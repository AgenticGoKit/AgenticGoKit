<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>advanced :: AgenticGoKit Docs</title>
    <link>http://localhost:1313/AgenticGoKitDocs/tutorials/advanced/index.html</link>
    <description>Advanced Tutorials Production patterns and optimization techniques&#xA;This section covers advanced topics for building production-ready agent systems with AgenticGoKit. These tutorials assume familiarity with basic AgenticGoKit concepts.&#xA;Advanced Topics Circuit Breaker Patterns - Implement resilient agent systems Retry Policies - Handle failures gracefully Testing Strategies - Test multi-agent systems effectively Load Balancing and Scaling - Scale agents horizontally Prerequisites Before starting these tutorials, you should:&#xA;Complete the Getting Started tutorials Understand Core Concepts Have experience with Go testing and production deployment Production Focus These tutorials emphasize:</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 25 Jul 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/AgenticGoKitDocs/tutorials/advanced/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>advanced</title>
      <link>http://localhost:1313/AgenticGoKitDocs/tutorials/advanced/readme/index.html</link>
      <pubDate>Fri, 25 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AgenticGoKitDocs/tutorials/advanced/readme/index.html</guid>
      <description>Advanced Tutorials Production patterns and optimization techniques&#xA;This section covers advanced topics for building production-ready agent systems with AgenticGoKit. These tutorials assume familiarity with basic AgenticGoKit concepts.&#xA;Advanced Topics Circuit Breaker Patterns - Implement resilient agent systems Retry Policies - Handle failures gracefully Testing Strategies - Test multi-agent systems effectively Load Balancing and Scaling - Scale agents horizontally Prerequisites Before starting these tutorials, you should:&#xA;Complete the Getting Started tutorials Understand Core Concepts Have experience with Go testing and production deployment Production Focus These tutorials emphasize:</description>
    </item>
    <item>
      <title>circuit-breaker-patterns</title>
      <link>http://localhost:1313/AgenticGoKitDocs/tutorials/advanced/circuit-breaker-patterns/index.html</link>
      <pubDate>Fri, 25 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AgenticGoKitDocs/tutorials/advanced/circuit-breaker-patterns/index.html</guid>
      <description>Circuit Breaker Patterns in AgenticGoKit Building fault-tolerant agent systems with automatic failure detection and recovery&#xA;Circuit breakers are essential for building resilient multi-agent systems. They automatically detect failures and prevent cascade failures by temporarily stopping requests to failing services. This guide shows you how to implement and configure circuit breaker patterns in AgenticGoKit.&#xA;üîå Understanding Circuit Breakers A circuit breaker works like an electrical circuit breaker - it ‚Äúopens‚Äù when it detects too many failures, preventing further requests until the service recovers.</description>
    </item>
    <item>
      <title>load-balancing-scaling</title>
      <link>http://localhost:1313/AgenticGoKitDocs/tutorials/advanced/load-balancing-scaling/index.html</link>
      <pubDate>Fri, 25 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AgenticGoKitDocs/tutorials/advanced/load-balancing-scaling/index.html</guid>
      <description>Load Balancing and Scaling Patterns Building horizontally scalable agent systems with intelligent load distribution&#xA;As your agent systems grow, you need strategies to distribute load effectively and scale horizontally. This guide covers load balancing patterns, horizontal scaling strategies, and performance optimization techniques for AgenticGoKit applications.&#xA;üèóÔ∏è Scaling Architecture Overview flowchart TB&#xD;subgraph &#34;Load Balancer Layer&#34;&#xD;LB[Load Balancer]&#xD;HEALTH[Health Checker]&#xD;end&#xD;subgraph &#34;Agent Instances&#34;&#xD;AGENT1[Agent Instance 1]&#xD;AGENT2[Agent Instance 2]&#xD;AGENT3[Agent Instance 3]&#xD;AGENTN[Agent Instance N]&#xD;end&#xD;subgraph &#34;Shared Resources&#34;&#xD;REDIS[Redis Cache]&#xD;POSTGRES[PostgreSQL]&#xD;VECTOR[Vector DB]&#xD;MCP[MCP Servers]&#xD;end&#xD;subgraph &#34;Monitoring&#34;&#xD;METRICS[Metrics Collector]&#xD;ALERTS[Alert Manager]&#xD;end&#xD;CLIENT[Client Requests] --&gt; LB&#xD;LB --&gt; AGENT1&#xD;LB --&gt; AGENT2&#xD;LB --&gt; AGENT3&#xD;LB --&gt; AGENTN&#xD;HEALTH --&gt; AGENT1&#xD;HEALTH --&gt; AGENT2&#xD;HEALTH --&gt; AGENT3&#xD;HEALTH --&gt; AGENTN&#xD;AGENT1 --&gt; REDIS&#xD;AGENT2 --&gt; REDIS&#xD;AGENT3 --&gt; REDIS&#xD;AGENTN --&gt; REDIS&#xD;AGENT1 --&gt; POSTGRES&#xD;AGENT2 --&gt; POSTGRES&#xD;AGENT3 --&gt; POSTGRES&#xD;AGENTN --&gt; POSTGRES&#xD;AGENT1 --&gt; VECTOR&#xD;AGENT2 --&gt; VECTOR&#xD;AGENT3 --&gt; VECTOR&#xD;AGENTN --&gt; VECTOR&#xD;AGENT1 --&gt; MCP&#xD;AGENT2 --&gt; MCP&#xD;AGENT3 --&gt; MCP&#xD;AGENTN --&gt; MCP&#xD;AGENT1 --&gt; METRICS&#xD;AGENT2 --&gt; METRICS&#xD;AGENT3 --&gt; METRICS&#xD;AGENTN --&gt; METRICS&#xD;METRICS --&gt; ALERTS&#xD;classDef lb fill:#e3f2fd,stroke:#1976d2,stroke-width:2px&#xD;classDef agent fill:#e8f5e8,stroke:#388e3c,stroke-width:2px&#xD;classDef shared fill:#fff3e0,stroke:#f57c00,stroke-width:2px&#xD;classDef monitor fill:#fce4ec,stroke:#c2185b,stroke-width:2px&#xD;class LB,HEALTH lb&#xD;class AGENT1,AGENT2,AGENT3,AGENTN agent&#xD;class REDIS,POSTGRES,VECTOR,MCP shared&#xD;class METRICS,ALERTS monitor&#xD;‚öñÔ∏è Load Balancing Strategies 1. Round Robin Load Balancing package balancer import ( &#34;context&#34; &#34;sync&#34; &#34;sync/atomic&#34; &#34;github.com/kunalkushwaha/agenticgokit/core&#34; ) type RoundRobinBalancer struct { agents []core.AgentHandler counter int64 mu sync.RWMutex } func NewRoundRobinBalancer(agents []core.AgentHandler) *RoundRobinBalancer { return &amp;RoundRobinBalancer{ agents: agents, } } func (rb *RoundRobinBalancer) Execute(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { if len(rb.agents) == 0 { return nil, errors.New(&#34;no agents available&#34;) } // Get next agent in round-robin fashion index := atomic.AddInt64(&amp;rb.counter, 1) % int64(len(rb.agents)) agent := rb.agents[index] return agent.Execute(ctx, event, state) } func (rb *RoundRobinBalancer) AddAgent(agent core.AgentHandler) { rb.mu.Lock() defer rb.mu.Unlock() rb.agents = append(rb.agents, agent) } func (rb *RoundRobinBalancer) RemoveAgent(index int) { rb.mu.Lock() defer rb.mu.Unlock() if index &gt;= 0 &amp;&amp; index &lt; len(rb.agents) { rb.agents = append(rb.agents[:index], rb.agents[index+1:]...) } }&#xD;2. Weighted Load Balancing type WeightedBalancer struct { agents []WeightedAgent totalWeight int mu sync.RWMutex } type WeightedAgent struct { Agent core.AgentHandler Weight int CurrentWeight int } func NewWeightedBalancer() *WeightedBalancer { return &amp;WeightedBalancer{ agents: make([]WeightedAgent, 0), } } func (wb *WeightedBalancer) AddAgent(agent core.AgentHandler, weight int) { wb.mu.Lock() defer wb.mu.Unlock() wb.agents = append(wb.agents, WeightedAgent{ Agent: agent, Weight: weight, CurrentWeight: 0, }) wb.totalWeight += weight } func (wb *WeightedBalancer) Execute(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { wb.mu.Lock() defer wb.mu.Unlock() if len(wb.agents) == 0 { return nil, errors.New(&#34;no agents available&#34;) } // Weighted round-robin algorithm selected := wb.selectAgent() if selected == nil { return nil, errors.New(&#34;no agent selected&#34;) } return selected.Execute(ctx, event, state) } func (wb *WeightedBalancer) selectAgent() core.AgentHandler { var selected *WeightedAgent for i := range wb.agents { agent := &amp;wb.agents[i] agent.CurrentWeight += agent.Weight if selected == nil || agent.CurrentWeight &gt; selected.CurrentWeight { selected = agent } } if selected != nil { selected.CurrentWeight -= wb.totalWeight return selected.Agent } return nil }&#xD;3. Least Connections Load Balancing type LeastConnectionsBalancer struct { agents []ConnectionTrackingAgent mu sync.RWMutex } type ConnectionTrackingAgent struct { Agent core.AgentHandler Connections int64 mu sync.RWMutex } func NewLeastConnectionsBalancer(agents []core.AgentHandler) *LeastConnectionsBalancer { trackingAgents := make([]ConnectionTrackingAgent, len(agents)) for i, agent := range agents { trackingAgents[i] = ConnectionTrackingAgent{ Agent: agent, } } return &amp;LeastConnectionsBalancer{ agents: trackingAgents, } } func (lcb *LeastConnectionsBalancer) Execute(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { agent := lcb.selectLeastConnectedAgent() if agent == nil { return nil, errors.New(&#34;no agents available&#34;) } // Increment connection count atomic.AddInt64(&amp;agent.Connections, 1) defer atomic.AddInt64(&amp;agent.Connections, -1) return agent.Agent.Execute(ctx, event, state) } func (lcb *LeastConnectionsBalancer) selectLeastConnectedAgent() *ConnectionTrackingAgent { lcb.mu.RLock() defer lcb.mu.RUnlock() if len(lcb.agents) == 0 { return nil } var selected *ConnectionTrackingAgent minConnections := int64(^uint64(0) &gt;&gt; 1) // Max int64 for i := range lcb.agents { agent := &amp;lcb.agents[i] connections := atomic.LoadInt64(&amp;agent.Connections) if connections &lt; minConnections { minConnections = connections selected = agent } } return selected }&#xD;4. Performance-Based Load Balancing type PerformanceBasedBalancer struct { agents []PerformanceTrackingAgent mu sync.RWMutex } type PerformanceTrackingAgent struct { Agent core.AgentHandler ResponseTimes []time.Duration SuccessRate float64 LastUpdate time.Time mu sync.RWMutex } func NewPerformanceBasedBalancer(agents []core.AgentHandler) *PerformanceBasedBalancer { trackingAgents := make([]PerformanceTrackingAgent, len(agents)) for i, agent := range agents { trackingAgents[i] = PerformanceTrackingAgent{ Agent: agent, ResponseTimes: make([]time.Duration, 0, 100), SuccessRate: 1.0, LastUpdate: time.Now(), } } return &amp;PerformanceBasedBalancer{ agents: trackingAgents, } } func (pbb *PerformanceBasedBalancer) Execute(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { agent := pbb.selectBestPerformingAgent() if agent == nil { return nil, errors.New(&#34;no agents available&#34;) } start := time.Now() result, err := agent.Agent.Execute(ctx, event, state) duration := time.Since(start) // Record performance metrics pbb.recordPerformance(agent, duration, err == nil) return result, err } func (pbb *PerformanceBasedBalancer) selectBestPerformingAgent() *PerformanceTrackingAgent { pbb.mu.RLock() defer pbb.mu.RUnlock() if len(pbb.agents) == 0 { return nil } var selected *PerformanceTrackingAgent bestScore := -1.0 for i := range pbb.agents { agent := &amp;pbb.agents[i] score := pbb.calculatePerformanceScore(agent) if score &gt; bestScore { bestScore = score selected = agent } } return selected } func (pbb *PerformanceBasedBalancer) calculatePerformanceScore(agent *PerformanceTrackingAgent) float64 { agent.mu.RLock() defer agent.mu.RUnlock() // Calculate average response time avgResponseTime := time.Duration(0) if len(agent.ResponseTimes) &gt; 0 { total := time.Duration(0) for _, rt := range agent.ResponseTimes { total += rt } avgResponseTime = total / time.Duration(len(agent.ResponseTimes)) } // Normalize response time (lower is better) responseTimeScore := 1.0 / (1.0 + avgResponseTime.Seconds()) // Combine success rate and response time return (agent.SuccessRate * 0.7) + (responseTimeScore * 0.3) } func (pbb *PerformanceBasedBalancer) recordPerformance(agent *PerformanceTrackingAgent, duration time.Duration, success bool) { agent.mu.Lock() defer agent.mu.Unlock() // Record response time agent.ResponseTimes = append(agent.ResponseTimes, duration) if len(agent.ResponseTimes) &gt; 100 { agent.ResponseTimes = agent.ResponseTimes[1:] } // Update success rate (exponential moving average) alpha := 0.1 if success { agent.SuccessRate = agent.SuccessRate*(1-alpha) + alpha } else { agent.SuccessRate = agent.SuccessRate * (1 - alpha) } agent.LastUpdate = time.Now() }&#xD;üîÑ Horizontal Scaling Patterns 1. Stateless Agent Design // Stateless agent that can be scaled horizontally type StatelessAgent struct { name string llmProvider core.ModelProvider cache CacheProvider // Shared cache memory MemoryProvider // Shared memory } func NewStatelessAgent(name string, provider core.ModelProvider, cache CacheProvider, memory MemoryProvider) *StatelessAgent { return &amp;StatelessAgent{ name: name, llmProvider: provider, cache: cache, memory: memory, } } func (sa *StatelessAgent) Execute(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { // All state is passed in or retrieved from shared resources // No local state that would prevent horizontal scaling // Get context from shared memory context := sa.memory.GetContext(state.SessionID) // Check shared cache cacheKey := sa.generateCacheKey(event) if cached := sa.cache.Get(cacheKey); cached != nil { return cached.(*core.AgentResult), nil } // Process request result, err := sa.processRequest(ctx, event, context) if err != nil { return nil, err } // Store in shared cache sa.cache.Set(cacheKey, result, 5*time.Minute) return result, nil } func (sa *StatelessAgent) generateCacheKey(event core.Event) string { return fmt.Sprintf(&#34;%s:%s:%x&#34;, sa.name, event.Type, sha256.Sum256([]byte(fmt.Sprintf(&#34;%v&#34;, event.Data)))) }&#xD;2. Auto-Scaling Controller type AutoScaler struct { minInstances int maxInstances int currentInstances int targetCPU float64 targetMemory float64 scaleUpCooldown time.Duration scaleDownCooldown time.Duration lastScaleAction time.Time metrics MetricsProvider orchestrator Orchestrator mu sync.RWMutex } func NewAutoScaler(min, max int, orchestrator Orchestrator) *AutoScaler { return &amp;AutoScaler{ minInstances: min, maxInstances: max, currentInstances: min, targetCPU: 70.0, // 70% CPU utilization targetMemory: 80.0, // 80% memory utilization scaleUpCooldown: 2 * time.Minute, scaleDownCooldown: 5 * time.Minute, orchestrator: orchestrator, } } func (as *AutoScaler) Start(ctx context.Context) { ticker := time.NewTicker(30 * time.Second) defer ticker.Stop() for { select { case &lt;-ticker.C: as.evaluateScaling() case &lt;-ctx.Done(): return } } } func (as *AutoScaler) evaluateScaling() { as.mu.Lock() defer as.mu.Unlock() // Get current metrics cpuUsage := as.metrics.GetCPUUsage() memoryUsage := as.metrics.GetMemoryUsage() requestRate := as.metrics.GetRequestRate() // Determine if scaling is needed shouldScaleUp := (cpuUsage &gt; as.targetCPU || memoryUsage &gt; as.targetMemory) &amp;&amp; as.currentInstances &lt; as.maxInstances &amp;&amp; time.Since(as.lastScaleAction) &gt; as.scaleUpCooldown shouldScaleDown := cpuUsage &lt; as.targetCPU*0.5 &amp;&amp; memoryUsage &lt; as.targetMemory*0.5 &amp;&amp; as.currentInstances &gt; as.minInstances &amp;&amp; time.Since(as.lastScaleAction) &gt; as.scaleDownCooldown if shouldScaleUp { as.scaleUp() } else if shouldScaleDown { as.scaleDown() } } func (as *AutoScaler) scaleUp() { newInstances := min(as.currentInstances+1, as.maxInstances) if newInstances &gt; as.currentInstances { err := as.orchestrator.AddInstance() if err == nil { as.currentInstances = newInstances as.lastScaleAction = time.Now() log.Printf(&#34;Scaled up to %d instances&#34;, as.currentInstances) } } } func (as *AutoScaler) scaleDown() { newInstances := max(as.currentInstances-1, as.minInstances) if newInstances &lt; as.currentInstances { err := as.orchestrator.RemoveInstance() if err == nil { as.currentInstances = newInstances as.lastScaleAction = time.Now() log.Printf(&#34;Scaled down to %d instances&#34;, as.currentInstances) } } }&#xD;3. Health-Aware Load Balancing type HealthAwareBalancer struct { agents []HealthTrackingAgent healthCheck time.Duration mu sync.RWMutex } type HealthTrackingAgent struct { Agent core.AgentHandler Healthy bool LastCheck time.Time Endpoint string } func NewHealthAwareBalancer(agents []core.AgentHandler) *HealthAwareBalancer { trackingAgents := make([]HealthTrackingAgent, len(agents)) for i, agent := range agents { trackingAgents[i] = HealthTrackingAgent{ Agent: agent, Healthy: true, } } hab := &amp;HealthAwareBalancer{ agents: trackingAgents, healthCheck: 30 * time.Second, } // Start health checking go hab.startHealthChecking() return hab } func (hab *HealthAwareBalancer) Execute(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { healthyAgents := hab.getHealthyAgents() if len(healthyAgents) == 0 { return nil, errors.New(&#34;no healthy agents available&#34;) } // Use round-robin among healthy agents index := rand.Intn(len(healthyAgents)) return healthyAgents[index].Execute(ctx, event, state) } func (hab *HealthAwareBalancer) getHealthyAgents() []core.AgentHandler { hab.mu.RLock() defer hab.mu.RUnlock() var healthy []core.AgentHandler for _, agent := range hab.agents { if agent.Healthy { healthy = append(healthy, agent.Agent) } } return healthy } func (hab *HealthAwareBalancer) startHealthChecking() { ticker := time.NewTicker(hab.healthCheck) defer ticker.Stop() for range ticker.C { hab.checkHealth() } } func (hab *HealthAwareBalancer) checkHealth() { hab.mu.Lock() defer hab.mu.Unlock() for i := range hab.agents { agent := &amp;hab.agents[i] // Perform health check healthy := hab.performHealthCheck(agent) if agent.Healthy != healthy { agent.Healthy = healthy if healthy { log.Printf(&#34;Agent %d is now healthy&#34;, i) } else { log.Printf(&#34;Agent %d is now unhealthy&#34;, i) } } agent.LastCheck = time.Now() } } func (hab *HealthAwareBalancer) performHealthCheck(agent *HealthTrackingAgent) bool { // Simple health check - try to execute a test request ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second) defer cancel() testEvent := core.NewEvent(&#34;health_check&#34;, &#34;ping&#34;) testState := &amp;core.State{} _, err := agent.Agent.Execute(ctx, testEvent, testState) return err == nil }&#xD;üê≥ Container Orchestration Docker Compose for Development # docker-compose.yml version: &#39;3.8&#39; services: agent-1: build: . environment: - INSTANCE_ID=1 - REDIS_URL=redis://redis:6379 - POSTGRES_URL=postgres://user:pass@postgres:5432/agentflow depends_on: - redis - postgres deploy: resources: limits: cpus: &#39;0.5&#39; memory: 512M reservations: cpus: &#39;0.25&#39; memory: 256M agent-2: build: . environment: - INSTANCE_ID=2 - REDIS_URL=redis://redis:6379 - POSTGRES_URL=postgres://user:pass@postgres:5432/agentflow depends_on: - redis - postgres deploy: resources: limits: cpus: &#39;0.5&#39; memory: 512M agent-3: build: . environment: - INSTANCE_ID=3 - REDIS_URL=redis://redis:6379 - POSTGRES_URL=postgres://user:pass@postgres:5432/agentflow depends_on: - redis - postgres nginx: image: nginx:alpine ports: - &#34;80:80&#34; volumes: - ./nginx.conf:/etc/nginx/nginx.conf depends_on: - agent-1 - agent-2 - agent-3 redis: image: redis:7-alpine volumes: - redis_data:/data postgres: image: pgvector/pgvector:pg15 environment: POSTGRES_DB: agentflow POSTGRES_USER: user POSTGRES_PASSWORD: pass volumes: - postgres_data:/var/lib/postgresql/data volumes: redis_data: postgres_data:&#xD;Kubernetes Deployment # k8s/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: agenticgokit-agents spec: replicas: 3 selector: matchLabels: app: agenticgokit-agents template: metadata: labels: app: agenticgokit-agents spec: containers: - name: agent image: agenticgokit:latest ports: - containerPort: 8080 env: - name: REDIS_URL value: &#34;redis://redis-service:6379&#34; - name: POSTGRES_URL valueFrom: secretKeyRef: name: db-secret key: postgres-url resources: requests: memory: &#34;256Mi&#34; cpu: &#34;250m&#34; limits: memory: &#34;512Mi&#34; cpu: &#34;500m&#34; livenessProbe: httpGet: path: /health port: 8080 initialDelaySeconds: 30 periodSeconds: 10 readinessProbe: httpGet: path: /ready port: 8080 initialDelaySeconds: 5 periodSeconds: 5 --- apiVersion: v1 kind: Service metadata: name: agenticgokit-service spec: selector: app: agenticgokit-agents ports: - port: 80 targetPort: 8080 type: LoadBalancer --- apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: agenticgokit-hpa spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: agenticgokit-agents minReplicas: 2 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80&#xD;üìä Performance Monitoring Load Balancer Metrics type LoadBalancerMetrics struct { totalRequests int64 requestsPerAgent map[string]int64 responseTimesByAgent map[string][]time.Duration errorsByAgent map[string]int64 mu sync.RWMutex } func NewLoadBalancerMetrics() *LoadBalancerMetrics { return &amp;LoadBalancerMetrics{ requestsPerAgent: make(map[string]int64), responseTimesByAgent: make(map[string][]time.Duration), errorsByAgent: make(map[string]int64), } } func (lbm *LoadBalancerMetrics) RecordRequest(agentID string, duration time.Duration, success bool) { lbm.mu.Lock() defer lbm.mu.Unlock() lbm.totalRequests++ lbm.requestsPerAgent[agentID]++ // Record response time if lbm.responseTimesByAgent[agentID] == nil { lbm.responseTimesByAgent[agentID] = make([]time.Duration, 0, 100) } lbm.responseTimesByAgent[agentID] = append(lbm.responseTimesByAgent[agentID], duration) // Keep only recent response times if len(lbm.responseTimesByAgent[agentID]) &gt; 100 { lbm.responseTimesByAgent[agentID] = lbm.responseTimesByAgent[agentID][1:] } // Record errors if !success { lbm.errorsByAgent[agentID]++ } } func (lbm *LoadBalancerMetrics) GetStats() map[string]interface{} { lbm.mu.RLock() defer lbm.mu.RUnlock() stats := map[string]interface{}{ &#34;total_requests&#34;: lbm.totalRequests, &#34;agents&#34;: make(map[string]interface{}), } for agentID, requests := range lbm.requestsPerAgent { agentStats := map[string]interface{}{ &#34;requests&#34;: requests, &#34;errors&#34;: lbm.errorsByAgent[agentID], } // Calculate average response time if responseTimes := lbm.responseTimesByAgent[agentID]; len(responseTimes) &gt; 0 { total := time.Duration(0) for _, rt := range responseTimes { total += rt } agentStats[&#34;avg_response_time&#34;] = total / time.Duration(len(responseTimes)) } stats[&#34;agents&#34;].(map[string]interface{})[agentID] = agentStats } return stats }&#xD;üéØ Best Practices 1. Configuration for Scaling # agentflow.toml - Production scaling configuration [runtime] max_concurrent_agents = 50 timeout_seconds = 30 enable_metrics = true metrics_port = 8080 [load_balancer] strategy = &#34;performance_based&#34; # round_robin, weighted, least_connections, performance_based health_check_interval = &#34;30s&#34; unhealthy_threshold = 3 healthy_threshold = 2 [scaling] min_instances = 2 max_instances = 20 target_cpu_utilization = 70 target_memory_utilization = 80 scale_up_cooldown = &#34;2m&#34; scale_down_cooldown = &#34;5m&#34; [shared_resources] redis_url = &#34;${REDIS_URL}&#34; postgres_url = &#34;${POSTGRES_URL}&#34; vector_db_url = &#34;${VECTOR_DB_URL}&#34;&#xD;2. Health Check Implementation func (a *ScalableAgent) HealthCheck() map[string]interface{} { health := map[string]interface{}{ &#34;status&#34;: &#34;healthy&#34;, &#34;timestamp&#34;: time.Now().Format(time.RFC3339), } // Check LLM provider if err := a.checkLLMProvider(); err != nil { health[&#34;status&#34;] = &#34;unhealthy&#34; health[&#34;llm_provider_error&#34;] = err.Error() } // Check shared cache if err := a.checkCache(); err != nil { health[&#34;status&#34;] = &#34;degraded&#34; health[&#34;cache_error&#34;] = err.Error() } // Check memory system if err := a.checkMemory(); err != nil { health[&#34;status&#34;] = &#34;degraded&#34; health[&#34;memory_error&#34;] = err.Error() } // Add performance metrics health[&#34;metrics&#34;] = map[string]interface{}{ &#34;requests_per_minute&#34;: a.getRequestsPerMinute(), &#34;avg_response_time&#34;: a.getAverageResponseTime(), &#34;error_rate&#34;: a.getErrorRate(), } return health }&#xD;3. Graceful Shutdown func (a *ScalableAgent) GracefulShutdown(ctx context.Context) error { log.Println(&#34;Starting graceful shutdown...&#34;) // Stop accepting new requests a.stopAcceptingRequests() // Wait for ongoing requests to complete done := make(chan struct{}) go func() { a.waitForOngoingRequests() close(done) }() select { case &lt;-done: log.Println(&#34;All requests completed&#34;) case &lt;-ctx.Done(): log.Println(&#34;Shutdown timeout reached, forcing shutdown&#34;) } // Clean up resources a.cleanup() log.Println(&#34;Graceful shutdown completed&#34;) return nil }&#xD;Load balancing and scaling are essential for building production-ready agent systems that can handle varying loads efficiently and maintain high availability.</description>
    </item>
    <item>
      <title>retry-policies</title>
      <link>http://localhost:1313/AgenticGoKitDocs/tutorials/advanced/retry-policies/index.html</link>
      <pubDate>Fri, 25 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AgenticGoKitDocs/tutorials/advanced/retry-policies/index.html</guid>
      <description>Retry Policies and Error Handling Building resilient agent systems with intelligent retry strategies and comprehensive error handling&#xA;Retry policies are crucial for handling transient failures in distributed agent systems. This guide shows you how to implement intelligent retry strategies, handle different types of errors appropriately, and build robust error recovery mechanisms in AgenticGoKit.&#xA;üîÑ Understanding Retry Patterns Different types of failures require different retry strategies:&#xA;flowchart TD&#xD;REQUEST[Request] --&gt; EXECUTE[Execute]&#xD;EXECUTE --&gt; SUCCESS{Success?}&#xD;SUCCESS --&gt;|Yes| RETURN[Return Result]&#xD;SUCCESS --&gt;|No| CLASSIFY[Classify Error]&#xD;CLASSIFY --&gt; TRANSIENT{Transient Error?}&#xD;CLASSIFY --&gt; PERMANENT{Permanent Error?}&#xD;CLASSIFY --&gt; RATE_LIMIT{Rate Limited?}&#xD;TRANSIENT --&gt;|Yes| BACKOFF[Apply Backoff]&#xD;PERMANENT --&gt;|Yes| FAIL[Fail Immediately]&#xD;RATE_LIMIT --&gt;|Yes| WAIT[Wait for Reset]&#xD;BACKOFF --&gt; RETRY_CHECK{Retries Left?}&#xD;WAIT --&gt; RETRY_CHECK&#xD;RETRY_CHECK --&gt;|Yes| EXECUTE&#xD;RETRY_CHECK --&gt;|No| FAIL&#xD;classDef success fill:#d4edda,stroke:#155724&#xD;classDef error fill:#f8d7da,stroke:#721c24&#xD;classDef retry fill:#fff3cd,stroke:#856404&#xD;class RETURN success&#xD;class FAIL error&#xD;class BACKOFF,WAIT,RETRY_CHECK retry&#xD;üõ†Ô∏è Built-in Retry Configuration AgenticGoKit provides built-in retry policies for LLM providers and MCP tools:</description>
    </item>
    <item>
      <title>testing-strategies</title>
      <link>http://localhost:1313/AgenticGoKitDocs/tutorials/advanced/testing-strategies/index.html</link>
      <pubDate>Fri, 25 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AgenticGoKitDocs/tutorials/advanced/testing-strategies/index.html</guid>
      <description>Testing Strategies for Multi-Agent Systems Comprehensive testing approaches for building reliable and maintainable agent systems&#xA;Testing multi-agent systems presents unique challenges due to their distributed nature, asynchronous communication, and external dependencies. This guide covers testing strategies, patterns, and tools specifically designed for AgenticGoKit applications.&#xA;üß™ Testing Pyramid for Agent Systems flowchart TD&#xD;subgraph &#34;End-to-End Tests (10%)&#34;&#xD;E2E1[Full System Tests]&#xD;E2E2[User Journey Tests]&#xD;E2E3[Performance Tests]&#xD;end&#xD;subgraph &#34;Integration Tests (30%)&#34;&#xD;INT1[Agent Interaction Tests]&#xD;INT2[External Service Tests]&#xD;INT3[Orchestration Tests]&#xD;INT4[Memory System Tests]&#xD;INT5[MCP Tool Tests]&#xD;end&#xD;subgraph &#34;Unit Tests (60%)&#34;&#xD;UNIT1[Individual Agent Tests]&#xD;UNIT2[Business Logic Tests]&#xD;UNIT3[Utility Function Tests]&#xD;UNIT4[Error Handling Tests]&#xD;UNIT5[State Management Tests]&#xD;end&#xD;E2E1 --&gt; INT1&#xD;E2E2 --&gt; INT2&#xD;E2E3 --&gt; INT3&#xD;INT1 --&gt; UNIT1&#xD;INT2 --&gt; UNIT2&#xD;INT3 --&gt; UNIT3&#xD;INT4 --&gt; UNIT4&#xD;INT5 --&gt; UNIT5&#xD;classDef e2e fill:#ffebee,stroke:#c62828,stroke-width:2px&#xD;classDef integration fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px&#xD;classDef unit fill:#e3f2fd,stroke:#1565c0,stroke-width:2px&#xD;class E2E1,E2E2,E2E3 e2e&#xD;class INT1,INT2,INT3,INT4,INT5 integration&#xD;class UNIT1,UNIT2,UNIT3,UNIT4,UNIT5 unit&#xD;üîß Unit Testing Strategies 1. Testing Individual Agents package agents import ( &#34;context&#34; &#34;testing&#34; &#34;time&#34; &#34;github.com/stretchr/testify/assert&#34; &#34;github.com/stretchr/testify/mock&#34; &#34;github.com/kunalkushwaha/agenticgokit/core&#34; ) // Mock LLM Provider for testing type MockLLMProvider struct { mock.Mock } func (m *MockLLMProvider) GenerateResponse(ctx context.Context, prompt string, options map[string]interface{}) (string, error) { args := m.Called(ctx, prompt, options) return args.String(0), args.Error(1) } func TestAnalyzerAgent_Execute(t *testing.T) { tests := []struct { name string event core.Event state *core.State mockResponse string mockError error expectedResult map[string]interface{} expectedError string }{ { name: &#34;successful analysis&#34;, event: core.NewEvent(&#34;analyze&#34;, map[string]interface{}{ &#34;data&#34;: &#34;Sample data to analyze&#34;, }), state: &amp;core.State{ SessionID: &#34;test-session&#34;, Data: map[string]interface{}{ &#34;context&#34;: &#34;test context&#34;, }, }, mockResponse: &#34;Analysis complete: The data shows positive trends&#34;, expectedResult: map[string]interface{}{ &#34;analysis&#34;: &#34;Analysis complete: The data shows positive trends&#34;, &#34;agent&#34;: &#34;analyzer&#34;, &#34;status&#34;: &#34;completed&#34;, }, }, { name: &#34;empty data handling&#34;, event: core.NewEvent(&#34;analyze&#34;, map[string]interface{}{ &#34;data&#34;: &#34;&#34;, }), state: &amp;core.State{SessionID: &#34;test-session&#34;}, expectedError: &#34;no data provided for analysis&#34;, }, { name: &#34;LLM provider error&#34;, event: core.NewEvent(&#34;analyze&#34;, map[string]interface{}{ &#34;data&#34;: &#34;Sample data&#34;, }), state: &amp;core.State{SessionID: &#34;test-session&#34;}, mockError: errors.New(&#34;LLM service unavailable&#34;), expectedError: &#34;LLM request failed: LLM service unavailable&#34;, }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { // Setup mock mockProvider := new(MockLLMProvider) if tt.mockResponse != &#34;&#34; || tt.mockError != nil { mockProvider.On(&#34;GenerateResponse&#34;, mock.Anything, mock.AnythingOfType(&#34;string&#34;), mock.Anything). Return(tt.mockResponse, tt.mockError) } // Create agent with mock agent := NewAnalyzerAgent(&#34;analyzer&#34;, mockProvider) // Execute test result, err := agent.Execute(context.Background(), tt.event, tt.state) // Assertions if tt.expectedError != &#34;&#34; { assert.Error(t, err) assert.Contains(t, err.Error(), tt.expectedError) assert.Nil(t, result) } else { assert.NoError(t, err) assert.NotNil(t, result) for key, expectedValue := range tt.expectedResult { assert.Equal(t, expectedValue, result.Data[key]) } } // Verify mock expectations mockProvider.AssertExpectations(t) }) } }&#xD;2. Testing State Management func TestStateTransformations(t *testing.T) { tests := []struct { name string initialState *core.State transformation func(*core.State) *core.State expectedData map[string]interface{} }{ { name: &#34;add analysis result&#34;, initialState: &amp;core.State{ SessionID: &#34;test&#34;, Data: map[string]interface{}{ &#34;input&#34;: &#34;test data&#34;, }, }, transformation: func(state *core.State) *core.State { newState := state.Clone() newState.Data[&#34;analysis&#34;] = &#34;completed&#34; newState.Data[&#34;confidence&#34;] = 0.95 return newState }, expectedData: map[string]interface{}{ &#34;input&#34;: &#34;test data&#34;, &#34;analysis&#34;: &#34;completed&#34;, &#34;confidence&#34;: 0.95, }, }, { name: &#34;merge states&#34;, initialState: &amp;core.State{ SessionID: &#34;test&#34;, Data: map[string]interface{}{ &#34;step1&#34;: &#34;completed&#34;, }, }, transformation: func(state *core.State) *core.State { additionalData := map[string]interface{}{ &#34;step2&#34;: &#34;completed&#34;, &#34;final&#34;: true, } return state.Merge(additionalData) }, expectedData: map[string]interface{}{ &#34;step1&#34;: &#34;completed&#34;, &#34;step2&#34;: &#34;completed&#34;, &#34;final&#34;: true, }, }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { result := tt.transformation(tt.initialState) for key, expected := range tt.expectedData { assert.Equal(t, expected, result.Data[key]) } }) } }&#xD;3. Testing Error Handling func TestErrorHandling(t *testing.T) { tests := []struct { name string setupMock func(*MockLLMProvider) expectedError string shouldRetry bool }{ { name: &#34;timeout error&#34;, setupMock: func(m *MockLLMProvider) { m.On(&#34;GenerateResponse&#34;, mock.Anything, mock.Anything, mock.Anything). Return(&#34;&#34;, context.DeadlineExceeded) }, expectedError: &#34;context deadline exceeded&#34;, shouldRetry: true, }, { name: &#34;rate limit error&#34;, setupMock: func(m *MockLLMProvider) { m.On(&#34;GenerateResponse&#34;, mock.Anything, mock.Anything, mock.Anything). Return(&#34;&#34;, errors.New(&#34;rate limit exceeded&#34;)) }, expectedError: &#34;rate limit exceeded&#34;, shouldRetry: true, }, { name: &#34;authentication error&#34;, setupMock: func(m *MockLLMProvider) { m.On(&#34;GenerateResponse&#34;, mock.Anything, mock.Anything, mock.Anything). Return(&#34;&#34;, errors.New(&#34;invalid API key&#34;)) }, expectedError: &#34;invalid API key&#34;, shouldRetry: false, }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { mockProvider := new(MockLLMProvider) tt.setupMock(mockProvider) agent := NewResilientAgent(&#34;test&#34;, mockProvider) event := core.NewEvent(&#34;test&#34;, &#34;data&#34;) state := &amp;core.State{SessionID: &#34;test&#34;} _, err := agent.Execute(context.Background(), event, state) assert.Error(t, err) assert.Contains(t, err.Error(), tt.expectedError) // Test retry behavior if tt.shouldRetry { // Verify that retries were attempted assert.True(t, mockProvider.AssertNumberOfCalls(t, &#34;GenerateResponse&#34;, 3)) } else { // Verify that no retries were attempted assert.True(t, mockProvider.AssertNumberOfCalls(t, &#34;GenerateResponse&#34;, 1)) } }) } }&#xD;üîó Integration Testing 1. Testing Agent Interactions func TestAgentOrchestration(t *testing.T) { // Setup test agents mockProvider := new(MockLLMProvider) mockProvider.On(&#34;GenerateResponse&#34;, mock.Anything, mock.MatchedBy(func(prompt string) bool { return strings.Contains(prompt, &#34;analyze&#34;) }), mock.Anything).Return(&#34;Analysis: Data processed&#34;, nil) mockProvider.On(&#34;GenerateResponse&#34;, mock.Anything, mock.MatchedBy(func(prompt string) bool { return strings.Contains(prompt, &#34;validate&#34;) }), mock.Anything).Return(&#34;Validation: Data is valid&#34;, nil) analyzer := NewAnalyzerAgent(&#34;analyzer&#34;, mockProvider) validator := NewValidatorAgent(&#34;validator&#34;, mockProvider) // Create sequential runner agents := map[string]core.AgentHandler{ &#34;analyzer&#34;: analyzer, &#34;validator&#34;: validator, } config := core.RunnerConfig{ OrchestrationMode: core.OrchestrationSequential, SequentialAgents: []string{&#34;analyzer&#34;, &#34;validator&#34;}, Timeout: 30 * time.Second, } runner := core.NewRunnerWithConfig(config) for name, agent := range agents { runner.RegisterAgent(name, agent) } // Test sequential execution event := core.NewEvent(&#34;process&#34;, map[string]interface{}{ &#34;data&#34;: &#34;test data for processing&#34;, }) results, err := runner.ProcessEvent(context.Background(), event) assert.NoError(t, err) assert.Len(t, results, 2) // Verify execution order and data flow analyzerResult := results[0] validatorResult := results[1] assert.Equal(t, &#34;analyzer&#34;, analyzerResult.Agent) assert.Contains(t, analyzerResult.Data[&#34;analysis&#34;], &#34;Data processed&#34;) assert.Equal(t, &#34;validator&#34;, validatorResult.Agent) assert.Contains(t, validatorResult.Data[&#34;validation&#34;], &#34;Data is valid&#34;) }&#xD;2. Testing Memory System Integration func TestMemorySystemIntegration(t *testing.T) { // Setup in-memory test database memoryProvider := memory.NewInMemoryProvider() // Create agent with memory agent := NewMemoryEnabledAgent(&#34;test-agent&#34;, mockLLMProvider, memoryProvider) // Test storing and retrieving memories t.Run(&#34;store and retrieve memory&#34;, func(t *testing.T) { event := core.NewEvent(&#34;remember&#34;, map[string]interface{}{ &#34;fact&#34;: &#34;The sky is blue&#34;, &#34;category&#34;: &#34;general_knowledge&#34;, }) state := &amp;core.State{ SessionID: &#34;test-session&#34;, UserID: &#34;test-user&#34;, } result, err := agent.Execute(context.Background(), event, state) assert.NoError(t, err) assert.Equal(t, &#34;stored&#34;, result.Data[&#34;status&#34;]) // Verify memory was stored memories, err := memoryProvider.Search(context.Background(), &#34;sky blue&#34;, 5) assert.NoError(t, err) assert.Len(t, memories, 1) assert.Contains(t, memories[0].Content, &#34;sky is blue&#34;) }) t.Run(&#34;RAG retrieval&#34;, func(t *testing.T) { // Setup mock to expect context from memory mockProvider.On(&#34;GenerateResponse&#34;, mock.Anything, mock.MatchedBy(func(prompt string) bool { return strings.Contains(prompt, &#34;sky is blue&#34;) }), mock.Anything).Return(&#34;Based on the stored knowledge, the sky appears blue due to light scattering&#34;, nil) event := core.NewEvent(&#34;query&#34;, map[string]interface{}{ &#34;question&#34;: &#34;What color is the sky?&#34;, }) state := &amp;core.State{SessionID: &#34;test-session&#34;} result, err := agent.Execute(context.Background(), event, state) assert.NoError(t, err) assert.Contains(t, result.Data[&#34;response&#34;], &#34;blue due to light scattering&#34;) }) }&#xD;3. Testing MCP Tool Integration func TestMCPToolIntegration(t *testing.T) { // Setup mock MCP server mockServer := &amp;MockMCPServer{} mockServer.On(&#34;ExecuteTool&#34;, &#34;web_search&#34;, map[string]interface{}{ &#34;query&#34;: &#34;AgenticGoKit testing&#34;, }).Return(map[string]interface{}{ &#34;results&#34;: []string{&#34;AgenticGoKit is a Go framework for building multi-agent systems&#34;}, }, nil) // Create agent with MCP tools toolManager := mcp.NewToolManager() toolManager.RegisterServer(&#34;test-server&#34;, mockServer) agent := NewToolEnabledAgent(&#34;test-agent&#34;, mockLLMProvider, toolManager) event := core.NewEvent(&#34;search&#34;, map[string]interface{}{ &#34;query&#34;: &#34;AgenticGoKit testing&#34;, &#34;tool&#34;: &#34;web_search&#34;, }) state := &amp;core.State{SessionID: &#34;test-session&#34;} result, err := agent.Execute(context.Background(), event, state) assert.NoError(t, err) assert.Contains(t, result.Data[&#34;search_results&#34;], &#34;Go framework&#34;) mockServer.AssertExpectations(t) }&#xD;üåê End-to-End Testing 1. Full System Tests func TestFullSystemWorkflow(t *testing.T) { // Setup complete system system := setupTestSystem(t) defer system.Cleanup() // Test complete user workflow t.Run(&#34;research assistant workflow&#34;, func(t *testing.T) { // Step 1: User asks a question request := &amp;api.Request{ Type: &#34;research_query&#34;, Data: map[string]interface{}{ &#34;question&#34;: &#34;What are the latest developments in AI agent frameworks?&#34;, &#34;depth&#34;: &#34;comprehensive&#34;, }, UserID: &#34;test-user&#34;, SessionID: &#34;test-session&#34;, } response, err := system.ProcessRequest(context.Background(), request) assert.NoError(t, err) // Verify response structure assert.NotNil(t, response) assert.Equal(t, &#34;success&#34;, response.Status) assert.Contains(t, response.Data, &#34;research_results&#34;) assert.Contains(t, response.Data, &#34;sources&#34;) assert.Contains(t, response.Data, &#34;summary&#34;) // Verify agent execution trace trace := response.Trace assert.NotEmpty(t, trace) // Should have executed: researcher -&gt; analyzer -&gt; validator -&gt; formatter expectedAgents := []string{&#34;researcher&#34;, &#34;analyzer&#34;, &#34;validator&#34;, &#34;formatter&#34;} assert.Len(t, trace, len(expectedAgents)) for i, expectedAgent := range expectedAgents { assert.Equal(t, expectedAgent, trace[i].Agent) assert.Equal(t, &#34;completed&#34;, trace[i].Status) } }) t.Run(&#34;error handling workflow&#34;, func(t *testing.T) { // Simulate external service failure system.SimulateServiceFailure(&#34;web_search&#34;) request := &amp;api.Request{ Type: &#34;research_query&#34;, Data: map[string]interface{}{ &#34;question&#34;: &#34;Test question&#34;, }, UserID: &#34;test-user&#34;, SessionID: &#34;test-session-2&#34;, } response, err := system.ProcessRequest(context.Background(), request) // Should handle gracefully with fallback assert.NoError(t, err) assert.Equal(t, &#34;partial_success&#34;, response.Status) assert.Contains(t, response.Data, &#34;fallback_response&#34;) assert.Contains(t, response.Warnings, &#34;web search unavailable&#34;) }) }&#xD;2. Performance Testing func TestSystemPerformance(t *testing.T) { system := setupTestSystem(t) defer system.Cleanup() t.Run(&#34;concurrent request handling&#34;, func(t *testing.T) { concurrency := 10 requestsPerWorker := 5 var wg sync.WaitGroup results := make(chan time.Duration, concurrency*requestsPerWorker) errors := make(chan error, concurrency*requestsPerWorker) for i := 0; i &lt; concurrency; i++ { wg.Add(1) go func(workerID int) { defer wg.Done() for j := 0; j &lt; requestsPerWorker; j++ { start := time.Now() request := &amp;api.Request{ Type: &#34;simple_query&#34;, Data: map[string]interface{}{ &#34;question&#34;: fmt.Sprintf(&#34;Test question %d-%d&#34;, workerID, j), }, UserID: fmt.Sprintf(&#34;user-%d&#34;, workerID), SessionID: fmt.Sprintf(&#34;session-%d-%d&#34;, workerID, j), } _, err := system.ProcessRequest(context.Background(), request) duration := time.Since(start) if err != nil { errors &lt;- err } else { results &lt;- duration } } }(i) } wg.Wait() close(results) close(errors) // Collect results var durations []time.Duration for duration := range results { durations = append(durations, duration) } var errorList []error for err := range errors { errorList = append(errorList, err) } // Assertions assert.Empty(t, errorList, &#34;No errors should occur during concurrent processing&#34;) assert.Len(t, durations, concurrency*requestsPerWorker) // Performance assertions avgDuration := calculateAverage(durations) maxDuration := calculateMax(durations) assert.Less(t, avgDuration, 5*time.Second, &#34;Average response time should be under 5 seconds&#34;) assert.Less(t, maxDuration, 10*time.Second, &#34;Max response time should be under 10 seconds&#34;) t.Logf(&#34;Performance results: avg=%v, max=%v, total_requests=%d&#34;, avgDuration, maxDuration, len(durations)) }) }&#xD;üé≠ Test Doubles and Mocking 1. Mock External Services type MockExternalService struct { mock.Mock responses map[string]interface{} delays map[string]time.Duration failures map[string]error } func NewMockExternalService() *MockExternalService { return &amp;MockExternalService{ responses: make(map[string]interface{}), delays: make(map[string]time.Duration), failures: make(map[string]error), } } func (m *MockExternalService) SetResponse(endpoint string, response interface{}) { m.responses[endpoint] = response } func (m *MockExternalService) SetDelay(endpoint string, delay time.Duration) { m.delays[endpoint] = delay } func (m *MockExternalService) SetFailure(endpoint string, err error) { m.failures[endpoint] = err } func (m *MockExternalService) Call(ctx context.Context, endpoint string, params map[string]interface{}) (interface{}, error) { // Simulate delay if delay, exists := m.delays[endpoint]; exists { select { case &lt;-time.After(delay): case &lt;-ctx.Done(): return nil, ctx.Err() } } // Simulate failure if err, exists := m.failures[endpoint]; exists { return nil, err } // Return mock response if response, exists := m.responses[endpoint]; exists { return response, nil } return nil, errors.New(&#34;endpoint not mocked&#34;) }&#xD;2. Test Fixtures and Builders type TestSystemBuilder struct { agents map[string]core.AgentHandler mockServices map[string]*MockExternalService config core.RunnerConfig } func NewTestSystemBuilder() *TestSystemBuilder { return &amp;TestSystemBuilder{ agents: make(map[string]core.AgentHandler), mockServices: make(map[string]*MockExternalService), config: core.RunnerConfig{ OrchestrationMode: core.OrchestrationSequential, Timeout: 30 * time.Second, }, } } func (b *TestSystemBuilder) WithAgent(name string, agent core.AgentHandler) *TestSystemBuilder { b.agents[name] = agent return b } func (b *TestSystemBuilder) WithMockService(name string, service *MockExternalService) *TestSystemBuilder { b.mockServices[name] = service return b } func (b *TestSystemBuilder) WithOrchestration(mode core.OrchestrationMode, agents []string) *TestSystemBuilder { b.config.OrchestrationMode = mode switch mode { case core.OrchestrationSequential: b.config.SequentialAgents = agents case core.OrchestrationCollaborative: b.config.CollaborativeAgents = agents } return b } func (b *TestSystemBuilder) Build() *TestSystem { runner := core.NewRunnerWithConfig(b.config) for name, agent := range b.agents { runner.RegisterAgent(name, agent) } return &amp;TestSystem{ runner: runner, mockServices: b.mockServices, } } type TestSystem struct { runner core.Runner mockServices map[string]*MockExternalService } func (ts *TestSystem) ProcessRequest(ctx context.Context, request *api.Request) (*api.Response, error) { event := core.NewEvent(request.Type, request.Data) state := &amp;core.State{ SessionID: request.SessionID, UserID: request.UserID, } results, err := ts.runner.ProcessEvent(ctx, event) if err != nil { return nil, err } return &amp;api.Response{ Status: &#34;success&#34;, Data: results[len(results)-1].Data, Trace: convertToTrace(results), }, nil } func (ts *TestSystem) SimulateServiceFailure(serviceName string) { if service, exists := ts.mockServices[serviceName]; exists { service.SetFailure(&#34;*&#34;, errors.New(&#34;simulated service failure&#34;)) } } func (ts *TestSystem) Cleanup() { // Cleanup resources for _, service := range ts.mockServices { service.AssertExpectations(&amp;testing.T{}) } }&#xD;üìä Test Data Management 1. Test Data Builders type EventBuilder struct { eventType string data map[string]interface{} metadata map[string]interface{} } func NewEventBuilder(eventType string) *EventBuilder { return &amp;EventBuilder{ eventType: eventType, data: make(map[string]interface{}), metadata: make(map[string]interface{}), } } func (b *EventBuilder) WithData(key string, value interface{}) *EventBuilder { b.data[key] = value return b } func (b *EventBuilder) WithMetadata(key string, value interface{}) *EventBuilder { b.metadata[key] = value return b } func (b *EventBuilder) Build() core.Event { event := core.NewEvent(b.eventType, b.data) for key, value := range b.metadata { event.Metadata[key] = value } return event } type StateBuilder struct { sessionID string userID string data map[string]interface{} } func NewStateBuilder() *StateBuilder { return &amp;StateBuilder{ data: make(map[string]interface{}), } } func (b *StateBuilder) WithSession(sessionID string) *StateBuilder { b.sessionID = sessionID return b } func (b *StateBuilder) WithUser(userID string) *StateBuilder { b.userID = userID return b } func (b *StateBuilder) WithData(key string, value interface{}) *StateBuilder { b.data[key] = value return b } func (b *StateBuilder) Build() *core.State { return &amp;core.State{ SessionID: b.sessionID, UserID: b.userID, Data: b.data, } }&#xD;2. Test Scenarios func TestCommonScenarios(t *testing.T) { scenarios := []struct { name string setup func() (*TestSystem, core.Event, *core.State) validate func(*testing.T, *core.AgentResult, error) }{ { name: &#34;successful_analysis&#34;, setup: func() (*TestSystem, core.Event, *core.State) { system := NewTestSystemBuilder(). WithAgent(&#34;analyzer&#34;, NewMockAnalyzer(&#34;success&#34;)). Build() event := NewEventBuilder(&#34;analyze&#34;). WithData(&#34;content&#34;, &#34;test data&#34;). Build() state := NewStateBuilder(). WithSession(&#34;test-session&#34;). Build() return system, event, state }, validate: func(t *testing.T, result *core.AgentResult, err error) { assert.NoError(t, err) assert.Equal(t, &#34;completed&#34;, result.Data[&#34;status&#34;]) }, }, { name: &#34;timeout_handling&#34;, setup: func() (*TestSystem, core.Event, *core.State) { system := NewTestSystemBuilder(). WithAgent(&#34;slow_agent&#34;, NewSlowMockAgent(10*time.Second)). Build() event := NewEventBuilder(&#34;process&#34;). WithData(&#34;timeout&#34;, &#34;5s&#34;). Build() state := NewStateBuilder(). WithSession(&#34;timeout-test&#34;). Build() return system, event, state }, validate: func(t *testing.T, result *core.AgentResult, err error) { assert.Error(t, err) assert.Contains(t, err.Error(), &#34;timeout&#34;) }, }, } for _, scenario := range scenarios { t.Run(scenario.name, func(t *testing.T) { system, event, state := scenario.setup() defer system.Cleanup() result, err := system.ProcessSingleEvent(context.Background(), event, state) scenario.validate(t, result, err) }) } }&#xD;üéØ Best Practices 1. Test Organization // Package structure for tests // agents/ // analyzer_test.go - Unit tests for analyzer agent // validator_test.go - Unit tests for validator agent // integration_test.go - Integration tests between agents // orchestration/ // sequential_test.go - Sequential orchestration tests // collaborative_test.go - Collaborative orchestration tests // system/ // e2e_test.go - End-to-end system tests // performance_test.go - Performance and load tests // testutils/ // builders.go - Test data builders // mocks.go - Mock implementations // fixtures.go - Test fixtures and helpers&#xD;2. Test Configuration // Test configuration management type TestConfig struct { DatabaseURL string RedisURL string MockServices bool LogLevel string TestTimeout time.Duration } func LoadTestConfig() *TestConfig { return &amp;TestConfig{ DatabaseURL: getEnvOrDefault(&#34;TEST_DATABASE_URL&#34;, &#34;postgres://localhost/agentflow_test&#34;), RedisURL: getEnvOrDefault(&#34;TEST_REDIS_URL&#34;, &#34;redis://localhost:6379/1&#34;), MockServices: getEnvOrDefault(&#34;MOCK_SERVICES&#34;, &#34;true&#34;) == &#34;true&#34;, LogLevel: getEnvOrDefault(&#34;TEST_LOG_LEVEL&#34;, &#34;error&#34;), TestTimeout: 30 * time.Second, } }&#xD;3. Continuous Integration # .github/workflows/test.yml name: Test Suite on: [push, pull_request] jobs: unit-tests: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - uses: actions/setup-go@v3 with: go-version: &#39;1.21&#39; - name: Run unit tests run: go test -v -race -coverprofile=coverage.out ./... - name: Upload coverage uses: codecov/codecov-action@v3 integration-tests: runs-on: ubuntu-latest services: postgres: image: pgvector/pgvector:pg15 env: POSTGRES_PASSWORD: test POSTGRES_DB: agentflow_test options: &gt;- --health-cmd pg_isready --health-interval 10s --health-timeout 5s --health-retries 5 redis: image: redis:7-alpine options: &gt;- --health-cmd &#34;redis-cli ping&#34; --health-interval 10s --health-timeout 5s --health-retries 5 steps: - uses: actions/checkout@v3 - uses: actions/setup-go@v3 with: go-version: &#39;1.21&#39; - name: Run integration tests run: go test -v -tags=integration ./... env: TEST_DATABASE_URL: postgres://postgres:test@localhost:5432/agentflow_test TEST_REDIS_URL: redis://localhost:6379/1 e2e-tests: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - uses: actions/setup-go@v3 with: go-version: &#39;1.21&#39; - name: Start test environment run: docker-compose -f docker-compose.test.yml up -d - name: Wait for services run: ./scripts/wait-for-services.sh - name: Run E2E tests run: go test -v -tags=e2e ./... - name: Cleanup run: docker-compose -f docker-compose.test.yml down&#xD;Testing multi-agent systems requires a comprehensive approach that covers unit, integration, and end-to-end scenarios. By following these strategies and patterns, you can build reliable and maintainable agent systems with confidence.</description>
    </item>
  </channel>
</rss>