<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>memory-systems :: AgenticGoKit Docs</title>
    <link>http://localhost:1313/AgenticGoKitDocs/tutorials/memory-systems/index.html</link>
    <description>Memory Systems in AgenticGoKit Navigation: Documentation Home → Tutorials → Memory Systems&#xA;Overview Memory systems are crucial for building intelligent agents that can learn, remember, and build upon previous interactions. This tutorial series explores AgenticGoKit’s memory capabilities, from basic in-memory storage to advanced RAG (Retrieval-Augmented Generation) systems with vector databases.&#xA;Memory systems enable agents to maintain context across conversations, store knowledge, and retrieve relevant information to enhance their responses.&#xA;Prerequisites Understanding of Core Concepts Basic knowledge of databases and data storage Familiarity with vector embeddings and similarity search Memory System Architecture AgenticGoKit’s memory system is built on a flexible architecture that supports multiple storage backends and retrieval strategies:</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 25 Jul 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/AgenticGoKitDocs/tutorials/memory-systems/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>memory-systems</title>
      <link>http://localhost:1313/AgenticGoKitDocs/tutorials/memory-systems/readme/index.html</link>
      <pubDate>Fri, 25 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AgenticGoKitDocs/tutorials/memory-systems/readme/index.html</guid>
      <description>Memory Systems in AgenticGoKit Navigation: Documentation Home → Tutorials → Memory Systems&#xA;Overview Memory systems are crucial for building intelligent agents that can learn, remember, and build upon previous interactions. This tutorial series explores AgenticGoKit’s memory capabilities, from basic in-memory storage to advanced RAG (Retrieval-Augmented Generation) systems with vector databases.&#xA;Memory systems enable agents to maintain context across conversations, store knowledge, and retrieve relevant information to enhance their responses.&#xA;Prerequisites Understanding of Core Concepts Basic knowledge of databases and data storage Familiarity with vector embeddings and similarity search Memory System Architecture AgenticGoKit’s memory system is built on a flexible architecture that supports multiple storage backends and retrieval strategies:</description>
    </item>
    <item>
      <title>basic-memory</title>
      <link>http://localhost:1313/AgenticGoKitDocs/tutorials/memory-systems/basic-memory/index.html</link>
      <pubDate>Fri, 25 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AgenticGoKitDocs/tutorials/memory-systems/basic-memory/index.html</guid>
      <description>Basic Memory Operations in AgenticGoKit Overview This tutorial covers the fundamentals of memory operations in AgenticGoKit, including storing information, retrieving data, and managing conversation history. We’ll start with simple in-memory storage and progress to more advanced concepts.&#xA;Basic memory operations form the foundation for all memory-enabled agents, providing the essential building blocks for more sophisticated memory systems.&#xA;Prerequisites Understanding of Core Concepts Basic knowledge of Go programming Familiarity with key-value storage concepts Setting Up Basic Memory 1. In-Memory Storage The simplest memory provider stores data in RAM:</description>
    </item>
    <item>
      <title>document-ingestion</title>
      <link>http://localhost:1313/AgenticGoKitDocs/tutorials/memory-systems/document-ingestion/index.html</link>
      <pubDate>Fri, 25 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AgenticGoKitDocs/tutorials/memory-systems/document-ingestion/index.html</guid>
      <description>Document Ingestion and Knowledge Base Management Overview Document ingestion is a critical component of building comprehensive knowledge bases in AgenticGoKit. This tutorial covers the complete pipeline from raw documents to searchable knowledge, including document processing, chunking strategies, metadata extraction, and optimization techniques.&#xA;Effective document ingestion enables agents to access and reason over large collections of structured and unstructured data.&#xA;Prerequisites Understanding of Memory Systems Overview Familiarity with Vector Databases Knowledge of document formats (PDF, Markdown, HTML, etc.) Basic understanding of text processing and NLP concepts Document Ingestion Pipeline Architecture Overview ┌─────────────────┐ ┌──────────────────┐ ┌─────────────────┐&#xD;│ Raw │───▶│ Document │───▶│ Text │&#xD;│ Documents │ │ Parser │ │ Extraction │&#xD;└─────────────────┘ └──────────────────┘ └─────────────────┘&#xD;│&#xD;▼&#xD;┌─────────────────┐ ┌──────────────────┐ ┌─────────────────┐&#xD;│ Vector │◀───│ Embedding │◀───│ Text │&#xD;│ Storage │ │ Generation │ │ Chunking │&#xD;└─────────────────┘ └──────────────────┘ └─────────────────┘&#xD;│&#xD;▼&#xD;┌─────────────────┐&#xD;│ Metadata │&#xD;│ Extraction │&#xD;└─────────────────┘&#xD;Document Types and Processing 1. Supported Document Types // Document types supported by AgenticGoKit const ( DocumentTypePDF DocumentType = &#34;pdf&#34; DocumentTypeText DocumentType = &#34;txt&#34; DocumentTypeMarkdown DocumentType = &#34;md&#34; DocumentTypeWeb DocumentType = &#34;web&#34; DocumentTypeCode DocumentType = &#34;code&#34; DocumentTypeJSON DocumentType = &#34;json&#34; ) // Document structure for ingestion type Document struct { ID string `json:&#34;id&#34;` Title string `json:&#34;title,omitempty&#34;` Content string `json:&#34;content&#34;` Source string `json:&#34;source,omitempty&#34;` // URL, file path, etc. Type DocumentType `json:&#34;type,omitempty&#34;` Metadata map[string]any `json:&#34;metadata,omitempty&#34;` Tags []string `json:&#34;tags,omitempty&#34;` CreatedAt time.Time `json:&#34;created_at&#34;` UpdatedAt time.Time `json:&#34;updated_at,omitempty&#34;` ChunkIndex int `json:&#34;chunk_index,omitempty&#34;` // For chunked documents ChunkTotal int `json:&#34;chunk_total,omitempty&#34;` }&#xD;2. Basic Document Ingestion package main import ( &#34;context&#34; &#34;fmt&#34; &#34;log&#34; &#34;time&#34; &#34;github.com/kunalkushwaha/agenticgokit/core&#34; ) func ingestBasicDocument(memory core.Memory) error { ctx := context.Background() // Create a document doc := core.Document{ ID: &#34;doc-001&#34;, Title: &#34;Introduction to Machine Learning&#34;, Content: `Machine learning is a subset of artificial intelligence that enables computers to learn and make decisions from data without being explicitly programmed for every task. It involves algorithms that can identify patterns, make predictions, and improve their performance over time.`, Source: &#34;textbook-chapter-1.pdf&#34;, Type: core.DocumentTypePDF, Metadata: map[string]any{ &#34;author&#34;: &#34;Dr. Jane Smith&#34;, &#34;chapter&#34;: 1, &#34;subject&#34;: &#34;machine-learning&#34;, &#34;difficulty&#34;: &#34;beginner&#34;, &#34;language&#34;: &#34;english&#34;, }, Tags: []string{&#34;ml&#34;, &#34;ai&#34;, &#34;introduction&#34;}, CreatedAt: time.Now(), } // Ingest the document err := memory.IngestDocument(ctx, doc) if err != nil { return fmt.Errorf(&#34;failed to ingest document: %w&#34;, err) } fmt.Printf(&#34;Successfully ingested document: %s\n&#34;, doc.Title) return nil }&#xD;3. Batch Document Ingestion func ingestMultipleDocuments(memory core.Memory) error { ctx := context.Background() // Prepare multiple documents documents := []core.Document{ { ID: &#34;doc-002&#34;, Title: &#34;Neural Networks Fundamentals&#34;, Content: &#34;Neural networks are computing systems inspired by biological neural networks...&#34;, Source: &#34;textbook-chapter-2.pdf&#34;, Type: core.DocumentTypePDF, Metadata: map[string]any{ &#34;author&#34;: &#34;Dr. Jane Smith&#34;, &#34;chapter&#34;: 2, &#34;subject&#34;: &#34;neural-networks&#34;, &#34;difficulty&#34;: &#34;intermediate&#34;, }, Tags: []string{&#34;neural-networks&#34;, &#34;deep-learning&#34;}, }, { ID: &#34;doc-003&#34;, Title: &#34;Data Preprocessing Techniques&#34;, Content: &#34;Data preprocessing is a crucial step in machine learning pipelines...&#34;, Source: &#34;textbook-chapter-3.pdf&#34;, Type: core.DocumentTypePDF, Metadata: map[string]any{ &#34;author&#34;: &#34;Dr. Jane Smith&#34;, &#34;chapter&#34;: 3, &#34;subject&#34;: &#34;data-preprocessing&#34;, &#34;difficulty&#34;: &#34;beginner&#34;, }, Tags: []string{&#34;data-science&#34;, &#34;preprocessing&#34;}, }, } // Batch ingest documents err := memory.IngestDocuments(ctx, documents) if err != nil { return fmt.Errorf(&#34;failed to ingest documents: %w&#34;, err) } fmt.Printf(&#34;Successfully ingested %d documents\n&#34;, len(documents)) return nil }&#xD;Text Chunking Strategies 1. Fixed-Size Chunking type FixedSizeChunker struct { ChunkSize int ChunkOverlap int } func NewFixedSizeChunker(chunkSize, overlap int) *FixedSizeChunker { return &amp;FixedSizeChunker{ ChunkSize: chunkSize, ChunkOverlap: overlap, } } func (c *FixedSizeChunker) ChunkText(text string) []string { if len(text) &lt;= c.ChunkSize { return []string{text} } var chunks []string start := 0 for start &lt; len(text) { end := start + c.ChunkSize if end &gt; len(text) { end = len(text) } chunk := text[start:end] chunks = append(chunks, chunk) // Move start position considering overlap start += c.ChunkSize - c.ChunkOverlap if start &gt;= len(text) { break } } return chunks } // Example usage func chunkLargeDocument(memory core.Memory, largeText string) error { ctx := context.Background() chunker := NewFixedSizeChunker(1000, 200) chunks := chunker.ChunkText(largeText) for i, chunk := range chunks { doc := core.Document{ ID: fmt.Sprintf(&#34;large-doc-chunk-%d&#34;, i), Title: fmt.Sprintf(&#34;Large Document - Chunk %d&#34;, i+1), Content: chunk, Source: &#34;large-document.pdf&#34;, Type: core.DocumentTypePDF, ChunkIndex: i, ChunkTotal: len(chunks), Metadata: map[string]any{ &#34;chunk_method&#34;: &#34;fixed-size&#34;, &#34;chunk_size&#34;: 1000, &#34;chunk_overlap&#34;: 200, }, CreatedAt: time.Now(), } err := memory.IngestDocument(ctx, doc) if err != nil { return fmt.Errorf(&#34;failed to ingest chunk %d: %w&#34;, i, err) } } return nil }&#xD;2. Semantic Chunking type SemanticChunker struct { MaxChunkSize int MinChunkSize int } func NewSemanticChunker(minSize, maxSize int) *SemanticChunker { return &amp;SemanticChunker{ MinChunkSize: minSize, MaxChunkSize: maxSize, } } func (c *SemanticChunker) ChunkText(text string) []string { // Split by paragraphs first paragraphs := strings.Split(text, &#34;\n\n&#34;) var chunks []string var currentChunk strings.Builder for _, paragraph := range paragraphs { paragraph = strings.TrimSpace(paragraph) if paragraph == &#34;&#34; { continue } // Check if adding this paragraph would exceed max size if currentChunk.Len() &gt; 0 &amp;&amp; currentChunk.Len()+len(paragraph) &gt; c.MaxChunkSize { // Finalize current chunk if it meets minimum size if currentChunk.Len() &gt;= c.MinChunkSize { chunks = append(chunks, currentChunk.String()) currentChunk.Reset() } } // Add paragraph to current chunk if currentChunk.Len() &gt; 0 { currentChunk.WriteString(&#34;\n\n&#34;) } currentChunk.WriteString(paragraph) } // Add final chunk if it has content if currentChunk.Len() &gt; 0 { chunks = append(chunks, currentChunk.String()) } return chunks }&#xD;3. Sentence-Based Chunking type SentenceChunker struct { MaxSentences int Overlap int } func NewSentenceChunker(maxSentences, overlap int) *SentenceChunker { return &amp;SentenceChunker{ MaxSentences: maxSentences, Overlap: overlap, } } func (c *SentenceChunker) ChunkText(text string) []string { sentences := c.splitIntoSentences(text) if len(sentences) &lt;= c.MaxSentences { return []string{text} } var chunks []string start := 0 for start &lt; len(sentences) { end := start + c.MaxSentences if end &gt; len(sentences) { end = len(sentences) } chunk := strings.Join(sentences[start:end], &#34; &#34;) chunks = append(chunks, chunk) start += c.MaxSentences - c.Overlap if start &gt;= len(sentences) { break } } return chunks } func (c *SentenceChunker) splitIntoSentences(text string) []string { // Simple sentence splitting (in production, use a proper NLP library) sentences := strings.FieldsFunc(text, func(r rune) bool { return r == &#39;.&#39; || r == &#39;!&#39; || r == &#39;?&#39; }) // Clean up sentences var cleanSentences []string for _, sentence := range sentences { sentence = strings.TrimSpace(sentence) if len(sentence) &gt; 10 { // Filter out very short fragments cleanSentences = append(cleanSentences, sentence) } } return cleanSentences }&#xD;Advanced Document Processing 1. Document Processor with Multiple Strategies type DocumentProcessor struct { memory core.Memory chunkers map[string]TextChunker extractors map[core.DocumentType]MetadataExtractor config ProcessorConfig } type TextChunker interface { ChunkText(text string) []string } type MetadataExtractor interface { ExtractMetadata(doc core.Document) (map[string]any, error) } type ProcessorConfig struct { DefaultChunkStrategy string MaxConcurrentDocs int EnableMetadataExtraction bool EnableContentCleaning bool } func NewDocumentProcessor(memory core.Memory, config ProcessorConfig) *DocumentProcessor { dp := &amp;DocumentProcessor{ memory: memory, chunkers: make(map[string]TextChunker), extractors: make(map[core.DocumentType]MetadataExtractor), config: config, } // Register default chunkers dp.chunkers[&#34;fixed&#34;] = NewFixedSizeChunker(1000, 200) dp.chunkers[&#34;semantic&#34;] = NewSemanticChunker(500, 1500) dp.chunkers[&#34;sentence&#34;] = NewSentenceChunker(10, 2) // Register metadata extractors dp.extractors[core.DocumentTypePDF] = &amp;PDFMetadataExtractor{} dp.extractors[core.DocumentTypeMarkdown] = &amp;MarkdownMetadataExtractor{} dp.extractors[core.DocumentTypeCode] = &amp;CodeMetadataExtractor{} return dp } func (dp *DocumentProcessor) ProcessDocument(ctx context.Context, doc core.Document, chunkStrategy string) error { // Clean content if enabled if dp.config.EnableContentCleaning { doc.Content = dp.cleanContent(doc.Content) } // Extract metadata if enabled if dp.config.EnableMetadataExtraction { if extractor, exists := dp.extractors[doc.Type]; exists { metadata, err := extractor.ExtractMetadata(doc) if err == nil { // Merge extracted metadata with existing if doc.Metadata == nil { doc.Metadata = make(map[string]any) } for k, v := range metadata { doc.Metadata[k] = v } } } } // Choose chunking strategy if chunkStrategy == &#34;&#34; { chunkStrategy = dp.config.DefaultChunkStrategy } chunker, exists := dp.chunkers[chunkStrategy] if !exists { return fmt.Errorf(&#34;unknown chunking strategy: %s&#34;, chunkStrategy) } // Chunk the document chunks := chunker.ChunkText(doc.Content) // Process chunks if len(chunks) == 1 { // Single chunk - ingest as-is return dp.memory.IngestDocument(ctx, doc) } // Multiple chunks - create separate documents var documents []core.Document for i, chunk := range chunks { chunkDoc := doc // Copy original document chunkDoc.ID = fmt.Sprintf(&#34;%s-chunk-%d&#34;, doc.ID, i) chunkDoc.Content = chunk chunkDoc.ChunkIndex = i chunkDoc.ChunkTotal = len(chunks) // Add chunking metadata if chunkDoc.Metadata == nil { chunkDoc.Metadata = make(map[string]any) } chunkDoc.Metadata[&#34;chunk_strategy&#34;] = chunkStrategy chunkDoc.Metadata[&#34;original_doc_id&#34;] = doc.ID documents = append(documents, chunkDoc) } return dp.memory.IngestDocuments(ctx, documents) } func (dp *DocumentProcessor) cleanContent(content string) string { // Remove excessive whitespace content = regexp.MustCompile(`\s+`).ReplaceAllString(content, &#34; &#34;) // Remove special characters that might interfere with processing content = regexp.MustCompile(`[^\w\s\.,!?;:()\-&#34;&#39;]`).ReplaceAllString(content, &#34;&#34;) // Trim whitespace content = strings.TrimSpace(content) return content }&#xD;2. Metadata Extractors // PDF Metadata Extractor type PDFMetadataExtractor struct{} func (e *PDFMetadataExtractor) ExtractMetadata(doc core.Document) (map[string]any, error) { metadata := make(map[string]any) // Extract basic statistics metadata[&#34;word_count&#34;] = len(strings.Fields(doc.Content)) metadata[&#34;char_count&#34;] = len(doc.Content) metadata[&#34;paragraph_count&#34;] = len(strings.Split(doc.Content, &#34;\n\n&#34;)) // Extract potential headings (lines that are short and followed by longer content) lines := strings.Split(doc.Content, &#34;\n&#34;) var headings []string for i, line := range lines { line = strings.TrimSpace(line) if len(line) &gt; 0 &amp;&amp; len(line) &lt; 100 &amp;&amp; i+1 &lt; len(lines) { nextLine := strings.TrimSpace(lines[i+1]) if len(nextLine) &gt; len(line)*2 { headings = append(headings, line) } } } metadata[&#34;potential_headings&#34;] = headings // Detect language (simple heuristic) metadata[&#34;detected_language&#34;] = detectLanguage(doc.Content) return metadata, nil } // Markdown Metadata Extractor type MarkdownMetadataExtractor struct{} func (e *MarkdownMetadataExtractor) ExtractMetadata(doc core.Document) (map[string]any, error) { metadata := make(map[string]any) // Extract headings headings := extractMarkdownHeadings(doc.Content) metadata[&#34;headings&#34;] = headings metadata[&#34;heading_count&#34;] = len(headings) // Extract links links := extractMarkdownLinks(doc.Content) metadata[&#34;links&#34;] = links metadata[&#34;link_count&#34;] = len(links) // Extract code blocks codeBlocks := extractMarkdownCodeBlocks(doc.Content) metadata[&#34;code_blocks&#34;] = len(codeBlocks) // Extract front matter if present frontMatter := extractFrontMatter(doc.Content) if frontMatter != nil { metadata[&#34;front_matter&#34;] = frontMatter } return metadata, nil } // Code Metadata Extractor type CodeMetadataExtractor struct{} func (e *CodeMetadataExtractor) ExtractMetadata(doc core.Document) (map[string]any, error) { metadata := make(map[string]any) // Detect programming language language := detectProgrammingLanguage(doc.Source, doc.Content) metadata[&#34;programming_language&#34;] = language // Count lines of code lines := strings.Split(doc.Content, &#34;\n&#34;) metadata[&#34;total_lines&#34;] = len(lines) // Count non-empty lines nonEmptyLines := 0 commentLines := 0 for _, line := range lines { line = strings.TrimSpace(line) if line != &#34;&#34; { nonEmptyLines++ if isCommentLine(line, language) { commentLines++ } } } metadata[&#34;code_lines&#34;] = nonEmptyLines metadata[&#34;comment_lines&#34;] = commentLines // Extract functions/methods (basic pattern matching) functions := extractFunctions(doc.Content, language) metadata[&#34;functions&#34;] = functions metadata[&#34;function_count&#34;] = len(functions) return metadata, nil } // Helper functions for metadata extraction func detectLanguage(content string) string { // Simple language detection based on common words englishWords := []string{&#34;the&#34;, &#34;and&#34;, &#34;is&#34;, &#34;in&#34;, &#34;to&#34;, &#34;of&#34;, &#34;a&#34;, &#34;that&#34;, &#34;it&#34;, &#34;with&#34;} words := strings.Fields(strings.ToLower(content)) englishCount := 0 for _, word := range words { for _, englishWord := range englishWords { if word == englishWord { englishCount++ break } } } if float64(englishCount)/float64(len(words)) &gt; 0.1 { return &#34;english&#34; } return &#34;unknown&#34; } func extractMarkdownHeadings(content string) []string { var headings []string lines := strings.Split(content, &#34;\n&#34;) for _, line := range lines { line = strings.TrimSpace(line) if strings.HasPrefix(line, &#34;#&#34;) { headings = append(headings, line) } } return headings } func extractMarkdownLinks(content string) []string { // Simple regex for markdown links [text](url) linkRegex := regexp.MustCompile(`\[([^\]]+)\]\(([^)]+)\)`) matches := linkRegex.FindAllStringSubmatch(content, -1) var links []string for _, match := range matches { if len(match) &gt;= 3 { links = append(links, match[2]) // URL part } } return links } func extractMarkdownCodeBlocks(content string) []string { // Simple extraction of code blocks codeBlockRegex := regexp.MustCompile(&#34;```[\\s\\S]*?```&#34;) matches := codeBlockRegex.FindAllString(content, -1) return matches } func extractFrontMatter(content string) map[string]any { // Extract YAML front matter if !strings.HasPrefix(content, &#34;---&#34;) { return nil } parts := strings.SplitN(content, &#34;---&#34;, 3) if len(parts) &lt; 3 { return nil } // Simple key-value extraction (in production, use a YAML parser) frontMatter := make(map[string]any) lines := strings.Split(parts[1], &#34;\n&#34;) for _, line := range lines { line = strings.TrimSpace(line) if strings.Contains(line, &#34;:&#34;) { parts := strings.SplitN(line, &#34;:&#34;, 2) if len(parts) == 2 { key := strings.TrimSpace(parts[0]) value := strings.TrimSpace(parts[1]) frontMatter[key] = value } } } return frontMatter } func detectProgrammingLanguage(filename, content string) string { // Detect by file extension ext := strings.ToLower(filepath.Ext(filename)) switch ext { case &#34;.go&#34;: return &#34;go&#34; case &#34;.py&#34;: return &#34;python&#34; case &#34;.js&#34;: return &#34;javascript&#34; case &#34;.ts&#34;: return &#34;typescript&#34; case &#34;.java&#34;: return &#34;java&#34; case &#34;.cpp&#34;, &#34;.cc&#34;, &#34;.cxx&#34;: return &#34;cpp&#34; case &#34;.c&#34;: return &#34;c&#34; case &#34;.rs&#34;: return &#34;rust&#34; } // Detect by content patterns if strings.Contains(content, &#34;package main&#34;) || strings.Contains(content, &#34;func &#34;) { return &#34;go&#34; } if strings.Contains(content, &#34;def &#34;) || strings.Contains(content, &#34;import &#34;) { return &#34;python&#34; } return &#34;unknown&#34; } func isCommentLine(line, language string) bool { switch language { case &#34;go&#34;, &#34;javascript&#34;, &#34;typescript&#34;, &#34;java&#34;, &#34;cpp&#34;, &#34;c&#34;, &#34;rust&#34;: return strings.HasPrefix(line, &#34;//&#34;) || strings.HasPrefix(line, &#34;/*&#34;) case &#34;python&#34;: return strings.HasPrefix(line, &#34;#&#34;) } return false } func extractFunctions(content, language string) []string { var functions []string switch language { case &#34;go&#34;: funcRegex := regexp.MustCompile(`func\s+(\w+)\s*\(`) matches := funcRegex.FindAllStringSubmatch(content, -1) for _, match := range matches { if len(match) &gt;= 2 { functions = append(functions, match[1]) } } case &#34;python&#34;: funcRegex := regexp.MustCompile(`def\s+(\w+)\s*\(`) matches := funcRegex.FindAllStringSubmatch(content, -1) for _, match := range matches { if len(match) &gt;= 2 { functions = append(functions, match[1]) } } case &#34;javascript&#34;, &#34;typescript&#34;: funcRegex := regexp.MustCompile(`function\s+(\w+)\s*\(`) matches := funcRegex.FindAllStringSubmatch(content, -1) for _, match := range matches { if len(match) &gt;= 2 { functions = append(functions, match[1]) } } } return functions }&#xD;Knowledge Base Search and Retrieval 1. Advanced Search with Filters func performAdvancedKnowledgeSearch(memory core.Memory) error { ctx := context.Background() // Search with multiple filters results, err := memory.SearchKnowledge(ctx, &#34;machine learning algorithms&#34;, core.WithLimit(10), core.WithScoreThreshold(0.7), core.WithSources([]string{&#34;textbook-chapter-1.pdf&#34;, &#34;textbook-chapter-2.pdf&#34;}), core.WithDocumentTypes([]core.DocumentType{core.DocumentTypePDF}), core.WithTags([]string{&#34;ml&#34;, &#34;algorithms&#34;}), core.WithDateRange(&amp;core.DateRange{ Start: time.Now().Add(-30 * 24 * time.Hour), End: time.Now(), }), ) if err != nil { return fmt.Errorf(&#34;knowledge search failed: %w&#34;, err) } fmt.Printf(&#34;Found %d relevant knowledge items:\n&#34;, len(results)) for _, result := range results { fmt.Printf(&#34;- %s (Score: %.3f)\n&#34;, result.Title, result.Score) fmt.Printf(&#34; Source: %s\n&#34;, result.Source) fmt.Printf(&#34; Content: %s...\n&#34;, truncateString(result.Content, 100)) if result.ChunkIndex &gt; 0 { fmt.Printf(&#34; Chunk: %d/%d\n&#34;, result.ChunkIndex+1, result.ChunkTotal) } fmt.Println() } return nil } func truncateString(s string, maxLen int) string { if len(s) &lt;= maxLen { return s } return s[:maxLen] + &#34;...&#34; }&#xD;2. Hybrid Search (Personal + Knowledge) func performHybridSearch(memory core.Memory) error { ctx := context.Context() // Perform hybrid search combining personal memory and knowledge base result, err := memory.SearchAll(ctx, &#34;neural network implementation&#34;, core.WithLimit(15), core.WithScoreThreshold(0.6), core.WithIncludePersonal(true), core.WithIncludeKnowledge(true), core.WithHybridWeight(0.7), // 70% semantic, 30% keyword ) if err != nil { return fmt.Errorf(&#34;hybrid search failed: %w&#34;, err) } fmt.Printf(&#34;Hybrid Search Results for: %s\n&#34;, result.Query) fmt.Printf(&#34;Total Results: %d (Search Time: %v)\n\n&#34;, result.TotalResults, result.SearchTime) // Display personal memory results if len(result.PersonalMemory) &gt; 0 { fmt.Println(&#34;Personal Memory Results:&#34;) for _, item := range result.PersonalMemory { fmt.Printf(&#34;- %s (Score: %.3f)\n&#34;, truncateString(item.Content, 80), item.Score) } fmt.Println() } // Display knowledge base results if len(result.Knowledge) &gt; 0 { fmt.Println(&#34;Knowledge Base Results:&#34;) for _, item := range result.Knowledge { fmt.Printf(&#34;- %s (Score: %.3f)\n&#34;, item.Title, item.Score) fmt.Printf(&#34; Source: %s\n&#34;, item.Source) } } return nil }&#xD;3. RAG Context Building func buildRAGContext(memory core.Memory, query string) error { ctx := context.Background() // Build comprehensive RAG context ragContext, err := memory.BuildContext(ctx, query, core.WithMaxTokens(4000), core.WithPersonalWeight(0.3), core.WithKnowledgeWeight(0.7), core.WithHistoryLimit(5), core.WithIncludeSources(true), core.WithFormatTemplate(`Context Information: Personal Memory: {{range .PersonalMemory}} - {{.Content}} {{end}} Knowledge Base: {{range .Knowledge}} - {{.Content}} (Source: {{.Source}}) {{end}} Recent Conversation: {{range .ChatHistory}} {{.Role}}: {{.Content}} {{end}} Query: {{.Query}}`), ) if err != nil { return fmt.Errorf(&#34;failed to build RAG context: %w&#34;, err) } fmt.Printf(&#34;RAG Context for: %s\n&#34;, ragContext.Query) fmt.Printf(&#34;Token Count: %d\n&#34;, ragContext.TokenCount) fmt.Printf(&#34;Sources: %v\n&#34;, ragContext.Sources) fmt.Printf(&#34;Context Text:\n%s\n&#34;, ragContext.ContextText) return nil }&#xD;Production Optimization 1. Batch Processing Pipeline type BatchProcessor struct { memory core.Memory processor *DocumentProcessor concurrency int batchSize int } func NewBatchProcessor(memory core.Memory, concurrency, batchSize int) *BatchProcessor { return &amp;BatchProcessor{ memory: memory, processor: NewDocumentProcessor(memory, ProcessorConfig{ DefaultChunkStrategy: &#34;semantic&#34;, MaxConcurrentDocs: concurrency, EnableMetadataExtraction: true, EnableContentCleaning: true, }), concurrency: concurrency, batchSize: batchSize, } } func (bp *BatchProcessor) ProcessDocuments(ctx context.Context, documents []core.Document) error { // Process documents in batches for i := 0; i &lt; len(documents); i += bp.batchSize { end := i + bp.batchSize if end &gt; len(documents) { end = len(documents) } batch := documents[i:end] err := bp.processBatch(ctx, batch) if err != nil { return fmt.Errorf(&#34;failed to process batch %d-%d: %w&#34;, i, end-1, err) } fmt.Printf(&#34;Processed batch %d-%d (%d documents)\n&#34;, i, end-1, len(batch)) } return nil } func (bp *BatchProcessor) processBatch(ctx context.Context, documents []core.Document) error { // Use worker pool for concurrent processing jobs := make(chan core.Document, len(documents)) results := make(chan error, len(documents)) // Start workers for w := 0; w &lt; bp.concurrency; w++ { go bp.worker(ctx, jobs, results) } // Send jobs for _, doc := range documents { jobs &lt;- doc } close(jobs) // Collect results var errors []error for i := 0; i &lt; len(documents); i++ { if err := &lt;-results; err != nil { errors = append(errors, err) } } if len(errors) &gt; 0 { return fmt.Errorf(&#34;batch processing failed with %d errors: %v&#34;, len(errors), errors[0]) } return nil } func (bp *BatchProcessor) worker(ctx context.Context, jobs &lt;-chan core.Document, results chan&lt;- error) { for doc := range jobs { err := bp.processor.ProcessDocument(ctx, doc, &#34;&#34;) results &lt;- err } }&#xD;2. Performance Monitoring type IngestionMetrics struct { DocumentsProcessed int64 `json:&#34;documents_processed&#34;` ChunksCreated int64 `json:&#34;chunks_created&#34;` ProcessingTime time.Duration `json:&#34;processing_time&#34;` ErrorCount int64 `json:&#34;error_count&#34;` AverageChunkSize float64 `json:&#34;average_chunk_size&#34;` mu sync.RWMutex } func (m *IngestionMetrics) RecordDocument(chunkCount int, processingTime time.Duration, chunkSizes []int) { m.mu.Lock() defer m.mu.Unlock() m.DocumentsProcessed++ m.ChunksCreated += int64(chunkCount) m.ProcessingTime += processingTime // Update average chunk size if len(chunkSizes) &gt; 0 { totalSize := 0 for _, size := range chunkSizes { totalSize += size } avgSize := float64(totalSize) / float64(len(chunkSizes)) // Running average totalChunks := float64(m.ChunksCreated) m.AverageChunkSize = (m.AverageChunkSize*(totalChunks-float64(chunkCount)) + avgSize*float64(chunkCount)) / totalChunks } } func (m *IngestionMetrics) RecordError() { m.mu.Lock() defer m.mu.Unlock() m.ErrorCount++ } func (m *IngestionMetrics) GetStats() IngestionMetrics { m.mu.RLock() defer m.mu.RUnlock() return IngestionMetrics{ DocumentsProcessed: m.DocumentsProcessed, ChunksCreated: m.ChunksCreated, ProcessingTime: m.ProcessingTime, ErrorCount: m.ErrorCount, AverageChunkSize: m.AverageChunkSize, } }&#xD;Best Practices 1. Document Ingestion Guidelines Chunk Size: Balance between context preservation and retrieval precision Overlap: Use 10-20% overlap to maintain context continuity Metadata: Extract and store rich metadata for better filtering Batch Processing: Process documents in batches for better performance Error Handling: Implement robust error handling and retry mechanisms 2. Performance Optimization Concurrent Processing: Use worker pools for parallel document processing Embedding Caching: Cache embeddings to avoid recomputation Index Optimization: Optimize vector database indexes for your query patterns Memory Management: Monitor memory usage during large batch operations 3. Quality Assurance Content Validation: Validate document content before ingestion Duplicate Detection: Implement deduplication to avoid redundant storage Quality Metrics: Track ingestion quality and search relevance Regular Maintenance: Periodically clean up and optimize the knowledge base Conclusion Document ingestion and knowledge base management are critical for building effective RAG systems. By implementing proper chunking strategies, metadata extraction, and optimization techniques, you can create knowledge bases that provide accurate and relevant information to your agents.</description>
    </item>
    <item>
      <title>knowledge-bases</title>
      <link>http://localhost:1313/AgenticGoKitDocs/tutorials/memory-systems/knowledge-bases/index.html</link>
      <pubDate>Fri, 25 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AgenticGoKitDocs/tutorials/memory-systems/knowledge-bases/index.html</guid>
      <description>Knowledge Bases in AgenticGoKit Overview Knowledge bases are structured repositories of information that enable agents to access, search, and reason over large collections of documents and data. This tutorial covers building comprehensive knowledge bases with AgenticGoKit, including document ingestion, chunking strategies, metadata management, and search optimization.&#xA;Knowledge bases transform raw information into accessible, searchable knowledge that agents can use to provide accurate and contextual responses.&#xA;Prerequisites Understanding of RAG Implementation Familiarity with Vector Databases Knowledge of document processing and text extraction Basic understanding of information retrieval concepts Knowledge Base Architecture Components Overview ┌─────────────────┐ ┌──────────────────┐ ┌─────────────────┐&#xD;│ Documents │───▶│ Ingestion │───▶│ Processing │&#xD;│ (PDF, MD, │ │ Pipeline │ │ Pipeline │&#xD;│ HTML, etc.) │ └──────────────────┘ └─────────────────┘&#xD;└─────────────────┘ │&#xD;▼&#xD;┌─────────────────┐ ┌──────────────────┐ ┌─────────────────┐&#xD;│ Search &amp; │◀───│ Vector Store │◀───│ Chunking &amp; │&#xD;│ Retrieval │ │ (pgvector/ │ │ Embedding │&#xD;└─────────────────┘ │ Weaviate) │ └─────────────────┘&#xD;└──────────────────┘&#xD;Knowledge Base Layers Storage Layer: Vector database with metadata Processing Layer: Document parsing and chunking Embedding Layer: Vector representation generation Retrieval Layer: Search and ranking algorithms Management Layer: Updates, versioning, and maintenance Document Ingestion Pipeline 1. Basic Document Processor package main import ( &#34;context&#34; &#34;fmt&#34; &#34;io/ioutil&#34; &#34;log&#34; &#34;path/filepath&#34; &#34;strings&#34; &#34;time&#34; &#34;github.com/kunalkushwaha/agenticgokit/core&#34; ) type DocumentProcessor struct { memory core.Memory parsers map[string]DocumentParser chunker *DocumentChunker config ProcessingConfig } type ProcessingConfig struct { ChunkSize int ChunkOverlap int MaxFileSize int64 SupportedFormats []string BatchSize int } type DocumentParser interface { Parse(filePath string) (*Document, error) SupportedExtensions() []string } type Document struct { ID string Title string Content string Metadata map[string]string Source string CreatedAt time.Time UpdatedAt time.Time } func NewDocumentProcessor(memory core.Memory) *DocumentProcessor { dp := &amp;DocumentProcessor{ memory: memory, parsers: make(map[string]DocumentParser), chunker: NewDocumentChunker(ChunkingConfig{ ChunkSize: 1000, ChunkOverlap: 200, Strategy: &#34;semantic&#34;, }), config: ProcessingConfig{ ChunkSize: 1000, ChunkOverlap: 200, MaxFileSize: 10 * 1024 * 1024, // 10MB SupportedFormats: []string{&#34;.txt&#34;, &#34;.md&#34;, &#34;.pdf&#34;, &#34;.html&#34;}, BatchSize: 10, }, } // Register parsers dp.registerParsers() return dp } func (dp *DocumentProcessor) registerParsers() { dp.parsers[&#34;.txt&#34;] = &amp;TextParser{} dp.parsers[&#34;.md&#34;] = &amp;MarkdownParser{} dp.parsers[&#34;.pdf&#34;] = &amp;PDFParser{} dp.parsers[&#34;.html&#34;] = &amp;HTMLParser{} } func (dp *DocumentProcessor) ProcessFile(ctx context.Context, filePath string) error { // Check file size fileInfo, err := os.Stat(filePath) if err != nil { return fmt.Errorf(&#34;failed to stat file: %w&#34;, err) } if fileInfo.Size() &gt; dp.config.MaxFileSize { return fmt.Errorf(&#34;file too large: %d bytes&#34;, fileInfo.Size()) } // Get parser for file extension ext := strings.ToLower(filepath.Ext(filePath)) parser, exists := dp.parsers[ext] if !exists { return fmt.Errorf(&#34;unsupported file format: %s&#34;, ext) } // Parse document doc, err := parser.Parse(filePath) if err != nil { return fmt.Errorf(&#34;failed to parse document: %w&#34;, err) } // Add file metadata doc.Metadata[&#34;file_path&#34;] = filePath doc.Metadata[&#34;file_size&#34;] = fmt.Sprintf(&#34;%d&#34;, fileInfo.Size()) doc.Metadata[&#34;processed_at&#34;] = time.Now().Format(time.RFC3339) // Process document return dp.ProcessDocument(ctx, doc) } func (dp *DocumentProcessor) ProcessDocument(ctx context.Context, doc *Document) error { // Chunk the document chunks, err := dp.chunker.ChunkDocument(doc) if err != nil { return fmt.Errorf(&#34;failed to chunk document: %w&#34;, err) } log.Printf(&#34;Processing document &#39;%s&#39; with %d chunks&#34;, doc.Title, len(chunks)) // Store chunks in batches for i := 0; i &lt; len(chunks); i += dp.config.BatchSize { end := i + dp.config.BatchSize if end &gt; len(chunks) { end = len(chunks) } batch := chunks[i:end] err := dp.storeBatch(ctx, batch) if err != nil { return fmt.Errorf(&#34;failed to store batch: %w&#34;, err) } } log.Printf(&#34;Successfully processed document &#39;%s&#39;&#34;, doc.Title) return nil } func (dp *DocumentProcessor) storeBatch(ctx context.Context, chunks []*DocumentChunk) error { for _, chunk := range chunks { err := dp.memory.Store(ctx, chunk.Content, &#34;document-chunk&#34;, core.WithMetadata(chunk.Metadata), core.WithTimestamp(chunk.CreatedAt), ) if err != nil { return fmt.Errorf(&#34;failed to store chunk %s: %w&#34;, chunk.ID, err) } } return nil } // Simple text parser type TextParser struct{} func (tp *TextParser) Parse(filePath string) (*Document, error) { content, err := ioutil.ReadFile(filePath) if err != nil { return nil, err } return &amp;Document{ ID: generateDocumentID(filePath), Title: filepath.Base(filePath), Content: string(content), Source: filePath, CreatedAt: time.Now(), UpdatedAt: time.Now(), Metadata: map[string]string{ &#34;format&#34;: &#34;text&#34;, &#34;parser&#34;: &#34;text&#34;, }, }, nil } func (tp *TextParser) SupportedExtensions() []string { return []string{&#34;.txt&#34;} } // Markdown parser type MarkdownParser struct{} func (mp *MarkdownParser) Parse(filePath string) (*Document, error) { content, err := ioutil.ReadFile(filePath) if err != nil { return nil, err } // Extract title from first heading lines := strings.Split(string(content), &#34;\n&#34;) title := filepath.Base(filePath) for _, line := range lines { if strings.HasPrefix(line, &#34;# &#34;) { title = strings.TrimPrefix(line, &#34;# &#34;) break } } return &amp;Document{ ID: generateDocumentID(filePath), Title: title, Content: string(content), Source: filePath, CreatedAt: time.Now(), UpdatedAt: time.Now(), Metadata: map[string]string{ &#34;format&#34;: &#34;markdown&#34;, &#34;parser&#34;: &#34;markdown&#34;, }, }, nil } func (mp *MarkdownParser) SupportedExtensions() []string { return []string{&#34;.md&#34;, &#34;.markdown&#34;} } func generateDocumentID(filePath string) string { // Generate unique ID based on file path hash := sha256.Sum256([]byte(filePath + time.Now().String())) return fmt.Sprintf(&#34;%x&#34;, hash)[:16] }&#xD;Document Chunking Strategies 1. Document Chunker type DocumentChunker struct { config ChunkingConfig } type ChunkingConfig struct { ChunkSize int ChunkOverlap int Strategy string // &#34;fixed&#34;, &#34;semantic&#34;, &#34;sentence&#34;, &#34;paragraph&#34; MinChunkSize int MaxChunkSize int } type DocumentChunk struct { ID string DocumentID string Content string ChunkIndex int StartOffset int EndOffset int Metadata map[string]string CreatedAt time.Time } func NewDocumentChunker(config ChunkingConfig) *DocumentChunker { return &amp;DocumentChunker{config: config} } func (dc *DocumentChunker) ChunkDocument(doc *Document) ([]*DocumentChunk, error) { switch dc.config.Strategy { case &#34;fixed&#34;: return dc.fixedSizeChunking(doc) case &#34;semantic&#34;: return dc.semanticChunking(doc) case &#34;sentence&#34;: return dc.sentenceChunking(doc) case &#34;paragraph&#34;: return dc.paragraphChunking(doc) default: return dc.fixedSizeChunking(doc) } } func (dc *DocumentChunker) fixedSizeChunking(doc *Document) ([]*DocumentChunk, error) { content := doc.Content chunks := make([]*DocumentChunk, 0) for i := 0; i &lt; len(content); i += dc.config.ChunkSize - dc.config.ChunkOverlap { end := i + dc.config.ChunkSize if end &gt; len(content) { end = len(content) } chunkContent := content[i:end] // Skip chunks that are too small if len(chunkContent) &lt; dc.config.MinChunkSize { continue } chunk := &amp;DocumentChunk{ ID: fmt.Sprintf(&#34;%s_chunk_%d&#34;, doc.ID, len(chunks)), DocumentID: doc.ID, Content: chunkContent, ChunkIndex: len(chunks), StartOffset: i, EndOffset: end, CreatedAt: time.Now(), Metadata: map[string]string{ &#34;document_id&#34;: doc.ID, &#34;document_title&#34;: doc.Title, &#34;document_source&#34;: doc.Source, &#34;chunk_strategy&#34;: &#34;fixed&#34;, &#34;chunk_index&#34;: fmt.Sprintf(&#34;%d&#34;, len(chunks)), }, } chunks = append(chunks, chunk) if end &gt;= len(content) { break } } return chunks, nil } func (dc *DocumentChunker) semanticChunking(doc *Document) ([]*DocumentChunk, error) { content := doc.Content // Split by double newlines (paragraphs) paragraphs := strings.Split(content, &#34;\n\n&#34;) chunks := make([]*DocumentChunk, 0) currentChunk := &#34;&#34; startOffset := 0 for _, paragraph := range paragraphs { paragraph = strings.TrimSpace(paragraph) if paragraph == &#34;&#34; { continue } // Check if adding this paragraph would exceed chunk size if len(currentChunk)+len(paragraph)+2 &gt; dc.config.ChunkSize &amp;&amp; currentChunk != &#34;&#34; { // Create chunk from current content chunk := dc.createChunk(doc, currentChunk, len(chunks), startOffset, startOffset+len(currentChunk)) chunks = append(chunks, chunk) // Start new chunk with overlap overlapSize := min(dc.config.ChunkOverlap, len(currentChunk)) currentChunk = currentChunk[len(currentChunk)-overlapSize:] + &#34;\n\n&#34; + paragraph startOffset = startOffset + len(currentChunk) - overlapSize } else { // Add paragraph to current chunk if currentChunk != &#34;&#34; { currentChunk += &#34;\n\n&#34; } currentChunk += paragraph } } // Add final chunk if there&#39;s content if currentChunk != &#34;&#34; &amp;&amp; len(currentChunk) &gt;= dc.config.MinChunkSize { chunk := dc.createChunk(doc, currentChunk, len(chunks), startOffset, startOffset+len(currentChunk)) chunks = append(chunks, chunk) } return chunks, nil } func (dc *DocumentChunker) createChunk(doc *Document, content string, index, startOffset, endOffset int) *DocumentChunk { return &amp;DocumentChunk{ ID: fmt.Sprintf(&#34;%s_chunk_%d&#34;, doc.ID, index), DocumentID: doc.ID, Content: content, ChunkIndex: index, StartOffset: startOffset, EndOffset: endOffset, CreatedAt: time.Now(), Metadata: map[string]string{ &#34;document_id&#34;: doc.ID, &#34;document_title&#34;: doc.Title, &#34;document_source&#34;: doc.Source, &#34;chunk_strategy&#34;: dc.config.Strategy, &#34;chunk_index&#34;: fmt.Sprintf(&#34;%d&#34;, index), }, } } func min(a, b int) int { if a &lt; b { return a } return b }&#xD;Knowledge Base Management 1. Knowledge Base Manager type KnowledgeBaseManager struct { memory core.Memory processor *DocumentProcessor config ManagerConfig } type ManagerConfig struct { AutoIndexing bool IndexingInterval time.Duration BackupEnabled bool BackupInterval time.Duration } func NewKnowledgeBaseManager(memory core.Memory) *KnowledgeBaseManager { processor := NewDocumentProcessor(memory) return &amp;KnowledgeBaseManager{ memory: memory, processor: processor, config: ManagerConfig{ AutoIndexing: true, IndexingInterval: 1 * time.Hour, BackupEnabled: true, BackupInterval: 24 * time.Hour, }, } } func (kbm *KnowledgeBaseManager) AddDocument(ctx context.Context, filePath string) error { return kbm.processor.ProcessFile(ctx, filePath) } func (kbm *KnowledgeBaseManager) AddDocumentFromContent(ctx context.Context, title, content string, metadata map[string]string) error { doc := &amp;Document{ ID: generateDocumentID(title + content), Title: title, Content: content, Source: &#34;direct-input&#34;, CreatedAt: time.Now(), UpdatedAt: time.Now(), Metadata: metadata, } return kbm.processor.ProcessDocument(ctx, doc) } func (kbm *KnowledgeBaseManager) Search(ctx context.Context, query string, options ...core.SearchOption) ([]core.MemoryResult, error) { return kbm.memory.Search(ctx, query, options...) } func (kbm *KnowledgeBaseManager) GetStats(ctx context.Context) (*KnowledgeBaseStats, error) { memoryStats, err := kbm.memory.GetStats(ctx) if err != nil { return nil, err } return &amp;KnowledgeBaseStats{ TotalDocuments: memoryStats.ItemCount, TotalChunks: memoryStats.ItemCount, IndexSize: memoryStats.SizeBytes, LastUpdated: time.Now(), }, nil } type KnowledgeBaseStats struct { TotalDocuments int64 `json:&#34;total_documents&#34;` TotalChunks int64 `json:&#34;total_chunks&#34;` IndexSize int64 `json:&#34;index_size_bytes&#34;` LastUpdated time.Time `json:&#34;last_updated&#34;` }&#xD;Usage Example Complete Knowledge Base Example func main() { // Setup memory with vector database memory, err := core.NewMemory(core.AgentMemoryConfig{ Provider: &#34;pgvector&#34;, Connection: &#34;postgres://user:pass@localhost:5432/agentdb&#34;, EnableRAG: true, Dimensions: 1536, Embedding: core.EmbeddingConfig{ Provider: &#34;openai&#34;, Model: &#34;text-embedding-3-small&#34;, APIKey: os.Getenv(&#34;OPENAI_API_KEY&#34;), Dimensions: 1536, }, }) if err != nil { log.Fatalf(&#34;Failed to create memory: %v&#34;, err) } // Create knowledge base manager kbManager := NewKnowledgeBaseManager(memory) ctx := context.Background() // Add documents to knowledge base documents := []string{ &#34;./docs/tutorial1.md&#34;, &#34;./docs/tutorial2.md&#34;, &#34;./docs/api-reference.md&#34;, } for _, docPath := range documents { err := kbManager.AddDocument(ctx, docPath) if err != nil { log.Printf(&#34;Failed to add document %s: %v&#34;, docPath, err) } else { log.Printf(&#34;Successfully added document: %s&#34;, docPath) } } // Add content directly err = kbManager.AddDocumentFromContent(ctx, &#34;AgenticGoKit Overview&#34;, &#34;AgenticGoKit is a Go framework for building multi-agent systems...&#34;, map[string]string{ &#34;category&#34;: &#34;overview&#34;, &#34;author&#34;: &#34;AgenticGoKit Team&#34;, }, ) if err != nil { log.Printf(&#34;Failed to add content: %v&#34;, err) } // Search the knowledge base results, err := kbManager.Search(ctx, &#34;How to build multi-agent systems?&#34;, core.WithLimit(5), core.WithScoreThreshold(0.7), ) if err != nil { log.Printf(&#34;Search failed: %v&#34;, err) } else { fmt.Printf(&#34;Found %d results:\n&#34;, len(results)) for i, result := range results { fmt.Printf(&#34;%d. %s (Score: %.3f)\n&#34;, i+1, result.Content[:100]+&#34;...&#34;, result.Score) } } // Get knowledge base statistics stats, err := kbManager.GetStats(ctx) if err != nil { log.Printf(&#34;Failed to get stats: %v&#34;, err) } else { fmt.Printf(&#34;Knowledge Base Stats:\n&#34;) fmt.Printf(&#34; Documents: %d\n&#34;, stats.TotalDocuments) fmt.Printf(&#34; Chunks: %d\n&#34;, stats.TotalChunks) fmt.Printf(&#34; Index Size: %d MB\n&#34;, stats.IndexSize/1024/1024) } }&#xD;Best Practices 1. Document Processing Format Support: Implement parsers for all relevant document formats Error Handling: Gracefully handle parsing errors and corrupted files Batch Processing: Process multiple documents efficiently Progress Tracking: Provide feedback on processing progress Validation: Validate documents before processing 2. Chunking Strategy Content-Aware: Use semantic chunking for better context preservation Overlap Management: Balance overlap size with storage efficiency Size Optimization: Optimize chunk size for your embedding model Metadata Preservation: Maintain document context in chunks Quality Control: Validate chunk quality and coherence 3. Search Optimization Index Tuning: Optimize vector database indexes Query Enhancement: Improve query understanding Result Ranking: Implement effective ranking algorithms Caching: Cache frequent searches Performance Monitoring: Track search performance metrics Conclusion Knowledge bases in AgenticGoKit provide the foundation for intelligent information retrieval and RAG systems. Key takeaways:</description>
    </item>
    <item>
      <title>memory-optimization</title>
      <link>http://localhost:1313/AgenticGoKitDocs/tutorials/memory-systems/memory-optimization/index.html</link>
      <pubDate>Fri, 25 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AgenticGoKitDocs/tutorials/memory-systems/memory-optimization/index.html</guid>
      <description>Memory Optimization in AgenticGoKit Overview Memory optimization is crucial for building scalable, high-performance agent systems. This tutorial covers advanced techniques for optimizing memory systems in AgenticGoKit, including performance tuning, caching strategies, resource management, and scaling patterns.&#xA;Prerequisites Understanding of Vector Databases Familiarity with RAG Implementation Knowledge of Knowledge Bases Basic understanding of database performance tuning Performance Optimization Strategies 1. Vector Database Optimization // Optimized pgvector configuration func createOptimizedPgvectorMemory() (core.Memory, error) { config := core.AgentMemoryConfig{ Provider: &#34;pgvector&#34;, Connection: &#34;postgres://user:pass@localhost:5432/agentdb&#34;, EnableRAG: true, Dimensions: 1536, Embedding: core.EmbeddingConfig{ Provider: &#34;openai&#34;, Model: &#34;text-embedding-3-small&#34;, APIKey: os.Getenv(&#34;OPENAI_API_KEY&#34;), Dimensions: 1536, BatchSize: 100, }, Options: map[string]interface{}{ // Connection pool settings &#34;max_connections&#34;: 20, &#34;max_idle_connections&#34;: 5, &#34;connection_timeout&#34;: &#34;30s&#34;, // Index optimization &#34;index_type&#34;: &#34;hnsw&#34;, &#34;hnsw_m&#34;: 16, &#34;hnsw_ef_construction&#34;: 64, // Memory settings &#34;work_mem&#34;: &#34;256MB&#34;, &#34;effective_cache_size&#34;: &#34;4GB&#34;, }, } return core.NewMemory(config) }&#xD;2. Caching Strategies type OptimizedMemoryCache struct { queryCache *LRUCache embeddingCache *LRUCache resultCache *LRUCache config CacheConfig } type CacheConfig struct { QueryCacheSize int EmbeddingCacheSize int ResultCacheSize int TTL time.Duration } func NewOptimizedMemoryCache(config CacheConfig) *OptimizedMemoryCache { return &amp;OptimizedMemoryCache{ queryCache: NewLRUCache(config.QueryCacheSize, config.TTL), embeddingCache: NewLRUCache(config.EmbeddingCacheSize, config.TTL), resultCache: NewLRUCache(config.ResultCacheSize, config.TTL), config: config, } } func (omc *OptimizedMemoryCache) GetCachedResults(query string) ([]core.MemoryResult, bool) { if results, found := omc.queryCache.Get(query); found { return results.([]core.MemoryResult), true } return nil, false } func (omc *OptimizedMemoryCache) CacheResults(query string, results []core.MemoryResult) { omc.queryCache.Set(query, results) } func (omc *OptimizedMemoryCache) GetCachedEmbedding(text string) ([]float32, bool) { if embedding, found := omc.embeddingCache.Get(text); found { return embedding.([]float32), true } return nil, false } func (omc *OptimizedMemoryCache) CacheEmbedding(text string, embedding []float32) { omc.embeddingCache.Set(text, embedding) }&#xD;3. Batch Processing Optimization type BatchProcessor struct { memory core.Memory batchSize int timeout time.Duration semaphore chan struct{} } func NewBatchProcessor(memory core.Memory, batchSize int, concurrency int) *BatchProcessor { return &amp;BatchProcessor{ memory: memory, batchSize: batchSize, timeout: 30 * time.Second, semaphore: make(chan struct{}, concurrency), } } func (bp *BatchProcessor) ProcessBatch(ctx context.Context, items []string) error { // Process items in batches for i := 0; i &lt; len(items); i += bp.batchSize { end := i + bp.batchSize if end &gt; len(items) { end = len(items) } batch := items[i:end] // Acquire semaphore for concurrency control select { case bp.semaphore &lt;- struct{}{}: go func(b []string) { defer func() { &lt;-bp.semaphore }() bp.processBatch(ctx, b) }(batch) case &lt;-ctx.Done(): return ctx.Err() } } return nil } func (bp *BatchProcessor) processBatch(ctx context.Context, batch []string) error { ctx, cancel := context.WithTimeout(ctx, bp.timeout) defer cancel() for _, item := range batch { err := bp.memory.Store(ctx, item, &#34;batch-item&#34;) if err != nil { log.Printf(&#34;Failed to store batch item: %v&#34;, err) } } return nil }&#xD;Resource Management 1. Connection Pool Optimization type OptimizedConnectionManager struct { pool *sql.DB metrics *ConnectionMetrics config PoolConfig } type PoolConfig struct { MaxOpenConns int MaxIdleConns int ConnMaxLifetime time.Duration ConnMaxIdleTime time.Duration } func NewOptimizedConnectionManager(dsn string, config PoolConfig) (*OptimizedConnectionManager, error) { db, err := sql.Open(&#34;postgres&#34;, dsn) if err != nil { return nil, err } // Configure connection pool db.SetMaxOpenConns(config.MaxOpenConns) db.SetMaxIdleConns(config.MaxIdleConns) db.SetConnMaxLifetime(config.ConnMaxLifetime) db.SetConnMaxIdleTime(config.ConnMaxIdleTime) return &amp;OptimizedConnectionManager{ pool: db, metrics: NewConnectionMetrics(), config: config, }, nil } func (ocm *OptimizedConnectionManager) GetConnection(ctx context.Context) (*sql.Conn, error) { start := time.Now() conn, err := ocm.pool.Conn(ctx) if err != nil { ocm.metrics.RecordError() return nil, err } ocm.metrics.RecordAcquisition(time.Since(start)) return conn, nil }&#xD;2. Memory Usage Monitoring type MemoryMonitor struct { thresholds MemoryThresholds alerts chan Alert interval time.Duration } type MemoryThresholds struct { WarningMB int64 CriticalMB int64 } type Alert struct { Level string Message string Time time.Time } func NewMemoryMonitor(thresholds MemoryThresholds) *MemoryMonitor { return &amp;MemoryMonitor{ thresholds: thresholds, alerts: make(chan Alert, 100), interval: 30 * time.Second, } } func (mm *MemoryMonitor) Start(ctx context.Context) { ticker := time.NewTicker(mm.interval) defer ticker.Stop() for { select { case &lt;-ctx.Done(): return case &lt;-ticker.C: mm.checkMemoryUsage() } } } func (mm *MemoryMonitor) checkMemoryUsage() { var m runtime.MemStats runtime.ReadMemStats(&amp;m) heapMB := int64(m.HeapAlloc / 1024 / 1024) if heapMB &gt; mm.thresholds.CriticalMB { mm.alerts &lt;- Alert{ Level: &#34;CRITICAL&#34;, Message: fmt.Sprintf(&#34;Memory usage: %d MB&#34;, heapMB), Time: time.Now(), } runtime.GC() // Force garbage collection } else if heapMB &gt; mm.thresholds.WarningMB { mm.alerts &lt;- Alert{ Level: &#34;WARNING&#34;, Message: fmt.Sprintf(&#34;Memory usage: %d MB&#34;, heapMB), Time: time.Now(), } } } func (mm *MemoryMonitor) GetAlerts() &lt;-chan Alert { return mm.alerts }&#xD;Performance Monitoring 1. Metrics Collection type PerformanceMetrics struct { searchLatency []time.Duration cacheHitRate float64 throughput int64 errorCount int64 mu sync.RWMutex } func NewPerformanceMetrics() *PerformanceMetrics { return &amp;PerformanceMetrics{ searchLatency: make([]time.Duration, 0, 1000), } } func (pm *PerformanceMetrics) RecordSearch(duration time.Duration, cached bool) { pm.mu.Lock() defer pm.mu.Unlock() pm.searchLatency = append(pm.searchLatency, duration) pm.throughput++ // Keep only recent measurements if len(pm.searchLatency) &gt; 1000 { pm.searchLatency = pm.searchLatency[len(pm.searchLatency)-1000:] } // Update cache hit rate if cached { pm.cacheHitRate = (pm.cacheHitRate + 1.0) / 2.0 } else { pm.cacheHitRate = pm.cacheHitRate / 2.0 } } func (pm *PerformanceMetrics) GetAverageLatency() time.Duration { pm.mu.RLock() defer pm.mu.RUnlock() if len(pm.searchLatency) == 0 { return 0 } var total time.Duration for _, latency := range pm.searchLatency { total += latency } return total / time.Duration(len(pm.searchLatency)) } func (pm *PerformanceMetrics) GetReport() map[string]interface{} { pm.mu.RLock() defer pm.mu.RUnlock() return map[string]interface{}{ &#34;avg_latency_ms&#34;: pm.GetAverageLatency().Milliseconds(), &#34;cache_hit_rate&#34;: pm.cacheHitRate, &#34;throughput&#34;: pm.throughput, &#34;error_count&#34;: pm.errorCount, } }&#xD;Best Practices 1. Configuration Optimization Connection Pooling: Configure appropriate pool sizes based on workload Index Selection: Choose HNSW for better query performance, IVFFlat for faster indexing Memory Settings: Tune PostgreSQL memory settings for vector operations Batch Sizes: Optimize batch sizes for embedding API calls Cache Sizes: Size caches based on available memory and hit rate requirements 2. Performance Monitoring Latency Tracking: Monitor search and indexing latencies Resource Usage: Track memory, CPU, and disk usage Cache Performance: Monitor cache hit rates and effectiveness Error Rates: Track and alert on error rates Throughput: Monitor requests per second and concurrent users 3. Scaling Strategies Vertical Scaling: Increase memory and CPU for single-node performance Read Replicas: Use read replicas for read-heavy workloads Sharding: Distribute data across multiple nodes for large datasets Caching Layers: Implement multiple caching layers for frequently accessed data Load Balancing: Distribute requests across multiple instances Conclusion Memory optimization is essential for production-ready agent systems. Key takeaways:</description>
    </item>
    <item>
      <title>rag-implementation</title>
      <link>http://localhost:1313/AgenticGoKitDocs/tutorials/memory-systems/rag-implementation/index.html</link>
      <pubDate>Fri, 25 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AgenticGoKitDocs/tutorials/memory-systems/rag-implementation/index.html</guid>
      <description>RAG Implementation in AgenticGoKit Overview Retrieval-Augmented Generation (RAG) combines the power of large language models with external knowledge retrieval to provide more accurate, up-to-date, and contextually relevant responses. This tutorial covers implementing RAG systems in AgenticGoKit, from basic retrieval to advanced techniques.&#xA;RAG enables agents to access vast amounts of information while maintaining the conversational abilities of language models, making them more knowledgeable and reliable.&#xA;Prerequisites Understanding of Vector Databases Familiarity with Basic Memory Operations Knowledge of language model APIs Basic understanding of information retrieval concepts RAG Architecture Basic RAG Flow ┌─────────────┐ ┌──────────────┐ ┌─────────────────┐&#xD;│ User Query │───▶│ Retrieval │───▶│ Context + Query │&#xD;└─────────────┘ │ System │ └─────────────────┘&#xD;└──────────────┘ │&#xD;│ ▼&#xD;▼ ┌─────────────────┐&#xD;┌──────────────┐ │ Language Model │&#xD;│ Vector Store │ │ Generation │&#xD;└──────────────┘ └─────────────────┘&#xD;│&#xD;▼&#xD;┌─────────────────┐&#xD;│ Enhanced │&#xD;│ Response │&#xD;└─────────────────┘&#xD;Advanced RAG Components Query Processing: Understanding and reformulating user queries Retrieval: Finding relevant information from knowledge base Reranking: Improving relevance of retrieved results Context Building: Constructing effective prompts Generation: Producing responses with retrieved context Response Enhancement: Post-processing and validation Basic RAG Implementation 1. Simple RAG Agent package main import ( &#34;context&#34; &#34;fmt&#34; &#34;log&#34; &#34;os&#34; &#34;strings&#34; &#34;github.com/kunalkushwaha/agenticgokit/core&#34; ) type BasicRAGAgent struct { name string memory core.Memory llm core.ModelProvider config RAGConfig } type RAGConfig struct { MaxRetrievalResults int ScoreThreshold float32 MaxContextLength int ContextTemplate string } func NewBasicRAGAgent(name string, memory core.Memory, llm core.ModelProvider) *BasicRAGAgent { return &amp;BasicRAGAgent{ name: name, memory: memory, llm: llm, config: RAGConfig{ MaxRetrievalResults: 5, ScoreThreshold: 0.7, MaxContextLength: 2000, ContextTemplate: `Based on the following information: %s Please answer the question: %s`, }, } } func (r *BasicRAGAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Extract user query query, ok := state.Get(&#34;message&#34;) if !ok { return core.AgentResult{}, fmt.Errorf(&#34;no message in state&#34;) } queryStr := query.(string) // Retrieve relevant context contextStr, sources, err := r.retrieveContext(ctx, queryStr) if err != nil { return core.AgentResult{}, fmt.Errorf(&#34;retrieval failed: %w&#34;, err) } // Generate response with context response, err := r.generateResponse(ctx, queryStr, contextStr) if err != nil { return core.AgentResult{}, fmt.Errorf(&#34;generation failed: %w&#34;, err) } // Store the interaction sessionID, _ := event.GetMetadataValue(core.SessionIDKey) err = r.storeInteraction(ctx, sessionID, queryStr, response, sources) if err != nil { log.Printf(&#34;Failed to store interaction: %v&#34;, err) } // Return result outputState := state.Clone() outputState.Set(&#34;response&#34;, response) outputState.Set(&#34;sources&#34;, sources) outputState.Set(&#34;context_used&#34;, len(sources) &gt; 0) return core.AgentResult{OutputState: outputState}, nil } func (r *BasicRAGAgent) retrieveContext(ctx context.Context, query string) (string, []string, error) { // Search for relevant information results, err := r.memory.Search(ctx, query, core.WithLimit(r.config.MaxRetrievalResults), core.WithScoreThreshold(r.config.ScoreThreshold), ) if err != nil { return &#34;&#34;, nil, fmt.Errorf(&#34;search failed: %w&#34;, err) } if len(results) == 0 { return &#34;&#34;, nil, nil // No relevant context found } // Build context string var contextBuilder strings.Builder sources := make([]string, 0, len(results)) for i, result := range results { // Add numbered context item contextBuilder.WriteString(fmt.Sprintf(&#34;%d. %s\n&#34;, i+1, result.Content)) // Track sources if source, ok := result.Metadata[&#34;source&#34;]; ok { sources = append(sources, source) } else { sources = append(sources, fmt.Sprintf(&#34;Document %s&#34;, result.ID)) } } context := contextBuilder.String() // Truncate if too long if len(context) &gt; r.config.MaxContextLength { context = context[:r.config.MaxContextLength] + &#34;...&#34; } return context, sources, nil } func (r *BasicRAGAgent) generateResponse(ctx context.Context, query, context string) (string, error) { var prompt string if context != &#34;&#34; { // Use context template prompt = fmt.Sprintf(r.config.ContextTemplate, context, query) } else { // Fallback to direct query prompt = fmt.Sprintf(&#34;Please answer the following question: %s&#34;, query) } // Generate response response, err := r.llm.Generate(ctx, prompt) if err != nil { return &#34;&#34;, fmt.Errorf(&#34;LLM generation failed: %w&#34;, err) } return response, nil } func (r *BasicRAGAgent) storeInteraction(ctx context.Context, sessionID, query, response string, sources []string) error { // Store user query err := r.memory.Store(ctx, query, &#34;user-query&#34;, core.WithSession(sessionID), core.WithTimestamp(time.Now()), core.WithMetadata(map[string]string{ &#34;interaction_type&#34;: &#34;rag-query&#34;, }), ) if err != nil { return err } // Store agent response with sources sourcesStr := strings.Join(sources, &#34;, &#34;) err = r.memory.Store(ctx, response, &#34;agent-response&#34;, core.WithSession(sessionID), core.WithTimestamp(time.Now()), core.WithMetadata(map[string]string{ &#34;interaction_type&#34;: &#34;rag-response&#34;, &#34;sources_used&#34;: sourcesStr, &#34;sources_count&#34;: fmt.Sprintf(&#34;%d&#34;, len(sources)), }), ) return err } func main() { // Setup memory with vector database memory, err := core.NewMemory(core.AgentMemoryConfig{ Provider: &#34;pgvector&#34;, Connection: &#34;postgres://user:pass@localhost:5432/agentdb&#34;, EnableRAG: true, Dimensions: 1536, Embedding: core.EmbeddingConfig{ Provider: &#34;openai&#34;, Model: &#34;text-embedding-3-small&#34;, APIKey: os.Getenv(&#34;OPENAI_API_KEY&#34;), Dimensions: 1536, }, }) if err != nil { log.Fatalf(&#34;Failed to create memory: %v&#34;, err) } // Setup LLM provider llm, err := core.NewOpenAIAdapter( os.Getenv(&#34;OPENAI_API_KEY&#34;), &#34;gpt-3.5-turbo&#34;, 2000, 0.7, ) if err != nil { log.Fatalf(&#34;Failed to create LLM: %v&#34;, err) } // Create RAG agent ragAgent := NewBasicRAGAgent(&#34;rag-assistant&#34;, memory, llm) // Test the agent ctx := context.Background() // First, populate some knowledge knowledge := []string{ &#34;AgenticGoKit is a Go framework for building multi-agent systems with support for LLM integration, memory systems, and orchestration patterns.&#34;, &#34;Vector databases like pgvector and Weaviate are used in AgenticGoKit for semantic search and RAG implementations.&#34;, &#34;The framework supports multiple orchestration patterns including route, collaborative, sequential, and loop modes.&#34;, } for _, info := range knowledge { memory.Store(ctx, info, &#34;knowledge&#34;, core.WithMetadata(map[string]string{ &#34;source&#34;: &#34;documentation&#34;, &#34;topic&#34;: &#34;agenticgokit&#34;, }), ) } // Test query event := core.NewEvent( &#34;rag-assistant&#34;, core.EventData{&#34;message&#34;: &#34;What is AgenticGoKit and what databases does it support?&#34;}, map[string]string{&#34;session_id&#34;: &#34;test-session&#34;}, ) state := core.NewState() state.Set(&#34;message&#34;, &#34;What is AgenticGoKit and what databases does it support?&#34;) result, err := ragAgent.Run(ctx, event, state) if err != nil { log.Fatalf(&#34;RAG agent failed: %v&#34;, err) } response, _ := result.OutputState.Get(&#34;response&#34;) sources, _ := result.OutputState.Get(&#34;sources&#34;) fmt.Printf(&#34;Response: %s\n&#34;, response) fmt.Printf(&#34;Sources: %v\n&#34;, sources) }&#xD;Advanced RAG Techniques 1. Query Enhancement type QueryEnhancer struct { llm core.LLMProvider } func NewQueryEnhancer(llm core.LLMProvider) *QueryEnhancer { return &amp;QueryEnhancer{llm: llm} } func (qe *QueryEnhancer) EnhanceQuery(ctx context.Context, originalQuery string, conversationHistory []core.Message) (string, error) { // Build context from conversation history var historyBuilder strings.Builder for _, msg := range conversationHistory { historyBuilder.WriteString(fmt.Sprintf(&#34;%s: %s\n&#34;, msg.Role, msg.Content)) } // Create enhancement prompt prompt := fmt.Sprintf(`Given the conversation history: %s The user&#39;s current query is: &#34;%s&#34; Please rewrite this query to be more specific and searchable, incorporating relevant context from the conversation history. The enhanced query should be optimized for semantic search. Enhanced query:`, historyBuilder.String(), originalQuery) enhancedQuery, err := qe.llm.Generate(ctx, prompt) if err != nil { // Fallback to original query return originalQuery, nil } return strings.TrimSpace(enhancedQuery), nil } // Multi-query generation for better retrieval func (qe *QueryEnhancer) GenerateMultipleQueries(ctx context.Context, originalQuery string) ([]string, error) { prompt := fmt.Sprintf(`Given the query: &#34;%s&#34; Generate 3 different ways to ask the same question that would help find relevant information: 1. 2. 3.`, originalQuery) response, err := qe.llm.Generate(ctx, prompt) if err != nil { return []string{originalQuery}, nil } // Parse the numbered list lines := strings.Split(response, &#34;\n&#34;) queries := make([]string, 0, 3) for _, line := range lines { line = strings.TrimSpace(line) if strings.HasPrefix(line, &#34;1.&#34;) || strings.HasPrefix(line, &#34;2.&#34;) || strings.HasPrefix(line, &#34;3.&#34;) { query := strings.TrimSpace(line[2:]) if query != &#34;&#34; { queries = append(queries, query) } } } // Always include original query if len(queries) == 0 { queries = append(queries, originalQuery) } return queries, nil }&#xD;2. Advanced Retrieval with Reranking type AdvancedRetriever struct { memory core.Memory reranker *Reranker config RetrievalConfig } type RetrievalConfig struct { InitialRetrievalLimit int FinalResultLimit int ScoreThreshold float64 RerankingEnabled bool DiversityThreshold float64 } type Reranker struct { llm core.LLMProvider } func NewReranker(llm core.LLMProvider) *Reranker { return &amp;Reranker{llm: llm} } func (r *Reranker) Rerank(ctx context.Context, query string, results []core.MemoryResult) ([]core.MemoryResult, error) { if len(results) &lt;= 1 { return results, nil } // Create reranking prompt var resultsBuilder strings.Builder resultsBuilder.WriteString(&#34;Rank the following passages by relevance to the query:\n\n&#34;) resultsBuilder.WriteString(fmt.Sprintf(&#34;Query: %s\n\n&#34;, query)) for i, result := range results { resultsBuilder.WriteString(fmt.Sprintf(&#34;Passage %d: %s\n\n&#34;, i+1, result.Content)) } resultsBuilder.WriteString(&#34;Please rank these passages from most relevant (1) to least relevant, providing only the numbers separated by commas (e.g., 3,1,4,2):&#34;) response, err := r.llm.Generate(ctx, resultsBuilder.String()) if err != nil { // Fallback to original order return results, nil } // Parse ranking ranking := r.parseRanking(response, len(results)) // Reorder results based on ranking rerankedResults := make([]core.MemoryResult, 0, len(results)) for _, idx := range ranking { if idx &gt;= 0 &amp;&amp; idx &lt; len(results) { rerankedResults = append(rerankedResults, results[idx]) } } // Add any missing results used := make(map[int]bool) for _, idx := range ranking { used[idx] = true } for i, result := range results { if !used[i] { rerankedResults = append(rerankedResults, result) } } return rerankedResults, nil } func (r *Reranker) parseRanking(response string, maxItems int) []int { // Clean and split the response response = strings.TrimSpace(response) parts := strings.Split(response, &#34;,&#34;) ranking := make([]int, 0, len(parts)) for _, part := range parts { part = strings.TrimSpace(part) if num, err := strconv.Atoi(part); err == nil &amp;&amp; num &gt;= 1 &amp;&amp; num &lt;= maxItems { ranking = append(ranking, num-1) // Convert to 0-based index } } return ranking } func (ar *AdvancedRetriever) Retrieve(ctx context.Context, query string) ([]core.MemoryResult, error) { // Initial retrieval with higher limit results, err := ar.memory.Search(ctx, query, core.WithLimit(ar.config.InitialRetrievalLimit), core.WithScoreThreshold(ar.config.ScoreThreshold*0.8), // Lower threshold initially ) if err != nil { return nil, fmt.Errorf(&#34;initial retrieval failed: %w&#34;, err) } if len(results) == 0 { return results, nil } // Apply reranking if enabled if ar.config.RerankingEnabled &amp;&amp; ar.reranker != nil { results, err = ar.reranker.Rerank(ctx, query, results) if err != nil { log.Printf(&#34;Reranking failed, using original order: %v&#34;, err) } } // Apply diversity filtering if ar.config.DiversityThreshold &gt; 0 { results = ar.applyDiversityFilter(results) } // Limit final results if len(results) &gt; ar.config.FinalResultLimit { results = results[:ar.config.FinalResultLimit] } return results, nil } func (ar *AdvancedRetriever) applyDiversityFilter(results []core.MemoryResult) []core.MemoryResult { if len(results) &lt;= 1 { return results } filtered := []core.MemoryResult{results[0]} // Always include the top result for _, candidate := range results[1:] { isDiverse := true for _, selected := range filtered { similarity := ar.calculateSimilarity(candidate.Content, selected.Content) if similarity &gt; ar.config.DiversityThreshold { isDiverse = false break } } if isDiverse { filtered = append(filtered, candidate) } } return filtered } func (ar *AdvancedRetriever) calculateSimilarity(text1, text2 string) float64 { // Simple similarity calculation (in production, use proper similarity metrics) words1 := strings.Fields(strings.ToLower(text1)) words2 := strings.Fields(strings.ToLower(text2)) wordSet1 := make(map[string]bool) for _, word := range words1 { wordSet1[word] = true } common := 0 for _, word := range words2 { if wordSet1[word] { common++ } } if len(words1) == 0 || len(words2) == 0 { return 0 } return float64(common) / float64(len(words1)+len(words2)-common) // Jaccard similarity }&#xD;3. Context-Aware Response Generation type ContextAwareGenerator struct { llm core.LLMProvider config GenerationConfig } type GenerationConfig struct { MaxContextLength int ResponseMaxLength int IncludeSources bool FactCheckingEnabled bool TemperatureAdjustment float32 } func NewContextAwareGenerator(llm core.LLMProvider, config GenerationConfig) *ContextAwareGenerator { return &amp;ContextAwareGenerator{ llm: llm, config: config, } } func (cag *ContextAwareGenerator) Generate(ctx context.Context, query string, retrievedContext []core.MemoryResult, conversationHistory []core.Message) (string, error) { // Build comprehensive context context := cag.buildContext(query, retrievedContext, conversationHistory) // Create generation prompt prompt := cag.createPrompt(query, context, retrievedContext) // Generate response response, err := cag.llm.Generate(ctx, prompt) if err != nil { return &#34;&#34;, fmt.Errorf(&#34;generation failed: %w&#34;, err) } // Post-process response response = cag.postProcessResponse(response, retrievedContext) // Fact-check if enabled if cag.config.FactCheckingEnabled { response, err = cag.factCheck(ctx, response, retrievedContext) if err != nil { log.Printf(&#34;Fact-checking failed: %v&#34;, err) } } return response, nil } func (cag *ContextAwareGenerator) buildContext(query string, retrievedContext []core.MemoryResult, history []core.Message) string { var contextBuilder strings.Builder // Add conversation history if relevant if len(history) &gt; 0 { contextBuilder.WriteString(&#34;Recent conversation:\n&#34;) for _, msg := range history { contextBuilder.WriteString(fmt.Sprintf(&#34;%s: %s\n&#34;, msg.Role, msg.Content)) } contextBuilder.WriteString(&#34;\n&#34;) } // Add retrieved context if len(retrievedContext) &gt; 0 { contextBuilder.WriteString(&#34;Relevant information:\n&#34;) for i, result := range retrievedContext { contextBuilder.WriteString(fmt.Sprintf(&#34;%d. %s&#34;, i+1, result.Content)) // Add source information if available if source, ok := result.Metadata[&#34;source&#34;]; ok { contextBuilder.WriteString(fmt.Sprintf(&#34; (Source: %s)&#34;, source)) } contextBuilder.WriteString(&#34;\n&#34;) } } context := contextBuilder.String() // Truncate if too long if len(context) &gt; cag.config.MaxContextLength { context = context[:cag.config.MaxContextLength] + &#34;...\n[Context truncated]&#34; } return context } func (cag *ContextAwareGenerator) createPrompt(query, context string, retrievedContext []core.MemoryResult) string { var promptBuilder strings.Builder promptBuilder.WriteString(&#34;You are a knowledgeable assistant. Use the provided context to answer the user&#39;s question accurately and helpfully.\n\n&#34;) if context != &#34;&#34; { promptBuilder.WriteString(&#34;Context:\n&#34;) promptBuilder.WriteString(context) promptBuilder.WriteString(&#34;\n&#34;) } promptBuilder.WriteString(fmt.Sprintf(&#34;Question: %s\n\n&#34;, query)) promptBuilder.WriteString(&#34;Instructions:\n&#34;) promptBuilder.WriteString(&#34;- Answer based on the provided context\n&#34;) promptBuilder.WriteString(&#34;- If the context doesn&#39;t contain enough information, say so\n&#34;) promptBuilder.WriteString(&#34;- Be accurate and cite sources when possible\n&#34;) if cag.config.IncludeSources &amp;&amp; len(retrievedContext) &gt; 0 { promptBuilder.WriteString(&#34;- Include source references in your response\n&#34;) } promptBuilder.WriteString(&#34;\nAnswer:&#34;) return promptBuilder.String() } func (cag *ContextAwareGenerator) postProcessResponse(response string, context []core.MemoryResult) string { // Clean up response response = strings.TrimSpace(response) // Add source citations if configured if cag.config.IncludeSources &amp;&amp; len(context) &gt; 0 { response = cag.addSourceCitations(response, context) } // Truncate if too long if len(response) &gt; cag.config.ResponseMaxLength { response = response[:cag.config.ResponseMaxLength] + &#34;...&#34; } return response } func (cag *ContextAwareGenerator) addSourceCitations(response string, context []core.MemoryResult) string { if len(context) == 0 { return response } var sourcesBuilder strings.Builder sourcesBuilder.WriteString(&#34;\n\nSources:\n&#34;) for i, result := range context { if source, ok := result.Metadata[&#34;source&#34;]; ok { sourcesBuilder.WriteString(fmt.Sprintf(&#34;[%d] %s\n&#34;, i+1, source)) } else { sourcesBuilder.WriteString(fmt.Sprintf(&#34;[%d] Internal knowledge base\n&#34;, i+1)) } } return response + sourcesBuilder.String() } func (cag *ContextAwareGenerator) factCheck(ctx context.Context, response string, context []core.MemoryResult) (string, error) { // Create fact-checking prompt var contextBuilder strings.Builder for _, result := range context { contextBuilder.WriteString(fmt.Sprintf(&#34;- %s\n&#34;, result.Content)) } prompt := fmt.Sprintf(`Please fact-check the following response against the provided context: Context: %s Response to check: %s Is the response factually accurate based on the context? If there are any inaccuracies, please provide a corrected version. Fact-check result:`, contextBuilder.String(), response) factCheckResult, err := cag.llm.Generate(ctx, prompt) if err != nil { return response, err // Return original response if fact-checking fails } // Simple heuristic: if the fact-check suggests corrections, use them if strings.Contains(strings.ToLower(factCheckResult), &#34;corrected version&#34;) || strings.Contains(strings.ToLower(factCheckResult), &#34;inaccurate&#34;) { // Extract corrected version (this is a simplified approach) lines := strings.Split(factCheckResult, &#34;\n&#34;) for i, line := range lines { if strings.Contains(strings.ToLower(line), &#34;corrected&#34;) &amp;&amp; i+1 &lt; len(lines) { return strings.TrimSpace(lines[i+1]), nil } } } return response, nil }&#xD;RAG Agent Integration 1. Complete RAG Agent type ComprehensiveRAGAgent struct { name string memory core.Memory llm core.LLMProvider queryEnhancer *QueryEnhancer retriever *AdvancedRetriever generator *ContextAwareGenerator conversationMgr *ConversationManager } type ConversationManager struct { memory core.Memory } func NewConversationManager(memory core.Memory) *ConversationManager { return &amp;ConversationManager{memory: memory} } func (cm *ConversationManager) GetRecentHistory(ctx context.Context, sessionID string, limit int) ([]core.Message, error) { return cm.memory.GetHistory(ctx, limit, core.WithSession(sessionID), core.WithTimeRange(time.Now().Add(-24*time.Hour), time.Now()), ) } func NewComprehensiveRAGAgent(name string, memory core.Memory, llm core.LLMProvider) *ComprehensiveRAGAgent { return &amp;ComprehensiveRAGAgent{ name: name, memory: memory, llm: llm, queryEnhancer: NewQueryEnhancer(llm), retriever: &amp;AdvancedRetriever{ memory: memory, reranker: NewReranker(llm), config: RetrievalConfig{ InitialRetrievalLimit: 10, FinalResultLimit: 5, ScoreThreshold: 0.7, RerankingEnabled: true, DiversityThreshold: 0.8, }, }, generator: NewContextAwareGenerator(llm, GenerationConfig{ MaxContextLength: 3000, ResponseMaxLength: 1500, IncludeSources: true, FactCheckingEnabled: true, TemperatureAdjustment: 0.1, }), conversationMgr: NewConversationManager(memory), } } func (cra *ComprehensiveRAGAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Extract query query, ok := state.Get(&#34;message&#34;) if !ok { return core.AgentResult{}, fmt.Errorf(&#34;no message in state&#34;) } queryStr := query.(string) sessionID := event.GetSessionID() // Get conversation history history, err := cra.conversationMgr.GetRecentHistory(ctx, sessionID, 5) if err != nil { log.Printf(&#34;Failed to get conversation history: %v&#34;, err) history = []core.Message{} // Continue without history } // Enhance query with conversation context enhancedQuery, err := cra.queryEnhancer.EnhanceQuery(ctx, queryStr, history) if err != nil { log.Printf(&#34;Query enhancement failed: %v&#34;, err) enhancedQuery = queryStr // Fallback to original } // Retrieve relevant context retrievedContext, err := cra.retriever.Retrieve(ctx, enhancedQuery) if err != nil { return core.AgentResult{}, fmt.Errorf(&#34;retrieval failed: %w&#34;, err) } // Generate response with context response, err := cra.generator.Generate(ctx, queryStr, retrievedContext, history) if err != nil { return core.AgentResult{}, fmt.Errorf(&#34;generation failed: %w&#34;, err) } // Store interaction err = cra.storeInteraction(ctx, sessionID, queryStr, response, retrievedContext) if err != nil { log.Printf(&#34;Failed to store interaction: %v&#34;, err) } // Prepare result outputState := state.Clone() outputState.Set(&#34;response&#34;, response) outputState.Set(&#34;enhanced_query&#34;, enhancedQuery) outputState.Set(&#34;sources_count&#34;, len(retrievedContext)) outputState.Set(&#34;context_used&#34;, len(retrievedContext) &gt; 0) // Add source information sources := make([]string, 0, len(retrievedContext)) for _, result := range retrievedContext { if source, ok := result.Metadata[&#34;source&#34;]; ok { sources = append(sources, source) } } outputState.Set(&#34;sources&#34;, sources) return core.AgentResult{OutputState: outputState}, nil } func (cra *ComprehensiveRAGAgent) storeInteraction(ctx context.Context, sessionID, query, response string, context []core.MemoryResult) error { // Store user query err := cra.memory.Store(ctx, query, &#34;user-message&#34;, core.WithSession(sessionID), core.WithTimestamp(time.Now()), core.WithMetadata(map[string]string{ &#34;agent_type&#34;: &#34;comprehensive-rag&#34;, }), ) if err != nil { return err } // Store agent response with context metadata contextSources := make([]string, 0, len(context)) for _, result := range context { if source, ok := result.Metadata[&#34;source&#34;]; ok { contextSources = append(contextSources, source) } } err = cra.memory.Store(ctx, response, &#34;assistant-message&#34;, core.WithSession(sessionID), core.WithTimestamp(time.Now()), core.WithMetadata(map[string]string{ &#34;agent_type&#34;: &#34;comprehensive-rag&#34;, &#34;sources_used&#34;: strings.Join(contextSources, &#34;, &#34;), &#34;sources_count&#34;: fmt.Sprintf(&#34;%d&#34;, len(context)), &#34;context_length&#34;: fmt.Sprintf(&#34;%d&#34;, len(strings.Join(contextSources, &#34; &#34;))), }), ) return err }&#xD;RAG Performance Optimization 1. Caching Strategies type RAGCache struct { retrievalCache map[string][]core.MemoryResult responseCache map[string]string mu sync.RWMutex ttl time.Duration timestamps map[string]time.Time } func NewRAGCache(ttl time.Duration) *RAGCache { cache := &amp;RAGCache{ retrievalCache: make(map[string][]core.MemoryResult), responseCache: make(map[string]string), timestamps: make(map[string]time.Time), ttl: ttl, } // Start cleanup goroutine go cache.cleanup() return cache } func (rc *RAGCache) GetRetrievalResults(query string) ([]core.MemoryResult, bool) { rc.mu.RLock() defer rc.mu.RUnlock() if timestamp, exists := rc.timestamps[query]; exists { if time.Since(timestamp) &lt; rc.ttl { if results, exists := rc.retrievalCache[query]; exists { return results, true } } } return nil, false } func (rc *RAGCache) SetRetrievalResults(query string, results []core.MemoryResult) { rc.mu.Lock() defer rc.mu.Unlock() rc.retrievalCache[query] = results rc.timestamps[query] = time.Now() } func (rc *RAGCache) cleanup() { ticker := time.NewTicker(rc.ttl / 2) defer ticker.Stop() for range ticker.C { rc.mu.Lock() now := time.Now() for query, timestamp := range rc.timestamps { if now.Sub(timestamp) &gt; rc.ttl { delete(rc.retrievalCache, query) delete(rc.responseCache, query) delete(rc.timestamps, query) } } rc.mu.Unlock() } }&#xD;2. Batch Processing type BatchRAGProcessor struct { agent *ComprehensiveRAGAgent batchSize int timeout time.Duration } func NewBatchRAGProcessor(agent *ComprehensiveRAGAgent, batchSize int, timeout time.Duration) *BatchRAGProcessor { return &amp;BatchRAGProcessor{ agent: agent, batchSize: batchSize, timeout: timeout, } } func (brp *BatchRAGProcessor) ProcessBatch(ctx context.Context, queries []string, sessionID string) ([]string, error) { responses := make([]string, len(queries)) // Process in batches for i := 0; i &lt; len(queries); i += brp.batchSize { end := i + brp.batchSize if end &gt; len(queries) { end = len(queries) } batch := queries[i:end] batchResponses, err := brp.processBatch(ctx, batch, sessionID) if err != nil { return nil, fmt.Errorf(&#34;batch processing failed: %w&#34;, err) } copy(responses[i:], batchResponses) } return responses, nil } func (brp *BatchRAGProcessor) processBatch(ctx context.Context, queries []string, sessionID string) ([]string, error) { ctx, cancel := context.WithTimeout(ctx, brp.timeout) defer cancel() responses := make([]string, len(queries)) var wg sync.WaitGroup var mu sync.Mutex var firstError error for i, query := range queries { wg.Add(1) go func(index int, q string) { defer wg.Done() event := core.NewEvent( brp.agent.name, core.EventData{&#34;message&#34;: q}, map[string]string{&#34;session_id&#34;: sessionID}, ) state := core.NewState() state.Set(&#34;message&#34;, q) result, err := brp.agent.Run(ctx, event, state) mu.Lock() defer mu.Unlock() if err != nil &amp;&amp; firstError == nil { firstError = err } else if err == nil { if response, ok := result.OutputState.Get(&#34;response&#34;); ok { responses[index] = response.(string) } } }(i, query) } wg.Wait() if firstError != nil { return nil, firstError } return responses, nil }&#xD;RAG Evaluation and Monitoring 1. RAG Metrics type RAGMetrics struct { RetrievalLatency []time.Duration GenerationLatency []time.Duration RetrievalAccuracy float64 ResponseQuality float64 SourceUtilization map[string]int mu sync.RWMutex } func NewRAGMetrics() *RAGMetrics { return &amp;RAGMetrics{ SourceUtilization: make(map[string]int), } } func (rm *RAGMetrics) RecordRetrieval(latency time.Duration, resultsCount int, accuracy float64) { rm.mu.Lock() defer rm.mu.Unlock() rm.RetrievalLatency = append(rm.RetrievalLatency, latency) rm.RetrievalAccuracy = (rm.RetrievalAccuracy + accuracy) / 2 // Simple moving average } func (rm *RAGMetrics) RecordGeneration(latency time.Duration, quality float64) { rm.mu.Lock() defer rm.mu.Unlock() rm.GenerationLatency = append(rm.GenerationLatency, latency) rm.ResponseQuality = (rm.ResponseQuality + quality) / 2 } func (rm *RAGMetrics) RecordSourceUsage(sources []string) { rm.mu.Lock() defer rm.mu.Unlock() for _, source := range sources { rm.SourceUtilization[source]++ } } func (rm *RAGMetrics) GetAverageRetrievalLatency() time.Duration { rm.mu.RLock() defer rm.mu.RUnlock() if len(rm.RetrievalLatency) == 0 { return 0 } var total time.Duration for _, latency := range rm.RetrievalLatency { total += latency } return total / time.Duration(len(rm.RetrievalLatency)) }&#xD;2. Quality Assessment type RAGQualityAssessor struct { llm core.LLMProvider } func NewRAGQualityAssessor(llm core.LLMProvider) *RAGQualityAssessor { return &amp;RAGQualityAssessor{llm: llm} } func (rqa *RAGQualityAssessor) AssessResponse(ctx context.Context, query, response string, sources []core.MemoryResult) (float64, error) { // Build assessment prompt var sourcesBuilder strings.Builder for i, source := range sources { sourcesBuilder.WriteString(fmt.Sprintf(&#34;%d. %s\n&#34;, i+1, source.Content)) } prompt := fmt.Sprintf(`Please assess the quality of this RAG response on a scale of 0.0 to 1.0: Query: %s Sources used: %s Response: %s Assessment criteria: - Accuracy: Is the response factually correct based on the sources? - Relevance: Does the response directly address the query? - Completeness: Does the response provide sufficient information? - Coherence: Is the response well-structured and clear? Please provide only a numerical score between 0.0 and 1.0:`, query, sourcesBuilder.String(), response) scoreStr, err := rqa.llm.Generate(ctx, prompt) if err != nil { return 0.0, fmt.Errorf(&#34;quality assessment failed: %w&#34;, err) } // Parse score scoreStr = strings.TrimSpace(scoreStr) score, err := strconv.ParseFloat(scoreStr, 64) if err != nil { return 0.0, fmt.Errorf(&#34;failed to parse quality score: %w&#34;, err) } // Clamp score to valid range if score &lt; 0.0 { score = 0.0 } else if score &gt; 1.0 { score = 1.0 } return score, nil }&#xD;Best Practices 1. RAG System Design Chunk Size Optimization: Balance context and specificity Embedding Quality: Use appropriate embedding models for your domain Retrieval Tuning: Optimize similarity thresholds and result limits Context Management: Manage context length to avoid token limits Source Attribution: Always track and cite information sources 2. Performance Optimization Caching: Cache frequent queries and embeddings Batch Processing: Process multiple queries efficiently Index Optimization: Use appropriate vector database indexes Async Processing: Use asynchronous operations where possible 3. Quality Assurance Evaluation Metrics: Implement comprehensive evaluation Human Feedback: Collect and incorporate user feedback Continuous Monitoring: Monitor system performance and quality A/B Testing: Test different RAG configurations Conclusion RAG implementation in AgenticGoKit enables agents to provide accurate, contextual, and up-to-date responses by combining retrieval and generation. Key takeaways:</description>
    </item>
    <item>
      <title>vector-databases</title>
      <link>http://localhost:1313/AgenticGoKitDocs/tutorials/memory-systems/vector-databases/index.html</link>
      <pubDate>Fri, 25 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AgenticGoKitDocs/tutorials/memory-systems/vector-databases/index.html</guid>
      <description>Vector Databases in AgenticGoKit Overview Vector databases enable sophisticated similarity search and retrieval-augmented generation (RAG) by storing and querying high-dimensional embeddings. This tutorial covers setting up and using vector databases with AgenticGoKit, including pgvector and Weaviate integration.&#xA;Vector databases are essential for production-ready memory systems that need to handle large amounts of data with fast, semantically-aware search capabilities.&#xA;Prerequisites Understanding of Basic Memory Operations Knowledge of vector embeddings and similarity search Basic database administration skills Familiarity with Docker (for setup examples) Vector Database Concepts What are Vector Embeddings? Vector embeddings are numerical representations of text, images, or other data that capture semantic meaning in high-dimensional space:</description>
    </item>
  </channel>
</rss>