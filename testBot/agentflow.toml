# AgentFlow Configuration

[agent_flow]
name = "testBot"
version = "1.0.0"
description = "Configuration-driven multi-agent system"

# Global LLM configuration - can be overridden per agent
[llm]
provider = "ollama"
model = "smollm2:latest"
temperature = 0.7
max_tokens = 2000
timeout_seconds = 30

[logging]
level = "info"
format = "console"

[runtime]
max_concurrent_agents = 10
timeout_seconds = 30

[providers.azure]
# API key will be read from AZURE_OPENAI_API_KEY environment variable
# Endpoint will be read from AZURE_OPENAI_ENDPOINT environment variable
# Deployment will be read from AZURE_OPENAI_DEPLOYMENT environment variable

[providers.openai]
# API key will be read from OPENAI_API_KEY environment variable

[providers.ollama]
base_url = "http://localhost:11434"
model = "llama2"

[providers.mock]
# Mock provider for testing - no configuration needed

# Agent Definitions
# Each agent has its own configuration including role, capabilities, and LLM settings

[agents.agent1]
role = "agent1"
description = "Processes tasks in sequence as part of a processing pipeline"
system_prompt = "You are Agent1, Processes tasks in sequence as part of a processing pipeline You are the first agent in a sequential workflow. Your capabilities include: analysis, text_analysis, data_processing. Always provide helpful, accurate, and relevant responses."
capabilities = ["analysis", "text_analysis", "data_processing"]
enabled = true
auto_llm = true

# Agent-specific LLM settings (overrides global settings)
[agents.agent1.llm]
temperature = 0.7
max_tokens = 2000

# Retry policy for Agent1
[agents.agent1.retry_policy]
max_retries = 3
base_delay_ms = 1000
max_delay_ms = 10000
backoff_factor = 2.0

[agents.agent2]
role = "agent2"
description = "Processes tasks in sequence as part of a processing pipeline"
system_prompt = "You are Agent2, Processes tasks in sequence as part of a processing pipeline You are the final agent in a sequential workflow. Your capabilities include: analysis, text_analysis, data_processing. Always provide helpful, accurate, and relevant responses."
capabilities = ["analysis", "text_analysis", "data_processing"]
enabled = true
auto_llm = true

# Agent-specific LLM settings (overrides global settings)
[agents.agent2.llm]
temperature = 0.5
max_tokens = 2000

# Retry policy for Agent2
[agents.agent2.retry_policy]
max_retries = 3
base_delay_ms = 1000
max_delay_ms = 10000
backoff_factor = 2.0

[orchestration]
mode = "sequential"
timeout_seconds = 0
sequential_agents = ["agent1", "agent2"]
