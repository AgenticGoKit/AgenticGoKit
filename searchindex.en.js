var relearn_searchindex = [
  {
    "breadcrumb": "content \u003e tutorials \u003e advanced",
    "content": "Advanced Tutorials Production patterns and optimization techniques\nThis section covers advanced topics for building production-ready agent systems with AgenticGoKit. These tutorials assume familiarity with basic AgenticGoKit concepts.\nAdvanced Topics Circuit Breaker Patterns - Implement resilient agent systems Retry Policies - Handle failures gracefully Testing Strategies - Test multi-agent systems effectively Load Balancing and Scaling - Scale agents horizontally Prerequisites Before starting these tutorials, you should:\nComplete the Getting Started tutorials Understand Core Concepts Have experience with Go testing and production deployment Production Focus These tutorials emphasize:\nReliability: Building fault-tolerant systems Performance: Optimizing for speed and efficiency Scalability: Handling increased load Maintainability: Testing and monitoring strategies Next Steps After mastering these advanced concepts:\nExplore How-To Guides for specific implementation tasks Check API Reference for detailed interface documentation Consider Contributing to the AgenticGoKit project",
    "description": "Advanced Tutorials Production patterns and optimization techniques\nThis section covers advanced topics for building production-ready agent systems with AgenticGoKit. These tutorials assume familiarity with basic AgenticGoKit concepts.\nAdvanced Topics Circuit Breaker Patterns - Implement resilient agent systems Retry Policies - Handle failures gracefully Testing Strategies - Test multi-agent systems effectively Load Balancing and Scaling - Scale agents horizontally Prerequisites Before starting these tutorials, you should:\nComplete the Getting Started tutorials Understand Core Concepts Have experience with Go testing and production deployment Production Focus These tutorials emphasize:",
    "tags": [],
    "title": "advanced",
    "uri": "/AgenticGoKitDocs/tutorials/advanced/readme/index.html"
  },
  {
    "breadcrumb": "content",
    "content": "AgenticGoKit Documentation The Complete Guide to Building AI Agent Systems in Go\nAgenticGoKit is a production-ready Go framework for building intelligent agent workflows with dynamic tool integration, multi-provider LLM support, and enterprise-grade patterns.\n📚 Documentation Structure 🚀 Learning Paths New to AgenticGoKit? Follow these guided paths:\nBeginner Path (30 minutes) 5-Minute Quickstart - Get running immediately Your First Agent - Build a simple agent Multi-Agent Collaboration - Agents working together Intermediate Path (1 hour) Memory \u0026 RAG - Add knowledge capabilities Tool Integration - Connect external tools Core Concepts - Deep dive into fundamentals Advanced Path (2+ hours) Advanced Patterns - Complex orchestration patterns Production Deployment - Deploy to production Performance Optimization - Scale your systems Getting Started 5-Minute Quickstart - Get running immediately Your First Agent - Build a simple agent from scratch Multi-Agent Collaboration - Agents working together Memory \u0026 RAG - Add knowledge capabilities Tool Integration - Connect external tools Production Deployment - Deploy to production Core Concepts Agent Fundamentals - Understanding AgentHandler interface and patterns Memory \u0026 RAG - Persistent memory, vector search, and knowledge bases Multi-Agent Orchestration - Orchestration patterns and API reference Orchestration Configuration - Complete guide to configuration-based orchestration Examples \u0026 Tutorials - Practical examples and code samples Tool Integration - MCP protocol and dynamic tool discovery LLM Providers - Azure, OpenAI, Ollama, and custom providers Configuration - Managing agentflow.toml and environment setup Advanced Usage Advanced Patterns - Advanced orchestration patterns and configuration RAG Configuration - Retrieval-Augmented Generation setup and tuning Memory Provider Setup - PostgreSQL, Weaviate, and in-memory setup guides Workflow Visualization - Generate and customize Mermaid diagrams Production Deployment - Scaling, monitoring, and best practices Error Handling - Resilient agent workflows Custom Tools - Building your own MCP servers Performance Tuning - Optimization and benchmarking API Reference Core Package API - Complete public API reference Agent Interface - AgentHandler and related types Memory API - Memory system and RAG APIs MCP Integration - Tool discovery and execution APIs CLI Commands - agentcli reference 🔧 For AgenticGoKit Contributors Want to contribute to AgenticGoKit? See our Contributor Documentation for:\nDevelopment Setup - Getting started with the codebase Architecture Overview - Understanding the project structure Contribution Guidelines - Code style, testing, and documentation standards Development Workflow - How to submit changes and new features Quick Start Installation # Install the CLI go install github.com/kunalkushwaha/agenticgokit/cmd/agentcli@latest # Create a collaborative multi-agent system agentcli create research-system \\ --orchestration-mode collaborative \\ --agents 3 \\ --visualize \\ --mcp-enabled cd research-system # Set your API key export AZURE_OPENAI_API_KEY=your-key-here export AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/ export AZURE_OPENAI_DEPLOYMENT=your-deployment-name # Run with any message - agents work together intelligently go run . -m \"research AI trends and provide comprehensive analysis\"\rMulti-Agent Orchestration # Sequential processing pipeline agentcli create data-pipeline \\ --orchestration-mode sequential \\ --agents 3 \\ --orchestration-timeout 45 \\ --visualize # Loop-based workflow with conditions agentcli create quality-loop \\ --orchestration-mode loop \\ --agents 1 \\ --max-iterations 5 \\ --orchestration-timeout 120 \\ --visualize # Mixed collaborative + sequential workflow agentcli create complex-workflow \\ --orchestration-mode collaborative \\ --agents 4 \\ --orchestration-timeout 90 \\ --visualize-output \"docs/workflows\"\rConfiguration-Based Orchestration All generated projects use configuration-driven orchestration via agentflow.toml:\n[orchestration] mode = \"sequential\" # sequential, collaborative, loop, mixed, route timeout_seconds = 30 # Timeout for orchestration operations sequential_agents = [\"agent1\", \"agent2\", \"agent3\"]\rpackage main import ( \"context\" \"log\" \"time\" \"github.com/kunalkushwaha/agenticgokit/core\" ) func main() { // Load configuration from agentflow.toml config, err := core.LoadConfigFromWorkingDir() if err != nil { log.Fatal(err) } // Create provider from configuration provider, err := config.InitializeProvider() if err != nil { log.Fatal(err) } // Create agents agents := map[string]core.AgentHandler{ \"agent1\": core.NewLLMAgent(\"agent1\", provider), \"agent2\": core.NewLLMAgent(\"agent2\", provider), \"agent3\": core.NewLLMAgent(\"agent3\", provider), } // Create runner based on configuration var runner core.Runner switch config.Orchestration.Mode { case \"sequential\": runner = core.NewOrchestrationBuilder(core.OrchestrationSequential). WithAgents(agents). WithTimeout(time.Duration(config.Orchestration.TimeoutSeconds)*time.Second). Build() case \"collaborative\": runner = core.CreateCollaborativeRunner(agents, time.Duration(config.Orchestration.TimeoutSeconds)*time.Second) default: runner = core.CreateRouteRunner(agents) } // Start the runner ctx := context.Background() if err := runner.Start(ctx); err != nil { log.Fatal(err) } defer runner.Stop() }\rFirst Agent # Generate a single agent project agentcli create simple-agent --visualize # The generated agent1.go will look like this:\rpackage main import ( \"context\" \"fmt\" agentflow \"github.com/kunalkushwaha/agenticgokit/core\" ) type Agent1Handler struct { llm agentflow.ModelProvider mcpManager agentflow.MCPManager } func NewAgent1(llm agentflow.ModelProvider, mcp agentflow.MCPManager) *Agent1Handler { return \u0026Agent1Handler{llm: llm, mcpManager: mcp} } func (a *Agent1Handler) Run(ctx context.Context, event agentflow.Event, state agentflow.State) (agentflow.AgentResult, error) { // Extract user message message := event.GetData()[\"message\"] // Build prompt with available tools systemPrompt := \"You are a helpful assistant that uses tools when needed.\" toolPrompt := agentflow.FormatToolsForPrompt(ctx, a.mcpManager) fullPrompt := fmt.Sprintf(\"%s\\n%s\\nUser: %s\", systemPrompt, toolPrompt, message) // Get LLM response response, err := a.llm.Generate(ctx, fullPrompt) if err != nil { return agentflow.AgentResult{}, err } // Execute any tool calls toolResults := agentflow.ParseAndExecuteToolCalls(ctx, a.mcpManager, response) if len(toolResults) \u003e 0 { // Synthesize tool results with response finalPrompt := fmt.Sprintf(\"Response: %s\\nTool Results: %v\\nProvide final answer:\", response, toolResults) response, _ = a.llm.Generate(ctx, finalPrompt) } // Return result state.Set(\"response\", response) return agentflow.AgentResult{Result: response, State: state}, nil }\rMulti-Agent # Generate a collaborative multi-agent workflow agentcli create research-system \\ --orchestration-mode collaborative \\ --collaborative-agents \"researcher,analyzer,validator\" \\ --visualize # This creates: # - researcher.go (Research agent - gathers information) # - analyzer.go (Analysis agent - processes data) # - validator.go (Validation agent - ensures quality) # - main.go (Collaborative orchestration) # - workflow.mmd (Mermaid diagram)\rCollaborative Orchestration Code:\npackage main import ( \"context\" \"fmt\" \"log\" \"time\" \"github.com/kunalkushwaha/agenticgokit/core\" ) func main() { // Initialize agents agents := map[string]core.AgentHandler{ \"researcher\": NewResearcher(), \"analyzer\": NewAnalyzer(), \"validator\": NewValidator(), } // Create collaborative orchestration runner := core.NewOrchestrationBuilder(core.OrchestrationCollaborate). WithAgents(agents). WithTimeout(2 * time.Minute). WithFailureThreshold(0.8). WithMaxConcurrency(10). Build() // Create event event := core.NewEvent(\"all\", map[string]interface{}{ \"task\": \"research AI trends and provide comprehensive analysis\", }, nil) // All agents process the event in parallel result, err := runner.Run(context.Background(), event) if err != nil { log.Fatal(err) } fmt.Printf(\"Collaborative Result: %s\\n\", result.GetResult()) }\rWhy AgenticGoKit? For Users: ⚡ Fast Setup: Working agents in 5 minutes with CLI scaffolding 🔧 Tool-Rich: Dynamic tool discovery via MCP protocol 🌐 Provider Agnostic: Works with any LLM (Azure, OpenAI, Ollama) 🏗️ Production Ready: Built-in error handling, monitoring, scaling patterns For Contributors: 🎯 Clear Architecture: Separation between core (public API) and internal (implementation) 📝 Documentation First: Every feature documented with examples 🧪 Test Coverage: Comprehensive unit and integration tests 🔄 Continuous Integration: Automated testing and release workflows 📖 Documentation Sections 📚 Tutorials Learning-oriented guides to help you understand AgenticGoKit:\nGetting Started - Step-by-step beginner tutorials Core Concepts - Fundamental concepts and patterns Memory Systems - RAG and knowledge management MCP Tools - Tool integration and development Advanced Patterns - Complex orchestration patterns Debugging - Debugging and troubleshooting 🛠️ How-To Guides Task-oriented guides for specific scenarios:\nSetup - Configuration and environment setup Development - Development patterns and best practices Deployment - Production deployment and scaling 📋 Reference Information-oriented documentation:\nAPI Reference - Complete API documentation CLI Reference - Command-line interface documentation Configuration Reference - Configuration options 👥 Contributors For developers contributing to AgenticGoKit:\nContributor Guide - Development setup and workflow Code Style - Coding standards and conventions Testing - Testing strategies and guidelines Contributing We welcome contributions! See our Contributor Guide for details.\n# Quick start for contributors git clone https://github.com/kunalkushwaha/agenticgokit.git cd agenticgokit go mod tidy go test ./...\rCommunity GitHub Discussions - Q\u0026A and community Issues - Bug reports and feature requests Contributing - How to contribute code and documentation ⭐ Star us on GitHub | 📖 Full Documentation | 🚀 Examples",
    "description": "AgenticGoKit Documentation The Complete Guide to Building AI Agent Systems in Go\nAgenticGoKit is a production-ready Go framework for building intelligent agent workflows with dynamic tool integration, multi-provider LLM support, and enterprise-grade patterns.\n📚 Documentation Structure 🚀 Learning Paths New to AgenticGoKit? Follow these guided paths:\nBeginner Path (30 minutes) 5-Minute Quickstart - Get running immediately Your First Agent - Build a simple agent Multi-Agent Collaboration - Agents working together Intermediate Path (1 hour) Memory \u0026 RAG - Add knowledge capabilities Tool Integration - Connect external tools Core Concepts - Deep dive into fundamentals Advanced Path (2+ hours) Advanced Patterns - Complex orchestration patterns Production Deployment - Deploy to production Performance Optimization - Scale your systems Getting Started 5-Minute Quickstart - Get running immediately Your First Agent - Build a simple agent from scratch Multi-Agent Collaboration - Agents working together Memory \u0026 RAG - Add knowledge capabilities Tool Integration - Connect external tools Production Deployment - Deploy to production Core Concepts Agent Fundamentals - Understanding AgentHandler interface and patterns Memory \u0026 RAG - Persistent memory, vector search, and knowledge bases Multi-Agent Orchestration - Orchestration patterns and API reference Orchestration Configuration - Complete guide to configuration-based orchestration Examples \u0026 Tutorials - Practical examples and code samples Tool Integration - MCP protocol and dynamic tool discovery LLM Providers - Azure, OpenAI, Ollama, and custom providers Configuration - Managing agentflow.toml and environment setup Advanced Usage Advanced Patterns - Advanced orchestration patterns and configuration RAG Configuration - Retrieval-Augmented Generation setup and tuning Memory Provider Setup - PostgreSQL, Weaviate, and in-memory setup guides Workflow Visualization - Generate and customize Mermaid diagrams Production Deployment - Scaling, monitoring, and best practices Error Handling - Resilient agent workflows Custom Tools - Building your own MCP servers Performance Tuning - Optimization and benchmarking API Reference Core Package API - Complete public API reference Agent Interface - AgentHandler and related types Memory API - Memory system and RAG APIs MCP Integration - Tool discovery and execution APIs CLI Commands - agentcli reference 🔧 For AgenticGoKit Contributors Want to contribute to AgenticGoKit? See our Contributor Documentation for:",
    "tags": [],
    "title": "content",
    "uri": "/AgenticGoKitDocs/readme/index.html"
  },
  {
    "breadcrumb": "content \u003e contributors",
    "content": "Contributor Documentation Navigation: Documentation Home → Contributors\nFor developers contributing to AgenticGoKit\nThis section contains documentation specifically for contributors to the AgenticGoKit project. If you’re looking to use AgenticGoKit in your projects, see the main documentation.\n🚀 Getting Started Essential Reading Contributor Guide - Start here! Development setup and contribution workflow Core vs Internal - Understanding the public API vs implementation details Code Style - Go standards and project conventions Development Process Adding Features - How to extend AgenticGoKit with new features Testing Strategy - Unit tests, integration tests, and benchmarks Documentation Standards - Writing user-focused documentation Project Management Release Process - How releases are managed and versioned 🏗️ Architecture Overview AgenticGoKit is designed with a clear separation between public APIs and internal implementation:\ncore/ package: Public API that users interact with internal/ package: Implementation details not exposed to users cmd/ package: CLI tools and utilities examples/ directory: Working examples and tutorials 🧪 Development Workflow Fork and Clone: Fork the repository and clone your fork Create Branch: Create a feature branch for your changes Develop: Make your changes following our code style Test: Run tests and add new tests for your changes Document: Update documentation as needed Submit PR: Create a pull request with a clear description 📋 Contribution Guidelines Code Quality Follow Go best practices and our Code Style Write comprehensive tests for new features Ensure all tests pass before submitting Use meaningful commit messages Documentation Update user documentation for new features Follow our Documentation Standards Include code examples in documentation Keep contributor docs up to date Communication Use GitHub Issues for bug reports and feature requests Use GitHub Discussions for questions and community interaction Be respectful and constructive in all interactions 🔧 Development Tools Required Tools # Go (1.21+) go version # Linting go install github.com/golangci/golangci-lint/cmd/golangci-lint@latest # Testing go test ./... # Documentation generation (if applicable) go run tools/docgen/main.go\rRecommended IDE Setup VS Code with Go extension GoLand by JetBrains Vim/Neovim with Go plugins 📚 Additional Resources GitHub Repository GitHub Issues GitHub Discussions Main Documentation - For users of AgenticGoKit Thank you for contributing to AgenticGoKit! 🎉",
    "description": "Contributor Documentation Navigation: Documentation Home → Contributors\nFor developers contributing to AgenticGoKit\nThis section contains documentation specifically for contributors to the AgenticGoKit project. If you’re looking to use AgenticGoKit in your projects, see the main documentation.\n🚀 Getting Started Essential Reading Contributor Guide - Start here! Development setup and contribution workflow Core vs Internal - Understanding the public API vs implementation details Code Style - Go standards and project conventions Development Process Adding Features - How to extend AgenticGoKit with new features Testing Strategy - Unit tests, integration tests, and benchmarks Documentation Standards - Writing user-focused documentation Project Management Release Process - How releases are managed and versioned 🏗️ Architecture Overview AgenticGoKit is designed with a clear separation between public APIs and internal implementation:",
    "tags": [],
    "title": "contributors",
    "uri": "/AgenticGoKitDocs/contributors/readme/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials \u003e core-concepts",
    "content": "Core Concepts Overview Navigation: Documentation Home → Tutorials → Core Concepts\nUnderstanding AgenticGoKit’s core concepts is essential for building effective multi-agent systems. This section covers the fundamental building blocks that power the framework.\nThe Big Picture AgenticGoKit is built around a few key concepts that work together to create a powerful multi-agent system:\ngraph TB\rEvent[Event] --\u003e Runner[Runner]\rRunner --\u003e Orchestrator[Orchestrator]\rOrchestrator --\u003e Agent[Agent]\rAgent --\u003e State[State]\rState --\u003e Memory[Memory]\rsubgraph \"Core Flow\"\rEvent --\u003e State\rState --\u003e Agent\rAgent --\u003e State\rend\rsubgraph \"Orchestration\"\rOrchestrator --\u003e AgentA[Agent A]\rOrchestrator --\u003e AgentB[Agent B]\rOrchestrator --\u003e AgentC[Agent C]\rend\rKey Components 1. Events - The Message System Events are the messages that flow through your agent system. They carry data, metadata, and routing information.\n// Create an event event := core.NewEvent(\"target-agent\", core.EventData{\"message\": \"Hello, world!\"}, map[string]string{\"priority\": \"high\"}) // Events have IDs, timestamps, and routing info fmt.Println(\"Event ID:\", event.GetID()) fmt.Println(\"Target:\", event.GetTargetAgentID())\r2. State - The Data Container State objects carry data between agents and persist information across interactions.\n// Create and manipulate state state := core.NewState() state.Set(\"user_input\", \"What's the weather like?\") state.SetMeta(\"session_id\", \"user-123\") // State is thread-safe and can be cloned/merged clonedState := state.Clone()\r3. Agents - The Processing Units Agents are the core processing units that transform input state into output state.\n// Agents implement a simple interface type Agent interface { Run(ctx context.Context, inputState State) (State, error) Name() string } // Create a simple agent agent := core.NewLLMAgent(\"assistant\", llmProvider) agentResult, err := agent.Run(ctx, event, state)\r4. Runner - The Event Processor The Runner manages the event processing loop, routing events to the appropriate agents.\n// Create and start a runner runner := core.NewRunner(100) // queue size runner.RegisterAgent(\"assistant\", agentHandler) runner.Start(ctx) // Emit events for processing runner.Emit(event)\r5. Orchestrator - The Coordination Engine Orchestrators determine how events are distributed to agents (single, parallel, sequential, etc.).\n// Different orchestration modes collaborativeRunner := core.CreateCollaborativeRunner(agents, 30*time.Second) sequentialRunner := core.NewOrchestrationBuilder(core.OrchestrationSequential). WithAgents(agents). Build()\rData Flow Architecture Understanding how data flows through the system is crucial:\nsequenceDiagram\rparticipant Client\rparticipant Runner\rparticipant Orchestrator\rparticipant Agent\rparticipant Memory\rClient-\u003e\u003eRunner: Emit Event\rRunner-\u003e\u003eOrchestrator: Dispatch Event\rOrchestrator-\u003e\u003eAgent: Run(ctx, event, state)\rAgent-\u003e\u003eMemory: Store/Retrieve Data\rMemory--\u003e\u003eAgent: Return Results\rAgent--\u003e\u003eOrchestrator: Return AgentResult\rOrchestrator--\u003e\u003eRunner: Return Result\rRunner-\u003e\u003eRunner: Process Result (emit new events)\rCore Patterns 1. Simple Agent Execution package main import ( \"context\" \"fmt\" \"log\" \"github.com/kunalkushwaha/agentflow/core\" ) // Simple agent implementation type SimpleAgent struct { name string llm core.ModelProvider } func (a *SimpleAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Get message from event message, ok := event.GetData()[\"message\"].(string) if !ok { return core.AgentResult{}, fmt.Errorf(\"no message found\") } // Create prompt prompt := core.Prompt{ System: \"You are a helpful assistant.\", User: message, } // Call LLM response, err := a.llm.Call(ctx, prompt) if err != nil { return core.AgentResult{}, err } // Create output state outputState := core.NewState() outputState.Set(\"response\", response.Content) return core.AgentResult{OutputState: outputState}, nil } func main() { // Create LLM provider from configuration provider, err := core.NewProviderFromWorkingDir() if err != nil { log.Fatal(err) } // Create agent agent := \u0026SimpleAgent{ name: \"assistant\", llm: provider, } // Create state with input state := core.NewState() state.Set(\"message\", \"Hello, world!\") // Create event event := core.NewEvent(\"assistant\", core.EventData{ \"message\": \"Hello, world!\", }, nil) // Run agent result, err := agent.Run(context.Background(), event, state) if err != nil { log.Fatal(err) } // Get response from output state if response, ok := result.OutputState.Get(\"response\"); ok { fmt.Println(response) } }\r2. Event-Driven Processing func main() { // Create agents agents := map[string]core.AgentHandler{ \"processor\": \u0026ProcessorAgent{}, \"responder\": \u0026ResponderAgent{}, } // Create collaborative runner runner := core.CreateCollaborativeRunner(agents, 30*time.Second) }) // Start processing runner.Start(context.Background()) // Emit event event := core.NewEvent(\"processor\", core.EventData{\"task\": \"analyze data\"}, nil) runner.Emit(event) }\r3. Multi-Agent Collaboration func main() { agents := map[string]core.AgentHandler{ \"researcher\": \u0026ResearchAgent{}, \"analyzer\": \u0026AnalysisAgent{}, \"writer\": \u0026WritingAgent{}, } // All agents work on the same input runner := core.CreateCollaborativeRunner(agents, 60*time.Second) ctx := context.Background() runner.Start(ctx) defer runner.Stop() event := core.NewEvent(\"researcher\", // Start with researcher core.EventData{\"topic\": \"AI trends\"}, map[string]string{\"route\": \"researcher\"}) err := runner.Emit(event) if err != nil { log.Fatal(err) } // Wait for processing time.Sleep(10 * time.Second) }\rMemory Integration AgenticGoKit provides powerful memory capabilities for persistent storage and RAG:\n// Configure memory memoryConfig := core.AgentMemoryConfig{ Provider: \"pgvector\", Connection: \"postgres://user:pass@localhost/db\", EnableRAG: true, } memory, err := core.NewMemory(memoryConfig) // Store information memory.Store(ctx, core.MemoryItem{ Content: \"Important information\", Tags: []string{\"important\", \"user-data\"}, }) // Search with RAG results, err := memory.Search(ctx, \"find important information\")\rError Handling Patterns AgenticGoKit provides sophisticated error handling and recovery:\n// Configure error routing errorConfig := core.DefaultErrorRouterConfig() errorConfig.MaxRetries = 3 errorConfig.BackoffFactor = 2.0 runner := core.NewRunnerWithConfig(core.RunnerConfig{ ErrorRouterConfig: errorConfig, Agents: map[string]core.AgentHandler{ \"main-agent\": mainAgent, \"error-handler\": errorHandlerAgent, }, })\rNext Steps Now that you understand the core concepts, dive deeper into specific areas:\nMessage Passing - Learn how events flow through the system State Management - Master data handling between agents Agent Lifecycle - Understand agent creation and execution Error Handling - Build robust error management Or jump to specific orchestration patterns:\nOrchestration Overview - Learn about different orchestration modes Key Takeaways Events carry messages and data through the system State objects persist data between agent interactions Agents are the core processing units that transform state Runners manage the event processing loop Orchestrators coordinate how agents work together Memory provides persistent storage and RAG capabilities The system is designed for scalability, fault tolerance, and flexibility Understanding these concepts will help you build more effective and maintainable multi-agent systems with AgenticGoKit.",
    "description": "Core Concepts Overview Navigation: Documentation Home → Tutorials → Core Concepts\nUnderstanding AgenticGoKit’s core concepts is essential for building effective multi-agent systems. This section covers the fundamental building blocks that power the framework.\nThe Big Picture AgenticGoKit is built around a few key concepts that work together to create a powerful multi-agent system:\ngraph TB\rEvent[Event] --\u003e Runner[Runner]\rRunner --\u003e Orchestrator[Orchestrator]\rOrchestrator --\u003e Agent[Agent]\rAgent --\u003e State[State]\rState --\u003e Memory[Memory]\rsubgraph \"Core Flow\"\rEvent --\u003e State\rState --\u003e Agent\rAgent --\u003e State\rend\rsubgraph \"Orchestration\"\rOrchestrator --\u003e AgentA[Agent A]\rOrchestrator --\u003e AgentB[Agent B]\rOrchestrator --\u003e AgentC[Agent C]\rend\rKey Components 1. Events - The Message System Events are the messages that flow through your agent system. They carry data, metadata, and routing information.",
    "tags": [],
    "title": "core-concepts",
    "uri": "/AgenticGoKitDocs/tutorials/core-concepts/readme/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials \u003e debugging",
    "content": "Debugging and Monitoring in AgenticGoKit Overview This section covers comprehensive debugging and monitoring strategies for AgenticGoKit applications. You’ll learn how to troubleshoot multi-agent systems, implement effective logging, set up monitoring, and optimize performance in production environments.\nDebugging multi-agent systems presents unique challenges due to their distributed nature, asynchronous execution, and complex interaction patterns. This guide provides practical tools and techniques to help you identify, diagnose, and resolve issues effectively.\nWhat You’ll Learn Debugging Multi-Agent Systems: Techniques for troubleshooting complex agent interactions Logging and Tracing: Implementing structured logging and distributed tracing Performance Monitoring: Setting up metrics, alerts, and performance optimization Production Troubleshooting: Common issues and their solutions in production environments Prerequisites Before diving into debugging and monitoring, you should be familiar with:\nAgent Lifecycle Message Passing and Event Flow Orchestration Patterns State Management Why Debugging Multi-Agent Systems is Different Multi-agent systems introduce several debugging challenges:\n1. Concurrent Execution Agents may run on different threads within the same process State changes occur across multiple components simultaneously Race conditions and timing issues are more common with concurrent execution 2. Asynchronous Communication Events flow through the system asynchronously Cause and effect relationships may be separated in time Error propagation can be complex and delayed 3. Complex State Management State is shared and modified by multiple agents State transformations may be non-deterministic Debugging requires understanding the entire state flow 4. Orchestration Complexity Different orchestration patterns have different failure modes Agent interactions can create emergent behaviors System behavior may vary based on timing and load Debugging Philosophy Effective debugging of multi-agent systems requires a systematic approach:\n1. Observability First Implement comprehensive logging before you need it Use structured logging with consistent formats Include correlation IDs to track requests across agents 2. Fail Fast and Clearly Validate inputs and state at agent boundaries Use clear, actionable error messages Implement circuit breakers to prevent cascade failures 3. Isolate and Reproduce Create minimal test cases that reproduce issues Use deterministic testing with controlled inputs Implement agent mocking for isolated testing 4. Monitor Continuously Set up metrics and alerts for key system behaviors Track performance trends over time Use health checks to detect issues early Getting Started Start with Debugging Multi-Agent Systems to learn fundamental debugging techniques, then progress through the other guides based on your specific needs.\nQuick Reference Common Debugging Commands # List available trace sessions agentcli list # View execution trace for a session agentcli trace \u003csession-id\u003e # View only agent flow without state details agentcli trace --flow-only \u003csession-id\u003e # Filter trace to specific agent agentcli trace --filter agent=\u003cagent-name\u003e \u003csession-id\u003e # View verbose trace with full state details agentcli trace --verbose \u003csession-id\u003e # Debug trace structure agentcli trace --debug \u003csession-id\u003e # Check MCP servers and tools agentcli mcp servers agentcli mcp tools # View memory system status agentcli memory --stats # Check cache statistics agentcli cache stats\rConfiguration in agentflow.toml [logging] level = \"debug\" # debug, info, warn, error format = \"json\" # json or text [agent_flow] name = \"my-agent-system\" version = \"1.0.0\" # Enable tracing (traces are saved as .trace.json files) [runtime] max_concurrent_agents = 10\rKey Metrics to Monitor Agent Response Time: Time from event receipt to result Error Rate: Percentage of failed agent executions Queue Depth: Number of pending events in orchestrator Memory Usage: Memory consumption per agent Tool Call Latency: Time for external tool executions",
    "description": "Debugging and Monitoring in AgenticGoKit Overview This section covers comprehensive debugging and monitoring strategies for AgenticGoKit applications. You’ll learn how to troubleshoot multi-agent systems, implement effective logging, set up monitoring, and optimize performance in production environments.\nDebugging multi-agent systems presents unique challenges due to their distributed nature, asynchronous execution, and complex interaction patterns. This guide provides practical tools and techniques to help you identify, diagnose, and resolve issues effectively.\nWhat You’ll Learn Debugging Multi-Agent Systems: Techniques for troubleshooting complex agent interactions Logging and Tracing: Implementing structured logging and distributed tracing Performance Monitoring: Setting up metrics, alerts, and performance optimization Production Troubleshooting: Common issues and their solutions in production environments Prerequisites Before diving into debugging and monitoring, you should be familiar with:",
    "tags": [],
    "title": "debugging",
    "uri": "/AgenticGoKitDocs/tutorials/debugging/readme/index.html"
  },
  {
    "breadcrumb": "content \u003e guides \u003e deployment",
    "content": "Deployment \u0026 Operations Guides Guides for deploying and operating AgenticGoKit applications in production.\nAvailable Guides Docker Deployment Containerize your AgenticGoKit applications with Docker, including multi-stage builds and optimization techniques.\nWhen to use: Deploying agents in containerized environments or cloud platforms.\nMonitoring Set up comprehensive monitoring for agent performance, including metrics, logging, and alerting.\nWhen to use: Running agents in production and need visibility into system health and performance.\nScaling Scale AgenticGoKit applications horizontally, including load balancing and distributed deployment patterns.\nWhen to use: Handling increased load or building high-availability agent systems.\nDeployment Patterns Common deployment patterns:\nSingle Instance Simple applications with low traffic Development and testing environments Proof-of-concept deployments Load Balanced Multiple instances behind a load balancer Horizontal scaling for increased throughput High availability with failover Microservices Agents deployed as separate services Independent scaling and deployment Service mesh integration Production Checklist Before deploying to production:\nConfiguration management - Externalized configuration Security - Proper authentication and authorization Monitoring - Metrics, logging, and alerting set up Backup and recovery - Data backup and disaster recovery plans Performance testing - Load testing and capacity planning Documentation - Operational runbooks and procedures Infrastructure Requirements Typical production requirements:\nCompute - CPU and memory based on agent complexity Storage - Persistent storage for state and memory systems Network - Reliable network connectivity for LLM APIs Monitoring - Observability infrastructure Next Steps For production deployment:\nStart with Docker containerization Add monitoring for observability Scale with appropriate patterns as needed Reference Best Practices for operational excellence",
    "description": "Deployment \u0026 Operations Guides Guides for deploying and operating AgenticGoKit applications in production.\nAvailable Guides Docker Deployment Containerize your AgenticGoKit applications with Docker, including multi-stage builds and optimization techniques.\nWhen to use: Deploying agents in containerized environments or cloud platforms.\nMonitoring Set up comprehensive monitoring for agent performance, including metrics, logging, and alerting.\nWhen to use: Running agents in production and need visibility into system health and performance.\nScaling Scale AgenticGoKit applications horizontally, including load balancing and distributed deployment patterns.",
    "tags": [],
    "title": "deployment",
    "uri": "/AgenticGoKitDocs/guides/deployment/readme/index.html"
  },
  {
    "breadcrumb": "content \u003e guides \u003e development",
    "content": "Development Guides Guides for developing, testing, and debugging AgenticGoKit applications.\nAvailable Guides Testing Agents Comprehensive guide to testing multi-agent systems, including unit tests, integration tests, and end-to-end testing strategies.\nWhen to use: Building reliable agent systems that need thorough testing coverage.\nDebugging Debug agent interactions, trace execution flows, and troubleshoot common issues in multi-agent systems.\nWhen to use: When agents aren’t behaving as expected or you need to understand execution flow.\nBest Practices Development best practices for building maintainable, scalable, and robust agent systems.\nWhen to use: Starting a new project or improving existing agent implementations.\nDevelopment Workflow Recommended development workflow:\nDesign your agents - Plan agent responsibilities and interactions Implement incrementally - Start with simple agents, add complexity gradually Test continuously - Use Testing Agents strategies Debug systematically - Apply Debugging techniques Follow best practices - Reference Best Practices Development Tools AgenticGoKit provides several tools to help with development:\nagentcli - Command-line tool for tracing and debugging Configuration validation - Built-in config validation Logging and tracing - Comprehensive observability features Code Quality All guides emphasize:\nError handling - Proper error management patterns Testing - Comprehensive test coverage Documentation - Clear code documentation Performance - Efficient resource usage Next Steps After development:\nDeployment Guides for production deployment Setup Guides for configuration management API Reference for detailed documentation",
    "description": "Development Guides Guides for developing, testing, and debugging AgenticGoKit applications.\nAvailable Guides Testing Agents Comprehensive guide to testing multi-agent systems, including unit tests, integration tests, and end-to-end testing strategies.\nWhen to use: Building reliable agent systems that need thorough testing coverage.\nDebugging Debug agent interactions, trace execution flows, and troubleshoot common issues in multi-agent systems.\nWhen to use: When agents aren’t behaving as expected or you need to understand execution flow.",
    "tags": [],
    "title": "development",
    "uri": "/AgenticGoKitDocs/guides/development/readme/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials \u003e getting-started",
    "content": "Getting Started Tutorials Beginner-friendly tutorials to get you up and running with AgenticGoKit quickly.\nTutorial Series Follow these tutorials in order for the best learning experience:\n0. 5-Minute Quickstart Time: 5 minutes\nGet a multi-agent system running immediately with minimal setup.\nYou’ll learn:\nFastest way to create agents Basic orchestration patterns CLI vs code-first approaches 1. Your First Agent Time: 10 minutes\nCreate your first AgenticGoKit agent and understand the basic concepts.\nYou’ll learn:\nHow to set up a basic agent Understanding the Agent interface Running your first agent interaction 2. Multi-Agent Collaboration Time: 15 minutes\nLearn how multiple agents can work together using orchestration patterns.\nYou’ll learn:\nSequential and collaborative orchestration Agent communication patterns State management between agents 3. Memory and RAG Time: 20 minutes\nAdd memory capabilities and implement Retrieval-Augmented Generation (RAG).\nYou’ll learn:\nSetting up vector databases Document ingestion and retrieval Building knowledge-aware agents 4. Tool Integration Time: 15 minutes\nConnect your agents to external tools using the Model Context Protocol (MCP).\nYou’ll learn:\nMCP tool integration Custom tool development Tool caching and optimization 5. Production Deployment Time: 25 minutes\nDeploy your agents to production with proper monitoring and scaling.\nYou’ll learn:\nDocker containerization Configuration management Monitoring and logging Basic scaling patterns Prerequisites Go 1.21 or later installed Basic familiarity with Go programming Docker installed (for later tutorials) Next Steps After completing these tutorials, explore:\nCore Concepts for deeper understanding How-To Guides for specific tasks Advanced Tutorials for complex scenarios",
    "description": "Getting Started Tutorials Beginner-friendly tutorials to get you up and running with AgenticGoKit quickly.\nTutorial Series Follow these tutorials in order for the best learning experience:\n0. 5-Minute Quickstart Time: 5 minutes\nGet a multi-agent system running immediately with minimal setup.\nYou’ll learn:\nFastest way to create agents Basic orchestration patterns CLI vs code-first approaches 1. Your First Agent Time: 10 minutes\nCreate your first AgenticGoKit agent and understand the basic concepts.",
    "tags": [],
    "title": "getting-started",
    "uri": "/AgenticGoKitDocs/tutorials/getting-started/readme/index.html"
  },
  {
    "breadcrumb": "content \u003e guides",
    "content": "How-To Guides Navigation: Documentation Home → Guides\nTask-oriented guides for specific development scenarios and common tasks.\nQuick Navigation Setup \u0026 Configuration LLM Providers - Configure different LLM providers Vector Databases - Set up vector storage for RAG MCP Tools - Integrate Model Context Protocol tools Development Testing Agents - Test multi-agent systems Debugging - Debug agent interactions Best Practices - Development best practices Web Search Integration - Add web search capabilities to agents Research Assistant - Build multi-agent research systems Visualization - Generate workflow diagrams with Mermaid Framework \u0026 Tools Framework Comparison - Compare AgenticGoKit with other frameworks Troubleshooting - Common issues and solutions Deployment \u0026 Operations Production Deployment - Deploy and scale your agents Guide Structure Each guide follows this format:\nProblem statement - What you’re trying to accomplish Solution overview - High-level approach Step-by-step instructions - Detailed implementation Code examples - Working code snippets Troubleshooting - Common issues and solutions Related guides - Links to related content When to Use Guides vs Tutorials Use Tutorials when you want to learn concepts step-by-step Use Guides when you need to solve a specific problem or implement a particular feature Getting Help If you can’t find what you’re looking for:\nCheck the API Reference for detailed function documentation Review Troubleshooting for common issues Browse Tutorials for learning-oriented content",
    "description": "How-To Guides Navigation: Documentation Home → Guides\nTask-oriented guides for specific development scenarios and common tasks.\nQuick Navigation Setup \u0026 Configuration LLM Providers - Configure different LLM providers Vector Databases - Set up vector storage for RAG MCP Tools - Integrate Model Context Protocol tools Development Testing Agents - Test multi-agent systems Debugging - Debug agent interactions Best Practices - Development best practices Web Search Integration - Add web search capabilities to agents Research Assistant - Build multi-agent research systems Visualization - Generate workflow diagrams with Mermaid Framework \u0026 Tools Framework Comparison - Compare AgenticGoKit with other frameworks Troubleshooting - Common issues and solutions Deployment \u0026 Operations Production Deployment - Deploy and scale your agents Guide Structure Each guide follows this format:",
    "tags": [],
    "title": "guides",
    "uri": "/AgenticGoKitDocs/guides/readme/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials \u003e mcp",
    "content": "Model Context Protocol (MCP) in AgenticGoKit Navigation: Documentation Home → Tutorials → MCP (Tools)\nOverview The Model Context Protocol (MCP) is a powerful framework within AgenticGoKit that enables agents to interact with external tools, APIs, and services. MCP bridges the gap between language models and the outside world, allowing agents to perform actions beyond text generation.\nWith MCP, agents can search the web, access databases, call APIs, manipulate files, perform calculations, and much more. This capability transforms agents from simple text processors into powerful assistants that can take meaningful actions.\nKey Concepts What is MCP? MCP (Model Context Protocol) is a standardized interface for connecting language models to external tools and capabilities. It defines:\nTool Registration: How tools are defined and registered with the system Tool Discovery: How agents discover available tools Tool Invocation: How agents call tools and receive results Tool Response Handling: How tool results are processed and incorporated into agent responses MCP Architecture ┌─────────────┐ ┌───────────────┐ ┌─────────────┐\r│ │ │ │ │ │\r│ Agent │────▶│ MCP Manager │────▶│ Tool │\r│ │ │ │ │ │\r└─────────────┘ └───────────────┘ └─────────────┘\r▲ │ │\r│ ▼ ▼\r│ ┌───────────────┐ ┌─────────────┐\r└──────────────│ Tool Result │◀───│ External │\r│ Processor │ │ Service │\r└───────────────┘ └─────────────┘\rTool Types AgenticGoKit supports various types of tools:\nBuilt-in Tools: Core functionality provided by the framework Custom Tools: User-defined tools for specific use cases API Tools: Wrappers around external APIs and services Stateful Tools: Tools that maintain state between invocations Composite Tools: Tools composed of multiple sub-tools Why Use MCP? MCP provides several key benefits:\nExtended Capabilities: Enables agents to perform actions beyond text generation Modularity: Tools can be developed and maintained independently Flexibility: Mix and match tools based on specific requirements Standardization: Consistent interface for all tool interactions Security: Controlled access to external systems MCP vs. Function Calling MCP is similar to function calling in LLMs but provides additional capabilities:\nFeature MCP Function Calling Tool Discovery Dynamic Static Tool Registration Runtime Design time Tool Composition Supported Limited State Management Built-in Manual Error Handling Comprehensive Basic Security Controls Fine-grained Limited Getting Started with MCP To start using MCP in AgenticGoKit, you’ll need to:\nCreate Tools: Define the tools your agents will use Register Tools: Make tools available to the MCP manager Configure Agents: Set up agents to use MCP Handle Tool Results: Process and incorporate tool outputs The following tutorials will guide you through these steps in detail:\nTool Development - Creating custom tools Tool Integration - Integrating tools with agents Advanced Tool Patterns - Complex tool usage patterns Example: Simple MCP Setup package main import ( \"context\" \"fmt\" \"log\" \"os\" \"time\" \"github.com/kunalkushwaha/agenticgokit/core\" ) func main() { // Initialize MCP for tool discovery core.QuickStartMCP() // Create LLM provider provider, err := core.NewOpenAIAdapter( os.Getenv(\"OPENAI_API_KEY\"), \"gpt-3.5-turbo\", 1000, 0.7, ) if err != nil { log.Fatalf(\"Failed to create provider: %v\", err) } // Create agent with MCP tools agent := core.NewLLMAgent(\"assistant\", provider). WithSystemPrompt(\"You are a helpful assistant with access to tools. Use them when needed.\"). WithTools([]string{\"calculator\", \"weather\"}) // Tools discovered via MCP // Create agents map agents := map[string]core.AgentHandler{ \"assistant\": agent, } // Create runner runner := core.CreateRouteRunner(agents) // Start runner ctx := context.Background() if err := runner.Start(ctx); err != nil { log.Fatalf(\"Failed to start runner: %v\", err) } defer runner.Stop() // Create event with user query event := core.NewEvent(\"assistant\", map[string]interface{}{ \"message\": \"What's 25 * 16 and what's the weather in New York?\", }) event.SetMetadata(core.RouteMetadataKey, \"assistant\") // Emit the event if err := runner.Emit(event); err != nil { log.Fatalf(\"Failed to emit event: %v\", err) } // Wait for processing time.Sleep(5 * time.Second) fmt.Println(\"MCP tool integration complete!\") }\rNext Steps Now that you understand the basics of MCP, proceed to the following tutorials to learn more:\nTool Development - Learn how to create custom tools Tool Integration - Integrate tools with your agents Advanced Tool Patterns - Explore complex tool usage patterns Further Reading API Reference: MCP Examples: Tool Usage Advanced Patterns - Advanced multi-agent patterns",
    "description": "Model Context Protocol (MCP) in AgenticGoKit Navigation: Documentation Home → Tutorials → MCP (Tools)\nOverview The Model Context Protocol (MCP) is a powerful framework within AgenticGoKit that enables agents to interact with external tools, APIs, and services. MCP bridges the gap between language models and the outside world, allowing agents to perform actions beyond text generation.\nWith MCP, agents can search the web, access databases, call APIs, manipulate files, perform calculations, and much more. This capability transforms agents from simple text processors into powerful assistants that can take meaningful actions.",
    "tags": [],
    "title": "mcp",
    "uri": "/AgenticGoKitDocs/tutorials/mcp/readme/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials \u003e memory-systems",
    "content": "Memory Systems in AgenticGoKit Navigation: Documentation Home → Tutorials → Memory Systems\nOverview Memory systems are crucial for building intelligent agents that can learn, remember, and build upon previous interactions. This tutorial series explores AgenticGoKit’s memory capabilities, from basic in-memory storage to advanced RAG (Retrieval-Augmented Generation) systems with vector databases.\nMemory systems enable agents to maintain context across conversations, store knowledge, and retrieve relevant information to enhance their responses.\nPrerequisites Understanding of Core Concepts Basic knowledge of databases and data storage Familiarity with vector embeddings and similarity search Memory System Architecture AgenticGoKit’s memory system is built on a flexible architecture that supports multiple storage backends and retrieval strategies:\n┌─────────────┐ ┌──────────────┐ ┌─────────────────┐\r│ Agent │───▶│ Memory │───▶│ Storage Backend │\r│ │ │ Interface │ │ │\r└─────────────┘ └──────────────┘ └─────────────────┘\r│\r▼\r┌──────────────┐\r│ Embedding │\r│ Provider │\r└──────────────┘\rMemory Types 1. Conversational Memory Stores chat history and conversation context:\nMessage history User preferences Session context Conversation metadata 2. Knowledge Memory Stores factual information and documents:\nDocument chunks Factual knowledge Reference materials Structured data 3. Episodic Memory Stores experiences and events:\nPast interactions Learning experiences Feedback and corrections Temporal sequences 4. Working Memory Temporary storage during processing:\nIntermediate results Processing context Temporary variables Computation state Memory Interface The core memory interface provides a unified API for all memory operations including RAG:\ntype Memory interface { // Personal memory operations Store(ctx context.Context, content string, tags ...string) error Query(ctx context.Context, query string, limit ...int) ([]Result, error) Remember(ctx context.Context, key string, value any) error Recall(ctx context.Context, key string) (any, error) // Chat history management AddMessage(ctx context.Context, role, content string) error GetHistory(ctx context.Context, limit ...int) ([]Message, error) // Session management NewSession() string SetSession(ctx context.Context, sessionID string) context.Context ClearSession(ctx context.Context) error Close() error // RAG-Enhanced Knowledge Base Operations IngestDocument(ctx context.Context, doc Document) error IngestDocuments(ctx context.Context, docs []Document) error SearchKnowledge(ctx context.Context, query string, options ...SearchOption) ([]KnowledgeResult, error) // Hybrid Search (Personal Memory + Knowledge Base) SearchAll(ctx context.Context, query string, options ...SearchOption) (*HybridResult, error) // RAG Context Assembly for LLM Prompts BuildContext(ctx context.Context, query string, options ...ContextOption) (*RAGContext, error) }\rMemory Providers 1. In-Memory Provider Fast, non-persistent storage for development and testing:\n// Create in-memory storage memory, err := core.NewMemory(core.AgentMemoryConfig{ Provider: \"memory\", Connection: \"memory\", MaxResults: 10, Dimensions: 1536, AutoEmbed: true, })\rUse Cases:\nDevelopment and testing Temporary storage High-speed access Stateless applications 2. PostgreSQL with pgvector Production-ready vector storage with SQL capabilities:\n// Create pgvector storage memory, err := core.NewMemory(core.AgentMemoryConfig{ Provider: \"pgvector\", Connection: \"postgres://user:pass@localhost:5432/agentdb\", EnableRAG: true, EnableKnowledgeBase: true, Dimensions: 1536, // OpenAI embedding dimensions KnowledgeMaxResults: 20, KnowledgeScoreThreshold: 0.7, Embedding: core.EmbeddingConfig{ Provider: \"openai\", Model: \"text-embedding-3-small\", APIKey: os.Getenv(\"OPENAI_API_KEY\"), MaxBatchSize: 100, TimeoutSeconds: 30, CacheEmbeddings: true, }, })\rUse Cases:\nProduction applications Complex queries ACID compliance Structured and unstructured data 3. Weaviate Vector Database Specialized vector database with advanced features:\n// Create Weaviate storage memory, err := core.NewMemory(core.AgentMemoryConfig{ Provider: \"weaviate\", Connection: \"http://localhost:8080\", EnableRAG: true, EnableKnowledgeBase: true, Dimensions: 1536, KnowledgeMaxResults: 20, KnowledgeScoreThreshold: 0.7, Embedding: core.EmbeddingConfig{ Provider: \"openai\", Model: \"text-embedding-3-small\", APIKey: os.Getenv(\"OPENAI_API_KEY\"), MaxBatchSize: 100, TimeoutSeconds: 30, CacheEmbeddings: true, }, })\rUse Cases:\nLarge-scale vector search Multi-modal data Advanced filtering Real-time updates Basic Memory Operations 1. Storing Information // Store simple text err := memory.Store(ctx, \"Paris is the capital of France\", \"fact\") // Store with metadata err = memory.Store(ctx, \"The user prefers technical explanations\", \"preference\", core.WithSession(\"user-123\"), core.WithMetadata(map[string]string{ \"category\": \"user-preference\", \"priority\": \"high\", }), ) // Store conversation message err = memory.Store(ctx, \"How do I implement a binary search?\", \"user-message\", core.WithSession(\"user-123\"), core.WithTimestamp(time.Now()), )\r2. Searching Memory // Basic search results, err := memory.Search(ctx, \"capital of France\") // Search with options results, err = memory.Search(ctx, \"user preferences\", core.WithLimit(5), core.WithScoreThreshold(0.7), core.WithSession(\"user-123\"), core.WithContentType(\"preference\"), ) // Process results for _, result := range results { fmt.Printf(\"Content: %s (Score: %.3f)\\n\", result.Content, result.Score) }\r3. Conversation History // Get recent conversation history messages, err := memory.GetHistory(ctx, 10, core.WithSession(\"user-123\"), core.WithTimeRange(time.Now().Add(-24*time.Hour), time.Now()), ) // Process conversation history for _, msg := range messages { fmt.Printf(\"[%s] %s: %s\\n\", msg.Timestamp.Format(\"15:04\"), msg.Role, msg.Content) }\rRAG (Retrieval-Augmented Generation) RAG enhances agent responses by retrieving relevant information from memory:\n1. Basic RAG Implementation type RAGAgent struct { name string llm LLMProvider memory Memory } func (r *RAGAgent) Run(ctx context.Context, event Event, state State) (AgentResult, error) { query, _ := state.Get(\"message\") queryStr := query.(string) // Retrieve relevant context results, err := r.memory.Search(ctx, queryStr, core.WithLimit(3), core.WithScoreThreshold(0.7), ) if err != nil { return AgentResult{}, err } // Build context from retrieved results var context strings.Builder for _, result := range results { context.WriteString(fmt.Sprintf(\"- %s\\n\", result.Content)) } // Create enhanced prompt prompt := fmt.Sprintf(`Context: %s Question: %s Please answer the question using the provided context.`, context.String(), queryStr) // Generate response with context response, err := r.llm.Generate(ctx, prompt) if err != nil { return AgentResult{}, err } // Store the interaction r.memory.Store(ctx, queryStr, \"user-message\") r.memory.Store(ctx, response, \"assistant-response\") outputState := state.Clone() outputState.Set(\"response\", response) outputState.Set(\"context_used\", len(results)) return AgentResult{OutputState: outputState}, nil }\r2. Advanced RAG with Reranking type AdvancedRAGAgent struct { name string llm LLMProvider memory Memory reranker Reranker } func (r *AdvancedRAGAgent) Run(ctx context.Context, event Event, state State) (AgentResult, error) { query, _ := state.Get(\"message\") queryStr := query.(string) // Initial retrieval with higher limit results, err := r.memory.Search(ctx, queryStr, core.WithLimit(10), core.WithScoreThreshold(0.5), ) if err != nil { return AgentResult{}, err } // Rerank results for better relevance rerankedResults, err := r.reranker.Rerank(ctx, queryStr, results) if err != nil { return AgentResult{}, err } // Take top results after reranking topResults := rerankedResults[:min(3, len(rerankedResults))] // Build enhanced context context := r.buildContext(topResults) // Generate response response, err := r.generateWithContext(ctx, queryStr, context) if err != nil { return AgentResult{}, err } // Store interaction with metadata r.storeInteraction(ctx, queryStr, response, topResults) outputState := state.Clone() outputState.Set(\"response\", response) outputState.Set(\"sources\", r.extractSources(topResults)) return AgentResult{OutputState: outputState}, nil }\rMemory Configuration 1. Embedding Configuration type EmbeddingConfig struct { Provider string `yaml:\"provider\"` // \"openai\", \"huggingface\", \"local\" Model string `yaml:\"model\"` // Model name APIKey string `yaml:\"api_key\"` // API key if needed Dimensions int `yaml:\"dimensions\"` // Embedding dimensions BatchSize int `yaml:\"batch_size\"` // Batch size for processing Options map[string]string `yaml:\"options\"` // Provider-specific options } // OpenAI embeddings embeddingConfig := core.EmbeddingConfig{ Provider: \"openai\", Model: \"text-embedding-3-small\", APIKey: os.Getenv(\"OPENAI_API_KEY\"), Dimensions: 1536, BatchSize: 100, } // Hugging Face embeddings embeddingConfig := core.EmbeddingConfig{ Provider: \"huggingface\", Model: \"sentence-transformers/all-MiniLM-L6-v2\", Dimensions: 384, BatchSize: 32, }\r2. Memory Configuration type AgentMemoryConfig struct { // Core memory settings Provider string `toml:\"provider\"` // pgvector, weaviate, memory Connection string `toml:\"connection\"` // postgres://..., http://..., or \"memory\" MaxResults int `toml:\"max_results\"` // default: 10 Dimensions int `toml:\"dimensions\"` // default: 1536 AutoEmbed bool `toml:\"auto_embed\"` // default: true // RAG-enhanced settings EnableKnowledgeBase bool `toml:\"enable_knowledge_base\"` // default: true KnowledgeMaxResults int `toml:\"knowledge_max_results\"` // default: 20 KnowledgeScoreThreshold float32 `toml:\"knowledge_score_threshold\"` // default: 0.7 ChunkSize int `toml:\"chunk_size\"` // default: 1000 ChunkOverlap int `toml:\"chunk_overlap\"` // default: 200 // RAG context assembly settings EnableRAG bool `toml:\"enable_rag\"` // default: true RAGMaxContextTokens int `toml:\"rag_max_context_tokens\"` // default: 4000 RAGPersonalWeight float32 `toml:\"rag_personal_weight\"` // default: 0.3 RAGKnowledgeWeight float32 `toml:\"rag_knowledge_weight\"` // default: 0.7 RAGIncludeSources bool `toml:\"rag_include_sources\"` // default: true // Document processing settings Documents DocumentConfig `toml:\"documents\"` // Embedding service settings Embedding EmbeddingConfig `toml:\"embedding\"` // Search settings Search SearchConfigToml `toml:\"search\"` } // Complete memory configuration config := core.AgentMemoryConfig{ Provider: \"pgvector\", Connection: \"postgres://user:pass@localhost:5432/agentdb\", EnableRAG: true, EnableKnowledgeBase: true, ChunkSize: 1000, ChunkOverlap: 200, Dimensions: 1536, KnowledgeMaxResults: 20, KnowledgeScoreThreshold: 0.7, RAGMaxContextTokens: 4000, RAGPersonalWeight: 0.3, RAGKnowledgeWeight: 0.7, RAGIncludeSources: true, Embedding: core.EmbeddingConfig{ Provider: \"openai\", Model: \"text-embedding-3-small\", APIKey: os.Getenv(\"OPENAI_API_KEY\"), MaxBatchSize: 100, TimeoutSeconds: 30, CacheEmbeddings: true, }, Documents: core.DocumentConfig{ AutoChunk: true, SupportedTypes: []string{\"pdf\", \"txt\", \"md\", \"web\", \"code\"}, MaxFileSize: \"10MB\", EnableMetadataExtraction: true, EnableURLScraping: true, }, Search: core.SearchConfigToml{ HybridSearch: true, KeywordWeight: 0.3, SemanticWeight: 0.7, EnableReranking: false, EnableQueryExpansion: false, }, }\rMemory Integration with Agents 1. Agent Builder Integration // Create agent with memory agent, err := core.NewAgent(\"knowledge-agent\"). WithLLMAndConfig(llmProvider, llmConfig). WithMemory(memory). WithMCPAndConfig(mcpManager, mcpConfig). Build()\r2. Custom Memory Integration type MemoryEnabledAgent struct { name string llm LLMProvider memory Memory config MemoryConfig } func (m *MemoryEnabledAgent) Run(ctx context.Context, event Event, state State) (AgentResult, error) { // Extract user message message, _ := state.Get(\"message\") messageStr := message.(string) // Get conversation history for context history, err := m.memory.GetHistory(ctx, 5, core.WithSession(event.GetSessionID()), ) if err != nil { return AgentResult{}, err } // Search for relevant knowledge knowledge, err := m.memory.Search(ctx, messageStr, core.WithLimit(3), core.WithScoreThreshold(0.7), core.WithContentType(\"knowledge\"), ) if err != nil { return AgentResult{}, err } // Build enhanced prompt with history and knowledge prompt := m.buildEnhancedPrompt(messageStr, history, knowledge) // Generate response response, err := m.llm.Generate(ctx, prompt) if err != nil { return AgentResult{}, err } // Store the interaction m.storeInteraction(ctx, event.GetSessionID(), messageStr, response) outputState := state.Clone() outputState.Set(\"response\", response) outputState.Set(\"knowledge_used\", len(knowledge)) outputState.Set(\"history_length\", len(history)) return AgentResult{OutputState: outputState}, nil }\rTutorial Series Structure This memory systems tutorial series covers:\n1. Basic Memory In-memory storage Simple operations Session management Basic retrieval 2. Vector Databases pgvector setup and usage Weaviate integration Embedding strategies Performance optimization 3. Document Ingestion Document processing pipeline Text chunking strategies Metadata extraction Batch processing optimization 4. RAG Implementation Retrieval-Augmented Generation Context building Prompt engineering Response enhancement 5. Knowledge Bases Knowledge base architecture Advanced search patterns Multi-modal content Production deployment 6. Memory Optimization Performance tuning Scaling strategies Caching mechanisms Resource management Best Practices 1. Memory Design Principles Separate concerns: Use different memory types for different purposes Optimize for retrieval: Design storage for efficient search Manage lifecycle: Clean up old or irrelevant memories Monitor performance: Track memory usage and search performance 2. RAG Best Practices Chunk appropriately: Balance context size and relevance Use metadata: Enhance retrieval with structured metadata Implement reranking: Improve relevance with secondary ranking Handle failures: Gracefully degrade when memory is unavailable 3. Production Considerations Scale horizontally: Use distributed storage for large datasets Implement caching: Cache frequent queries and embeddings Monitor costs: Track embedding API usage and storage costs Backup data: Ensure memory data is backed up and recoverable Common Use Cases 1. Conversational AI Chat history maintenance User preference learning Context-aware responses Personalization 2. Knowledge Management Document Q\u0026A systems Technical support bots Research assistants Information retrieval 3. Learning Systems Adaptive agents Feedback incorporation Experience replay Continuous improvement 4. Multi-Agent Coordination Shared knowledge bases Inter-agent communication Collaborative learning Distributed memory Conclusion Memory systems are fundamental to building intelligent agents that can learn, remember, and improve over time. AgenticGoKit provides flexible memory abstractions that support various storage backends and retrieval strategies.\nThe key to effective memory systems is choosing the right combination of storage backend, embedding strategy, and retrieval approach for your specific use case.\nNext Steps Basic Memory - Start with simple memory operations Vector Databases - Learn about production storage RAG Implementation - Build retrieval-augmented systems Knowledge Bases - Create comprehensive knowledge systems Further Reading API Reference: Memory Interface Examples: Memory Systems Configuration Guide: Memory Settings",
    "description": "Memory Systems in AgenticGoKit Navigation: Documentation Home → Tutorials → Memory Systems\nOverview Memory systems are crucial for building intelligent agents that can learn, remember, and build upon previous interactions. This tutorial series explores AgenticGoKit’s memory capabilities, from basic in-memory storage to advanced RAG (Retrieval-Augmented Generation) systems with vector databases.\nMemory systems enable agents to maintain context across conversations, store knowledge, and retrieve relevant information to enhance their responses.\nPrerequisites Understanding of Core Concepts Basic knowledge of databases and data storage Familiarity with vector embeddings and similarity search Memory System Architecture AgenticGoKit’s memory system is built on a flexible architecture that supports multiple storage backends and retrieval strategies:",
    "tags": [],
    "title": "memory-systems",
    "uri": "/AgenticGoKitDocs/tutorials/memory-systems/readme/index.html"
  },
  {
    "breadcrumb": "content \u003e reference",
    "content": "AgenticGoKit API Reference Navigation: Documentation Home → API Reference\nComplete API documentation for building multi-agent systems\nThis section provides comprehensive documentation for all public APIs in AgenticGoKit. The framework is designed with a clean, intuitive interface that makes building complex agent systems straightforward.\n📚 API Overview AgenticGoKit’s API is organized around several core concepts:\nAgent API - Individual agents that process events and states Orchestration API - Multi-agent coordination and workflow patterns State \u0026 Events - Data flow and communication between agents Memory API - Persistent storage, RAG, and knowledge management MCP Integration - Tool integration via Model Context Protocol Configuration API - System configuration and setup 🏗️ Architecture Overview graph TB\rsubgraph \"Core Interfaces\"\rAgent[Agent Interface]\rAgentHandler[AgentHandler Interface]\rRunner[Runner Interface]\rOrchestrator[Orchestrator Interface]\rend\rsubgraph \"Data Flow\"\rEvent[Event]\rState[State]\rAgentResult[AgentResult]\rend\rsubgraph \"Advanced Features\"\rMemory[Memory System]\rMCP[MCP Tools]\rConfig[Configuration]\rend\rAgent --\u003e AgentHandler\rAgentHandler --\u003e Runner\rRunner --\u003e Orchestrator\rEvent --\u003e AgentHandler\rState --\u003e AgentHandler\rAgentHandler --\u003e AgentResult\rMemory --\u003e Agent\rMCP --\u003e Agent\rConfig --\u003e Runner\r🚀 Quick Start Basic Agent Creation package main import ( \"context\" \"fmt\" \"github.com/kunalkushwaha/agenticgokit/core\" ) func main() { // Create a simple agent agent := core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { message := event.Data[\"message\"].(string) return core.AgentResult{ Data: map[string]interface{}{ \"response\": fmt.Sprintf(\"Processed: %s\", message), }, }, nil }) // Create a runner and register the agent runner := core.NewRunner() runner.RegisterAgent(\"processor\", agent) // Process an event event := core.NewEvent(\"process\", map[string]interface{}{ \"message\": \"Hello, AgenticGoKit!\", }) results, err := runner.ProcessEvent(context.Background(), event) if err != nil { panic(err) } fmt.Printf(\"Response: %s\\n\", results[\"processor\"].Data[\"response\"]) }\rMulti-Agent Collaboration func collaborativeExample() { // Create multiple agents agents := map[string]core.AgentHandler{ \"analyzer\": core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { text := event.Data[\"text\"].(string) analysis := analyzeText(text) // Your analysis logic return core.AgentResult{ Data: map[string]interface{}{ \"analysis\": analysis, \"word_count\": len(strings.Fields(text)), }, }, nil }), \"summarizer\": core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { text := event.Data[\"text\"].(string) summary := summarizeText(text) // Your summarization logic return core.AgentResult{ Data: map[string]interface{}{ \"summary\": summary, }, }, nil }), } // Create collaborative runner runner := core.CreateCollaborativeRunner(agents, 30*time.Second) // Process with multiple agents event := core.NewEvent(\"analyze\", map[string]interface{}{ \"text\": \"Long document text here...\", }) results, _ := runner.ProcessEvent(context.Background(), event) // Both agents processed the event in parallel fmt.Printf(\"Analysis: %v\\n\", results[\"analyzer\"].Data[\"analysis\"]) fmt.Printf(\"Summary: %s\\n\", results[\"summarizer\"].Data[\"summary\"]) }\r🔧 Core Interfaces Agent vs AgentHandler AgenticGoKit provides two main interfaces for creating agents:\nAgent Interface - Simple, state-based processing:\ntype Agent interface { Run(ctx context.Context, inputState State) (State, error) Name() string }\rAgentHandler Interface - Event-driven processing with rich results:\ntype AgentHandler interface { Run(ctx context.Context, event Event, state State) (AgentResult, error) }\rWhen to use which:\nUse Agent for simple, stateful transformations Use AgentHandler for event-driven systems with complex orchestration Use AgentHandlerFunc for quick function-based agents Conversion Between Interfaces // Convert Agent to AgentHandler func ConvertAgentToHandler(agent Agent) AgentHandler { return core.ConvertAgentToHandler(agent) } // Convert function to AgentHandler func ConvertFuncToHandler(fn func(context.Context, Event, State) (AgentResult, error)) AgentHandler { return core.AgentHandlerFunc(fn) }\r📊 Orchestration Patterns AgenticGoKit supports multiple orchestration patterns:\nPattern Description Use Case Route Single agent per event (default) Simple request-response Collaborate All agents process in parallel Analysis, multiple perspectives Sequential Agents process in order Data pipelines, workflows Mixed Hybrid collaborative + sequential Complex business processes Loop Single agent iterative processing Refinement, optimization Pattern Selection Guide // Choose orchestration pattern based on your needs func chooseOrchestration(useCase string) core.OrchestrationMode { switch useCase { case \"simple_processing\": return core.OrchestrationRoute case \"parallel_analysis\": return core.OrchestrationCollaborate case \"data_pipeline\": return core.OrchestrationSequential case \"complex_workflow\": return core.OrchestrationMixed case \"iterative_refinement\": return core.OrchestrationLoop default: return core.OrchestrationRoute } }\r🧠 Memory and RAG AgenticGoKit provides built-in support for persistent memory and RAG (Retrieval-Augmented Generation):\n// Create memory-enabled agent memoryConfig := core.AgentMemoryConfig{ Provider: \"pgvector\", Connection: \"postgres://user:pass@localhost:5432/agents\", AutoEmbed: true, } memory, _ := core.NewMemory(memoryConfig) llmProvider, _ := core.NewOpenAIProvider() agent := core.NewMemoryEnabledAgent(\"assistant\", llmProvider, memory)\r🔧 Tool Integration Integrate external tools via MCP (Model Context Protocol):\n// Initialize MCP core.QuickStartMCP() // Create MCP-aware agent llmProvider, _ := core.NewOpenAIProvider() agent, _ := core.NewMCPAgent(\"assistant\", llmProvider) // Agent can now discover and use MCP tools automatically\rCommand Line Interface CLI Reference Complete reference for the agentcli command-line tool.\nAvailable commands:\ncreate - Create new projects with multi-agent orchestration trace - View execution traces and debugging information mcp - Manage MCP servers and tools cache - Cache management and optimization run - Run agents interactively for testing test - Run automated agent tests benchmark - Performance benchmarking config - Configuration management API Design Principles AgenticGoKit APIs follow these principles:\nInterface-based design - Clear separation of concerns Context-aware - All operations accept Go context Error handling - Explicit error returns and handling Configuration-driven - Behavior controlled through configuration Extensible - Plugin architecture for custom implementations Type Definitions All APIs use consistent type definitions:\n// Core types type State map[string]interface{} type Event struct { /* ... */ } type AgentResult struct { /* ... */ } // Configuration types type Config struct { /* ... */ } type OrchestrationMode string\rUsage Patterns Common patterns across all APIs:\nContext Usage ctx := context.Background() result, err := agent.Run(ctx, inputState)\rError Handling if err != nil { log.Printf(\"Operation failed: %v\", err) return err }\rConfiguration Loading config, err := core.LoadConfigFromWorkingDir() if err != nil { return fmt.Errorf(\"failed to load config: %w\", err) }\rVersioning AgenticGoKit follows semantic versioning:\nMajor versions - Breaking API changes Minor versions - New features, backward compatible Patch versions - Bug fixes, backward compatible Migration Guides When APIs change between versions, migration guides are provided:\nBreaking changes are documented Migration examples are provided Deprecation notices give advance warning Getting Help For API questions:\nCheck this reference documentation Review code examples in tutorials See how-to guides for specific use cases Check the troubleshooting guide for common issues",
    "description": "AgenticGoKit API Reference Navigation: Documentation Home → API Reference\nComplete API documentation for building multi-agent systems\nThis section provides comprehensive documentation for all public APIs in AgenticGoKit. The framework is designed with a clean, intuitive interface that makes building complex agent systems straightforward.\n📚 API Overview AgenticGoKit’s API is organized around several core concepts:\nAgent API - Individual agents that process events and states Orchestration API - Multi-agent coordination and workflow patterns State \u0026 Events - Data flow and communication between agents Memory API - Persistent storage, RAG, and knowledge management MCP Integration - Tool integration via Model Context Protocol Configuration API - System configuration and setup 🏗️ Architecture Overview graph TB\rsubgraph \"Core Interfaces\"\rAgent[Agent Interface]\rAgentHandler[AgentHandler Interface]\rRunner[Runner Interface]\rOrchestrator[Orchestrator Interface]\rend\rsubgraph \"Data Flow\"\rEvent[Event]\rState[State]\rAgentResult[AgentResult]\rend\rsubgraph \"Advanced Features\"\rMemory[Memory System]\rMCP[MCP Tools]\rConfig[Configuration]\rend\rAgent --\u003e AgentHandler\rAgentHandler --\u003e Runner\rRunner --\u003e Orchestrator\rEvent --\u003e AgentHandler\rState --\u003e AgentHandler\rAgentHandler --\u003e AgentResult\rMemory --\u003e Agent\rMCP --\u003e Agent\rConfig --\u003e Runner\r🚀 Quick Start Basic Agent Creation package main import ( \"context\" \"fmt\" \"github.com/kunalkushwaha/agenticgokit/core\" ) func main() { // Create a simple agent agent := core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { message := event.Data[\"message\"].(string) return core.AgentResult{ Data: map[string]interface{}{ \"response\": fmt.Sprintf(\"Processed: %s\", message), }, }, nil }) // Create a runner and register the agent runner := core.NewRunner() runner.RegisterAgent(\"processor\", agent) // Process an event event := core.NewEvent(\"process\", map[string]interface{}{ \"message\": \"Hello, AgenticGoKit!\", }) results, err := runner.ProcessEvent(context.Background(), event) if err != nil { panic(err) } fmt.Printf(\"Response: %s\\n\", results[\"processor\"].Data[\"response\"]) }\rMulti-Agent Collaboration func collaborativeExample() { // Create multiple agents agents := map[string]core.AgentHandler{ \"analyzer\": core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { text := event.Data[\"text\"].(string) analysis := analyzeText(text) // Your analysis logic return core.AgentResult{ Data: map[string]interface{}{ \"analysis\": analysis, \"word_count\": len(strings.Fields(text)), }, }, nil }), \"summarizer\": core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { text := event.Data[\"text\"].(string) summary := summarizeText(text) // Your summarization logic return core.AgentResult{ Data: map[string]interface{}{ \"summary\": summary, }, }, nil }), } // Create collaborative runner runner := core.CreateCollaborativeRunner(agents, 30*time.Second) // Process with multiple agents event := core.NewEvent(\"analyze\", map[string]interface{}{ \"text\": \"Long document text here...\", }) results, _ := runner.ProcessEvent(context.Background(), event) // Both agents processed the event in parallel fmt.Printf(\"Analysis: %v\\n\", results[\"analyzer\"].Data[\"analysis\"]) fmt.Printf(\"Summary: %s\\n\", results[\"summarizer\"].Data[\"summary\"]) }\r🔧 Core Interfaces Agent vs AgentHandler AgenticGoKit provides two main interfaces for creating agents:",
    "tags": [],
    "title": "reference",
    "uri": "/AgenticGoKitDocs/reference/readme/index.html"
  },
  {
    "breadcrumb": "content \u003e guides \u003e setup",
    "content": "Setup \u0026 Configuration Guides Guides for setting up and configuring AgenticGoKit components.\nAvailable Guides LLM Providers Configure different Large Language Model providers including OpenAI, Anthropic, and local models.\nWhen to use: Setting up your first agent or switching LLM providers.\nVector Databases Set up vector storage systems for Retrieval-Augmented Generation (RAG) including pgvector and Weaviate.\nWhen to use: Building agents that need to search through documents or knowledge bases.\nMCP Tools Integrate Model Context Protocol tools to extend agent capabilities with external services.\nWhen to use: Adding specific tools like web search, file operations, or API integrations.\nCommon Setup Patterns Most AgenticGoKit applications follow this setup pattern:\nChoose your LLM provider - Start with LLM Providers Configure memory (optional) - Add Vector Databases if needed Add tools (optional) - Integrate MCP Tools for extended capabilities Test your configuration - Use Testing Agents Configuration Files AgenticGoKit uses TOML configuration files (agentflow.toml) for most settings. Each guide shows you how to configure the relevant sections.\nNext Steps After setup, explore:\nDevelopment Guides for building and testing Deployment Guides for production deployment Getting Started Tutorials for hands-on learning",
    "description": "Setup \u0026 Configuration Guides Guides for setting up and configuring AgenticGoKit components.\nAvailable Guides LLM Providers Configure different Large Language Model providers including OpenAI, Anthropic, and local models.\nWhen to use: Setting up your first agent or switching LLM providers.\nVector Databases Set up vector storage systems for Retrieval-Augmented Generation (RAG) including pgvector and Weaviate.\nWhen to use: Building agents that need to search through documents or knowledge bases.\nMCP Tools Integrate Model Context Protocol tools to extend agent capabilities with external services.",
    "tags": [],
    "title": "setup",
    "uri": "/AgenticGoKitDocs/guides/setup/readme/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials",
    "content": "Tutorials Navigation: Documentation Home → Tutorials\nLearning-oriented guides to help you understand and use AgenticGoKit effectively.\nLearning Paths Beginner Path Start here if you’re new to AgenticGoKit:\nYour First Agent - Create a simple agent Multi-Agent Collaboration - Learn agent orchestration Memory and RAG - Add knowledge capabilities Tool Integration - Connect external tools Production Deployment - Deploy to production Core Concepts Path Understand the fundamental concepts:\nAgent Lifecycle - How agents work internally State Management - Data flow between agents Error Handling - Robust error management Orchestration Patterns - Multi-agent coordination Advanced Path For experienced users building complex systems:\nCircuit Breaker Patterns - Fault tolerance Retry Policies - Resilient operations Testing Strategies - Testing multi-agent systems Load Balancing \u0026 Scaling - Production scaling Tutorial Structure Each tutorial includes:\nLearning objectives - What you’ll accomplish Prerequisites - What you need to know first Step-by-step instructions - Clear, actionable steps Working code examples - Complete, runnable code What’s next - Suggested follow-up tutorials Getting Help Check the Troubleshooting Guide for common issues Review the API Reference for detailed documentation See How-To Guides for specific tasks",
    "description": "Tutorials Navigation: Documentation Home → Tutorials\nLearning-oriented guides to help you understand and use AgenticGoKit effectively.\nLearning Paths Beginner Path Start here if you’re new to AgenticGoKit:\nYour First Agent - Create a simple agent Multi-Agent Collaboration - Learn agent orchestration Memory and RAG - Add knowledge capabilities Tool Integration - Connect external tools Production Deployment - Deploy to production Core Concepts Path Understand the fundamental concepts:\nAgent Lifecycle - How agents work internally State Management - Data flow between agents Error Handling - Robust error management Orchestration Patterns - Multi-agent coordination Advanced Path For experienced users building complex systems:",
    "tags": [],
    "title": "tutorials",
    "uri": "/AgenticGoKitDocs/tutorials/readme/index.html"
  },
  {
    "breadcrumb": "content",
    "content": "How-To Guides Navigation: Documentation Home → Guides\nTask-oriented guides for specific development scenarios and common tasks.\nQuick Navigation Setup \u0026 Configuration LLM Providers - Configure different LLM providers Vector Databases - Set up vector storage for RAG MCP Tools - Integrate Model Context Protocol tools Development Testing Agents - Test multi-agent systems Debugging - Debug agent interactions Best Practices - Development best practices Web Search Integration - Add web search capabilities to agents Research Assistant - Build multi-agent research systems Visualization - Generate workflow diagrams with Mermaid Framework \u0026 Tools Framework Comparison - Compare AgenticGoKit with other frameworks Troubleshooting - Common issues and solutions Deployment \u0026 Operations Production Deployment - Deploy and scale your agents Guide Structure Each guide follows this format:\nProblem statement - What you’re trying to accomplish Solution overview - High-level approach Step-by-step instructions - Detailed implementation Code examples - Working code snippets Troubleshooting - Common issues and solutions Related guides - Links to related content When to Use Guides vs Tutorials Use Tutorials when you want to learn concepts step-by-step Use Guides when you need to solve a specific problem or implement a particular feature Getting Help If you can’t find what you’re looking for:\nCheck the API Reference for detailed function documentation Review Troubleshooting for common issues Browse Tutorials for learning-oriented content",
    "description": "How-To Guides Navigation: Documentation Home → Guides\nTask-oriented guides for specific development scenarios and common tasks.\nQuick Navigation Setup \u0026 Configuration LLM Providers - Configure different LLM providers Vector Databases - Set up vector storage for RAG MCP Tools - Integrate Model Context Protocol tools Development Testing Agents - Test multi-agent systems Debugging - Debug agent interactions Best Practices - Development best practices Web Search Integration - Add web search capabilities to agents Research Assistant - Build multi-agent research systems Visualization - Generate workflow diagrams with Mermaid Framework \u0026 Tools Framework Comparison - Compare AgenticGoKit with other frameworks Troubleshooting - Common issues and solutions Deployment \u0026 Operations Production Deployment - Deploy and scale your agents Guide Structure Each guide follows this format:",
    "tags": [],
    "title": "guides",
    "uri": "/AgenticGoKitDocs/guides/index.html"
  },
  {
    "breadcrumb": "content \u003e guides",
    "content": "Agent Basics Understanding the AgenticGoKit Agent System\nThis guide covers the fundamental concepts of building agents in AgenticGoKit, from the basic interfaces to advanced multi-agent orchestration patterns.\nCore Concepts AgentHandler Interface The AgentHandler is the primary interface for implementing agent logic in AgenticGoKit:\ntype AgentHandler interface { Run(ctx context.Context, event Event, state State) (AgentResult, error) }\rKey Components:\nEvent: Contains the user input and metadata State: Thread-safe storage for agent data AgentResult: The agent’s response and updated state Multi-Agent Orchestration AgenticGoKit supports multiple orchestration patterns for coordinating agents:\nCollaborative Orchestration All agents process the same event in parallel:\n// Create collaborative orchestration agents := map[string]core.AgentHandler{ \"researcher\": NewResearchAgent(), \"analyzer\": NewAnalysisAgent(), \"validator\": NewValidationAgent(), } runner := core.NewOrchestrationBuilder(core.OrchestrationCollaborate). WithAgents(agents). WithTimeout(2 * time.Minute). Build()\rSequential Orchestration Agents process events in pipeline order:\n// Create sequential orchestration agents := map[string]core.AgentHandler{ \"collector\": NewCollectorAgent(), \"processor\": NewProcessorAgent(), \"formatter\": NewFormatterAgent(), } runner := core.NewOrchestrationBuilder(core.OrchestrationSequential). WithAgents(agents). Build()\rLoop Orchestration Single agent repeats until conditions are met:\n// Create loop orchestration agents := map[string]core.AgentHandler{ \"quality-checker\": NewQualityCheckerAgent(), } runner := core.NewOrchestrationBuilder(core.OrchestrationLoop). WithAgents(agents). WithMaxIterations(10). Build()\rBasic Agent Structure Every agent follows this pattern:\npackage main import ( \"context\" \"fmt\" agentflow \"github.com/kunalkushwaha/agenticgokit/core\" ) type MyAgentHandler struct { llm agentflow.ModelProvider mcpManager agentflow.MCPManager name string } func NewMyAgent(name string, llm agentflow.ModelProvider, mcp agentflow.MCPManager) *MyAgentHandler { return \u0026MyAgentHandler{ name: name, llm: llm, mcpManager: mcp, } } func (a *MyAgentHandler) Run(ctx context.Context, event agentflow.Event, state agentflow.State) (agentflow.AgentResult, error) { logger := agentflow.Logger() logger.Info().Str(\"agent\", a.name).Msg(\"Processing request\") // 1. Extract input from event eventData := event.GetData() message, ok := eventData[\"message\"] if !ok { return agentflow.AgentResult{}, fmt.Errorf(\"no message in event data\") } // 2. Build system prompt systemPrompt := \"You are a helpful assistant.\" // 3. Add available tools to prompt toolPrompt := \"\" if a.mcpManager != nil { toolPrompt = agentflow.FormatToolsForPrompt(ctx, a.mcpManager) } fullPrompt := fmt.Sprintf(\"%s\\n%s\\nUser: %s\", systemPrompt, toolPrompt, message) // 4. Call LLM response, err := a.llm.Generate(ctx, fullPrompt) if err != nil { return agentflow.AgentResult{}, fmt.Errorf(\"LLM call failed: %w\", err) } // 5. Execute any tool calls var finalResponse string if a.mcpManager != nil { toolResults := agentflow.ParseAndExecuteToolCalls(ctx, a.mcpManager, response) if len(toolResults) \u003e 0 { // Synthesize tool results synthesisPrompt := fmt.Sprintf(\"Original response: %s\\nTool results: %v\\nProvide a comprehensive answer:\", response, toolResults) finalResponse, _ = a.llm.Generate(ctx, synthesisPrompt) } else { finalResponse = response } } else { finalResponse = response } // 6. Update state and return state.Set(\"response\", finalResponse) state.Set(\"processed_by\", a.name) return agentflow.AgentResult{ Result: finalResponse, State: state, }, nil }\rAgent Patterns 1. Information Gathering Agent Specializes in research and data collection:\ntype ResearchAgent struct { llm agentflow.ModelProvider mcpManager agentflow.MCPManager } func (a *ResearchAgent) Run(ctx context.Context, event agentflow.Event, state agentflow.State) (agentflow.AgentResult, error) { message := event.GetData()[\"message\"] systemPrompt := `You are a research agent. Your job is to gather comprehensive information using available tools. Key behaviors: - Use search tools for current information - Use fetch_content for specific URLs - Gather multiple perspectives - Organize findings clearly` // Include tools and generate research-focused response toolPrompt := agentflow.FormatToolsForPrompt(ctx, a.mcpManager) prompt := fmt.Sprintf(\"%s\\n%s\\nResearch query: %s\", systemPrompt, toolPrompt, message) response, err := a.llm.Generate(ctx, prompt) if err != nil { return agentflow.AgentResult{}, err } // Execute research tools toolResults := agentflow.ParseAndExecuteToolCalls(ctx, a.mcpManager, response) // Compile research findings if len(toolResults) \u003e 0 { compilationPrompt := fmt.Sprintf(`Research findings: %v Please compile these findings into a structured research report with: 1. Key findings 2. Sources 3. Important details 4. Areas for further investigation`, toolResults) response, _ = a.llm.Generate(ctx, compilationPrompt) } state.Set(\"research_findings\", response) return agentflow.AgentResult{Result: response, State: state}, nil }\r2. Analysis Agent Processes information and draws insights:\ntype AnalysisAgent struct { llm agentflow.ModelProvider } func (a *AnalysisAgent) Run(ctx context.Context, event agentflow.Event, state agentflow.State) (agentflow.AgentResult, error) { // Get previous research findings findings, exists := state.Get(\"research_findings\") if !exists { return agentflow.AgentResult{}, fmt.Errorf(\"no research findings to analyze\") } message := event.GetData()[\"message\"] systemPrompt := `You are an analysis agent. Your job is to analyze information and provide insights. Key behaviors: - Identify patterns and trends - Draw meaningful conclusions - Highlight important implications - Provide actionable insights` prompt := fmt.Sprintf(`%s Original query: %s Research findings: %s Please provide a thorough analysis with insights and implications.`, systemPrompt, message, findings) analysis, err := a.llm.Generate(ctx, prompt) if err != nil { return agentflow.AgentResult{}, err } state.Set(\"analysis\", analysis) return agentflow.AgentResult{Result: analysis, State: state}, nil }\r3. Synthesis Agent Combines multiple inputs into final output:\ntype SynthesisAgent struct { llm agentflow.ModelProvider } func (a *SynthesisAgent) Run(ctx context.Context, event agentflow.Event, state agentflow.State) (agentflow.AgentResult, error) { // Gather all previous work research, _ := state.Get(\"research_findings\") analysis, _ := state.Get(\"analysis\") message := event.GetData()[\"message\"] systemPrompt := `You are a synthesis agent. Your job is to create comprehensive, well-structured final responses. Key behaviors: - Integrate multiple information sources - Create coherent, flowing narrative - Ensure completeness and accuracy - Provide clear, actionable conclusions` prompt := fmt.Sprintf(`%s Original query: %s Research findings: %s Analysis: %s Please synthesize this into a comprehensive, well-structured response that fully addresses the original query.`, systemPrompt, message, research, analysis) synthesis, err := a.llm.Generate(ctx, prompt) if err != nil { return agentflow.AgentResult{}, err } state.Set(\"final_response\", synthesis) return agentflow.AgentResult{Result: synthesis, State: state}, nil }\rState Management Using State for Data Flow State allows agents to share data across the workflow:\n// Agent 1: Store research data state.Set(\"research_data\", researchResults) state.Set(\"sources\", sourceList) state.SetMeta(\"research_agent\", \"agent1\") // Agent 2: Access research data researchData, exists := state.Get(\"research_data\") if exists { // Process the research data } // Access metadata researchAgent, _ := state.GetMeta(\"research_agent\")\rState Best Practices Use descriptive keys: \"user_preferences\" not \"prefs\" Store structured data: Use structs or maps for complex data Set metadata: Track which agent processed what Handle missing data: Always check if data exists before using // Good: Structured data storage type UserProfile struct { Name string Preferences []string Context map[string]interface{} } profile := UserProfile{ Name: \"John\", Preferences: []string{\"technical\", \"detailed\"}, Context: map[string]interface{}{\"industry\": \"software\"}, } state.Set(\"user_profile\", profile) // Good: Metadata tracking state.SetMeta(\"processed_by\", \"agent1\") state.SetMeta(\"processing_time\", time.Now().Format(time.RFC3339)) state.SetMeta(\"data_sources\", \"research,analysis\")\rError Handling Graceful Error Management func (a *MyAgent) Run(ctx context.Context, event agentflow.Event, state agentflow.State) (agentflow.AgentResult, error) { // Validate inputs message, ok := event.GetData()[\"message\"] if !ok { return agentflow.AgentResult{}, fmt.Errorf(\"missing required field: message\") } // Handle LLM errors response, err := a.llm.Generate(ctx, prompt) if err != nil { // Log error for debugging agentflow.Logger().Error().Err(err).Msg(\"LLM generation failed\") // Return graceful fallback fallbackResponse := \"I apologize, but I'm having trouble processing your request right now. Please try again.\" state.Set(\"error\", err.Error()) state.Set(\"fallback_used\", true) return agentflow.AgentResult{ Result: fallbackResponse, State: state, }, nil // Don't propagate error, handle gracefully } // Handle tool execution errors toolResults := agentflow.ParseAndExecuteToolCalls(ctx, a.mcpManager, response) if len(toolResults) == 0 \u0026\u0026 strings.Contains(response, \"tool_call\") { // Tool call was attempted but failed agentflow.Logger().Warn().Msg(\"Tool calls failed, proceeding without tools\") // Continue with original response } return agentflow.AgentResult{Result: response, State: state}, nil }\rTesting Agents Unit Testing Agent Logic package main import ( \"context\" \"testing\" agentflow \"github.com/kunalkushwaha/agenticgokit/core\" ) func TestMyAgent(t *testing.T) { // Setup mockLLM := \u0026MockModelProvider{} mockMCP := \u0026MockMCPManager{} agent := NewMyAgent(\"test-agent\", mockLLM, mockMCP) // Create test event eventData := agentflow.EventData{\"message\": \"Hello, world!\"} event := agentflow.NewEvent(\"test\", eventData, nil) state := agentflow.NewState() // Execute result, err := agent.Run(context.Background(), event, state) // Assert if err != nil { t.Fatalf(\"Expected no error, got %v\", err) } if result.Result == \"\" { t.Error(\"Expected non-empty result\") } // Check state was updated response, exists := result.State.Get(\"response\") if !exists { t.Error(\"Expected response to be set in state\") } if response != result.Result { t.Error(\"State response should match result\") } } // Mock implementations for testing type MockModelProvider struct{} func (m *MockModelProvider) Generate(ctx context.Context, prompt string) (string, error) { return \"Mock response\", nil } func (m *MockModelProvider) Name() string { return \"mock\" } type MockMCPManager struct{} func (m *MockMCPManager) ListTools(ctx context.Context) ([]agentflow.ToolSchema, error) { return nil, nil } func (m *MockMCPManager) CallTool(ctx context.Context, name string, args map[string]interface{}) (interface{}, error) { return nil, nil } // ... implement other required methods\rNext Steps Tool Integration - Learn how to use MCP tools LLM Providers - Configure different LLM providers Multi-Agent Workflows - Orchestrate multiple agents Production Deployment - Scale your agents",
    "description": "Agent Basics Understanding the AgenticGoKit Agent System\nThis guide covers the fundamental concepts of building agents in AgenticGoKit, from the basic interfaces to advanced multi-agent orchestration patterns.\nCore Concepts AgentHandler Interface The AgentHandler is the primary interface for implementing agent logic in AgenticGoKit:\ntype AgentHandler interface { Run(ctx context.Context, event Event, state State) (AgentResult, error) }\rKey Components:\nEvent: Contains the user input and metadata State: Thread-safe storage for agent data AgentResult: The agent’s response and updated state Multi-Agent Orchestration AgenticGoKit supports multiple orchestration patterns for coordinating agents:",
    "tags": [],
    "title": "AgentBasics",
    "uri": "/AgenticGoKitDocs/guides/agentbasics/index.html"
  },
  {
    "breadcrumb": "content \u003e guides",
    "content": "AgenticGoKit Examples This guide provides practical examples of building AI agents and workflows with AgenticGoKit, from simple single-agent applications to complex multi-agent orchestrations.\nTable of Contents Quick Start Examples Single Agent Examples Multi-Agent Workflows Tool Integration Examples Production Examples Custom Provider Examples Quick Start Examples Simple Query Agent (5 minutes) The fastest way to create an agent that can answer questions:\npackage main import ( \"context\" \"fmt\" \"log\" \"github.com/kunalkushwaha/agenticgokit/core\" ) func main() { // Initialize MCP for tool discovery core.QuickStartMCP() // Create LLM provider (using mock for this example) llm := \u0026core.MockLLM{} // Create an agent agent, err := core.NewMCPAgent(\"helper\", llm) if err != nil { log.Fatal(err) } // Create state with query state := core.NewState() state.Set(\"query\", \"What is the capital of France?\") // Run agent result, err := agent.Run(context.Background(), state) if err != nil { log.Fatal(err) } fmt.Println(\"Response:\", result.GetResult()) }\rMulti-Agent Orchestration (Quick Start) Generate complete multi-agent workflows with the CLI:\n# Collaborative workflow - all agents work in parallel agentcli create research-system \\ --orchestration-mode collaborative \\ --collaborative-agents \"researcher,analyzer,validator\" \\ --visualize \\ --mcp-enabled # Sequential pipeline - agents process one after another agentcli create data-pipeline \\ --orchestration-mode sequential \\ --sequential-agents \"collector,processor,formatter\" \\ --visualize # Loop-based workflow - single agent repeats with conditions agentcli create quality-loop \\ --orchestration-mode loop \\ --loop-agent \"quality-checker\" \\ --max-iterations 5 \\ --visualize\rUsing CLI Scaffolding Generate a complete project in seconds:\n# Create a new project with mixed orchestration agentcli create my-ai-app \\ --orchestration-mode mixed \\ --collaborative-agents \"analyzer,validator\" \\ --sequential-agents \"processor,reporter\" \\ --visualize-output \"docs/diagrams\" \\ --mcp-enabled cd my-ai-app # Run with any query go run . -m \"analyze market trends and generate comprehensive report\"\rThe generated project includes:\nMulti-agent orchestration configuration Automatic workflow visualization (Mermaid diagrams) MCP tool integration Error handling and fault tolerance Logging and tracing Production-ready structure Single Agent Examples Research Agent An agent that searches for information and provides summaries:\npackage main import ( \"context\" \"fmt\" \"log\" \"github.com/kunalkushwaha/agentflow/core\" ) type ResearchAgent struct { agent core.Agent } func NewResearchAgent(llm core.ModelProvider) (*ResearchAgent, error) { agent, err := core.NewMCPAgent(\"researcher\", llm) if err != nil { return nil, err } return \u0026ResearchAgent{agent: agent}, nil } func (r *ResearchAgent) Research(ctx context.Context, topic string) (string, error) { state := core.NewState() state.Set(\"query\", fmt.Sprintf(\"Research the topic: %s. Provide a comprehensive summary with key points.\", topic)) result, err := r.agent.Run(ctx, state) if err != nil { return \"\", err } return result.GetResult(), nil } func main() { // Initialize with real LLM provider config := core.LLMConfig{ Provider: \"azure-openai\", APIKey: \"your-api-key\", BaseURL: \"https://your-resource.openai.azure.com\", } llm := core.NewAzureOpenAIAdapter(config) // Create research agent researcher, err := NewResearchAgent(llm) if err != nil { log.Fatal(err) } // Conduct research summary, err := researcher.Research(context.Background(), \"quantum computing applications\") if err != nil { log.Fatal(err) } fmt.Println(\"Research Summary:\", summary) }\rData Analysis Agent An agent specialized for analyzing and processing data:\ntype DataAnalysisAgent struct { agent core.Agent } func (d *DataAnalysisAgent) AnalyzeData(ctx context.Context, data map[string]interface{}) (*AnalysisResult, error) { state := core.NewState() state.Set(\"task\", \"data_analysis\") state.Set(\"data\", data) state.Set(\"query\", \"Analyze this data and provide insights, trends, and recommendations.\") result, err := d.agent.Run(ctx, state) if err != nil { return nil, err } return \u0026AnalysisResult{ Summary: result.GetResult(), Confidence: result.GetFloat(\"confidence\"), Recommendations: result.GetStringSlice(\"recommendations\"), }, nil } type AnalysisResult struct { Summary string Confidence float64 Recommendations []string }\rMulti-Agent Workflows Collaborative Research System All agents work in parallel to process the same task:\npackage main import ( \"context\" \"fmt\" \"log\" \"time\" \"github.com/kunalkushwaha/agentflow/core\" ) func main() { // Create specialized agents agents := map[string]core.AgentHandler{ \"researcher\": NewResearchAgent(), \"analyzer\": NewAnalysisAgent(), \"validator\": NewValidationAgent(), } // Create collaborative orchestration runner := core.NewOrchestrationBuilder(core.OrchestrationCollaborate). WithAgents(agents). WithTimeout(2 * time.Minute). WithFailureThreshold(0.8). WithMaxConcurrency(10). Build() // Create event event := core.NewEvent(\"all\", map[string]interface{}{ \"task\": \"research AI trends and provide comprehensive analysis\", }, nil) // All agents process in parallel result, err := runner.Run(context.Background(), event) if err != nil { log.Fatal(err) } fmt.Printf(\"Collaborative Result: %s\\n\", result.GetResult()) }\rSequential Data Pipeline Agents process data in sequence, each building on the previous result:\npackage main import ( \"context\" \"fmt\" \"log\" \"time\" \"github.com/kunalkushwaha/agentflow/core\" ) func main() { // Create pipeline agents agents := map[string]core.AgentHandler{ \"collector\": NewDataCollectorAgent(), \"processor\": NewDataProcessorAgent(), \"formatter\": NewDataFormatterAgent(), } // Create sequential orchestration runner := core.NewOrchestrationBuilder(core.OrchestrationSequential). WithAgents(agents). WithTimeout(5 * time.Minute). Build() // Create pipeline event event := core.NewEvent(\"pipeline\", map[string]interface{}{ \"data_source\": \"market_data\", \"format\": \"json\", }, nil) // Process through pipeline result, err := runner.Run(context.Background(), event) if err != nil { log.Fatal(err) } fmt.Printf(\"Pipeline Result: %s\\n\", result.GetResult()) }\rLoop-Based Quality Checker Single agent repeats execution until quality standards are met:\npackage main import ( \"context\" \"fmt\" \"log\" \"time\" \"github.com/kunalkushwaha/agentflow/core\" ) func main() { // Create quality checker agent agents := map[string]core.AgentHandler{ \"quality-checker\": NewQualityCheckerAgent(), } // Create loop orchestration runner := core.NewOrchestrationBuilder(core.OrchestrationLoop). WithAgents(agents). WithMaxIterations(10). WithTimeout(10 * time.Minute). Build() // Create quality check event event := core.NewEvent(\"loop\", map[string]interface{}{ \"content\": \"document to check\", \"quality_threshold\": 0.95, }, nil) // Loop until quality is met result, err := runner.Run(context.Background(), event) if err != nil { log.Fatal(err) } fmt.Printf(\"Quality Check Result: %s\\n\", result.GetResult()) }\rMixed Orchestration Workflow Combine collaborative and sequential patterns:\npackage main import ( \"context\" \"fmt\" \"log\" \"time\" \"github.com/kunalkushwaha/agentflow/core\" ) func main() { // Collaborative agents (parallel processing) collaborativeAgents := map[string]core.AgentHandler{ \"analyzer\": NewAnalyzerAgent(), \"validator\": NewValidatorAgent(), } // Sequential agents (pipeline processing) sequentialAgents := map[string]core.AgentHandler{ \"processor\": NewProcessorAgent(), \"reporter\": NewReporterAgent(), } // Create mixed orchestration runner := core.NewOrchestrationBuilder(core.OrchestrationMixed). WithCollaborativeAgents(collaborativeAgents). WithSequentialAgents(sequentialAgents). WithTimeout(8 * time.Minute). WithFailureThreshold(0.8). Build() // Create mixed workflow event event := core.NewEvent(\"mixed\", map[string]interface{}{ \"task\": \"analyze data, validate results, process, and generate report\", }, nil) // Execute mixed workflow result, err := runner.Run(context.Background(), event) if err != nil { log.Fatal(err) } fmt.Printf(\"Mixed Workflow Result: %s\\n\", result.GetResult()) }\rWorkflow Visualization Generate Mermaid diagrams for any orchestration:\npackage main import ( \"fmt\" \"os\" \"path/filepath\" \"github.com/kunalkushwaha/agentflow/core\" ) func main() { // Create agents agents := map[string]core.AgentHandler{ \"researcher\": NewResearchAgent(), \"analyzer\": NewAnalysisAgent(), \"validator\": NewValidationAgent(), } // Create orchestration builder builder := core.NewOrchestrationBuilder(core.OrchestrationCollaborate). WithAgents(agents). WithTimeout(2 * time.Minute). WithFailureThreshold(0.8) // Generate Mermaid diagram diagram := builder.GenerateMermaidDiagram() // Save to file outputDir := \"docs/diagrams\" os.MkdirAll(outputDir, 0755) filename := filepath.Join(outputDir, \"workflow.mmd\") err := os.WriteFile(filename, []byte(diagram), 0644) if err != nil { fmt.Printf(\"Error saving diagram: %v\\n\", err) return } fmt.Printf(\"Workflow diagram saved to: %s\\n\", filename) fmt.Println(\"Diagram content:\") fmt.Println(diagram) }\rResearch and Analysis Pipeline A workflow where one agent researches and another analyzes:\npackage main import ( \"context\" \"fmt\" \"log\" \"github.com/kunalkushwaha/agentflow/core\" ) func main() { // Initialize MCP core.QuickStartMCP() // Create LLM provider llm := core.NewAzureOpenAIAdapter(core.LLMConfig{ Provider: \"azure-openai\", APIKey: \"your-api-key\", BaseURL: \"https://your-resource.openai.azure.com\", }) // Create agents researcher, _ := core.NewMCPAgent(\"researcher\", llm) analyst, _ := core.NewMCPAgent(\"analyst\", llm) // Create workflow runner runner := core.NewSequentialRunner() runner.AddAgent(researcher) runner.AddAgent(analyst) // Create initial state state := core.NewState() state.Set(\"topic\", \"artificial intelligence trends 2024\") // Run workflow ctx := context.Background() // Step 1: Research state.Set(\"query\", \"Research the latest trends in artificial intelligence for 2024\") result1, err := researcher.Run(ctx, state) if err != nil { log.Fatal(err) } // Step 2: Analysis state.Set(\"research_data\", result1.GetResult()) state.Set(\"query\", \"Analyze the research data and provide key insights and predictions\") result2, err := analyst.Run(ctx, state) if err != nil { log.Fatal(err) } fmt.Println(\"Final Analysis:\", result2.GetResult()) }\rParallel Processing Workflow Process multiple tasks concurrently:\nfunc parallelWorkflow() { // Create parallel runner runner := core.NewParallelRunner() // Add multiple agents for different tasks webAgent, _ := core.NewMCPAgent(\"web-searcher\", llm) dbAgent, _ := core.NewMCPAgent(\"database-query\", llm) fileAgent, _ := core.NewMCPAgent(\"file-processor\", llm) runner.AddAgent(webAgent) runner.AddAgent(dbAgent) runner.AddAgent(fileAgent) // Create states for each agent states := []core.State{ core.NewStateWithQuery(\"search web for latest news\"), core.NewStateWithQuery(\"query database for user statistics\"), core.NewStateWithQuery(\"process uploaded files\"), } // Run all agents in parallel results, err := runner.RunParallel(context.Background(), states) if err != nil { log.Fatal(err) } // Process results for i, result := range results { fmt.Printf(\"Agent %d result: %s\\n\", i+1, result.GetResult()) } }\rConditional Workflow Route to different agents based on input:\ntype ConditionalWorkflow struct { router core.Agent classifier core.Agent processor core.Agent validator core.Agent } func (w *ConditionalWorkflow) Process(ctx context.Context, input string) (string, error) { state := core.NewState() state.Set(\"input\", input) // Step 1: Classify input type state.Set(\"query\", \"Classify this input type: text, code, data, or question\") classification, err := w.classifier.Run(ctx, state) if err != nil { return \"\", err } inputType := classification.GetString(\"classification\") // Step 2: Route to appropriate processor state.Set(\"input_type\", inputType) switch inputType { case \"code\": state.Set(\"query\", \"Analyze and review this code\") return w.processWithAgent(ctx, w.processor, state) case \"question\": state.Set(\"query\", \"Answer this question comprehensively\") return w.processWithAgent(ctx, w.processor, state) default: state.Set(\"query\", \"Process this general input\") return w.processWithAgent(ctx, w.processor, state) } } func (w *ConditionalWorkflow) processWithAgent(ctx context.Context, agent core.Agent, state core.State) (string, error) { result, err := agent.Run(ctx, state) if err != nil { return \"\", err } // Validate result state.Set(\"result_to_validate\", result.GetResult()) state.Set(\"query\", \"Validate this result for accuracy and completeness\") validation, err := w.validator.Run(ctx, state) if err != nil { return result.GetResult(), nil // Return original if validation fails } if validation.GetBool(\"is_valid\") { return result.GetResult(), nil } return \"Result validation failed: \" + validation.GetString(\"reason\"), nil }\rTool Integration Examples Web Search Integration Using MCP tools for web search:\nfunc webSearchExample() { // Initialize MCP with web search tools core.QuickStartMCP() agent, _ := core.NewMCPAgent(\"web-searcher\", llm) state := core.NewState() state.Set(\"query\", \"Search for the latest Docker tutorials and summarize the top 3 results\") result, err := agent.Run(context.Background(), state) if err != nil { log.Fatal(err) } // The agent automatically uses web search tools fmt.Println(\"Search Results:\", result.GetResult()) }\rDatabase Integration Connect to databases through MCP:\nfunc databaseExample() { agent, _ := core.NewMCPAgent(\"db-agent\", llm) state := core.NewState() state.Set(\"query\", \"Query the users table and find all active users created in the last 30 days\") // Agent will use database MCP tools automatically result, err := agent.Run(context.Background(), state) if err != nil { log.Fatal(err) } fmt.Println(\"Database Results:\", result.GetResult()) }\rFile Processing Process files using MCP tools:\nfunc fileProcessingExample() { agent, _ := core.NewMCPAgent(\"file-processor\", llm) state := core.NewState() state.Set(\"file_path\", \"/path/to/document.pdf\") state.Set(\"query\", \"Extract key information from this PDF document and summarize it\") result, err := agent.Run(context.Background(), state) if err != nil { log.Fatal(err) } fmt.Println(\"File Summary:\", result.GetResult()) }\rProduction Examples Error Handling and Retry Robust error handling for production systems:\ntype ProductionAgent struct { agent core.Agent retryCount int timeout time.Duration } func (p *ProductionAgent) ProcessWithRetry(ctx context.Context, query string) (string, error) { state := core.NewState() state.Set(\"query\", query) for attempt := 0; attempt \u003c p.retryCount; attempt++ { // Set timeout context ctxWithTimeout, cancel := context.WithTimeout(ctx, p.timeout) defer cancel() result, err := p.agent.Run(ctxWithTimeout, state) if err == nil { return result.GetResult(), nil } // Log retry attempt log.Printf(\"Attempt %d failed: %v\", attempt+1, err) // Exponential backoff time.Sleep(time.Duration(attempt+1) * time.Second) } return \"\", fmt.Errorf(\"failed after %d attempts\", p.retryCount) }\rMonitoring and Observability Add comprehensive monitoring:\nfunc monitoredWorkflow() { // Create runner with monitoring runner := core.NewSequentialRunner() // Register callbacks for monitoring runner.RegisterCallback(core.HookBeforeAgentRun, func(ctx context.Context, state core.State) { log.Printf(\"Starting agent run: %s\", state.Get(\"agent_name\")) }) runner.RegisterCallback(core.HookAfterAgentRun, func(ctx context.Context, result core.Result) { metrics := result.GetMetrics() log.Printf(\"Agent completed: duration=%v, tokens=%d\", metrics.Duration, metrics.TokensUsed) }) runner.RegisterCallback(core.HookOnError, func(ctx context.Context, err error) { log.Printf(\"Agent error: %v\", err) // Send to monitoring system }) // Run workflow with monitoring result, err := runner.Run(context.Background(), state) if err != nil { log.Fatal(err) } // Get comprehensive traces traces, _ := runner.DumpTrace(\"session-123\") for _, trace := range traces { fmt.Printf(\"Trace: %+v\\n\", trace) } }\rCaching for Performance Implement caching for better performance:\nfunc cachedAgent() { // Create agent with caching enabled config := core.AgentConfig{ CacheEnabled: true, CacheTTL: time.Hour, CacheProvider: \"redis\", } agent := core.NewAgentBuilder(\"cached-agent\"). WithLLM(llm). WithConfig(config). WithMCP(). WithCache(). Build() // First call - will hit LLM start := time.Now() result1, _ := agent.Run(context.Background(), state) duration1 := time.Since(start) fmt.Printf(\"First call: %v\\n\", duration1) // Second call - will hit cache start = time.Now() result2, _ := agent.Run(context.Background(), state) duration2 := time.Since(start) fmt.Printf(\"Cached call: %v\\n\", duration2) // Results should be identical, but second call much faster fmt.Printf(\"Results match: %v\\n\", result1.GetResult() == result2.GetResult()) }\rCustom Provider Examples Mock Provider for Testing Create mock providers for unit tests:\ntype TestMockLLM struct { responses map[string]string } func (t *TestMockLLM) Generate(ctx context.Context, prompt string) (*core.LLMResponse, error) { if response, exists := t.responses[prompt]; exists { return \u0026core.LLMResponse{ Content: response, TokensUsed: 100, }, nil } return \u0026core.LLMResponse{ Content: \"Mock response for: \" + prompt, TokensUsed: 100, }, nil } func TestAgentBehavior(t *testing.T) { // Create mock with predefined responses mock := \u0026TestMockLLM{ responses: map[string]string{ \"What is 2+2?\": \"The answer is 4.\", \"Hello\": \"Hello! How can I help you?\", }, } agent, err := core.NewMCPAgent(\"test-agent\", mock) require.NoError(t, err) state := core.NewState() state.Set(\"query\", \"What is 2+2?\") result, err := agent.Run(context.Background(), state) require.NoError(t, err) assert.Equal(t, \"The answer is 4.\", result.GetResult()) }\rCustom LLM Provider Implement your own LLM provider:\ntype CustomLLMProvider struct { apiKey string baseURL string client *http.Client } func (c *CustomLLMProvider) Generate(ctx context.Context, prompt string) (*core.LLMResponse, error) { // Implement your custom LLM API call request := CustomLLMRequest{ Prompt: prompt, MaxTokens: 1000, Temperature: 0.7, } response, err := c.callAPI(ctx, request) if err != nil { return nil, err } return \u0026core.LLMResponse{ Content: response.Text, TokensUsed: response.TokenCount, Model: response.Model, }, nil } func (c *CustomLLMProvider) callAPI(ctx context.Context, request CustomLLMRequest) (*CustomLLMResponse, error) { // Implement HTTP call to your LLM service // This is where you'd integrate with your specific LLM API return nil, nil } // Register and use custom provider func useCustomProvider() { provider := \u0026CustomLLMProvider{ apiKey: \"your-api-key\", baseURL: \"https://your-llm-service.com\", client: \u0026http.Client{Timeout: 30 * time.Second}, } agent, err := core.NewMCPAgent(\"custom-agent\", provider) if err != nil { log.Fatal(err) } // Use agent normally state := core.NewState() state.Set(\"query\", \"Your query here\") result, err := agent.Run(context.Background(), state) if err != nil { log.Fatal(err) } fmt.Println(\"Result:\", result.GetResult()) }\rBest Practices 1. Agent Specialization Create specialized agents for specific tasks:\n// Good: Specialized agents researchAgent := createResearchAgent() // Web search, data gathering analysisAgent := createAnalysisAgent() // Data processing, insights summaryAgent := createSummaryAgent() // Final presentation // Avoid: Generic \"do everything\" agents\r2. State Management Use state effectively to pass data between agents:\n// Good: Clear state management state := core.NewState() state.Set(\"task_type\", \"research\") state.Set(\"topic\", \"AI trends\") state.Set(\"output_format\", \"summary\") // Avoid: Putting everything in the query\r3. Error Handling Always implement proper error handling:\n// Good: Comprehensive error handling result, err := agent.Run(ctx, state) if err != nil { // Log error log.Printf(\"Agent failed: %v\", err) // Implement fallback return fallbackResponse, nil } // Avoid: Ignoring errors\r4. Testing Write tests for your agents:\nfunc TestResearchAgent(t *testing.T) { mock := \u0026TestMockLLM{ responses: map[string]string{ \"research prompt\": \"research results\", }, } agent, err := NewResearchAgent(mock) require.NoError(t, err) result, err := agent.Research(context.Background(), \"test topic\") require.NoError(t, err) assert.Contains(t, result, \"research results\") }\rNext Steps Production Guide - Deploy your agents to production Error Handling - Advanced error handling strategies Performance Guide - Optimize your agent performance Custom Tools - Build your own MCP tools API Reference - Complete API documentation Example Projects Check out the /examples directory for complete working projects:\nSimple Agent - Basic single-agent application Multi-Agent Workflow - Research and analysis pipeline Production App - Enterprise-ready agent system Custom Tools - MCP server implementation Performance Demo - Optimized high-throughput system Each example includes:\nComplete source code Configuration files Documentation Test cases Deployment instructions",
    "description": "AgenticGoKit Examples This guide provides practical examples of building AI agents and workflows with AgenticGoKit, from simple single-agent applications to complex multi-agent orchestrations.\nTable of Contents Quick Start Examples Single Agent Examples Multi-Agent Workflows Tool Integration Examples Production Examples Custom Provider Examples Quick Start Examples Simple Query Agent (5 minutes) The fastest way to create an agent that can answer questions:\npackage main import ( \"context\" \"fmt\" \"log\" \"github.com/kunalkushwaha/agenticgokit/core\" ) func main() { // Initialize MCP for tool discovery core.QuickStartMCP() // Create LLM provider (using mock for this example) llm := \u0026core.MockLLM{} // Create an agent agent, err := core.NewMCPAgent(\"helper\", llm) if err != nil { log.Fatal(err) } // Create state with query state := core.NewState() state.Set(\"query\", \"What is the capital of France?\") // Run agent result, err := agent.Run(context.Background(), state) if err != nil { log.Fatal(err) } fmt.Println(\"Response:\", result.GetResult()) }\rMulti-Agent Orchestration (Quick Start) Generate complete multi-agent workflows with the CLI:",
    "tags": [],
    "title": "Examples",
    "uri": "/AgenticGoKitDocs/guides/examples/index.html"
  },
  {
    "breadcrumb": "content \u003e reference",
    "content": "Configuration API System configuration and setup\nThis document covers AgenticGoKit’s Configuration API, which provides comprehensive configuration management for agents, orchestration, memory systems, and MCP integration. The configuration system uses TOML files and provides validation and defaults.\n📋 Core Configuration Types Main Configuration Structure The main configuration structure loaded from agentflow.toml:\ntype Config struct { AgentFlow struct { Name string `toml:\"name\"` Version string `toml:\"version\"` Provider string `toml:\"provider\"` } `toml:\"agent_flow\"` Logging struct { Level string `toml:\"level\"` Format string `toml:\"format\"` } `toml:\"logging\"` Runtime struct { MaxConcurrentAgents int `toml:\"max_concurrent_agents\"` TimeoutSeconds int `toml:\"timeout_seconds\"` } `toml:\"runtime\"` // Agent memory configuration AgentMemory AgentMemoryConfig `toml:\"agent_memory\"` // Error routing configuration ErrorRouting struct { Enabled bool `toml:\"enabled\"` MaxRetries int `toml:\"max_retries\"` RetryDelayMs int `toml:\"retry_delay_ms\"` EnableCircuitBreaker bool `toml:\"enable_circuit_breaker\"` ErrorHandlerName string `toml:\"error_handler_name\"` CategoryHandlers map[string]string `toml:\"category_handlers\"` SeverityHandlers map[string]string `toml:\"severity_handlers\"` CircuitBreaker CircuitBreakerConfigToml `toml:\"circuit_breaker\"` Retry RetryConfigToml `toml:\"retry\"` } `toml:\"error_routing\"` // Provider-specific configurations Providers map[string]map[string]interface{} `toml:\"providers\"` // MCP configuration MCP MCPConfigToml `toml:\"mcp\"` // Orchestration configuration Orchestration OrchestrationConfigToml `toml:\"orchestration\"` }\rMemory Configuration Configuration for memory systems and RAG capabilities:\ntype AgentMemoryConfig struct { // Core memory settings Provider string `toml:\\\"provider\\\"` // pgvector, weaviate, memory Connection string `toml:\\\"connection\\\"` // Connection string MaxResults int `toml:\\\"max_results\\\"` // Maximum search results Dimensions int `toml:\\\"dimensions\\\"` // Vector dimensions AutoEmbed bool `toml:\\\"auto_embed\\\"` // Auto-generate embeddings // RAG settings EnableKnowledgeBase bool `toml:\\\"enable_knowledge_base\\\"` KnowledgeMaxResults int `toml:\\\"knowledge_max_results\\\"` KnowledgeScoreThreshold float32 `toml:\\\"knowledge_score_threshold\\\"` ChunkSize int `toml:\\\"chunk_size\\\"` ChunkOverlap int `toml:\\\"chunk_overlap\\\"` // RAG context assembly EnableRAG bool `toml:\\\"enable_rag\\\"` RAGMaxContextTokens int `toml:\\\"rag_max_context_tokens\\\"` RAGPersonalWeight float32 `toml:\\\"rag_personal_weight\\\"` RAGKnowledgeWeight float32 `toml:\\\"rag_knowledge_weight\\\"` RAGIncludeSources bool `toml:\\\"rag_include_sources\\\"` // Document processing Documents DocumentConfig `toml:\\\"documents\\\"` // Embedding service Embedding EmbeddingConfig `toml:\\\"embedding\\\"` // Search configuration Search SearchConfigToml `toml:\\\"search\\\"` }\rMCP Configuration Configuration for Model Context Protocol integration:\ntype MCPConfigToml struct { Enabled bool `toml:\"enabled\"` EnableDiscovery bool `toml:\"enable_discovery\"` DiscoveryTimeout int `toml:\"discovery_timeout_ms\"` ScanPorts []int `toml:\"scan_ports\"` ConnectionTimeout int `toml:\"connection_timeout_ms\"` MaxRetries int `toml:\"max_retries\"` RetryDelay int `toml:\"retry_delay_ms\"` EnableCaching bool `toml:\"enable_caching\"` CacheTimeout int `toml:\"cache_timeout_ms\"` MaxConnections int `toml:\"max_connections\"` Servers []MCPServerConfigToml `toml:\"servers\"` } type MCPServerConfigToml struct { Name string `toml:\"name\"` Type string `toml:\"type\"` // tcp, stdio, docker, websocket Host string `toml:\"host,omitempty\"` Port int `toml:\"port,omitempty\"` Command string `toml:\"command,omitempty\"` // for stdio transport Enabled bool `toml:\"enabled\"` }\r🚀 Basic Configuration TOML Configuration Files AgentFlow supports TOML configuration files for easy setup:\n# agentflow.toml - Basic Configuration [agent_flow] name = \"my-agent-system\" version = \"1.0.0\" provider = \"azure\" [logging] level = \"info\" format = \"json\" [runtime] max_concurrent_agents = 10 timeout_seconds = 30 [providers.azure] # API key will be read from AZURE_OPENAI_API_KEY environment variable # Endpoint will be read from AZURE_OPENAI_ENDPOINT environment variable # Deployment will be read from AZURE_OPENAI_DEPLOYMENT environment variable [providers.openai] # API key will be read from OPENAI_API_KEY environment variable [providers.ollama] endpoint = \"http://localhost:11434\" model = \"llama2\" [providers.mock] # Mock provider for testing - no configuration needed\rWith Memory and RAG Configuration:\n# agentflow.toml - With Memory and RAG [agent_flow] name = \"my-rag-system\" version = \"1.0.0\" provider = \"azure\" [logging] level = \"info\" format = \"text\" [runtime] max_concurrent_agents = 5 timeout_seconds = 30 # RAG-Enhanced Memory Configuration [agent_memory] # Core memory settings provider = \"memory\" # Options: memory, pgvector, weaviate connection = \"memory\" # Connection string (for database providers) max_results = 10 # Maximum results per query dimensions = 1536 # Embedding dimensions auto_embed = true # Automatic embedding generation # RAG-enhanced settings enable_knowledge_base = true # Enable knowledge base functionality knowledge_max_results = 20 # Maximum results from knowledge base knowledge_score_threshold = 0.7 # Minimum relevance score for results chunk_size = 1000 # Document chunk size in characters chunk_overlap = 200 # Overlap between chunks in characters # RAG context assembly enable_rag = true # Enable RAG context building rag_max_context_tokens = 4000 # Maximum tokens in assembled context rag_personal_weight = 0.3 # Weight for personal memory (0.0-1.0) rag_knowledge_weight = 0.7 # Weight for knowledge base (0.0-1.0) rag_include_sources = true # Include source attribution in context # Document processing [agent_memory.documents] auto_chunk = true # Automatically chunk large documents supported_types = [\"pdf\", \"txt\", \"md\", \"web\", \"code\", \"json\"] # Supported file types max_file_size = \"10MB\" # Maximum file size for processing enable_metadata_extraction = true # Extract metadata from documents enable_url_scraping = true # Enable web scraping for URLs # Embedding service [agent_memory.embedding] provider = \"azure\" # Options: azure, openai, local model = \"text-embedding-ada-002\" # Embedding model to use cache_embeddings = true # Cache embeddings for performance max_batch_size = 100 # Maximum batch size for embeddings timeout_seconds = 30 # Request timeout in seconds # Search configuration [agent_memory.search] hybrid_search = true # Enable hybrid search (semantic + keyword) keyword_weight = 0.3 # Weight for keyword search (BM25) semantic_weight = 0.7 # Weight for semantic search (vector similarity) enable_reranking = false # Enable advanced re-ranking enable_query_expansion = false # Enable query expansion for better results [providers.azure] # API key will be read from AZURE_OPENAI_API_KEY environment variable # Endpoint will be read from AZURE_OPENAI_ENDPOINT environment variable # Deployment will be read from AZURE_OPENAI_DEPLOYMENT environment variable\rWith MCP Integration:\n# agentflow.toml - With MCP Integration [agent_flow] name = \"my-mcp-system\" version = \"1.0.0\" provider = \"azure\" [logging] level = \"info\" format = \"json\" [runtime] max_concurrent_agents = 10 timeout_seconds = 30 [providers.azure] # API key will be read from AZURE_OPENAI_API_KEY environment variable # Endpoint will be read from AZURE_OPENAI_ENDPOINT environment variable # Deployment will be read from AZURE_OPENAI_DEPLOYMENT environment variable [mcp] enabled = true enable_discovery = true connection_timeout = 5000 max_retries = 3 retry_delay = 1000 enable_caching = true cache_timeout = 300000 max_connections = 10 # Example MCP servers - configure as needed [[mcp.servers]] name = \"docker\" type = \"tcp\" host = \"localhost\" port = 8811 enabled = false [[mcp.servers]] name = \"filesystem\" type = \"stdio\" command = \"npx @modelcontextprotocol/server-filesystem /path/to/allowed/files\" enabled = false [[mcp.servers]] name = \"brave-search\" type = \"stdio\" command = \"npx @modelcontextprotocol/server-brave-search\" enabled = false\rLoading Configuration package main import ( \"fmt\" \"github.com/kunalkushwaha/agenticgokit/core\" ) func loadConfigurationExample() { // Load configuration from TOML file config, err := core.LoadConfig(\"agentflow.toml\") if err != nil { panic(fmt.Sprintf(\"Failed to load configuration: %v\", err)) } // Or load from working directory (looks for agentflow.toml) config, err = core.LoadConfigFromWorkingDir() if err != nil { panic(fmt.Sprintf(\"Failed to load configuration: %v\", err)) } // Apply logging configuration config.ApplyLoggingConfig() // Initialize LLM provider from configuration provider, err := config.InitializeProvider() if err != nil { panic(fmt.Sprintf(\"Failed to initialize provider: %v\", err)) } // Initialize memory if configured if config.AgentMemory.Provider != \"\" { memory, err := core.NewMemory(config.AgentMemory) if err != nil { panic(fmt.Sprintf(\"Failed to initialize memory: %v\", err)) } fmt.Printf(\"Memory initialized with provider: %s\\n\", config.AgentMemory.Provider) } // Initialize MCP if enabled if config.MCP.Enabled { mcpConfig := config.GetMCPConfig() // MCP initialization would be handled by the framework fmt.Println(\"MCP configuration loaded\") } fmt.Println(\"System initialized with configuration\") }\rEnvironment Variables AgenticGoKit supports environment variables for sensitive configuration like API keys:\n# LLM Provider Configuration export OPENAI_API_KEY=your-openai-key export AZURE_OPENAI_API_KEY=your-azure-key export AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/ export AZURE_OPENAI_DEPLOYMENT=your-deployment-name # Database Configuration (for memory providers) export DATABASE_URL=postgres://user:password@localhost:5432/agentflow # Optional: Override default settings export AGENTFLOW_LOG_LEVEL=debug export AGENTFLOW_TIMEOUT=60\rThese environment variables are automatically used when not specified in the TOML file.\n🔧 Configuration Examples Development vs Production Development Configuration (dev.toml):\n[agent_flow] name = \"my-dev-agent\" version = \"1.0.0\" provider = \"mock\" # Use mock provider for development [logging] level = \"debug\" format = \"text\" [runtime] max_concurrent_agents = 2 timeout_seconds = 10 [providers.mock] # Mock provider for testing - no configuration needed [agent_memory] provider = \"memory\" # In-memory for fast development connection = \"memory\" max_results = 5 dimensions = 384 # Smaller for faster testing [agent_memory.embedding] provider = \"dummy\" # No API calls in development\rProduction Configuration (prod.toml):\n[agent_flow] name = \"my-prod-agent\" version = \"1.0.0\" provider = \"azure\" [logging] level = \"info\" format = \"json\" [runtime] max_concurrent_agents = 20 timeout_seconds = 60 [providers.azure] # API key will be read from AZURE_OPENAI_API_KEY environment variable # Endpoint will be read from AZURE_OPENAI_ENDPOINT environment variable # Deployment will be read from AZURE_OPENAI_DEPLOYMENT environment variable [agent_memory] provider = \"pgvector\" connection = \"postgres://user:pass@db:5432/agentflow\" max_results = 20 dimensions = 1536 enable_knowledge_base = true enable_rag = true [agent_memory.embedding] provider = \"azure\" model = \"text-embedding-ada-002\" cache_embeddings = true [mcp] enabled = true enable_discovery = false # Use explicit servers in production max_connections = 20 [[mcp.servers]] name = \"search-service\" type = \"tcp\" host = \"search-service\" port = 8080 enabled = true\rMultiple Configuration Files You can use different configuration files for different environments:\n# Development go run . --config dev.toml # Production go run . --config prod.toml # Or use environment variable export AGENTFLOW_CONFIG=prod.toml go run .\r🔧 Configuration Validation AgenticGoKit automatically validates your configuration when loading. Common validation errors:\nInvalid provider: Must be one of openai, azure, ollama Missing API keys: Set environment variables for your chosen provider Invalid memory provider: Must be one of memory, pgvector, weaviate Invalid orchestration mode: Must be one of route, collaborative, sequential, loop, mixed # Test your configuration go run . --validate-config # Or check configuration loading go run . --dry-run\r📚 Best Practices 1. Keep It Simple Use agentflow.toml for all configuration Store sensitive data in environment variables Use different config files for different environments 2. Security Never commit API keys to version control Use environment variables for secrets: export OPENAI_API_KEY=your-key export DATABASE_URL=postgres://...\r3. Environment Management # Development cp agentflow.dev.toml agentflow.toml # Production cp agentflow.prod.toml agentflow.toml\r4. Common Configurations Minimal Configuration:\n[agent_flow] name = \"my-agent\" provider = \"openai\"\rFull-Featured Configuration:\n[agent_flow] name = \"my-agent\" provider = \"azure\" [agent_memory] provider = \"pgvector\" enable_rag = true [mcp] enabled = true [orchestration] mode = \"collaborative\"\rThis covers the essential configuration patterns you’ll need for AgenticGoKit systems.",
    "description": "Configuration API System configuration and setup\nThis document covers AgenticGoKit’s Configuration API, which provides comprehensive configuration management for agents, orchestration, memory systems, and MCP integration. The configuration system uses TOML files and provides validation and defaults.\n📋 Core Configuration Types Main Configuration Structure The main configuration structure loaded from agentflow.toml:\ntype Config struct { AgentFlow struct { Name string `toml:\"name\"` Version string `toml:\"version\"` Provider string `toml:\"provider\"` } `toml:\"agent_flow\"` Logging struct { Level string `toml:\"level\"` Format string `toml:\"format\"` } `toml:\"logging\"` Runtime struct { MaxConcurrentAgents int `toml:\"max_concurrent_agents\"` TimeoutSeconds int `toml:\"timeout_seconds\"` } `toml:\"runtime\"` // Agent memory configuration AgentMemory AgentMemoryConfig `toml:\"agent_memory\"` // Error routing configuration ErrorRouting struct { Enabled bool `toml:\"enabled\"` MaxRetries int `toml:\"max_retries\"` RetryDelayMs int `toml:\"retry_delay_ms\"` EnableCircuitBreaker bool `toml:\"enable_circuit_breaker\"` ErrorHandlerName string `toml:\"error_handler_name\"` CategoryHandlers map[string]string `toml:\"category_handlers\"` SeverityHandlers map[string]string `toml:\"severity_handlers\"` CircuitBreaker CircuitBreakerConfigToml `toml:\"circuit_breaker\"` Retry RetryConfigToml `toml:\"retry\"` } `toml:\"error_routing\"` // Provider-specific configurations Providers map[string]map[string]interface{} `toml:\"providers\"` // MCP configuration MCP MCPConfigToml `toml:\"mcp\"` // Orchestration configuration Orchestration OrchestrationConfigToml `toml:\"orchestration\"` }\rMemory Configuration Configuration for memory systems and RAG capabilities:",
    "tags": [],
    "title": "configuration",
    "uri": "/AgenticGoKitDocs/reference/api/configuration/index.html"
  },
  {
    "breadcrumb": "content \u003e guides",
    "content": "Configuration Management Managing AgentFlow Configuration with agentflow.toml\nAgentFlow uses TOML configuration files to manage all aspects of your agent system: LLM providers, MCP servers, multi-agent orchestration settings, workflow visualization, and more.\nBasic Configuration Structure agentflow.toml Template # Project metadata name = \"My Agent System\" version = \"1.0.0\" description = \"AgentFlow-powered agent workflow\" # Logging configuration log_level = \"info\" # debug, info, warn, error # Multi-Agent Orchestration Configuration [orchestration] mode = \"sequential\" # sequential, collaborative, loop, mixed, route timeout_seconds = 30 # Timeout for orchestration operations max_iterations = 5 # Maximum iterations for loop mode # Sequential mode: agents process in order sequential_agents = [\"agent1\", \"agent2\", \"agent3\"] # Collaborative mode: agents process in parallel collaborative_agents = [\"analyzer\", \"validator\", \"processor\"] # Loop mode: single agent repeats loop_agent = \"processor\" # Mixed mode: combine collaborative and sequential # collaborative_agents = [\"analyzer\", \"validator\"] # sequential_agents = [\"processor\", \"reporter\"] # Workflow Visualization [visualization] enabled = true output_dir = \"./docs/diagrams\" diagram_type = \"flowchart\" # flowchart, sequence, etc. direction = \"TD\" # TD, LR, BT, RL show_metadata = true show_agent_types = true # LLM Provider configuration [provider] type = \"azure\" # azure, openai, ollama, mock api_key = \"${AZURE_OPENAI_API_KEY}\" endpoint = \"https://your-resource.openai.azure.com\" deployment = \"gpt-4\" api_version = \"2024-02-15-preview\" model = \"gpt-4\" max_tokens = 2000 temperature = 0.7 timeout = \"30s\" # MCP (Model Context Protocol) configuration [mcp] enabled = true cache_enabled = true cache_ttl = \"5m\" connection_timeout = \"30s\" max_retries = 3 # MCP server definitions [mcp.servers.search] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-web-search\"] transport = \"stdio\" [mcp.servers.docker] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-docker\"] transport = \"stdio\" # Error handling and routing [error_routing] validation_errors = \"error_handler\" timeout_errors = \"timeout_handler\" critical_errors = \"critical_handler\" default_error_handler = \"error_handler\" # Optional: Metrics and monitoring [metrics] enabled = false prometheus_port = 8090\rProvider Configuration Azure OpenAI [provider] type = \"azure\" api_key = \"${AZURE_OPENAI_API_KEY}\" endpoint = \"${AZURE_OPENAI_ENDPOINT}\" deployment = \"${AZURE_OPENAI_DEPLOYMENT}\" api_version = \"2024-02-15-preview\" model = \"gpt-4\" max_tokens = 2000 temperature = 0.7 timeout = \"30s\" max_retries = 3\rRequired Environment Variables:\nexport AZURE_OPENAI_API_KEY=\"your-azure-api-key\" export AZURE_OPENAI_ENDPOINT=\"https://your-resource.openai.azure.com\" export AZURE_OPENAI_DEPLOYMENT=\"gpt-4\"\rOpenAI [provider] type = \"openai\" api_key = \"${OPENAI_API_KEY}\" model = \"gpt-4\" max_tokens = 2000 temperature = 0.7 organization = \"${OPENAI_ORG}\" # Optional timeout = \"30s\"\rRequired Environment Variables:\nexport OPENAI_API_KEY=\"your-openai-api-key\" export OPENAI_ORG=\"your-organization-id\" # Optional\rOllama (Local Models) [provider] type = \"ollama\" host = \"http://localhost:11434\" model = \"llama3.2:3b\" temperature = 0.7 context_window = 4096 timeout = \"60s\"\rSetup Ollama:\n# Install Ollama curl -fsSL https://ollama.ai/install.sh | sh # Pull your preferred model ollama pull llama3.2:3b ollama pull codellama:7b ollama pull mistral:7b # Start server (usually automatic) ollama serve\rMock Provider (Testing) [provider] type = \"mock\" response = \"This is a mock response for testing\" delay = \"100ms\" error_rate = 0.0 # 0.0 = no errors, 0.1 = 10% error rate\rMCP Configuration Basic MCP Setup [mcp] enabled = true cache_enabled = true cache_ttl = \"5m\" # Web search tools [mcp.servers.search] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-web-search\"] transport = \"stdio\" # Docker management [mcp.servers.docker] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-docker\"] transport = \"stdio\"\rProduction MCP Configuration [mcp] enabled = true cache_enabled = true cache_ttl = \"10m\" connection_timeout = \"30s\" max_retries = 3 max_concurrent_connections = 10 [mcp.cache] type = \"memory\" # memory, redis (future) max_size = 1000 cleanup_interval = \"1m\" # Production-ready servers with environment variables [mcp.servers.search] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-web-search\"] transport = \"stdio\" env = { \"SEARCH_API_KEY\" = \"${SEARCH_API_KEY}\" } [mcp.servers.database] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-postgres\"] transport = \"stdio\" env = { \"DATABASE_URL\" = \"${DATABASE_URL}\" } [mcp.servers.github] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-github\"] transport = \"stdio\" env = { \"GITHUB_TOKEN\" = \"${GITHUB_TOKEN}\" }\rAvailable MCP Servers # Development Tools [mcp.servers.filesystem] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-filesystem\"] transport = \"stdio\" [mcp.servers.docker] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-docker\"] transport = \"stdio\" # Web \u0026 Search [mcp.servers.brave_search] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-brave-search\"] transport = \"stdio\" env = { \"BRAVE_API_KEY\" = \"${BRAVE_API_KEY}\" } [mcp.servers.fetch] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-fetch\"] transport = \"stdio\" # Databases [mcp.servers.postgres] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-postgres\"] transport = \"stdio\" env = { \"DATABASE_URL\" = \"${DATABASE_URL}\" } [mcp.servers.sqlite] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-sqlite\"] transport = \"stdio\" # Cloud Services [mcp.servers.aws] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-aws\"] transport = \"stdio\" env = { \"AWS_ACCESS_KEY_ID\" = \"${AWS_ACCESS_KEY_ID}\", \"AWS_SECRET_ACCESS_KEY\" = \"${AWS_SECRET_ACCESS_KEY}\", \"AWS_REGION\" = \"${AWS_REGION}\" }\rEnvironment Variable Management .env File Support Create a .env file in your project root:\n# .env # LLM Provider AZURE_OPENAI_API_KEY=your-azure-api-key AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com AZURE_OPENAI_DEPLOYMENT=gpt-4 # MCP Tools SEARCH_API_KEY=your-search-api-key DATABASE_URL=postgresql://user:pass@localhost/db GITHUB_TOKEN=your-github-token BRAVE_API_KEY=your-brave-api-key # AWS (if using AWS MCP server) AWS_ACCESS_KEY_ID=your-access-key AWS_SECRET_ACCESS_KEY=your-secret-key AWS_REGION=us-east-1\rLoad environment variables:\nimport \"github.com/joho/godotenv\" func init() { // Load .env file if it exists _ = godotenv.Load() }\rEnvironment-Specific Configuration Create different config files for different environments:\n# Development agentflow.dev.toml # Staging agentflow.staging.toml # Production agentflow.prod.toml\rLoad specific config:\nconfig, err := core.LoadConfigFromFile(\"agentflow.prod.toml\") if err != nil { log.Fatal(err) }\rConfiguration Loading Automatic Loading AgentFlow automatically looks for configuration in this order:\nagentflow.toml in current directory agentflow.toml in parent directories (up to project root) Environment variables Default values // Automatic loading provider, err := core.NewProviderFromWorkingDir() runner, err := core.NewRunnerFromWorkingDir()\rExplicit Configuration // Load from specific file config, err := core.LoadConfig(\"path/to/agentflow.toml\") if err != nil { log.Fatal(err) } // Create provider from config provider, err := core.NewProviderFromConfig(config) if err != nil { log.Fatal(err) }\rProgrammatic Configuration // Create configuration in code config := core.Config{ Provider: core.ProviderConfig{ Type: \"azure\", APIKey: os.Getenv(\"AZURE_OPENAI_API_KEY\"), Endpoint: os.Getenv(\"AZURE_OPENAI_ENDPOINT\"), Deployment: \"gpt-4\", MaxTokens: 2000, Temperature: 0.7, }, MCP: core.MCPConfig{ Enabled: true, CacheEnabled: true, CacheTTL: 5 * time.Minute, Servers: map[string]core.MCPServerConfig{ \"search\": { Command: \"npx\", Args: []string{\"-y\", \"@modelcontextprotocol/server-web-search\"}, Transport: \"stdio\", }, }, }, } // Use programmatic config provider, err := core.NewProviderFromConfig(config) mcpManager, err := core.InitializeMCPFromConfig(ctx, config.MCP)\rConfiguration Validation Built-in Validation AgentFlow validates configuration automatically:\nconfig, err := core.LoadConfig(\"agentflow.toml\") if err != nil { // Configuration errors are descriptive log.Printf(\"Configuration error: %v\", err) // Example: \"provider.api_key is required when type is 'azure'\" }\rCustom Validation func validateConfig(config core.Config) error { if config.Provider.Type == \"azure\" { if config.Provider.APIKey == \"\" { return fmt.Errorf(\"Azure provider requires API key\") } if config.Provider.Endpoint == \"\" { return fmt.Errorf(\"Azure provider requires endpoint\") } } return nil }\rDynamic Configuration Hot Reloading (Future Feature) [config] hot_reload = true watch_files = [\"agentflow.toml\", \".env\"]\rRuntime Configuration Updates // Update provider configuration at runtime newConfig := core.ProviderConfig{ Temperature: 0.5, // Lower temperature for more deterministic responses } err := provider.UpdateConfig(newConfig) if err != nil { log.Printf(\"Failed to update provider config: %v\", err) }\rBest Practices 1. Environment Variable Naming Use consistent prefixes:\n# AgentFlow settings AGENTFLOW_LOG_LEVEL=debug AGENTFLOW_QUEUE_SIZE=200 # Provider settings AZURE_OPENAI_API_KEY=... OPENAI_API_KEY=... OLLAMA_HOST=... # Tool settings SEARCH_API_KEY=... DATABASE_URL=...\r2. Security Never commit secrets:\n# .gitignore .env *.key agentflow.prod.toml # If it contains secrets\rUse environment variables for secrets:\n[provider] api_key = \"${AZURE_OPENAI_API_KEY}\" # Good # api_key = \"sk-actual-key-here\" # Never do this\r3. Configuration Organization Separate concerns:\n# Base configuration [provider] type = \"azure\" model = \"gpt-4\" # Development overrides in agentflow.dev.toml [provider] type = \"mock\" response = \"Development response\" # Production overrides in agentflow.prod.toml [provider] max_tokens = 4000 timeout = \"60s\"\r4. Documentation Document your configuration:\n# agentflow.toml # Primary LLM provider for all agents [provider] type = \"azure\" # Using Azure OpenAI for enterprise compliance api_key = \"${AZURE_OPENAI_API_KEY}\" deployment = \"gpt-4\" # GPT-4 deployment for high-quality responses # Tools available to all agents [mcp] enabled = true # Web search for research agents [mcp.servers.search] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-web-search\"] transport = \"stdio\" # Docker management for DevOps agents [mcp.servers.docker] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-docker\"] transport = \"stdio\"\rTroubleshooting Common Configuration Issues 1. Environment Variables Not Loading\n# Check if environment variables are set echo $AZURE_OPENAI_API_KEY # Check if .env file is in the right location ls -la .env\r2. MCP Servers Not Starting\n# Test MCP server manually npx -y @modelcontextprotocol/server-web-search # Check Node.js is installed node --version npm --version\r3. Configuration File Not Found\n# Check current directory pwd ls -la agentflow.toml # Check if file has correct permissions chmod 644 agentflow.toml\rDebugging Configuration // Enable debug logging to see configuration loading config := core.Config{ LogLevel: \"debug\", } // Print loaded configuration (sanitized) fmt.Printf(\"Loaded config: %+v\\n\", config.Sanitized())\rConfiguration Templates Generate configuration templates:\n# Generate basic template agentcli config init # Generate with specific provider agentcli config init --provider azure # Generate with MCP servers agentcli config init --with-mcp # Generate production template agentcli config init --production\rNext Steps LLM Providers - Learn about specific provider configurations Tool Integration - Configure MCP servers for your agents Production Deployment - Production configuration patterns",
    "description": "Configuration Management Managing AgentFlow Configuration with agentflow.toml\nAgentFlow uses TOML configuration files to manage all aspects of your agent system: LLM providers, MCP servers, multi-agent orchestration settings, workflow visualization, and more.\nBasic Configuration Structure agentflow.toml Template # Project metadata name = \"My Agent System\" version = \"1.0.0\" description = \"AgentFlow-powered agent workflow\" # Logging configuration log_level = \"info\" # debug, info, warn, error # Multi-Agent Orchestration Configuration [orchestration] mode = \"sequential\" # sequential, collaborative, loop, mixed, route timeout_seconds = 30 # Timeout for orchestration operations max_iterations = 5 # Maximum iterations for loop mode # Sequential mode: agents process in order sequential_agents = [\"agent1\", \"agent2\", \"agent3\"] # Collaborative mode: agents process in parallel collaborative_agents = [\"analyzer\", \"validator\", \"processor\"] # Loop mode: single agent repeats loop_agent = \"processor\" # Mixed mode: combine collaborative and sequential # collaborative_agents = [\"analyzer\", \"validator\"] # sequential_agents = [\"processor\", \"reporter\"] # Workflow Visualization [visualization] enabled = true output_dir = \"./docs/diagrams\" diagram_type = \"flowchart\" # flowchart, sequence, etc. direction = \"TD\" # TD, LR, BT, RL show_metadata = true show_agent_types = true # LLM Provider configuration [provider] type = \"azure\" # azure, openai, ollama, mock api_key = \"${AZURE_OPENAI_API_KEY}\" endpoint = \"https://your-resource.openai.azure.com\" deployment = \"gpt-4\" api_version = \"2024-02-15-preview\" model = \"gpt-4\" max_tokens = 2000 temperature = 0.7 timeout = \"30s\" # MCP (Model Context Protocol) configuration [mcp] enabled = true cache_enabled = true cache_ttl = \"5m\" connection_timeout = \"30s\" max_retries = 3 # MCP server definitions [mcp.servers.search] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-web-search\"] transport = \"stdio\" [mcp.servers.docker] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-docker\"] transport = \"stdio\" # Error handling and routing [error_routing] validation_errors = \"error_handler\" timeout_errors = \"timeout_handler\" critical_errors = \"critical_handler\" default_error_handler = \"error_handler\" # Optional: Metrics and monitoring [metrics] enabled = false prometheus_port = 8090\rProvider Configuration Azure OpenAI [provider] type = \"azure\" api_key = \"${AZURE_OPENAI_API_KEY}\" endpoint = \"${AZURE_OPENAI_ENDPOINT}\" deployment = \"${AZURE_OPENAI_DEPLOYMENT}\" api_version = \"2024-02-15-preview\" model = \"gpt-4\" max_tokens = 2000 temperature = 0.7 timeout = \"30s\" max_retries = 3\rRequired Environment Variables:",
    "tags": [],
    "title": "Configuration",
    "uri": "/AgenticGoKitDocs/guides/configuration/index.html"
  },
  {
    "breadcrumb": "content \u003e guides",
    "content": "Custom Tools Guide This guide covers building custom tools for AgentFlow agents. Learn how to create AgentFlow-native tools using the FunctionTool interface and how to integrate external MCP servers for extended functionality.\n🎯 Overview AgentFlow supports two types of custom tools:\nAgentFlow-Native Tools: Built using the FunctionTool interface and registered with the tool registry External MCP Tools: Tools from external MCP servers that are discovered and integrated automatically Key Benefits:\nNative Integration: Direct tool registration with the AgentFlow tool registry Dynamic Discovery: Automatic discovery of external MCP server tools Standardized Interface: Consistent FunctionTool interface for all tools Type Safety: JSON-serializable arguments and return values 🏗️ AgentFlow Tool Architecture Tool System Overview graph TB\rAgent[Agent] --\u003e|Tool Call| Registry[Tool Registry]\rRegistry --\u003e|Native Tool| NativeTool[AgentFlow Native Tool]\rRegistry --\u003e|MCP Tool| MCPAdapter[MCP Tool Adapter]\rMCPAdapter --\u003e|JSON-RPC| MCPServer[External MCP Server]\rsubgraph \"AgentFlow Process\"\rRegistry\rNativeTool\rMCPAdapter\rend\rsubgraph \"External Process\"\rMCPServer\rend\rKey Components:\nTool Registry: Central registry for all available tools FunctionTool Interface: Standard interface for all tools MCP Tool Adapter: Bridge between AgentFlow and external MCP servers Built-in Tools: Web search, compute metrics, and other core functionality 🛠️ Building AgentFlow-Native Tools 1. FunctionTool Interface All AgentFlow tools implement the FunctionTool interface:\npackage tools import \"context\" // FunctionTool defines the interface for a callable tool that agents can use. type FunctionTool interface { // Name returns the unique identifier for the tool. Name() string // Call executes the tool's logic with the given arguments. // Arguments and return values must be JSON-serializable. Call(ctx context.Context, args map[string]any) (map[string]any, error) }\r2. Simple Tool Implementation package tools import ( \"context\" \"fmt\" ) // CalculatorTool performs mathematical calculations. type CalculatorTool struct{} // Name returns the tool's unique identifier. func (t *CalculatorTool) Name() string { return \"calculator\" } // Call performs the calculation based on the provided arguments. func (t *CalculatorTool) Call(ctx context.Context, args map[string]any) (map[string]any, error) { // Extract operation parameter opVal, ok := args[\"operation\"] if !ok { return nil, fmt.Errorf(\"missing required argument 'operation'\") } operation, ok := opVal.(string) if !ok { return nil, fmt.Errorf(\"argument 'operation' must be a string\") } // Extract numeric arguments aVal, ok := args[\"a\"] if !ok { return nil, fmt.Errorf(\"missing required argument 'a'\") } a, ok := aVal.(float64) if !ok { return nil, fmt.Errorf(\"argument 'a' must be a number\") } bVal, ok := args[\"b\"] if !ok { return nil, fmt.Errorf(\"missing required argument 'b'\") } b, ok := bVal.(float64) if !ok { return nil, fmt.Errorf(\"argument 'b' must be a number\") } // Perform calculation var result float64 switch operation { case \"add\": result = a + b case \"subtract\": result = a - b case \"multiply\": result = a * b case \"divide\": if b == 0 { return nil, fmt.Errorf(\"division by zero\") } result = a / b default: return nil, fmt.Errorf(\"unsupported operation: %s\", operation) } return map[string]any{ \"result\": result, \"operation\": operation, \"operands\": []float64{a, b}, }, nil }\r3. Advanced Tool with External API package tools import ( \"context\" \"encoding/json\" \"fmt\" \"net/http\" \"time\" ) // WeatherTool fetches weather information from an external API. type WeatherTool struct { apiKey string client *http.Client } // NewWeatherTool creates a new weather tool with the provided API key. func NewWeatherTool(apiKey string) *WeatherTool { return \u0026WeatherTool{ apiKey: apiKey, client: \u0026http.Client{Timeout: 10 * time.Second}, } } // Name returns the tool's unique identifier. func (t *WeatherTool) Name() string { return \"get_weather\" } // Call fetches weather information for the specified location. func (t *WeatherTool) Call(ctx context.Context, args map[string]any) (map[string]any, error) { // Extract location parameter locationVal, ok := args[\"location\"] if !ok { return nil, fmt.Errorf(\"missing required argument 'location'\") } location, ok := locationVal.(string) if !ok { return nil, fmt.Errorf(\"argument 'location' must be a string\") } // Optional parameters units := \"metric\" if unitsVal, ok := args[\"units\"].(string); ok { units = unitsVal } // Call weather API weather, err := t.fetchWeather(ctx, location, units) if err != nil { return nil, fmt.Errorf(\"failed to fetch weather: %w\", err) } return map[string]any{ \"location\": location, \"temperature\": weather.Temperature, \"conditions\": weather.Conditions, \"humidity\": weather.Humidity, \"units\": units, \"timestamp\": time.Now().Unix(), }, nil } type WeatherData struct { Temperature float64 `json:\"temperature\"` Conditions string `json:\"conditions\"` Humidity int `json:\"humidity\"` } func (t *WeatherTool) fetchWeather(ctx context.Context, location, units string) (*WeatherData, error) { // Implementation would call actual weather API // This is a stub implementation return \u0026WeatherData{ Temperature: 22.5, Conditions: \"Partly cloudy\", Humidity: 65, }, nil }\r},\r},\r},\r}\r}\nfunc (t *CalculatorTool) Execute(ctx context.Context, params map[string]interface{}) (*mcp.ToolResult, error) { expression, ok := params[“expression”].(string) if !ok { return nil, fmt.Errorf(“expression parameter is required”) }\nprecision := 2\rif p, ok := params[\"precision\"].(float64); ok {\rprecision = int(p)\r}\r// Parse and evaluate expression\rresult, err := evaluateExpression(expression)\rif err != nil {\rreturn \u0026mcp.ToolResult{\rSuccess: false,\rError: fmt.Sprintf(\"Calculation error: %v\", err),\r}, nil\r}\r// Format result with specified precision\rformatted := formatNumber(result, precision)\rreturn \u0026mcp.ToolResult{\rSuccess: true,\rContent: []mcp.Content{\r{\rType: \"text\",\rText: fmt.Sprintf(\"Result: %s\", formatted),\r📋 Tool Registration 1. Using the Tool Registry package main import ( \"context\" \"log\" \"github.com/zynkworks/agentflow/internal/tools\" ) func main() { // Create a new tool registry registry := tools.NewToolRegistry() // Register native tools err := registry.Register(\u0026CalculatorTool{}) if err != nil { log.Fatalf(\"Failed to register calculator tool: %v\", err) } err = registry.Register(NewWeatherTool(\"your-api-key\")) if err != nil { log.Fatalf(\"Failed to register weather tool: %v\", err) } // List all registered tools toolNames := registry.List() log.Printf(\"Registered tools: %v\", toolNames) // Call a tool ctx := context.Background() result, err := registry.CallTool(ctx, \"calculator\", map[string]any{ \"operation\": \"add\", \"a\": 10.0, \"b\": 5.0, }) if err != nil { log.Fatalf(\"Tool call failed: %v\", err) } log.Printf(\"Calculator result: %v\", result) }\r2. Agent Integration package main import ( \"context\" \"log\" \"github.com/zynkworks/agentflow/core\" \"github.com/zynkworks/agentflow/internal/factory\" ) func main() { // Create registry with built-in tools registry := factory.NewDefaultToolRegistry() // Add your custom tools registry.Register(\u0026CalculatorTool{}) registry.Register(NewWeatherTool(\"your-api-key\")) // Create agent configuration config := \u0026core.Config{ // ... other configuration } // Create agent with tools agent := core.NewAgent(config) // Now agents can discover and use your custom tools // Tools are automatically available in agent context }\r3. Tool Discovery package main import ( \"context\" \"fmt\" \"log\" \"github.com/zynkworks/agentflow/internal/tools\" ) func demonstrateToolDiscovery() { registry := tools.NewToolRegistry() // Register some tools registry.Register(\u0026CalculatorTool{}) registry.Register(NewWeatherTool(\"api-key\")) // Get all available tools toolNames := registry.List() fmt.Printf(\"Available tools: %v\\n\", toolNames) // Check if a specific tool exists if tool, exists := registry.Get(\"calculator\"); exists { fmt.Printf(\"Found calculator tool: %s\\n\", tool.Name()) } // Call tool with error handling ctx := context.Background() result, err := registry.CallTool(ctx, \"get_weather\", map[string]any{ \"location\": \"San Francisco\", \"units\": \"metric\", }) if err != nil { log.Printf(\"Weather tool error: %v\", err) return } fmt.Printf(\"Weather result: %v\\n\", result) }\r🔌 External MCP Server Integration 1. MCP Server Configuration AgentFlow can automatically discover and integrate tools from external MCP servers. Configure MCP servers in your agentflow.toml:\n[mcp] enabled = true enable_discovery = true discovery_timeout = \"30s\" connection_timeout = \"10s\" [[mcp.servers]] name = \"custom-tools\" command = \"node\" args = [\"custom-tools-server.js\"] working_dir = \"./tools\" env = { API_KEY = \"your-api-key\" } [[mcp.servers]] name = \"database-tools\" command = \"python\" args = [\"-m\", \"database_tools.server\"] working_dir = \"./tools\" env = { DATABASE_URL = \"postgresql://localhost/mydb\" }\r2. MCP Tool Auto-Discovery package main import ( \"context\" \"log\" \"github.com/zynkworks/agentflow/core\" \"github.com/zynkworks/agentflow/internal/factory\" ) func main() { // MCP configuration mcpConfig := core.MCPConfig{ Enabled: true, EnableDiscovery: true, DiscoveryTimeout: 30 * time.Second, ConnectionTimeout: 10 * time.Second, Servers: []core.MCPServerConfig{ { Name: \"custom-tools\", Command: \"node\", Args: []string{\"custom-tools-server.js\"}, WorkingDir: \"./tools\", Env: map[string]string{\"API_KEY\": \"your-api-key\"}, }, }, } // Create MCP-enabled registry registry, mcpManager, err := factory.NewMCPEnabledToolRegistry(mcpConfig) if err != nil { log.Fatalf(\"Failed to create MCP-enabled registry: %v\", err) } // Tools from MCP servers are automatically discovered and registered ctx := context.Background() tools := mcpManager.GetAvailableTools() log.Printf(\"Discovered %d MCP tools\", len(tools)) // List all available tools (native + MCP) allTools := registry.List() log.Printf(\"All available tools: %v\", allTools) }\r3. Using External MCP Tools package main import ( \"context\" \"log\" \"github.com/zynkworks/agentflow/core\" ) func useMCPTools() { // Get the global MCP manager mcpManager := core.GetMCPManager() if mcpManager == nil { log.Fatal(\"MCP manager not initialized\") } // Execute an MCP tool ctx := context.Background() result, err := mcpManager.ExecuteTool(ctx, \"mcp_custom-tools_weather\", map[string]interface{}{ \"location\": \"New York\", \"units\": \"fahrenheit\", }) if err != nil { log.Fatalf(\"MCP tool execution failed: %v\", err) } log.Printf(\"MCP tool result: %v\", result) }\r## � Advanced Tool Patterns\r### **1. Stateful Tools**\rTools that maintain state across calls:\r```go\rpackage tools\rimport (\r\"context\"\r\"fmt\"\r\"sync\"\r\"time\"\r)\rtype StatefulTool struct {\rsessions map[string]*ToolSession\rmutex sync.RWMutex\r}\rtype ToolSession struct {\rID string\rData map[string]interface{}\rCreatedAt time.Time\rLastUsed time.Time\r}\rfunc NewStatefulTool() *StatefulTool {\rreturn \u0026StatefulTool{\rsessions: make(map[string]*ToolSession),\r}\r}\rfunc (t *StatefulTool) Name() string {\rreturn \"stateful_processor\"\r}\rfunc (t *StatefulTool) Call(ctx context.Context, args map[string]any) (map[string]any, error) {\rsessionID := \"default\"\rif sid, ok := args[\"session_id\"].(string); ok {\rsessionID = sid\r}\rsession := t.getOrCreateSession(sessionID)\r// Process with session context\raction, ok := args[\"action\"].(string)\rif !ok {\rreturn nil, fmt.Errorf(\"missing required argument 'action'\")\r}\rswitch action {\rcase \"store\":\rkey := args[\"key\"].(string)\rvalue := args[\"value\"]\rsession.Data[key] = value\rcase \"retrieve\":\rkey := args[\"key\"].(string)\rvalue := session.Data[key]\rreturn map[string]any{\r\"session_id\": sessionID,\r\"key\": key,\r\"value\": value,\r}, nil\rcase \"list\":\rkeys := make([]string, 0, len(session.Data))\rfor k := range session.Data {\rkeys = append(keys, k)\r}\rreturn map[string]any{\r\"session_id\": sessionID,\r\"keys\": keys,\r}, nil\rdefault:\rreturn nil, fmt.Errorf(\"unsupported action: %s\", action)\r}\r// Update session timestamp\rsession.LastUsed = time.Now()\rreturn map[string]any{\r\"session_id\": sessionID,\r\"action\": action,\r\"success\": true,\r}, nil\r}\rfunc (t *StatefulTool) getOrCreateSession(sessionID string) *ToolSession {\rt.mutex.Lock()\rdefer t.mutex.Unlock()\rif session, exists := t.sessions[sessionID]; exists {\rreturn session\r}\rsession := \u0026ToolSession{\rID: sessionID,\rData: make(map[string]interface{}),\rCreatedAt: time.Now(),\rLastUsed: time.Now(),\r}\rt.sessions[sessionID] = session\rreturn session\r}\r2. Batch Processing Tools Tools that handle large datasets by processing them in batches:\npackage tools import ( \"context\" \"fmt\" \"strings\" ) type BatchProcessingTool struct { batchSize int } func NewBatchProcessingTool() *BatchProcessingTool { return \u0026BatchProcessingTool{ batchSize: 1000, } } func (t *BatchProcessingTool) Name() string { return \"batch_processor\" } func (t *BatchProcessingTool) Call(ctx context.Context, args map[string]any) (map[string]any, error) { dataVal, ok := args[\"data\"] if !ok { return nil, fmt.Errorf(\"missing required argument 'data'\") } data, ok := dataVal.(string) if !ok { return nil, fmt.Errorf(\"argument 'data' must be a string\") } operation, ok := args[\"operation\"].(string) if !ok { return nil, fmt.Errorf(\"missing required argument 'operation'\") } // Process data in batches results := make([]string, 0) batches := t.splitIntoBatches(data, t.batchSize) for i, batch := range batches { processed, err := t.processBatch(ctx, batch, operation) if err != nil { return nil, fmt.Errorf(\"failed to process batch %d: %w\", i, err) } results = append(results, processed) } return map[string]any{ \"operation\": operation, \"batches_processed\": len(batches), \"results\": results, \"total_length\": len(data), }, nil } func (t *BatchProcessingTool) splitIntoBatches(data string, batchSize int) []string { var batches []string runes := []rune(data) for i := 0; i \u003c len(runes); i += batchSize { end := i + batchSize if end \u003e len(runes) { end = len(runes) } batches = append(batches, string(runes[i:end])) } return batches } func (t *BatchProcessingTool) processBatch(ctx context.Context, batch, operation string) (string, error) { switch operation { case \"uppercase\": return strings.ToUpper(batch), nil case \"lowercase\": return strings.ToLower(batch), nil case \"reverse\": runes := []rune(batch) for i, j := 0, len(runes)-1; i \u003c j; i, j = i+1, j-1 { runes[i], runes[j] = runes[j], runes[i] } return string(runes), nil default: return \"\", fmt.Errorf(\"unsupported operation: %s\", operation) } }\r3. Composite Tools Tools that combine multiple operations:\npackage tools import ( \"context\" \"fmt\" \"time\" ) type CompositeAnalysisTool struct { registry *ToolRegistry } func NewCompositeAnalysisTool(registry *ToolRegistry) *CompositeAnalysisTool { return \u0026CompositeAnalysisTool{ registry: registry, } } func (t *CompositeAnalysisTool) Name() string { return \"composite_analysis\" } func (t *CompositeAnalysisTool) Call(ctx context.Context, args map[string]any) (map[string]any, error) { // Define analysis pipeline pipeline := []struct { toolName string args map[string]any }{ { toolName: \"web_search\", args: map[string]any{ \"query\": args[\"search_query\"], }, }, { toolName: \"compute_metric\", args: map[string]any{ \"operation\": \"add\", \"a\": float64(10), \"b\": float64(5), }, }, } results := make([]map[string]any, 0, len(pipeline)) // Execute pipeline steps for i, step := range pipeline { stepResult, err := t.registry.CallTool(ctx, step.toolName, step.args) if err != nil { return nil, fmt.Errorf(\"pipeline step %d failed: %w\", i, err) } results = append(results, stepResult) } // Combine results return map[string]any{ \"pipeline_steps\": len(pipeline), \"results\": results, \"execution_time\": time.Since(time.Now()).Milliseconds(), \"success\": true, }, nil }\rStreaming Tool Support ⚠️ Important: AgentFlow-native tools using the FunctionTool interface do NOT support streaming responses. The FunctionTool.Call() method is synchronous and returns results immediately.\nCurrent Limitations The FunctionTool interface is defined as:\ntype FunctionTool interface { Name() string Call(ctx context.Context, args map[string]any) (map[string]any, error) }\rThis interface only supports synchronous tool execution. There is no built-in streaming interface for AgentFlow-native tools.\nHandling Large Results For tools that need to process large datasets or return substantial results, consider these approaches:\n1. Batch Processing Process data in batches and return summaries:\nfunc (t *DataProcessingTool) Call(ctx context.Context, args map[string]any) (map[string]any, error) { // Process data in batches internally results := make([]BatchResult, 0) for batch := range t.getBatches(args) { batchResult := t.processBatch(ctx, batch) results = append(results, batchResult) } return map[string]any{ \"summary\": t.generateSummary(results), \"batch_count\": len(results), \"total_processed\": t.getTotalCount(results), }, nil }\r2. Pagination Support Return paginated results with continuation tokens:\nfunc (t *PaginatedTool) Call(ctx context.Context, args map[string]any) (map[string]any, error) { pageSize := getIntParam(args, \"page_size\", 100) pageToken := getStringParam(args, \"page_token\", \"\") results, nextToken, err := t.getPage(ctx, pageToken, pageSize) if err != nil { return nil, err } response := map[string]any{ \"results\": results, \"page_size\": pageSize, \"has_more\": nextToken != \"\", } if nextToken != \"\" { response[\"next_page_token\"] = nextToken } return response, nil }\r3. Result Truncation Limit output size and provide truncation indicators:\nfunc (t *LargeResultTool) Call(ctx context.Context, args map[string]any) (map[string]any, error) { maxResults := getIntParam(args, \"max_results\", 1000) allResults := t.getAllResults(ctx, args) results := allResults truncated := false if len(allResults) \u003e maxResults { results = allResults[:maxResults] truncated = true } return map[string]any{ \"results\": results, \"total_count\": len(allResults), \"truncated\": truncated, \"returned_count\": len(results), }, nil }\rExternal MCP Server Tools If you need true streaming capabilities, consider implementing an external MCP server. MCP servers can potentially support streaming through their own protocols, though this would be server-specific functionality.\nFuture Considerations The AgentFlow team may consider adding streaming support in future versions. This could involve:\nAdding a StreamingTool interface with channel-based returns Implementing async tool execution patterns Supporting progressive result delivery For now, use the batch processing and pagination patterns shown above to handle large datasets effectively.\nAdvanced Tool Patterns 1. Stateful Tools Tools that maintain state across calls:\npackage tools import ( \"context\" \"fmt\" \"sync\" \"time\" ) type StatefulTool struct { sessions map[string]*ToolSession mutex sync.RWMutex } type ToolSession struct { ID string Data map[string]interface{} CreatedAt time.Time LastUsed time.Time } func NewStatefulTool() *StatefulTool { return \u0026StatefulTool{ sessions: make(map[string]*ToolSession), } } func (t *StatefulTool) Name() string { return \"stateful_processor\" } func (t *StatefulTool) Call(ctx context.Context, args map[string]any) (map[string]any, error) { sessionID := \"default\" if sid, ok := args[\"session_id\"].(string); ok { sessionID = sid } session := t.getOrCreateSession(sessionID) // Process with session context result, err := t.processWithSession(ctx, session, params) // Update session session.LastUsed = time.Now() return result, err } func (t *StatefulTool) getOrCreateSession(sessionID string) *ToolSession { t.mutex.Lock() defer t.mutex.Unlock() if session, exists := t.sessions[sessionID]; exists { return session } session := \u0026ToolSession{ ID: sessionID, Data: make(map[string]interface{}), CreatedAt: time.Now(), LastUsed: time.Now(), } t.sessions[sessionID] = session return session }\r2. Paginated Tools Tools that handle large datasets using pagination:\ntype PaginatedDataTool struct { dataSource DataSource maxPageSize int } func NewPaginatedDataTool(dataSource DataSource) *PaginatedDataTool { return \u0026PaginatedDataTool{ dataSource: dataSource, maxPageSize: 1000, } } func (t *PaginatedDataTool) Name() string { return \"paginated_data\" } func (t *PaginatedDataTool) Call(ctx context.Context, args map[string]any) (map[string]any, error) { query, ok := args[\"query\"].(string) if !ok { return nil, fmt.Errorf(\"missing required argument 'query'\") } pageSize := getIntParam(args, \"page_size\", 100) if pageSize \u003e t.maxPageSize { pageSize = t.maxPageSize } pageToken := getStringParam(args, \"page_token\", \"\") // Fetch page of results results, nextToken, err := t.dataSource.QueryPage(ctx, query, pageToken, pageSize) if err != nil { return nil, fmt.Errorf(\"failed to query data: %w\", err) } response := map[string]any{ \"results\": results, \"page_size\": pageSize, \"has_more\": nextToken != \"\", \"result_count\": len(results), } if nextToken != \"\" { response[\"next_page_token\"] = nextToken } return response, nil } func getIntParam(args map[string]any, key string, defaultValue int) int { if val, ok := args[key]; ok { if intVal, ok := val.(int); ok { return intVal } if floatVal, ok := val.(float64); ok { return int(floatVal) } } return defaultValue } func getStringParam(args map[string]any, key string, defaultValue string) string { if val, ok := args[key]; ok { if strVal, ok := val.(string); ok { return strVal } } return defaultValue }\r3. Composite Tools Tools that combine multiple operations:\ntype CompositeAnalysisTool struct { dataSource DataSource analyzer Analyzer formatter Formatter } func (t *CompositeAnalysisTool) Execute(ctx context.Context, params map[string]interface{}) (*mcp.ToolResult, error) { pipeline := \u0026AnalysisPipeline{ steps: []PipelineStep{ \u0026DataExtractionStep{source: t.dataSource}, \u0026DataCleaningStep{}, \u0026AnalysisStep{analyzer: t.analyzer}, \u0026FormattingStep{formatter: t.formatter}, }, } result, err := pipeline.Execute(ctx, params) if err != nil { return \u0026mcp.ToolResult{ Success: false, Error: fmt.Sprintf(\"Pipeline failed: %v\", err), }, nil } return \u0026mcp.ToolResult{ Success: true, Content: result.Content, Metadata: map[string]interface{}{ \"pipeline_steps\": len(pipeline.steps), \"execution_time\": result.Duration, \"data_points\": result.DataPoints, }, }, nil }\r🧪 Testing Custom Tools 1. Unit Testing package tools import ( \"context\" \"testing\" \"github.com/stretchr/testify/assert\" \"github.com/stretchr/testify/require\" ) func TestCalculatorTool(t *testing.T) { tool := \u0026CalculatorTool{} tests := []struct { name string args map[string]any expected map[string]any wantErr bool }{ { name: \"Simple addition\", args: map[string]any{ \"operation\": \"add\", \"a\": 2.0, \"b\": 3.0, }, expected: map[string]any{ \"result\": 5.0, \"operation\": \"add\", \"operands\": []float64{2.0, 3.0}, }, wantErr: false, }, { name: \"Division\", args: map[string]any{ \"operation\": \"divide\", \"a\": 10.0, \"b\": 2.0, }, expected: map[string]any{ \"result\": 5.0, \"operation\": \"divide\", \"operands\": []float64{10.0, 2.0}, }, wantErr: false, }, { name: \"Division by zero\", args: map[string]any{ \"operation\": \"divide\", \"a\": 10.0, \"b\": 0.0, }, wantErr: true, }, { name: \"Invalid operation\", args: map[string]any{ \"operation\": \"invalid\", \"a\": 10.0, \"b\": 2.0, }, wantErr: true, }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { result, err := tool.Call(context.Background(), tt.args) if tt.wantErr { assert.Error(t, err) return } require.NoError(t, err) assert.Equal(t, tt.expected, result) }) } } func TestWeatherTool(t *testing.T) { tool := NewWeatherTool(\"test-api-key\") ctx := context.Background() args := map[string]any{ \"location\": \"San Francisco\", \"units\": \"metric\", } result, err := tool.Call(ctx, args) require.NoError(t, err) assert.Equal(t, \"San Francisco\", result[\"location\"]) assert.Equal(t, \"metric\", result[\"units\"]) assert.Contains(t, result, \"temperature\") assert.Contains(t, result, \"conditions\") }\r2. Tool Registry Testing package tools import ( \"context\" \"testing\" \"github.com/stretchr/testify/assert\" \"github.com/stretchr/testify/require\" ) func TestToolRegistry(t *testing.T) { registry := NewToolRegistry() // Test registration calculator := \u0026CalculatorTool{} err := registry.Register(calculator) require.NoError(t, err) // Test duplicate registration err = registry.Register(calculator) assert.Error(t, err) assert.Contains(t, err.Error(), \"already registered\") // Test retrieval tool, exists := registry.Get(\"calculator\") assert.True(t, exists) assert.Equal(t, calculator, tool) // Test listing tools := registry.List() assert.Contains(t, tools, \"calculator\") // Test tool calling ctx := context.Background() result, err := registry.CallTool(ctx, \"calculator\", map[string]any{ \"operation\": \"add\", \"a\": 5.0, \"b\": 3.0, }) require.NoError(t, err) assert.Equal(t, 8.0, result[\"result\"]) }\r3. Integration Testing package main import ( \"context\" \"testing\" \"github.com/stretchr/testify/assert\" \"github.com/stretchr/testify/require\" \"github.com/zynkworks/agentflow/core\" \"github.com/zynkworks/agentflow/internal/factory\" \"github.com/zynkworks/agentflow/internal/tools\" ) func TestAgentWithCustomTools(t *testing.T) { // Create registry with custom tools registry := factory.NewDefaultToolRegistry() // Add custom tools calculator := \u0026CalculatorTool{} err := registry.Register(calculator) require.NoError(t, err) // Test tool availability toolNames := registry.List() assert.Contains(t, toolNames, \"calculator\") assert.Contains(t, toolNames, \"web_search\") // Built-in tool assert.Contains(t, toolNames, \"compute_metric\") // Built-in tool // Test tool execution through registry ctx := context.Background() result, err := registry.CallTool(ctx, \"calculator\", map[string]any{ \"operation\": \"multiply\", \"a\": 6.0, \"b\": 7.0, }) require.NoError(t, err) assert.Equal(t, 42.0, result[\"result\"]) } func TestMCPToolIntegration(t *testing.T) { // MCP configuration for testing mcpConfig := core.MCPConfig{ Enabled: true, Servers: []core.MCPServerConfig{ { Name: \"test-server\", Command: \"echo\", Args: []string{\"test\"}, }, }, } // Create MCP-enabled registry registry, mcpManager, err := factory.NewMCPEnabledToolRegistry(mcpConfig) require.NoError(t, err) // Test registry includes both native and MCP tools tools := registry.List() assert.Contains(t, tools, \"web_search\") // Built-in tool assert.Contains(t, tools, \"compute_metric\") // Built-in tool // Test MCP manager assert.NotNil(t, mcpManager) mcpTools := mcpManager.GetAvailableTools() t.Logf(\"Available MCP tools: %d\", len(mcpTools)) }\r4. MCP Server Integration Testing package main import ( \"context\" \"testing\" \"github.com/stretchr/testify/assert\" \"github.com/stretchr/testify/require\" \"github.com/zynkworks/agentflow/core\" \"github.com/zynkworks/agentflow/internal/factory\" ) func TestMCPServerIntegration(t *testing.T) { // Start a local MCP server for testing listener, err := net.Listen(\"tcp\", \"localhost:0\") require.NoError(t, err) defer listener.Close() // MCP configuration mcpConfig := core.MCPConfig{ Enabled: true, Servers: []core.MCPServerConfig{ { Name: \"test-server\", Address: listener.Addr().String(), }, }, } runner, err := core.NewRunner(mcpConfig) require.NoError(t, err) // Test tool discovery tools, err := runner.GetMCPManager().ListTools(context.Background()) require.NoError(t, err) assert.Contains(t, tools, \"calculate\") // Test tool execution result, err := runner.GetMCPManager().ExecuteTool(context.Background(), \"calculate\", map[string]interface{}{ \"expression\": \"5 * 7\", }) require.NoError(t, err) assert.True(t, result.Success) }\r📈 Performance and Optimization 1. Connection Pooling package tools import ( \"context\" \"database/sql\" \"fmt\" \"time\" _ \"github.com/lib/pq\" ) type DatabaseTool struct { db *sql.DB } func NewDatabaseTool(connectionString string) (*DatabaseTool, error) { db, err := sql.Open(\"postgres\", connectionString) if err != nil { return nil, fmt.Errorf(\"failed to open database: %w\", err) } // Configure connection pool db.SetMaxOpenConns(25) db.SetMaxIdleConns(25) db.SetConnMaxLifetime(5 * time.Minute) return \u0026DatabaseTool{ db: db, }, nil } func (t *DatabaseTool) Name() string { return \"database_query\" } func (t *DatabaseTool) Call(ctx context.Context, args map[string]any) (map[string]any, error) { query, ok := args[\"query\"].(string) if !ok { return nil, fmt.Errorf(\"missing required argument 'query'\") } rows, err := t.db.QueryContext(ctx, query) if err != nil { return nil, fmt.Errorf(\"query failed: %w\", err) } defer rows.Close() // Process results columns, err := rows.Columns() if err != nil { return nil, fmt.Errorf(\"failed to get columns: %w\", err) } var results []map[string]interface{} for rows.Next() { values := make([]interface{}, len(columns)) valuePtrs := make([]interface{}, len(columns)) for i := range values { valuePtrs[i] = \u0026values[i] } if err := rows.Scan(valuePtrs...); err != nil { return nil, fmt.Errorf(\"scan failed: %w\", err) } row := make(map[string]interface{}) for i, col := range columns { row[col] = values[i] } results = append(results, row) } return map[string]any{ \"query\": query, \"results\": results, \"row_count\": len(results), }, nil }\r2. Caching package tools import ( \"context\" \"crypto/md5\" \"encoding/json\" \"fmt\" \"sync\" \"time\" ) type CachedTool struct { baseTool FunctionTool cache map[string]cacheEntry ttl time.Duration mutex sync.RWMutex } type cacheEntry struct { result map[string]any timestamp time.Time } func NewCachedTool(baseTool FunctionTool, ttl time.Duration) *CachedTool { return \u0026CachedTool{ baseTool: baseTool, cache: make(map[string]cacheEntry), ttl: ttl, } } func (t *CachedTool) Name() string { return t.baseTool.Name() } func (t *CachedTool) Call(ctx context.Context, args map[string]any) (map[string]any, error) { // Generate cache key key := t.generateCacheKey(args) // Check cache first t.mutex.RLock() if entry, found := t.cache[key]; found { if time.Since(entry.timestamp) \u003c t.ttl { t.mutex.RUnlock() return entry.result, nil } } t.mutex.RUnlock() // Execute tool result, err := t.baseTool.Call(ctx, args) if err != nil { return nil, err } // Cache successful results t.mutex.Lock() t.cache[key] = cacheEntry{ result: result, timestamp: time.Now(), } t.mutex.Unlock() return result, nil } func (t *CachedTool) generateCacheKey(args map[string]any) string { data, _ := json.Marshal(args) return fmt.Sprintf(\"%s:%x\", t.baseTool.Name(), md5.Sum(data)) }\r3. Rate Limiting package tools import ( \"context\" \"fmt\" \"sync\" \"time\" ) type RateLimitedTool struct { baseTool FunctionTool maxRequests int window time.Duration requests []time.Time mutex sync.Mutex } func NewRateLimitedTool(baseTool FunctionTool, maxRequests int, window time.Duration) *RateLimitedTool { return \u0026RateLimitedTool{ baseTool: baseTool, maxRequests: maxRequests, window: window, requests: make([]time.Time, 0), } } func (t *RateLimitedTool) Name() string { return t.baseTool.Name() } func (t *RateLimitedTool) Call(ctx context.Context, args map[string]any) (map[string]any, error) { t.mutex.Lock() defer t.mutex.Unlock() now := time.Now() // Remove old requests outside the window cutoff := now.Add(-t.window) validRequests := make([]time.Time, 0) for _, req := range t.requests { if req.After(cutoff) { validRequests = append(validRequests, req) } } t.requests = validRequests // Check rate limit if len(t.requests) \u003e= t.maxRequests { return nil, fmt.Errorf(\"rate limit exceeded: %d requests per %v\", t.maxRequests, t.window) } // Add current request t.requests = append(t.requests, now) // Execute tool return t.baseTool.Call(ctx, args) }\r📚 Best Practices and Guidelines 1. Tool Design Principles // Good: Clear, focused tool with single responsibility type WeatherTool struct { apiKey string client *http.Client } func (t *WeatherTool) Name() string { return \"get_weather\" } func (t *WeatherTool) Call(ctx context.Context, args map[string]any) (map[string]any, error) { // Validate required parameters location, ok := args[\"location\"].(string) if !ok || location == \"\" { return nil, fmt.Errorf(\"location is required and must be a non-empty string\") } // Provide sensible defaults units := \"metric\" if u, ok := args[\"units\"].(string); ok { units = u } // Return structured, JSON-serializable data return map[string]any{ \"location\": location, \"temperature\": 22.5, \"conditions\": \"Partly cloudy\", \"units\": units, \"timestamp\": time.Now().Unix(), }, nil }\r2. Error Handling func (t *MyTool) Call(ctx context.Context, args map[string]any) (map[string]any, error) { // Validate input parameters if err := t.validateArgs(args); err != nil { return nil, fmt.Errorf(\"invalid arguments: %w\", err) } // Handle external API calls with proper error handling result, err := t.callExternalAPI(ctx, args) if err != nil { // Provide meaningful error messages return nil, fmt.Errorf(\"external API call failed: %w\", err) } // Return structured results return map[string]any{ \"success\": true, \"data\": result, \"timestamp\": time.Now().Unix(), }, nil } func (t *MyTool) validateArgs(args map[string]any) error { required := []string{\"param1\", \"param2\"} for _, param := range required { if _, ok := args[param]; !ok { return fmt.Errorf(\"missing required parameter: %s\", param) } } return nil }\r3. Built-in vs Custom vs External Tools package main import ( \"context\" \"log\" \"github.com/zynkworks/agentflow/internal/factory\" \"github.com/zynkworks/agentflow/internal/tools\" \"github.com/zynkworks/agentflow/core\" ) func main() { // 1. Built-in tools (automatically included) registry := factory.NewDefaultToolRegistry() // Includes: web_search, compute_metric // 2. Custom native tools err := registry.Register(\u0026WeatherTool{apiKey: \"your-key\"}) if err != nil { log.Fatal(err) } // 3. External MCP tools (auto-discovered) mcpConfig := core.MCPConfig{ Enabled: true, Servers: []core.MCPServerConfig{ { Name: \"external-tools\", Command: \"node\", Args: []string{\"external-tools-server.js\"}, }, }, } mcpRegistry, mcpManager, err := factory.NewMCPEnabledToolRegistry(mcpConfig) if err != nil { log.Fatal(err) } // All tools are now available through the registry allTools := mcpRegistry.List() log.Printf(\"Available tools: %v\", allTools) // Tools can be called uniformly ctx := context.Background() result, err := mcpRegistry.CallTool(ctx, \"get_weather\", map[string]any{ \"location\": \"San Francisco\", }) if err != nil { log.Fatal(err) } log.Printf(\"Weather result: %v\", result) }\r📖 Summary This guide covered the complete AgentFlow tool system:\nAgentFlow-Native Tools Implement the FunctionTool interface Register with the ToolRegistry Direct integration with AgentFlow agents JSON-serializable arguments and return values External MCP Tools Configured in agentflow.toml Automatically discovered and registered Accessed through MCP tool adapters Seamlessly integrated with native tools Key Concepts Tool Registry: Central registry for all tools FunctionTool Interface: Standard synchronous interface for all tools MCP Integration: Bridge to external MCP servers Performance: Caching, rate limiting, and connection pooling Testing: Unit, integration, and registry testing Streaming Limitations: Native tools are synchronous; use pagination for large datasets Best Practices Single responsibility per tool Clear error messages Proper input validation Structured return values Comprehensive testing Use pagination for large result sets AgentFlow provides a flexible, extensible tool system that supports both native Go tools and external MCP servers, giving you the power to build sophisticated agents with domain-specific capabilities.\nImportant Notes:\nAgentFlow-native tools are synchronous and do not support streaming Use batch processing and pagination patterns for large datasets External MCP servers may implement their own streaming protocols Consider tool performance and resource usage when designing custom tools",
    "description": "Custom Tools Guide This guide covers building custom tools for AgentFlow agents. Learn how to create AgentFlow-native tools using the FunctionTool interface and how to integrate external MCP servers for extended functionality.\n🎯 Overview AgentFlow supports two types of custom tools:\nAgentFlow-Native Tools: Built using the FunctionTool interface and registered with the tool registry External MCP Tools: Tools from external MCP servers that are discovered and integrated automatically Key Benefits:",
    "tags": [],
    "title": "CustomTools",
    "uri": "/AgenticGoKitDocs/guides/customtools/index.html"
  },
  {
    "breadcrumb": "content \u003e guides",
    "content": "LLM Providers Multi-Provider LLM Integration in AgentFlow\nAgentFlow provides a unified interface for working with different LLM providers. This guide covers configuration, usage patterns, and provider-specific features.\nProvider Overview AgentFlow supports multiple LLM providers through a unified ModelProvider interface:\nAzure OpenAI (Default) - Enterprise-ready with robust scaling OpenAI - Direct API access to GPT models Ollama - Local models for privacy and cost control Mock - Testing and development ModelProvider Interface All providers implement the same interface:\ntype ModelProvider interface { Generate(ctx context.Context, prompt string) (string, error) GenerateWithHistory(ctx context.Context, messages []Message) (string, error) Name() string } type Message struct { Role string // \"system\", \"user\", \"assistant\" Content string }\rConfiguration Azure OpenAI (Default) agentflow.toml:\n[provider] type = \"azure\" api_key = \"${AZURE_OPENAI_API_KEY}\" endpoint = \"https://your-resource.openai.azure.com\" deployment = \"gpt-4\" api_version = \"2024-02-15-preview\" model = \"gpt-4\" max_tokens = 2000 temperature = 0.7\rEnvironment variables:\nexport AZURE_OPENAI_API_KEY=\"your-azure-api-key\" export AZURE_OPENAI_ENDPOINT=\"https://your-resource.openai.azure.com\" export AZURE_OPENAI_DEPLOYMENT=\"gpt-4\"\rProgrammatic configuration:\nimport agentflow \"github.com/kunalkushwaha/agentflow/core\" config := agentflow.AzureConfig{ APIKey: \"your-api-key\", Endpoint: \"https://your-resource.openai.azure.com\", Deployment: \"gpt-4\", APIVersion: \"2024-02-15-preview\", MaxTokens: 2000, Temperature: 0.7, } provider, err := agentflow.NewAzureProvider(config) if err != nil { log.Fatal(err) }\rOpenAI agentflow.toml:\n[provider] type = \"openai\" api_key = \"${OPENAI_API_KEY}\" model = \"gpt-4\" max_tokens = 2000 temperature = 0.7 organization = \"your-org-id\" # Optional\rEnvironment variables:\nexport OPENAI_API_KEY=\"your-openai-api-key\" export OPENAI_ORG=\"your-org-id\" # Optional\rProgrammatic configuration:\nconfig := agentflow.OpenAIConfig{ APIKey: \"your-api-key\", Model: \"gpt-4\", MaxTokens: 2000, Temperature: 0.7, Organization: \"your-org-id\", // Optional } provider, err := agentflow.NewOpenAIProvider(config)\rOllama (Local Models) agentflow.toml:\n[provider] type = \"ollama\" host = \"http://localhost:11434\" model = \"llama3.2:3b\" temperature = 0.7 context_window = 4096\rSetup Ollama:\n# Install Ollama curl -fsSL https://ollama.ai/install.sh | sh # Pull a model ollama pull llama3.2:3b # Start Ollama server (usually automatic) ollama serve\rProgrammatic configuration:\nconfig := agentflow.OllamaConfig{ Host: \"http://localhost:11434\", Model: \"llama3.2:3b\", Temperature: 0.7, ContextWindow: 4096, } provider, err := agentflow.NewOllamaProvider(config)\rMock Provider (Testing) agentflow.toml:\n[provider] type = \"mock\" response = \"This is a mock response\" delay = \"100ms\" # Simulate network delay\rProgrammatic configuration:\nconfig := agentflow.MockConfig{ Response: \"This is a mock response\", Delay: 100 * time.Millisecond, } provider := agentflow.NewMockProvider(config)\rProvider Factory AgentFlow provides factory functions for easy provider creation:\nFrom Configuration File // Automatically load from agentflow.toml provider, err := agentflow.NewProviderFromWorkingDir() if err != nil { log.Fatal(err) } // Or from specific directory provider, err := agentflow.NewProviderFromDir(\"/path/to/config\")\rFrom Environment // Auto-detect provider from environment variables provider, err := agentflow.NewProviderFromEnv() if err != nil { log.Fatal(err) }\rExplicit Configuration // Create specific provider azureProvider, err := agentflow.NewAzureProvider(azureConfig) openaiProvider, err := agentflow.NewOpenAIProvider(openaiConfig) ollamaProvider, err := agentflow.NewOllamaProvider(ollamaConfig)\rUsage Patterns Basic Usage package main import ( \"context\" \"fmt\" agentflow \"github.com/kunalkushwaha/agentflow/core\" ) func main() { // Load provider from configuration provider, err := agentflow.NewProviderFromWorkingDir() if err != nil { panic(err) } ctx := context.Background() // Simple generation response, err := provider.Generate(ctx, \"Explain Go interfaces in simple terms\") if err != nil { panic(err) } fmt.Println(\"Response:\", response) fmt.Println(\"Provider:\", provider.Name()) }\rConversation History func handleConversation(provider agentflow.ModelProvider) { ctx := context.Background() // Build conversation history messages := []agentflow.Message{ {Role: \"system\", Content: \"You are a helpful programming assistant.\"}, {Role: \"user\", Content: \"How do I create a web server in Go?\"}, {Role: \"assistant\", Content: \"You can create a web server in Go using the net/http package...\"}, {Role: \"user\", Content: \"Can you show me a more complex example?\"}, } response, err := provider.GenerateWithHistory(ctx, messages) if err != nil { log.Printf(\"Error: %v\", err) return } fmt.Println(\"Response:\", response) }\rAgent with Provider type MyAgent struct { provider agentflow.ModelProvider name string } func NewMyAgent(name string, provider agentflow.ModelProvider) *MyAgent { return \u0026MyAgent{ name: name, provider: provider, } } func (a *MyAgent) Run(ctx context.Context, event agentflow.Event, state agentflow.State) (agentflow.AgentResult, error) { message := event.GetData()[\"message\"] systemPrompt := fmt.Sprintf(\"You are %s, a specialized assistant.\", a.name) fullPrompt := fmt.Sprintf(\"%s\\n\\nUser: %s\", systemPrompt, message) response, err := a.provider.Generate(ctx, fullPrompt) if err != nil { return agentflow.AgentResult{}, fmt.Errorf(\"provider error: %w\", err) } state.Set(\"response\", response) state.Set(\"provider\", a.provider.Name()) return agentflow.AgentResult{ Result: response, State: state, }, nil }\rProvider Selection Strategies Environment-Based Selection func createProvider() (agentflow.ModelProvider, error) { providerType := os.Getenv(\"LLM_PROVIDER\") switch providerType { case \"azure\": return agentflow.NewAzureProvider(agentflow.AzureConfig{ APIKey: os.Getenv(\"AZURE_OPENAI_API_KEY\"), Endpoint: os.Getenv(\"AZURE_OPENAI_ENDPOINT\"), Deployment: os.Getenv(\"AZURE_OPENAI_DEPLOYMENT\"), }) case \"openai\": return agentflow.NewOpenAIProvider(agentflow.OpenAIConfig{ APIKey: os.Getenv(\"OPENAI_API_KEY\"), Model: \"gpt-4\", }) case \"ollama\": return agentflow.NewOllamaProvider(agentflow.OllamaConfig{ Host: \"http://localhost:11434\", Model: \"llama3.2:3b\", }) default: return agentflow.NewMockProvider(agentflow.MockConfig{ Response: \"Mock response for testing\", }), nil } }\rFeature-Based Selection type ProviderSelector struct { primary agentflow.ModelProvider fallback agentflow.ModelProvider local agentflow.ModelProvider } func NewProviderSelector() *ProviderSelector { azure, _ := agentflow.NewAzureProvider(azureConfig) openai, _ := agentflow.NewOpenAIProvider(openaiConfig) ollama, _ := agentflow.NewOllamaProvider(ollamaConfig) return \u0026ProviderSelector{ primary: azure, // Enterprise workloads fallback: openai, // Backup when Azure is down local: ollama, // Privacy-sensitive tasks } } func (p *ProviderSelector) SelectProvider(task string) agentflow.ModelProvider { if strings.Contains(task, \"confidential\") || strings.Contains(task, \"private\") { return p.local // Use local model for sensitive content } // Try primary first, fallback if needed return p.primary } func (p *ProviderSelector) GenerateWithFallback(ctx context.Context, prompt string) (string, error) { // Try primary provider response, err := p.primary.Generate(ctx, prompt) if err == nil { return response, nil } // Log primary failure and try fallback log.Printf(\"Primary provider failed: %v, trying fallback\", err) response, err = p.fallback.Generate(ctx, prompt) if err == nil { return response, nil } // Both failed return \"\", fmt.Errorf(\"all providers failed: %w\", err) }\rError Handling Provider-Specific Error Handling func handleProviderError(err error, provider agentflow.ModelProvider) string { providerName := provider.Name() switch { case strings.Contains(err.Error(), \"rate limit\"): return fmt.Sprintf(\"Rate limit reached for %s. Please try again later.\", providerName) case strings.Contains(err.Error(), \"authentication\"): return fmt.Sprintf(\"Authentication failed for %s. Please check your API key.\", providerName) case strings.Contains(err.Error(), \"timeout\"): return fmt.Sprintf(\"Request to %s timed out. Please try again.\", providerName) case strings.Contains(err.Error(), \"model\"): return fmt.Sprintf(\"Model not available on %s. Please check your configuration.\", providerName) default: return fmt.Sprintf(\"Error with %s: %v\", providerName, err) } } func (a *Agent) Run(ctx context.Context, event agentflow.Event, state agentflow.State) (agentflow.AgentResult, error) { message := event.GetData()[\"message\"] response, err := a.provider.Generate(ctx, prompt) if err != nil { errorMsg := handleProviderError(err, a.provider) // Set error in state for debugging state.Set(\"error\", errorMsg) state.Set(\"provider_error\", true) // Return user-friendly error return agentflow.AgentResult{ Result: \"I'm having trouble processing your request right now. Please try again.\", State: state, }, nil // Don't propagate error, handle gracefully } return agentflow.AgentResult{Result: response, State: state}, nil }\rRetry Logic func generateWithRetry(ctx context.Context, provider agentflow.ModelProvider, prompt string, maxRetries int) (string, error) { var lastErr error for i := 0; i \u003c maxRetries; i++ { response, err := provider.Generate(ctx, prompt) if err == nil { return response, nil } lastErr = err // Don't retry on authentication errors if strings.Contains(err.Error(), \"authentication\") { break } // Exponential backoff if i \u003c maxRetries-1 { backoff := time.Duration(i+1) * time.Second log.Printf(\"Retry %d/%d after %v: %v\", i+1, maxRetries, backoff, err) time.Sleep(backoff) } } return \"\", fmt.Errorf(\"all retries failed: %w\", lastErr) }\rTesting with Providers Mock Provider for Testing func TestAgentWithMockProvider(t *testing.T) { // Create mock provider mockProvider := agentflow.NewMockProvider(agentflow.MockConfig{ Response: \"This is a test response\", Delay: 10 * time.Millisecond, }) // Create agent with mock agent := NewMyAgent(\"test-agent\", mockProvider) // Test eventData := agentflow.EventData{\"message\": \"test message\"} event := agentflow.NewEvent(\"test\", eventData, nil) state := agentflow.NewState() result, err := agent.Run(context.Background(), event, state) assert.NoError(t, err) assert.Equal(t, \"This is a test response\", result.Result) assert.Equal(t, \"mock\", mockProvider.Name()) }\rProvider Integration Tests func TestProviderIntegration(t *testing.T) { if testing.Short() { t.Skip(\"Skipping integration test\") } tests := []struct { name string provider agentflow.ModelProvider prompt string }{ { name: \"azure\", provider: createAzureProvider(t), prompt: \"Say hello\", }, { name: \"openai\", provider: createOpenAIProvider(t), prompt: \"Say hello\", }, { name: \"ollama\", provider: createOllamaProvider(t), prompt: \"Say hello\", }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second) defer cancel() response, err := tt.provider.Generate(ctx, tt.prompt) assert.NoError(t, err) assert.NotEmpty(t, response) assert.Contains(t, strings.ToLower(response), \"hello\") }) } }\rPerformance Considerations Provider Performance Characteristics Provider Latency Throughput Cost Privacy Azure OpenAI Medium High Medium Enterprise OpenAI Medium High Medium Cloud Ollama Low Medium Free Full Mock Minimal Very High Free Full Optimization Strategies // Connection pooling for HTTP providers config := agentflow.AzureConfig{ APIKey: apiKey, Endpoint: endpoint, Deployment: deployment, // Performance settings MaxRetries: 3, RequestTimeout: 30 * time.Second, MaxConnections: 100, } // Context with timeout ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second) defer cancel() response, err := provider.Generate(ctx, prompt)\rProduction Deployment Environment Configuration # Production environment variables export LLM_PROVIDER=\"azure\" export AZURE_OPENAI_API_KEY=\"prod-api-key\" export AZURE_OPENAI_ENDPOINT=\"https://prod-resource.openai.azure.com\" export AZURE_OPENAI_DEPLOYMENT=\"gpt-4\" # Fallback provider export FALLBACK_PROVIDER=\"openai\" export OPENAI_API_KEY=\"fallback-api-key\" # Local development export LLM_PROVIDER=\"ollama\" export OLLAMA_HOST=\"http://localhost:11434\" export OLLAMA_MODEL=\"llama3.2:3b\"\rMonitoring Provider Health type ProviderHealthChecker struct { provider agentflow.ModelProvider } func (h *ProviderHealthChecker) HealthCheck(ctx context.Context) error { testPrompt := \"Health check\" ctx, cancel := context.WithTimeout(ctx, 10*time.Second) defer cancel() _, err := h.provider.Generate(ctx, testPrompt) return err } // Use in health endpoint func healthHandler(w http.ResponseWriter, r *http.Request) { checker := \u0026ProviderHealthChecker{provider: globalProvider} if err := checker.HealthCheck(r.Context()); err != nil { http.Error(w, \"Provider unhealthy: \"+err.Error(), http.StatusServiceUnavailable) return } w.WriteHeader(http.StatusOK) json.NewEncoder(w).Encode(map[string]string{\"status\": \"healthy\"}) }\rNext Steps Configuration - Advanced configuration management Tool Integration - Add MCP tools to your agents Production Deployment - Deploy at scale Custom Providers - Build custom provider adapters (for contributors)",
    "description": "LLM Providers Multi-Provider LLM Integration in AgentFlow\nAgentFlow provides a unified interface for working with different LLM providers. This guide covers configuration, usage patterns, and provider-specific features.\nProvider Overview AgentFlow supports multiple LLM providers through a unified ModelProvider interface:\nAzure OpenAI (Default) - Enterprise-ready with robust scaling OpenAI - Direct API access to GPT models Ollama - Local models for privacy and cost control Mock - Testing and development ModelProvider Interface All providers implement the same interface:",
    "tags": [],
    "title": "Providers",
    "uri": "/AgenticGoKitDocs/guides/providers/index.html"
  },
  {
    "breadcrumb": "content \u003e guides",
    "content": "Memory Provider Setup Guide Complete setup guides for AgenticGoKit memory providers\nAgenticGoKit supports multiple memory providers for different use cases. This guide provides step-by-step setup instructions for each provider, from development to production deployment.\n📚 Table of Contents Overview In-Memory Provider PostgreSQL + pgvector Setup Weaviate Setup Provider Comparison Migration Guide Troubleshooting 🎯 Overview Provider Selection Guide Provider Best For Persistence Scalability Setup Complexity memory Development, Testing ❌ No ⚠️ Single instance ✅ Minimal pgvector Production, Enterprise ✅ Yes ✅ High ⚠️ Moderate weaviate Large-scale Vector Ops ✅ Yes ✅ Very High ⚠️ Moderate Quick Start Recommendations Just getting started? → Use memory provider Building a production app? → Use pgvector provider Need advanced vector features? → Use weaviate provider 💾 In-Memory Provider Perfect for development, testing, and temporary sessions\nConfiguration [memory] enabled = true provider = \"memory\" max_results = 10 dimensions = 1536 auto_embed = true [memory.embedding] provider = \"dummy\" # For testing model = \"text-embedding-3-small\"\rUsage Example package main import ( \"context\" \"log\" agentflow \"github.com/kunalkushwaha/agenticgokit/core\" ) func main() { // Create memory configuration config := agentflow.AgentMemoryConfig{ Provider: \"memory\", Connection: \"memory\", Dimensions: 1536, Embedding: agentflow.EmbeddingConfig{ Provider: \"dummy\", Model: \"text-embedding-3-small\", }, } // Create memory provider memory, err := agentflow.NewMemory(config) if err != nil { log.Fatal(err) } defer memory.Close() // Use memory system ctx := memory.SetSession(context.Background(), \"test-session\") // Store and query err = memory.Store(ctx, \"I love programming in Go\", \"programming\") if err != nil { log.Fatal(err) } results, err := memory.Query(ctx, \"programming languages\", 5) if err != nil { log.Fatal(err) } log.Printf(\"Found %d results\", len(results)) }\rPros \u0026 Cons ✅ Advantages:\nZero setup required Fast performance Perfect for development No external dependencies ❌ Limitations:\nNo persistence (data lost on restart) Single instance only Limited to available RAM Not suitable for production 🐘 PostgreSQL + pgvector Setup Production-ready persistent memory with excellent performance\nPrerequisites PostgreSQL 12+ pgvector extension Docker (recommended) or native PostgreSQL installation Option 1: Docker Setup (Recommended) Step 1: Run PostgreSQL with pgvector # Create and start PostgreSQL container with pgvector docker run -d \\ --name agentflow-postgres \\ -e POSTGRES_DB=agentflow \\ -e POSTGRES_USER=agentflow \\ -e POSTGRES_PASSWORD=password \\ -p 5432:5432 \\ -v postgres_data:/var/lib/postgresql/data \\ pgvector/pgvector:pg16 # Wait for container to start sleep 10 # Verify container is running docker ps | grep agentflow-postgres\rStep 2: Initialize Database # Connect to database docker exec -it agentflow-postgres psql -U agentflow -d agentflow # Enable pgvector extension CREATE EXTENSION IF NOT EXISTS vector; # Verify extension SELECT * FROM pg_extension WHERE extname = 'vector'; # Exit psql \\q\rStep 3: Test Connection # Test connection string psql \"postgres://agentflow:password@localhost:5432/agentflow\" -c \"SELECT version();\"\rOption 2: Native Installation Ubuntu/Debian # Install PostgreSQL sudo apt update sudo apt install postgresql postgresql-contrib # Install pgvector sudo apt install postgresql-15-pgvector # Start PostgreSQL sudo systemctl start postgresql sudo systemctl enable postgresql # Create database and user sudo -u postgres createuser agentflow sudo -u postgres createdb agentflow -O agentflow sudo -u postgres psql -c \"ALTER USER agentflow PASSWORD 'password';\" # Enable pgvector sudo -u postgres psql -d agentflow -c \"CREATE EXTENSION IF NOT EXISTS vector;\"\rmacOS (Homebrew) # Install PostgreSQL brew install postgresql # Install pgvector brew install pgvector # Start PostgreSQL brew services start postgresql # Create database createdb agentflow psql agentflow -c \"CREATE EXTENSION IF NOT EXISTS vector;\"\rConfiguration agentflow.toml [memory] enabled = true provider = \"pgvector\" max_results = 10 dimensions = 1536 auto_embed = true [memory.pgvector] # Update with your PostgreSQL connection details connection = \"postgres://agentflow:password@localhost:5432/agentflow?sslmode=disable\" table_name = \"agent_memory\" [memory.embedding] provider = \"openai\" # or \"ollama\" for local model = \"text-embedding-3-small\" [memory.rag] enabled = true chunk_size = 1000 overlap = 100 top_k = 5 score_threshold = 0.7\rEnvironment Variables # Database connection export DATABASE_URL=\"postgres://agentflow:password@localhost:5432/agentflow?sslmode=disable\" # OpenAI API (if using OpenAI embeddings) export OPENAI_API_KEY=\"your-openai-api-key\" # Or Azure OpenAI export AZURE_OPENAI_API_KEY=\"your-azure-api-key\" export AZURE_OPENAI_ENDPOINT=\"https://your-resource.openai.azure.com/\"\rUsage Example package main import ( \"context\" \"log\" \"os\" agentflow \"github.com/kunalkushwaha/agentflow/core\" ) func main() { // Create pgvector configuration config := agentflow.AgentMemoryConfig{ Provider: \"pgvector\", Connection: os.Getenv(\"DATABASE_URL\"), Dimensions: 1536, Embedding: agentflow.EmbeddingConfig{ Provider: \"openai\", APIKey: os.Getenv(\"OPENAI_API_KEY\"), Model: \"text-embedding-3-small\", }, } // Create memory provider memory, err := agentflow.NewMemory(config) if err != nil { log.Fatal(err) } defer memory.Close() // Create session ctx := memory.SetSession(context.Background(), \"user-123\") // Store memories err = memory.Store(ctx, \"I prefer morning meetings\", \"scheduling\", \"preference\") if err != nil { log.Fatal(err) } // Query memories results, err := memory.Query(ctx, \"meeting preferences\", 5) if err != nil { log.Fatal(err) } for _, result := range results { log.Printf(\"Memory: %s (Score: %.2f)\", result.Content, result.Score) } // RAG example - ingest document doc := agentflow.Document{ ID: \"meeting-guide\", Title: \"Meeting Best Practices\", Content: \"Effective meetings should start on time, have clear agendas...\", Source: \"docs/meetings.md\", Type: agentflow.DocumentTypeText, Tags: []string{\"meetings\", \"productivity\"}, } err = memory.IngestDocument(ctx, doc) if err != nil { log.Fatal(err) } // Build RAG context ragContext, err := memory.BuildContext(ctx, \"How to run effective meetings?\") if err != nil { log.Fatal(err) } log.Printf(\"RAG Context (%d tokens):\\n%s\", ragContext.TokenCount, ragContext.ContextText) }\rProduction Optimization Connection Pooling [memory.pgvector] connection = \"postgres://agentflow:password@localhost:5432/agentflow?sslmode=disable\u0026pool_max_conns=25\u0026pool_min_conns=5\"\rPerformance Tuning -- Optimize PostgreSQL for vector operations -- Add to postgresql.conf # Memory settings shared_buffers = 256MB effective_cache_size = 1GB work_mem = 64MB # Vector-specific settings max_parallel_workers_per_gather = 2 max_parallel_workers = 8 # Connection settings max_connections = 100\rMonitoring -- Monitor vector operations SELECT schemaname, tablename, n_tup_ins as inserts, n_tup_upd as updates, n_tup_del as deletes FROM pg_stat_user_tables WHERE tablename LIKE '%memory%'; -- Check index usage SELECT indexrelname, idx_scan, idx_tup_read, idx_tup_fetch FROM pg_stat_user_indexes WHERE indexrelname LIKE '%memory%';\rPros \u0026 Cons ✅ Advantages:\nFull persistence Excellent performance (~45ms queries) ACID transactions Mature ecosystem Advanced indexing Production-ready ❌ Considerations:\nRequires PostgreSQL setup More complex than in-memory Database maintenance needed 🔍 Weaviate Setup Dedicated vector database for large-scale operations\nPrerequisites Docker or Kubernetes 4GB+ RAM recommended Network access for Weaviate Docker Setup Step 1: Run Weaviate # Create Weaviate container docker run -d \\ --name agentflow-weaviate \\ -p 8080:8080 \\ -e QUERY_DEFAULTS_LIMIT=25 \\ -e AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED='true' \\ -e PERSISTENCE_DATA_PATH='/var/lib/weaviate' \\ -e DEFAULT_VECTORIZER_MODULE='none' \\ -e CLUSTER_HOSTNAME='node1' \\ -e ENABLE_MODULES='text2vec-openai,text2vec-cohere,text2vec-huggingface,ref2vec-centroid,generative-openai,qna-openai' \\ -v weaviate_data:/var/lib/weaviate \\ semitechnologies/weaviate:1.22.4 # Wait for startup sleep 15 # Verify Weaviate is running curl http://localhost:8080/v1/.well-known/ready\rStep 2: Test Connection # Check Weaviate status curl http://localhost:8080/v1/meta # Expected response: JSON with version info\rDocker Compose Setup Create docker-compose.yml:\nversion: '3.8' services: weaviate: image: semitechnologies/weaviate:1.22.4 container_name: agentflow-weaviate ports: - \"8080:8080\" environment: QUERY_DEFAULTS_LIMIT: 25 AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true' PERSISTENCE_DATA_PATH: '/var/lib/weaviate' DEFAULT_VECTORIZER_MODULE: 'none' CLUSTER_HOSTNAME: 'node1' ENABLE_MODULES: 'text2vec-openai,text2vec-cohere,text2vec-huggingface,ref2vec-centroid,generative-openai,qna-openai' volumes: - weaviate_data:/var/lib/weaviate healthcheck: test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8080/v1/.well-known/ready\"] interval: 10s timeout: 5s retries: 5 restart: unless-stopped volumes: weaviate_data: driver: local\rStart with Docker Compose:\n# Start Weaviate docker-compose up -d # Check logs docker-compose logs weaviate # Stop when needed docker-compose down\rConfiguration agentflow.toml [memory] enabled = true provider = \"weaviate\" max_results = 10 dimensions = 1536 auto_embed = true [memory.weaviate] # Update with your Weaviate connection details connection = \"http://localhost:8080\" class_name = \"AgentMemory\" [memory.embedding] provider = \"openai\" model = \"text-embedding-3-small\" [memory.rag] enabled = true chunk_size = 1000 overlap = 100 top_k = 5 score_threshold = 0.7\rEnvironment Variables # Weaviate connection export WEAVIATE_URL=\"http://localhost:8080\" export WEAVIATE_CLASS_NAME=\"AgentMemory\" # OpenAI API (for embeddings) export OPENAI_API_KEY=\"your-openai-api-key\"\rUsage Example package main import ( \"context\" \"log\" \"os\" agentflow \"github.com/kunalkushwaha/agentflow/core\" ) func main() { // Create Weaviate configuration config := agentflow.AgentMemoryConfig{ Provider: \"weaviate\", Connection: \"http://localhost:8080\", Dimensions: 1536, Embedding: agentflow.EmbeddingConfig{ Provider: \"openai\", APIKey: os.Getenv(\"OPENAI_API_KEY\"), Model: \"text-embedding-3-small\", }, } // Create memory provider memory, err := agentflow.NewMemory(config) if err != nil { log.Fatal(err) } defer memory.Close() // Create session ctx := memory.SetSession(context.Background(), \"user-456\") // Store and query memories err = memory.Store(ctx, \"I work remotely from San Francisco\", \"location\", \"work\") if err != nil { log.Fatal(err) } results, err := memory.Query(ctx, \"work location\", 3) if err != nil { log.Fatal(err) } for _, result := range results { log.Printf(\"Memory: %s (Score: %.2f)\", result.Content, result.Score) } }\rAdvanced Configuration Authentication (Production) # docker-compose.yml with authentication services: weaviate: environment: AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'false' AUTHENTICATION_APIKEY_ENABLED: 'true' AUTHENTICATION_APIKEY_ALLOWED_KEYS: 'your-secret-key' AUTHENTICATION_APIKEY_USERS: 'admin'\r# agentflow.toml with authentication [memory.weaviate] connection = \"http://localhost:8080\" api_key = \"your-secret-key\" class_name = \"AgentMemory\"\rClustering (Production) # Multi-node Weaviate cluster services: weaviate-node1: image: semitechnologies/weaviate:1.22.4 environment: CLUSTER_HOSTNAME: 'node1' CLUSTER_GOSSIP_BIND_PORT: '7100' CLUSTER_DATA_BIND_PORT: '7101' weaviate-node2: image: semitechnologies/weaviate:1.22.4 environment: CLUSTER_HOSTNAME: 'node2' CLUSTER_GOSSIP_BIND_PORT: '7102' CLUSTER_DATA_BIND_PORT: '7103' CLUSTER_JOIN: 'node1:7100'\rMonitoring Health Checks # Check Weaviate health curl http://localhost:8080/v1/.well-known/ready # Check cluster status curl http://localhost:8080/v1/nodes # Check schema curl http://localhost:8080/v1/schema\rPerformance Monitoring # Check metrics curl http://localhost:8080/v1/meta # Monitor resource usage docker stats agentflow-weaviate\rPros \u0026 Cons ✅ Advantages:\nPurpose-built for vectors Excellent scalability Advanced search features Built-in clustering GraphQL API Rich ecosystem ❌ Considerations:\nMore complex setup Higher resource usage Learning curve Newer ecosystem 📊 Provider Comparison Performance Comparison Operation Memory PgVector Weaviate Store ~1ms ~50ms ~75ms Query ~5ms ~45ms ~60ms Batch Store ~10ms ~2.3s ~3.1s RAG Context ~15ms ~90ms ~120ms Feature Comparison Feature Memory PgVector Weaviate Persistence ❌ ✅ ✅ Scalability ❌ ✅ ✅ ACID Transactions ❌ ✅ ❌ Vector Indexing ❌ ✅ ✅ Clustering ❌ ⚠️ ✅ GraphQL API ❌ ❌ ✅ Setup Complexity ✅ Easy ⚠️ Moderate ⚠️ Moderate Use Case Recommendations Choose Memory when: Developing and testing Prototyping quickly Temporary sessions only Minimal setup required Choose PgVector when: Building production applications Need ACID transactions Have PostgreSQL expertise Want mature ecosystem Choose Weaviate when: Large-scale vector operations Need advanced search features Building vector-first applications Want purpose-built vector DB 🔄 Migration Guide From Memory to PgVector Setup PostgreSQL with pgvector Update configuration: [memory] provider = \"pgvector\" # Changed from \"memory\" [memory.pgvector] connection = \"postgres://user:password@localhost:5432/agentflow\"\rAdd environment variables Test connection Migrate data (if needed) From PgVector to Weaviate Setup Weaviate Update configuration: [memory] provider = \"weaviate\" # Changed from \"pgvector\" [memory.weaviate] connection = \"http://localhost:8080\"\rExport data from PostgreSQL Import data to Weaviate Test functionality Data Migration Script func migrateMemoryProvider(oldMemory, newMemory agentflow.Memory) error { // Get all sessions (implementation depends on provider) sessions := []string{\"session1\", \"session2\"} // Get from old provider for _, sessionID := range sessions { ctx := oldMemory.SetSession(context.Background(), sessionID) newCtx := newMemory.SetSession(context.Background(), sessionID) // Migrate personal memories results, err := oldMemory.Query(ctx, \"\", 1000) // Get all if err != nil { return err } for _, result := range results { err = newMemory.Store(newCtx, result.Content, result.Tags...) if err != nil { return err } } // Migrate chat history history, err := oldMemory.GetHistory(ctx, 1000) if err != nil { return err } for _, msg := range history { err = newMemory.AddMessage(newCtx, msg.Role, msg.Content) if err != nil { return err } } } return nil }\r🔧 Troubleshooting Common Issues Connection Problems PostgreSQL Connection Failed:\n# Check if PostgreSQL is running docker ps | grep postgres # or sudo systemctl status postgresql # Test connection psql \"postgres://user:password@localhost:5432/dbname\" -c \"SELECT 1;\" # Check firewall sudo ufw status\rWeaviate Connection Failed:\n# Check if Weaviate is running docker ps | grep weaviate curl http://localhost:8080/v1/.well-known/ready # Check logs docker logs agentflow-weaviate\rPerformance Issues Slow Queries:\n-- PostgreSQL: Check query performance EXPLAIN ANALYZE SELECT * FROM agent_memory WHERE embedding \u003c-\u003e '[0.1,0.2,...]' \u003c 0.5; -- Add indexes if needed CREATE INDEX CONCURRENTLY idx_memory_embedding ON agent_memory USING ivfflat (embedding vector_cosine_ops);\rHigh Memory Usage:\n# Monitor resource usage docker stats # Adjust memory limits docker run --memory=2g --name agentflow-postgres ...\rData Issues Missing Extensions:\n-- PostgreSQL: Install pgvector CREATE EXTENSION IF NOT EXISTS vector; -- Check extensions SELECT * FROM pg_extension WHERE extname = 'vector';\rSchema Issues:\n# Weaviate: Check schema curl http://localhost:8080/v1/schema # Reset schema (careful!) curl -X DELETE http://localhost:8080/v1/schema\rDebug Mode Enable debug logging:\n[logging] level = \"debug\"\r// Enable debug logging in code import \"log\" log.SetFlags(log.LstdFlags | log.Lshortfile)\rHealth Check Scripts PostgreSQL Health Check #!/bin/bash # check-postgres.sh DB_URL=\"postgres://agentflow:password@localhost:5432/agentflow\" echo \"Checking PostgreSQL connection...\" if psql \"$DB_URL\" -c \"SELECT 1;\" \u003e /dev/null 2\u003e\u00261; then echo \"✅ PostgreSQL connection successful\" else echo \"❌ PostgreSQL connection failed\" exit 1 fi echo \"Checking pgvector extension...\" if psql \"$DB_URL\" -c \"SELECT * FROM pg_extension WHERE extname = 'vector';\" | grep -q vector; then echo \"✅ pgvector extension installed\" else echo \"❌ pgvector extension missing\" exit 1 fi echo \"✅ All checks passed\"\rWeaviate Health Check #!/bin/bash # check-weaviate.sh WEAVIATE_URL=\"http://localhost:8080\" echo \"Checking Weaviate connection...\" if curl -f \"$WEAVIATE_URL/v1/.well-known/ready\" \u003e /dev/null 2\u003e\u00261; then echo \"✅ Weaviate connection successful\" else echo \"❌ Weaviate connection failed\" exit 1 fi echo \"Checking Weaviate schema...\" if curl -f \"$WEAVIATE_URL/v1/schema\" \u003e /dev/null 2\u003e\u00261; then echo \"✅ Weaviate schema accessible\" else echo \"❌ Weaviate schema not accessible\" exit 1 fi echo \"✅ All checks passed\"\r🎯 Summary This guide covered complete setup for all AgentFlow memory providers:\n✅ In-Memory: Perfect for development and testing\n✅ PostgreSQL + pgvector: Production-ready with excellent performance\n✅ Weaviate: Advanced vector database for large-scale operations\nNext Steps Choose your provider based on your use case Follow the setup guide for your chosen provider Configure AgentFlow with the appropriate settings Test your setup with the provided examples Monitor and optimize for production use For more information:\nMemory System Guide - Complete API reference Configuration Guide - Advanced configuration options RAG Configuration Guide - RAG-specific settings",
    "description": "Memory Provider Setup Guide Complete setup guides for AgenticGoKit memory providers\nAgenticGoKit supports multiple memory providers for different use cases. This guide provides step-by-step setup instructions for each provider, from development to production deployment.\n📚 Table of Contents Overview In-Memory Provider PostgreSQL + pgvector Setup Weaviate Setup Provider Comparison Migration Guide Troubleshooting 🎯 Overview Provider Selection Guide Provider Best For Persistence Scalability Setup Complexity memory Development, Testing ❌ No ⚠️ Single instance ✅ Minimal pgvector Production, Enterprise ✅ Yes ✅ High ⚠️ Moderate weaviate Large-scale Vector Ops ✅ Yes ✅ Very High ⚠️ Moderate Quick Start Recommendations Just getting started? → Use memory provider Building a production app? → Use pgvector provider Need advanced vector features? → Use weaviate provider 💾 In-Memory Provider Perfect for development, testing, and temporary sessions",
    "tags": [],
    "title": "MemoryProviderSetup",
    "uri": "/AgenticGoKitDocs/guides/memoryprovidersetup/index.html"
  },
  {
    "breadcrumb": "content \u003e guides",
    "content": "Memory System Troubleshooting Guide This guide helps you troubleshoot common issues with the AgentFlow memory system, especially when using the scaffold-generated projects.\nQuick Diagnosis Use the memory debug command to quickly diagnose issues:\n# Basic overview and connection test agentcli memory # Detailed configuration validation agentcli memory --validate # Show current configuration agentcli memory --config\rCommon Issues and Solutions 1. Dimension Mismatch Errors Error Message:\n❌ Configuration Error: nomic-embed-text requires 768 dimensions, but 1536 configured\r💡 Solution: Update agentflow.toml [agent_memory] dimensions = 768\rCause: The embedding model dimensions don’t match the configured dimensions in your agentflow.toml.\nSolution:\nCheck your embedding model’s actual dimensions:\nagentcli memory --config\rUpdate your agentflow.toml:\n[agent_memory] dimensions = 768 # Match your embedding model\rIf using pgvector, update your database schema:\n-- Connect to your database psql -h localhost -U user -d agentflow -- Drop and recreate tables with correct dimensions DROP TABLE IF EXISTS agent_memory; CREATE TABLE agent_memory ( id SERIAL PRIMARY KEY, content TEXT NOT NULL, embedding vector(768), -- Use correct dimensions tags TEXT[], metadata JSONB, created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW() );\r2. Database Connection Issues PostgreSQL/PgVector Issues Error Message:\n❌ Failed to connect to memory system: connection refused\rSolutions:\nStart the database:\ndocker compose up -d\rCheck if PostgreSQL is running:\ndocker ps | grep postgres\rVerify connection string in agentflow.toml:\n[agent_memory] connection = \"postgres://user:password@localhost:5432/agentflow?sslmode=disable\"\rTest connection manually:\npsql -h localhost -U user -d agentflow\rCheck database exists:\n\\l -- List databases \\c agentflow -- Connect to agentflow database \\dt -- List tables\rRecreate database if needed:\n./setup.sh # or setup.bat on Windows\rWeaviate Issues Error Message:\n❌ Cannot connect to Weaviate database\rSolutions:\nStart Weaviate:\ndocker compose up -d\rCheck Weaviate status:\ncurl http://localhost:8080/v1/meta\rVerify connection in agentflow.toml:\n[agent_memory] connection = \"http://localhost:8080\"\r3. Embedding Provider Issues Ollama Issues Error Message:\n❌ Connection Error: Cannot connect to Ollama service\rSolutions:\nStart Ollama:\nollama serve\rPull the embedding model:\nollama pull nomic-embed-text:latest\rTest Ollama connection:\ncurl http://localhost:11434/api/tags\rVerify configuration:\n[agent_memory.embedding] provider = \"ollama\" model = \"nomic-embed-text:latest\" base_url = \"http://localhost:11434\"\rOpenAI Issues Error Message:\n❌ OpenAI API Error: authentication failed\rSolutions:\nSet API key:\nexport OPENAI_API_KEY=\"your-api-key-here\"\rVerify API key is valid:\ncurl -H \"Authorization: Bearer $OPENAI_API_KEY\" https://api.openai.com/v1/models\rCheck model name:\n[agent_memory.embedding] provider = \"openai\" model = \"text-embedding-3-small\" # or text-embedding-3-large\r4. Configuration Structure Issues Error Message:\n❌ Memory system not configured in agentflow.toml\rCause: Using old [memory] section instead of [agent_memory].\nSolution: Update your configuration structure:\n# ❌ Old format (doesn't work) [memory] provider = \"pgvector\" # ✅ New format (correct) [agent_memory] provider = \"pgvector\" connection = \"postgres://user:password@localhost:5432/agentflow?sslmode=disable\" dimensions = 768 auto_embed = true [agent_memory.embedding] provider = \"ollama\" model = \"nomic-embed-text:latest\" base_url = \"http://localhost:11434\"\r5. RAG Configuration Issues Error Message:\n❌ RAG chunk overlap must be between 0 and chunk_size\rSolution: Fix RAG parameters:\n[agent_memory] enable_rag = true chunk_size = 1000 chunk_overlap = 100 # Must be less than chunk_size knowledge_score_threshold = 0.7 # Between 0.0 and 1.0\rRecommended RAG Settings:\nChunk Size: 500-2000 tokens Chunk Overlap: 10-20% of chunk size Score Threshold: 0.6-0.8 Top-K Results: 3-10 6. Performance Issues Slow Queries Symptoms: Memory queries taking too long.\nSolutions:\nReduce dimensions if possible:\n[agent_memory] dimensions = 768 # Instead of 1536 or 3072\rOptimize database (PostgreSQL):\n-- Create index on embedding column CREATE INDEX ON agent_memory USING ivfflat (embedding vector_cosine_ops); -- Analyze table for better query planning ANALYZE agent_memory;\rReduce result count:\n[agent_memory] max_results = 5 # Instead of 10 or more\rHigh Memory Usage Solutions:\nEnable embedding caching:\n[agent_memory.embedding] cache_embeddings = true\rReduce batch size:\n[agent_memory.embedding] max_batch_size = 50 # Instead of 100\rUse smaller embedding model:\n[agent_memory.embedding] model = \"all-minilm\" # 384 dimensions instead of 768\rDiagnostic Commands Memory Debug Commands # Show overview and basic stats agentcli memory # Detailed statistics agentcli memory --stats # List recent memories agentcli memory --list # Test search functionality agentcli memory --search \"your query\" # Validate configuration agentcli memory --validate # Show current configuration agentcli memory --config # Show active sessions agentcli memory --sessions # List knowledge base documents agentcli memory --docs\rDatabase Diagnostic Commands PostgreSQL # Connect to database psql -h localhost -U user -d agentflow # Check table structure \\d agent_memory # Count records SELECT COUNT(*) FROM agent_memory; # Check embedding dimensions SELECT array_length(embedding, 1) FROM agent_memory LIMIT 1; # Check recent entries SELECT id, content, created_at FROM agent_memory ORDER BY created_at DESC LIMIT 5;\rWeaviate # Check Weaviate status curl http://localhost:8080/v1/meta # List classes curl http://localhost:8080/v1/schema # Check objects count curl http://localhost:8080/v1/objects\rMigration Guide Upgrading from Old Configuration Format If you have an existing project with the old [memory] configuration:\nBackup your data (if using persistent storage)\nUpdate configuration structure:\n# Create backup cp agentflow.toml agentflow.toml.backup # Update to new format (manual edit required) # Change [memory] to [agent_memory] # Add [agent_memory.embedding] section\rRegenerate project files:\n# Generate new project with correct structure agentcli create myproject-new --memory-enabled --memory-provider pgvector \\ --embedding-provider ollama --embedding-model nomic-embed-text:latest # Copy your custom agent logic to new project\rMigrate data (if needed):\n-- Export data from old format pg_dump -h localhost -U user -d agentflow -t agent_memory \u003e backup.sql -- Import to new database psql -h localhost -U user -d agentflow \u003c backup.sql\rChanging Embedding Models When changing embedding models with different dimensions:\nUpdate configuration:\n[agent_memory] dimensions = 1536 # New model dimensions [agent_memory.embedding] model = \"text-embedding-3-small\" # New model\rMigrate database schema:\n-- For PostgreSQL ALTER TABLE agent_memory ALTER COLUMN embedding TYPE vector(1536); -- You may need to recreate the table if this fails\rRe-embed existing content:\n# This would require custom script or clearing and re-adding content agentcli memory --clear # Use with caution!\rGetting Help If you’re still experiencing issues:\nCheck the logs for detailed error messages Use the validation command: agentcli memory --validate Check the GitHub issues: AgentFlow Issues Create a new issue with: Your agentflow.toml configuration Error messages Output of agentcli memory --validate Your operating system and Docker version Best Practices Configuration Always use the new [agent_memory] configuration format Match embedding dimensions with your chosen model Use recommended RAG parameters Enable embedding caching for better performance Development Start with in-memory provider for development Use pgvector for production Test configuration with agentcli memory --validate Monitor performance with agentcli memory --stats Production Use persistent storage (pgvector or weaviate) Set up proper database backups Monitor memory usage and query performance Use appropriate embedding models for your use case",
    "description": "Memory System Troubleshooting Guide This guide helps you troubleshoot common issues with the AgentFlow memory system, especially when using the scaffold-generated projects.\nQuick Diagnosis Use the memory debug command to quickly diagnose issues:\n# Basic overview and connection test agentcli memory # Detailed configuration validation agentcli memory --validate # Show current configuration agentcli memory --config\rCommon Issues and Solutions 1. Dimension Mismatch Errors Error Message:\n❌ Configuration Error: nomic-embed-text requires 768 dimensions, but 1536 configured\r💡 Solution: Update agentflow.toml [agent_memory] dimensions = 768\rCause: The embedding model dimensions don’t match the configured dimensions in your agentflow.toml.",
    "tags": [],
    "title": "MemoryTroubleshooting",
    "uri": "/AgenticGoKitDocs/guides/memorytroubleshooting/index.html"
  },
  {
    "breadcrumb": "content \u003e guides",
    "content": "Embedding Model Intelligence Guide AgentFlow includes an intelligent embedding model system that automatically configures appropriate settings based on your embedding model choice. This guide explains how to use and benefit from this system.\nOverview The Embedding Model Intelligence system:\nAutomatically configures dimensions based on your chosen embedding model Validates compatibility between embedding models and memory providers Provides recommendations for optimal model selection Offers troubleshooting guidance for common issues Supported Models Ollama Models (Recommended for Local Development) nomic-embed-text:latest ⭐ Recommended Dimensions: 768 Provider: ollama Notes: Excellent general-purpose embedding model with good performance Best for: Local development, privacy-focused applications # Install and use ollama pull nomic-embed-text:latest agentcli create myproject --memory-enabled --embedding-provider ollama --embedding-model nomic-embed-text:latest\rmxbai-embed-large Dimensions: 1024 Provider: ollama Notes: Larger model with better quality, requires more resources Best for: Applications requiring higher embedding quality ollama pull mxbai-embed-large agentcli create myproject --memory-enabled --embedding-provider ollama --embedding-model mxbai-embed-large\rall-minilm Dimensions: 384 Provider: ollama Notes: Lightweight and fast, good for development Best for: Resource-constrained environments, rapid prototyping ollama pull all-minilm agentcli create myproject --memory-enabled --embedding-provider ollama --embedding-model all-minilm\rOpenAI Models (Production Ready) text-embedding-3-small ⭐ Recommended Dimensions: 1536 Provider: openai Notes: Cost-effective OpenAI embedding model with good performance Best for: Production applications with budget considerations export OPENAI_API_KEY=\"your-api-key\" agentcli create myproject --memory-enabled --embedding-provider openai --embedding-model text-embedding-3-small\rtext-embedding-3-large Dimensions: 3072 Provider: openai Notes: Highest quality OpenAI embedding model, more expensive Best for: Applications requiring maximum embedding quality agentcli create myproject --memory-enabled --embedding-provider openai --embedding-model text-embedding-3-large\rtext-embedding-ada-002 (Legacy) Dimensions: 1536 Provider: openai Notes: Legacy OpenAI model, use text-embedding-3-small instead Status: Not recommended for new projects Testing Models dummy Dimensions: 1536 Provider: dummy Notes: Simple embeddings for testing only, not suitable for production Best for: Testing, development without external dependencies Automatic Configuration When you create a project, the system automatically:\nDetects model dimensions:\nagentcli create myproject --memory-enabled --embedding-model nomic-embed-text:latest # Automatically configures 768 dimensions\rValidates compatibility:\n# System checks if the model works with your memory provider # Warns about potential issues\rProvides helpful information:\n✓ Using embedding model: nomic-embed-text:latest (768 dimensions)\rExcellent general-purpose embedding model with good performance\rModel Selection Guide For Local Development Recommended: nomic-embed-text:latest with Ollama\nNo API costs Good performance Privacy-friendly Easy to set up # Setup ollama serve ollama pull nomic-embed-text:latest # Create project agentcli create myproject --memory-enabled \\ --memory-provider pgvector \\ --embedding-provider ollama \\ --embedding-model nomic-embed-text:latest\rFor Production Applications Budget-conscious: text-embedding-3-small with OpenAI\nGood quality-to-cost ratio Reliable and fast Well-supported High-quality: text-embedding-3-large with OpenAI\nBest embedding quality Higher cost Suitable for critical applications # Setup export OPENAI_API_KEY=\"your-api-key\" # Create project agentcli create myproject --memory-enabled \\ --memory-provider pgvector \\ --embedding-provider openai \\ --embedding-model text-embedding-3-small\rFor Resource-Constrained Environments Lightweight: all-minilm with Ollama\nSmallest dimensions (384) Fast processing Lower memory usage agentcli create myproject --memory-enabled \\ --memory-provider memory \\ --embedding-provider ollama \\ --embedding-model all-minilm\rCompatibility Matrix Embedding Model Dimensions Memory Provider Compatibility Notes nomic-embed-text:latest 768 pgvector ✅ Excellent Recommended combination nomic-embed-text:latest 768 weaviate ✅ Excellent Good for large scale nomic-embed-text:latest 768 memory ✅ Good Development only text-embedding-3-small 1536 pgvector ✅ Excellent Production ready text-embedding-3-large 3072 pgvector ⚠️ Good May impact performance text-embedding-3-large 3072 weaviate ✅ Excellent Handles large dimensions well dummy 1536 weaviate ❌ Poor Not recommended all-minilm 384 pgvector ✅ Excellent Very fast Configuration Examples Complete Configuration Examples Local Development Setup [agent_memory] provider = \"pgvector\" connection = \"postgres://user:password@localhost:5432/agentflow?sslmode=disable\" dimensions = 768 auto_embed = true enable_rag = true chunk_size = 1000 chunk_overlap = 100 [agent_memory.embedding] provider = \"ollama\" model = \"nomic-embed-text:latest\" base_url = \"http://localhost:11434\" cache_embeddings = true timeout_seconds = 30\rProduction Setup [agent_memory] provider = \"pgvector\" connection = \"postgres://user:password@prod-db:5432/agentflow?sslmode=require\" dimensions = 1536 auto_embed = true enable_rag = true chunk_size = 1000 chunk_overlap = 100 knowledge_score_threshold = 0.75 [agent_memory.embedding] provider = \"openai\" model = \"text-embedding-3-small\" cache_embeddings = true max_batch_size = 100 timeout_seconds = 30\rHigh-Performance Setup [agent_memory] provider = \"weaviate\" connection = \"http://weaviate:8080\" dimensions = 3072 auto_embed = true enable_rag = true chunk_size = 2000 chunk_overlap = 200 [agent_memory.embedding] provider = \"openai\" model = \"text-embedding-3-large\" cache_embeddings = true max_batch_size = 50 timeout_seconds = 60\rValidation and Troubleshooting Using the Intelligence System The system provides automatic validation:\n# Check if your model is recognized agentcli create myproject --memory-enabled --embedding-model unknown-model # Output: ⚠️ Unknown embedding model: ollama/unknown-model # 💡 Recommended models for this provider: # • nomic-embed-text:latest (768 dimensions) - Excellent general-purpose...\rManual Validation # Validate your configuration agentcli memory --validate # Check current configuration agentcli memory --config # Test search with your embeddings agentcli memory --search \"test query\"\rCommon Issues and Solutions Unknown Model Warning ⚠️ Unknown embedding model: ollama/custom-model\rSolution: Use a recognized model or verify your custom model’s dimensions:\n# Check available models agentcli create --help | grep -A 10 \"EMBEDDING PROVIDERS\" # Or use a recommended model agentcli create myproject --embedding-model nomic-embed-text:latest\rDimension Mismatch ❌ Configuration Error: nomic-embed-text requires 768 dimensions, but 1536 configured\rSolution: The system automatically prevents this, but if you encounter it:\n[agent_memory] dimensions = 768 # Match your embedding model\rPerformance Warnings ⚠️ Large embedding dimensions (3072) may impact performance\rSolution: Consider using a smaller model for better performance:\n# Instead of text-embedding-3-large (3072 dims) agentcli create myproject --embedding-model text-embedding-3-small # 1536 dims\rAdvanced Usage Custom Model Integration If you need to use a custom embedding model:\nAdd it to the intelligence system (for developers):\n// In internal/scaffold/embedding_intelligence.go \"custom-provider\": { \"custom-model\": { Provider: \"custom-provider\", Model: \"custom-model\", Dimensions: 512, Notes: \"Custom embedding model\", }, }\rUse fallback dimensions:\n# System will use provider defaults agentcli create myproject --embedding-provider ollama --embedding-model custom-model\rPerformance Optimization Dimension Selection 384 dimensions: Fastest, lowest memory usage 768 dimensions: Good balance of speed and quality 1536 dimensions: Standard quality, moderate performance 3072 dimensions: Highest quality, slower performance Provider Selection Ollama: Best for local development, no API costs OpenAI: Best for production, consistent quality Dummy: Only for testing, no real embeddings Memory Provider Pairing Small dimensions (384-768) + PgVector: Excellent performance Large dimensions (1536-3072) + Weaviate: Better handling of high-dimensional vectors Any dimensions + Memory: Fast but non-persistent Migration Between Models Changing Embedding Models When switching embedding models:\nUpdate configuration:\n[agent_memory] dimensions = 1536 # New model dimensions [agent_memory.embedding] model = \"text-embedding-3-small\" # New model\rHandle existing data:\n# Option 1: Clear and restart (loses data) agentcli memory --clear # Option 2: Re-embed existing content (custom script needed) # Option 3: Keep old embeddings (may reduce search quality)\rTest the new setup:\nagentcli memory --validate agentcli memory --search \"test query\"\rBest Practices Model Selection Start with recommended models (nomic-embed-text:latest or text-embedding-3-small) Consider your use case (local vs. production, cost vs. quality) Test performance with your specific data and queries Monitor resource usage and adjust as needed Configuration Use the intelligence system - let it configure dimensions automatically Validate your setup with agentcli memory --validate Test thoroughly before production deployment Monitor performance and adjust settings as needed Development Workflow Start local with Ollama and nomic-embed-text:latest Test functionality with in-memory or pgvector Optimize configuration based on your data Deploy to production with appropriate model and provider This guide should help you make informed decisions about embedding models and get the most out of AgentFlow’s intelligent configuration system.",
    "description": "Embedding Model Intelligence Guide AgentFlow includes an intelligent embedding model system that automatically configures appropriate settings based on your embedding model choice. This guide explains how to use and benefit from this system.\nOverview The Embedding Model Intelligence system:\nAutomatically configures dimensions based on your chosen embedding model Validates compatibility between embedding models and memory providers Provides recommendations for optimal model selection Offers troubleshooting guidance for common issues Supported Models Ollama Models (Recommended for Local Development) nomic-embed-text:latest ⭐ Recommended Dimensions: 768 Provider: ollama Notes: Excellent general-purpose embedding model with good performance Best for: Local development, privacy-focused applications # Install and use ollama pull nomic-embed-text:latest agentcli create myproject --memory-enabled --embedding-provider ollama --embedding-model nomic-embed-text:latest\rmxbai-embed-large Dimensions: 1024 Provider: ollama Notes: Larger model with better quality, requires more resources Best for: Applications requiring higher embedding quality ollama pull mxbai-embed-large agentcli create myproject --memory-enabled --embedding-provider ollama --embedding-model mxbai-embed-large\rall-minilm Dimensions: 384 Provider: ollama Notes: Lightweight and fast, good for development Best for: Resource-constrained environments, rapid prototyping ollama pull all-minilm agentcli create myproject --memory-enabled --embedding-provider ollama --embedding-model all-minilm\rOpenAI Models (Production Ready) text-embedding-3-small ⭐ Recommended Dimensions: 1536 Provider: openai Notes: Cost-effective OpenAI embedding model with good performance Best for: Production applications with budget considerations export OPENAI_API_KEY=\"your-api-key\" agentcli create myproject --memory-enabled --embedding-provider openai --embedding-model text-embedding-3-small\rtext-embedding-3-large Dimensions: 3072 Provider: openai Notes: Highest quality OpenAI embedding model, more expensive Best for: Applications requiring maximum embedding quality agentcli create myproject --memory-enabled --embedding-provider openai --embedding-model text-embedding-3-large\rtext-embedding-ada-002 (Legacy) Dimensions: 1536 Provider: openai Notes: Legacy OpenAI model, use text-embedding-3-small instead Status: Not recommended for new projects Testing Models dummy Dimensions: 1536 Provider: dummy Notes: Simple embeddings for testing only, not suitable for production Best for: Testing, development without external dependencies Automatic Configuration When you create a project, the system automatically:",
    "tags": [],
    "title": "EmbeddingModelGuide",
    "uri": "/AgenticGoKitDocs/guides/embeddingmodelguide/index.html"
  },
  {
    "breadcrumb": "content \u003e guides",
    "content": "RAG Configuration Guide Configuring Retrieval-Augmented Generation in AgentFlow\nAgentFlow provides comprehensive RAG (Retrieval-Augmented Generation) capabilities through flexible TOML configuration. This guide covers all configuration options for building knowledge-aware agents with document understanding and context assembly.\n📚 Table of Contents Overview Basic Configuration Advanced Configuration Configuration Examples Best Practices Performance Tuning Migration Guide Troubleshooting 🎯 Overview What is RAG? RAG (Retrieval-Augmented Generation) enhances LLM responses by:\nRetrieving relevant information from knowledge bases and personal memory Augmenting prompts with contextual information Generating informed responses based on retrieved knowledge Providing source attribution for transparency AgentFlow RAG Features Hybrid Search: Combines semantic similarity and keyword matching Document Processing: Automatic chunking and metadata extraction Context Assembly: Intelligent context building with token management Session Memory: Isolated memory per user or conversation Multiple Providers: Support for various vector databases ⚙️ Basic Configuration Minimal RAG Setup [memory] enabled = true provider = \"memory\" dimensions = 1536 [memory.embedding] provider = \"openai\" model = \"text-embedding-3-small\" [memory.rag] enabled = true chunk_size = 1000 top_k = 5\rCore Memory Settings [memory] enabled = true provider = \"pgvector\" # Options: memory, pgvector, weaviate connection = \"postgres://user:password@localhost:5432/agentflow\" max_results = 10 # Maximum personal memory results dimensions = 1536 # Vector embedding dimensions auto_embed = true # Automatically generate embeddings\rRAG Configuration [memory.rag] enabled = true # Enable RAG functionality chunk_size = 1000 # Document chunk size in characters overlap = 200 # Overlap between chunks top_k = 5 # Number of results to retrieve score_threshold = 0.7 # Minimum similarity score hybrid_search = true # Enable hybrid search session_memory = true # Enable session isolation\r🔧 Advanced Configuration Document Processing [memory.documents] auto_chunk = true # Automatically chunk documents supported_types = [\"pdf\", \"txt\", \"md\", \"web\", \"code\", \"json\"] max_file_size = \"10MB\" # Maximum file size enable_metadata_extraction = true # Extract document metadata enable_url_scraping = true # Enable web content scraping\rEmbedding Configuration [memory.embedding] provider = \"openai\" # Options: openai, azure, ollama, dummy model = \"text-embedding-3-small\" # Embedding model api_key = \"${OPENAI_API_KEY}\" # API key (environment variable) base_url = \"\" # Custom endpoint (optional) max_batch_size = 50 # Batch size for embeddings timeout_seconds = 30 # Request timeout cache_embeddings = true # Cache embeddings for performance\rSearch Configuration [memory.search] hybrid_search = true # Enable hybrid search keyword_weight = 0.3 # Weight for keyword search (BM25) semantic_weight = 0.7 # Weight for semantic search enable_reranking = false # Enable result re-ranking reranking_model = \"\" # Re-ranking model (optional) enable_query_expansion = false # Enable query expansion\rContext Assembly [memory.context] max_tokens = 4000 # Maximum context tokens personal_weight = 0.3 # Weight for personal memory knowledge_weight = 0.7 # Weight for knowledge base include_sources = true # Include source attribution include_history = true # Include chat history history_limit = 5 # Number of history messages\r📋 Configuration Examples Development Configuration # agentflow.dev.toml - Development setup [memory] enabled = true provider = \"memory\" # In-memory for fast iteration max_results = 10 dimensions = 1536 auto_embed = true [memory.embedding] provider = \"dummy\" # Dummy embeddings for testing model = \"text-embedding-3-small\" cache_embeddings = true [memory.rag] enabled = true chunk_size = 800 # Smaller chunks for testing overlap = 150 top_k = 3 # Fewer results for faster testing score_threshold = 0.5 # Lower threshold for development hybrid_search = false # Disable for simplicity session_memory = false [memory.documents] auto_chunk = true supported_types = [\"txt\", \"md\", \"code\"] max_file_size = \"5MB\" enable_metadata_extraction = false # Disable for speed enable_url_scraping = false\rProduction Configuration # agentflow.prod.toml - Production deployment [memory] enabled = true provider = \"pgvector\" connection = \"postgres://user:password@localhost:5432/agentflow?sslmode=require\" max_results = 10 dimensions = 1536 auto_embed = true [memory.pgvector] table_name = \"agent_memory\" connection_pool_size = 25 [memory.embedding] provider = \"openai\" model = \"text-embedding-3-small\" api_key = \"${OPENAI_API_KEY}\" cache_embeddings = true max_batch_size = 100 timeout_seconds = 30 [memory.rag] enabled = true chunk_size = 1000 overlap = 200 top_k = 5 score_threshold = 0.7 hybrid_search = true session_memory = true [memory.documents] auto_chunk = true supported_types = [\"pdf\", \"txt\", \"md\", \"web\", \"code\", \"json\"] max_file_size = \"10MB\" enable_metadata_extraction = true enable_url_scraping = true [memory.context] max_tokens = 4000 personal_weight = 0.3 knowledge_weight = 0.7 include_sources = true include_history = true history_limit = 5 [memory.search] hybrid_search = true keyword_weight = 0.3 semantic_weight = 0.7 enable_reranking = false enable_query_expansion = false [memory.advanced] retry_max_attempts = 3 retry_base_delay = \"100ms\" retry_max_delay = \"5s\" health_check_interval = \"1m\"\rEnterprise Configuration # agentflow.enterprise.toml - Large-scale deployment [memory] enabled = true provider = \"weaviate\" connection = \"https://weaviate.company.com:8080\" max_results = 15 dimensions = 1536 auto_embed = true [memory.weaviate] api_key = \"${WEAVIATE_API_KEY}\" class_name = \"AgentMemory\" timeout = \"30s\" max_retries = 3 [memory.embedding] provider = \"azure\" model = \"text-embedding-ada-002\" api_key = \"${AZURE_OPENAI_API_KEY}\" endpoint = \"${AZURE_OPENAI_ENDPOINT}\" cache_embeddings = true max_batch_size = 100 timeout_seconds = 45 [memory.rag] enabled = true chunk_size = 1200 overlap = 240 top_k = 8 score_threshold = 0.75 hybrid_search = true session_memory = true [memory.context] max_tokens = 6000 # Larger context for enterprise personal_weight = 0.2 knowledge_weight = 0.8 # Knowledge-focused include_sources = true include_history = true history_limit = 10 [memory.search] hybrid_search = true keyword_weight = 0.25 semantic_weight = 0.75 enable_reranking = true reranking_model = \"cross-encoder/ms-marco-MiniLM-L-12-v2\" enable_query_expansion = true\r🚀 Best Practices Provider Selection Use Case Recommended Provider Reason Development memory Fast iteration, no setup Production pgvector Reliable, performant, mature Enterprise weaviate Advanced features, clustering Prototyping memory Quick testing, temporary data Chunking Strategy # Document type specific chunking [memory.rag] # For technical documentation chunk_size = 1000 overlap = 200 # For code files chunk_size = 800 overlap = 100 # For research papers chunk_size = 1500 overlap = 300\rGuidelines:\nSmall documents (\u003c 5KB): chunk_size = 500-800, overlap = 100-150 Medium documents (5-50KB): chunk_size = 1000-1200, overlap = 200-250 Large documents (\u003e 50KB): chunk_size = 1200-1500, overlap = 250-300 Code files: chunk_size = 600-1000, overlap = 50-100 Context Weighting [memory.context] # Knowledge-focused (documentation, Q\u0026A) personal_weight = 0.2 knowledge_weight = 0.8 # Balanced (general purpose) personal_weight = 0.5 knowledge_weight = 0.5 # Personal-focused (assistant, preferences) personal_weight = 0.7 knowledge_weight = 0.3\rScore Thresholds [memory.rag] # High precision (strict matching) score_threshold = 0.8 # Balanced (recommended) score_threshold = 0.7 # High recall (permissive) score_threshold = 0.5 # Development/testing score_threshold = 0.3\rContext Limits by Model [memory.context] # GPT-3.5 Turbo max_tokens = 3000 # GPT-4 max_tokens = 6000 # GPT-4 Turbo max_tokens = 8000 # Claude 3 max_tokens = 10000\r⚡ Performance Tuning Memory Provider Optimization [memory] # Reduce for faster queries max_results = 5 [memory.rag] # Optimize retrieval top_k = 3 # Fewer results = faster score_threshold = 0.8 # Higher threshold = fewer results\rEmbedding Optimization [memory.embedding] cache_embeddings = true # Essential for performance max_batch_size = 100 # Larger batches = fewer API calls timeout_seconds = 30 # Reasonable timeout # Provider-specific optimizations [memory.embedding.openai] max_requests_per_minute = 3000 # Respect rate limits [memory.embedding.azure] deployment_name = \"text-embedding-ada-002\" api_version = \"2023-05-15\"\rSearch Optimization [memory.search] hybrid_search = true # Best balance of speed/accuracy keyword_weight = 0.3 # Adjust based on query types semantic_weight = 0.7 # Higher for conceptual queries enable_reranking = false # Disable if not needed (faster)\rDatabase Optimization # PostgreSQL + pgvector [memory.pgvector] connection = \"postgres://user:password@localhost:5432/agentflow?pool_max_conns=25\u0026pool_min_conns=5\" # Weaviate [memory.weaviate] timeout = \"15s\" # Shorter timeout for faster failures max_retries = 2 # Fewer retries\r🔄 Migration Guide From Basic to RAG Configuration Step 1: Enable RAG # Add to existing configuration [memory.rag] enabled = true chunk_size = 1000 top_k = 5 score_threshold = 0.7\rStep 2: Configure Document Processing [memory.documents] auto_chunk = true supported_types = [\"txt\", \"md\", \"pdf\"] max_file_size = \"10MB\"\rStep 3: Set Up Embeddings [memory.embedding] provider = \"openai\" model = \"text-embedding-3-small\" api_key = \"${OPENAI_API_KEY}\"\rStep 4: Test Configuration # Test with AgentFlow CLI agentcli create test-rag --memory-enabled --rag-enabled --memory-provider pgvector # Verify configuration cd test-rag go run . -m \"Test RAG functionality\"\rFrom Memory to PgVector # Before [memory] provider = \"memory\" # After [memory] provider = \"pgvector\" connection = \"postgres://user:password@localhost:5432/agentflow\" [memory.pgvector] table_name = \"agent_memory\"\rAdding Hybrid Search # Add to existing RAG configuration [memory.search] hybrid_search = true keyword_weight = 0.3 semantic_weight = 0.7\r🔧 Troubleshooting Common Configuration Issues 1. Invalid Weight Configuration # ❌ Wrong - weights don't sum to 1.0 [memory.context] personal_weight = 0.5 knowledge_weight = 0.8 # ✅ Correct - weights sum to 1.0 [memory.context] personal_weight = 0.3 knowledge_weight = 0.7\r2. Chunk Overlap Too Large # ❌ Wrong - overlap \u003e= chunk_size [memory.rag] chunk_size = 1000 overlap = 1000 # ✅ Correct - overlap \u003c chunk_size [memory.rag] chunk_size = 1000 overlap = 200\r3. Missing Environment Variables # Check required variables echo $OPENAI_API_KEY echo $DATABASE_URL # Set if missing export OPENAI_API_KEY=\"your-api-key\" export DATABASE_URL=\"postgres://user:password@localhost:5432/agentflow\"\rValidation Errors AgentFlow automatically validates configurations:\n# Common validation messages \"RAG weights must sum to 1.0 (current: 1.3)\" \"Chunk overlap must be less than chunk size\" \"Score threshold must be between 0.0 and 1.0\" \"Provider 'invalid' not supported\"\rPerformance Issues Slow RAG Queries # Reduce context size [memory.context] max_tokens = 2000 # Smaller context [memory.rag] top_k = 3 # Fewer results score_threshold = 0.8 # Higher threshold\rHigh Memory Usage # Optimize embedding cache [memory.embedding] cache_embeddings = false # Disable if memory constrained max_batch_size = 25 # Smaller batches\rDebug Configuration # Enable debug logging [logging] level = \"debug\" # Test configuration [memory.advanced] health_check_interval = \"30s\" # More frequent health checks\r📊 Configuration Validation AgentFlow provides automatic validation with helpful error messages:\nWeight Validation RAG weights (personal + knowledge) must sum to 1.0 Search weights (keyword + semantic) must sum to 1.0 Range Validation Score thresholds must be between 0.0 and 1.0 Chunk overlap must be less than chunk size Token limits must be positive integers Provider Validation Ensures provider-specific settings are correct Validates connection strings and API keys Checks for required dependencies 🎛️ Environment Variables Required Variables # LLM Provider export OPENAI_API_KEY=\"your-openai-api-key\" # or export AZURE_OPENAI_API_KEY=\"your-azure-api-key\" export AZURE_OPENAI_ENDPOINT=\"https://your-resource.openai.azure.com/\" # Database (if using pgvector) export DATABASE_URL=\"postgres://user:password@localhost:5432/agentflow\" # Weaviate (if using weaviate) export WEAVIATE_URL=\"http://localhost:8080\" export WEAVIATE_API_KEY=\"your-weaviate-api-key\"\rOptional Variables # Ollama (for local embeddings) export OLLAMA_BASE_URL=\"http://localhost:11434\" export OLLAMA_MODEL=\"mxbai-embed-large\" # Configuration export AGENTFLOW_CONFIG_PATH=\"/path/to/agentflow.toml\" export AGENTFLOW_LOG_LEVEL=\"debug\"\rUsing in Configuration [memory.embedding] api_key = \"${OPENAI_API_KEY}\" endpoint = \"${AZURE_OPENAI_ENDPOINT}\" [memory.pgvector] connection = \"${DATABASE_URL}\"\r📚 Complete Configuration Reference [memory] # Core settings enabled = true provider = \"memory|pgvector|weaviate\" connection = \"connection-string\" max_results = 10 dimensions = 1536 auto_embed = true # RAG settings [memory.rag] enabled = true chunk_size = 1000 overlap = 200 top_k = 5 score_threshold = 0.7 hybrid_search = true session_memory = true # Document processing [memory.documents] auto_chunk = true supported_types = [\"pdf\", \"txt\", \"md\", \"web\", \"code\", \"json\"] max_file_size = \"10MB\" enable_metadata_extraction = true enable_url_scraping = true # Embedding service [memory.embedding] provider = \"openai|azure|ollama|dummy\" model = \"text-embedding-3-small\" api_key = \"${API_KEY}\" endpoint = \"${ENDPOINT}\" cache_embeddings = true max_batch_size = 50 timeout_seconds = 30 # Search configuration [memory.search] hybrid_search = true keyword_weight = 0.3 semantic_weight = 0.7 enable_reranking = false reranking_model = \"\" enable_query_expansion = false # Context assembly [memory.context] max_tokens = 4000 personal_weight = 0.3 knowledge_weight = 0.7 include_sources = true include_history = true history_limit = 5 # Advanced settings [memory.advanced] retry_max_attempts = 3 retry_base_delay = \"100ms\" retry_max_delay = \"5s\" connection_pool_size = 25 health_check_interval = \"1m\"\r🎯 Summary AgentFlow’s RAG configuration system provides:\n✅ Flexible Configuration: TOML-based configuration with environment variable support\n✅ Multiple Providers: Support for in-memory, PostgreSQL, and Weaviate backends\n✅ Advanced Features: Hybrid search, context assembly, session isolation\n✅ Performance Tuning: Extensive optimization options for production use\n✅ Validation: Built-in configuration validation with helpful error messages\n✅ Production Ready: Enterprise-grade features with monitoring and health checks\nThe RAG system enables building sophisticated knowledge-aware agents that can understand documents, maintain context, and provide informed responses with source attribution.\nFor more information:\nMemory System Guide - Complete memory API reference Memory Provider Setup - Provider installation guides Configuration Guide - General configuration patterns",
    "description": "RAG Configuration Guide Configuring Retrieval-Augmented Generation in AgentFlow\nAgentFlow provides comprehensive RAG (Retrieval-Augmented Generation) capabilities through flexible TOML configuration. This guide covers all configuration options for building knowledge-aware agents with document understanding and context assembly.\n📚 Table of Contents Overview Basic Configuration Advanced Configuration Configuration Examples Best Practices Performance Tuning Migration Guide Troubleshooting 🎯 Overview What is RAG? RAG (Retrieval-Augmented Generation) enhances LLM responses by:\nRetrieving relevant information from knowledge bases and personal memory Augmenting prompts with contextual information Generating informed responses based on retrieved knowledge Providing source attribution for transparency AgentFlow RAG Features Hybrid Search: Combines semantic similarity and keyword matching Document Processing: Automatic chunking and metadata extraction Context Assembly: Intelligent context building with token management Session Memory: Isolated memory per user or conversation Multiple Providers: Support for various vector databases ⚙️ Basic Configuration Minimal RAG Setup [memory] enabled = true provider = \"memory\" dimensions = 1536 [memory.embedding] provider = \"openai\" model = \"text-embedding-3-small\" [memory.rag] enabled = true chunk_size = 1000 top_k = 5\rCore Memory Settings [memory] enabled = true provider = \"pgvector\" # Options: memory, pgvector, weaviate connection = \"postgres://user:password@localhost:5432/agentflow\" max_results = 10 # Maximum personal memory results dimensions = 1536 # Vector embedding dimensions auto_embed = true # Automatically generate embeddings\rRAG Configuration [memory.rag] enabled = true # Enable RAG functionality chunk_size = 1000 # Document chunk size in characters overlap = 200 # Overlap between chunks top_k = 5 # Number of results to retrieve score_threshold = 0.7 # Minimum similarity score hybrid_search = true # Enable hybrid search session_memory = true # Enable session isolation\r🔧 Advanced Configuration Document Processing [memory.documents] auto_chunk = true # Automatically chunk documents supported_types = [\"pdf\", \"txt\", \"md\", \"web\", \"code\", \"json\"] max_file_size = \"10MB\" # Maximum file size enable_metadata_extraction = true # Extract document metadata enable_url_scraping = true # Enable web content scraping\rEmbedding Configuration [memory.embedding] provider = \"openai\" # Options: openai, azure, ollama, dummy model = \"text-embedding-3-small\" # Embedding model api_key = \"${OPENAI_API_KEY}\" # API key (environment variable) base_url = \"\" # Custom endpoint (optional) max_batch_size = 50 # Batch size for embeddings timeout_seconds = 30 # Request timeout cache_embeddings = true # Cache embeddings for performance\rSearch Configuration [memory.search] hybrid_search = true # Enable hybrid search keyword_weight = 0.3 # Weight for keyword search (BM25) semantic_weight = 0.7 # Weight for semantic search enable_reranking = false # Enable result re-ranking reranking_model = \"\" # Re-ranking model (optional) enable_query_expansion = false # Enable query expansion\rContext Assembly [memory.context] max_tokens = 4000 # Maximum context tokens personal_weight = 0.3 # Weight for personal memory knowledge_weight = 0.7 # Weight for knowledge base include_sources = true # Include source attribution include_history = true # Include chat history history_limit = 5 # Number of history messages\r📋 Configuration Examples Development Configuration # agentflow.dev.toml - Development setup [memory] enabled = true provider = \"memory\" # In-memory for fast iteration max_results = 10 dimensions = 1536 auto_embed = true [memory.embedding] provider = \"dummy\" # Dummy embeddings for testing model = \"text-embedding-3-small\" cache_embeddings = true [memory.rag] enabled = true chunk_size = 800 # Smaller chunks for testing overlap = 150 top_k = 3 # Fewer results for faster testing score_threshold = 0.5 # Lower threshold for development hybrid_search = false # Disable for simplicity session_memory = false [memory.documents] auto_chunk = true supported_types = [\"txt\", \"md\", \"code\"] max_file_size = \"5MB\" enable_metadata_extraction = false # Disable for speed enable_url_scraping = false\rProduction Configuration # agentflow.prod.toml - Production deployment [memory] enabled = true provider = \"pgvector\" connection = \"postgres://user:password@localhost:5432/agentflow?sslmode=require\" max_results = 10 dimensions = 1536 auto_embed = true [memory.pgvector] table_name = \"agent_memory\" connection_pool_size = 25 [memory.embedding] provider = \"openai\" model = \"text-embedding-3-small\" api_key = \"${OPENAI_API_KEY}\" cache_embeddings = true max_batch_size = 100 timeout_seconds = 30 [memory.rag] enabled = true chunk_size = 1000 overlap = 200 top_k = 5 score_threshold = 0.7 hybrid_search = true session_memory = true [memory.documents] auto_chunk = true supported_types = [\"pdf\", \"txt\", \"md\", \"web\", \"code\", \"json\"] max_file_size = \"10MB\" enable_metadata_extraction = true enable_url_scraping = true [memory.context] max_tokens = 4000 personal_weight = 0.3 knowledge_weight = 0.7 include_sources = true include_history = true history_limit = 5 [memory.search] hybrid_search = true keyword_weight = 0.3 semantic_weight = 0.7 enable_reranking = false enable_query_expansion = false [memory.advanced] retry_max_attempts = 3 retry_base_delay = \"100ms\" retry_max_delay = \"5s\" health_check_interval = \"1m\"\rEnterprise Configuration # agentflow.enterprise.toml - Large-scale deployment [memory] enabled = true provider = \"weaviate\" connection = \"https://weaviate.company.com:8080\" max_results = 15 dimensions = 1536 auto_embed = true [memory.weaviate] api_key = \"${WEAVIATE_API_KEY}\" class_name = \"AgentMemory\" timeout = \"30s\" max_retries = 3 [memory.embedding] provider = \"azure\" model = \"text-embedding-ada-002\" api_key = \"${AZURE_OPENAI_API_KEY}\" endpoint = \"${AZURE_OPENAI_ENDPOINT}\" cache_embeddings = true max_batch_size = 100 timeout_seconds = 45 [memory.rag] enabled = true chunk_size = 1200 overlap = 240 top_k = 8 score_threshold = 0.75 hybrid_search = true session_memory = true [memory.context] max_tokens = 6000 # Larger context for enterprise personal_weight = 0.2 knowledge_weight = 0.8 # Knowledge-focused include_sources = true include_history = true history_limit = 10 [memory.search] hybrid_search = true keyword_weight = 0.25 semantic_weight = 0.75 enable_reranking = true reranking_model = \"cross-encoder/ms-marco-MiniLM-L-12-v2\" enable_query_expansion = true\r🚀 Best Practices Provider Selection Use Case Recommended Provider Reason Development memory Fast iteration, no setup Production pgvector Reliable, performant, mature Enterprise weaviate Advanced features, clustering Prototyping memory Quick testing, temporary data Chunking Strategy # Document type specific chunking [memory.rag] # For technical documentation chunk_size = 1000 overlap = 200 # For code files chunk_size = 800 overlap = 100 # For research papers chunk_size = 1500 overlap = 300\rGuidelines:",
    "tags": [],
    "title": "RAGConfiguration",
    "uri": "/AgenticGoKitDocs/guides/ragconfiguration/index.html"
  },
  {
    "breadcrumb": "content \u003e guides",
    "content": "Tool Integration Dynamic Tool Discovery and Execution with MCP Protocol\nAgentFlow uses the Model Context Protocol (MCP) to provide agents with dynamic tool discovery and execution capabilities. This guide covers everything from basic tool usage to building custom MCP servers.\nOverview The MCP integration in AgentFlow provides:\nDynamic Discovery: Tools are discovered at runtime, not hard-coded Schema-Based: Tools provide their own descriptions and parameters LLM-Driven: The LLM decides which tools to use based on context Extensible: Add new tools by connecting MCP servers Core MCP Functions AgentFlow provides two key functions that handle all MCP complexity:\nFormatToolsForPrompt() Discovers available tools and formats them for LLM consumption:\ntoolPrompt := agentflow.FormatToolsForPrompt(ctx, mcpManager)\rExample output:\nAvailable tools:\r1. search(query: string) - Search the web for information\r2. docker(command: string, args: array) - Execute Docker commands\r3. fetch_content(url: string) - Fetch content from a URL\rParseAndExecuteToolCalls() Parses LLM responses and executes any tool calls found:\ntoolResults := agentflow.ParseAndExecuteToolCalls(ctx, mcpManager, llmResponse)\rHandles tool calls like:\nI'll search for that information.\r\u003ctool_call\u003e\r{\"name\": \"search\", \"args\": {\"query\": \"latest Go tutorials 2025\"}}\r\u003c/tool_call\u003e\rLet me also fetch the official Go documentation.\r\u003ctool_call\u003e\r{\"name\": \"fetch_content\", \"args\": {\"url\": \"https://golang.org/doc/\"}}\r\u003c/tool_call\u003e\rComplete Agent with Tools Here’s a complete agent that uses tools intelligently:\npackage main import ( \"context\" \"fmt\" agentflow \"github.com/kunalkushwaha/agentflow/core\" ) type ToolEnabledAgent struct { llm agentflow.ModelProvider mcpManager agentflow.MCPManager } func NewToolEnabledAgent(llm agentflow.ModelProvider, mcp agentflow.MCPManager) *ToolEnabledAgent { return \u0026ToolEnabledAgent{ llm: llm, mcpManager: mcp, } } func (a *ToolEnabledAgent) Run(ctx context.Context, event agentflow.Event, state agentflow.State) (agentflow.AgentResult, error) { logger := agentflow.Logger() // Extract user message message := event.GetData()[\"message\"] // Build system prompt with tool awareness systemPrompt := `You are a helpful assistant with access to various tools. Key principles: - Analyze what information the user needs - Use tools when they can provide current, specific, or actionable information - For factual queries, prefer search tools over general knowledge - For web content, use fetch_content for specific URLs - For technical tasks, use appropriate tools (docker, etc.) - Always explain what tools you're using and why When using tools, format calls exactly like this: \u003ctool_call\u003e {\"name\": \"tool_name\", \"args\": {\"param\": \"value\"}} \u003c/tool_call\u003e` // Get available tools and add to prompt toolPrompt := \"\" if a.mcpManager != nil { toolPrompt = agentflow.FormatToolsForPrompt(ctx, a.mcpManager) logger.Info().Int(\"tool_count\", len(strings.Split(toolPrompt, \"\\n\"))-1).Msg(\"Tools available\") } fullPrompt := fmt.Sprintf(\"%s\\n\\n%s\\n\\nUser: %s\", systemPrompt, toolPrompt, message) // Get initial LLM response response, err := a.llm.Generate(ctx, fullPrompt) if err != nil { return agentflow.AgentResult{}, fmt.Errorf(\"LLM generation failed: %w\", err) } // Execute any tool calls found in response var finalResponse string if a.mcpManager != nil { toolResults := agentflow.ParseAndExecuteToolCalls(ctx, a.mcpManager, response) if len(toolResults) \u003e 0 { logger.Info().Int(\"tool_calls\", len(toolResults)).Msg(\"Tools executed\") // Synthesize tool results with original response synthesisPrompt := fmt.Sprintf(`Original response: %s Tool execution results: %v Please provide a comprehensive final answer that incorporates the tool results. Be specific and cite the information sources when relevant.`, response, toolResults) finalResponse, err = a.llm.Generate(ctx, synthesisPrompt) if err != nil { // Fallback to original response if synthesis fails logger.Warn().Err(err).Msg(\"Tool result synthesis failed, using original response\") finalResponse = response } } else { finalResponse = response } } else { finalResponse = response } // Update state with results state.Set(\"response\", finalResponse) state.Set(\"tools_used\", len(toolResults) \u003e 0) if len(toolResults) \u003e 0 { state.Set(\"tool_results\", toolResults) } return agentflow.AgentResult{ Result: finalResponse, State: state, }, nil }\rMCP Configuration Basic Configuration (agentflow.toml) [mcp] enabled = true # Web search capabilities [mcp.servers.search] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-web-search\"] transport = \"stdio\" # Docker container management [mcp.servers.docker] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-docker\"] transport = \"stdio\" # File system operations [mcp.servers.filesystem] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-filesystem\"] transport = \"stdio\"\rProduction Configuration with Caching [mcp] enabled = true cache_enabled = true cache_ttl = \"5m\" connection_timeout = \"30s\" max_retries = 3 [mcp.cache] type = \"memory\" max_size = 1000 # Production-ready servers [mcp.servers.search] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-web-search\"] transport = \"stdio\" env = { \"SEARCH_API_KEY\" = \"${SEARCH_API_KEY}\" } [mcp.servers.database] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-postgres\"] transport = \"stdio\" env = { \"DATABASE_URL\" = \"${DATABASE_URL}\" }\rAvailable MCP Servers Development \u0026 System Tools # Docker management [mcp.servers.docker] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-docker\"] transport = \"stdio\" # File system operations [mcp.servers.filesystem] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-filesystem\"] transport = \"stdio\" # GitHub integration [mcp.servers.github] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-github\"] transport = \"stdio\" env = { \"GITHUB_TOKEN\" = \"${GITHUB_TOKEN}\" }\rWeb \u0026 Search Tools # Web search [mcp.servers.search] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-web-search\"] transport = \"stdio\" # URL content fetching [mcp.servers.fetch] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-fetch\"] transport = \"stdio\" # Brave search API [mcp.servers.brave] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-brave-search\"] transport = \"stdio\" env = { \"BRAVE_API_KEY\" = \"${BRAVE_API_KEY}\" }\rDatabase Tools # PostgreSQL [mcp.servers.postgres] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-postgres\"] transport = \"stdio\" env = { \"DATABASE_URL\" = \"${DATABASE_URL}\" } # SQLite [mcp.servers.sqlite] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-sqlite\"] transport = \"stdio\" # MongoDB [mcp.servers.mongodb] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-mongodb\"] transport = \"stdio\" env = { \"MONGODB_URI\" = \"${MONGODB_URI}\" }\rTool Usage Patterns Information Gathering Pattern func (a *ResearchAgent) Run(ctx context.Context, event agentflow.Event, state agentflow.State) (agentflow.AgentResult, error) { query := event.GetData()[\"message\"] // System prompt optimized for research systemPrompt := `You are a research agent. For any query: 1. First, search for current information using the search tool 2. If specific URLs are mentioned or found, fetch their content 3. Gather multiple perspectives and sources 4. Organize findings in a structured way Always use tools for factual, current information rather than relying on training data.` toolPrompt := agentflow.FormatToolsForPrompt(ctx, a.mcpManager) prompt := fmt.Sprintf(\"%s\\n%s\\nResearch: %s\", systemPrompt, toolPrompt, query) response, err := a.llm.Generate(ctx, prompt) if err != nil { return agentflow.AgentResult{}, err } // Execute research tools toolResults := agentflow.ParseAndExecuteToolCalls(ctx, a.mcpManager, response) // Compile comprehensive research report if len(toolResults) \u003e 0 { reportPrompt := fmt.Sprintf(`Based on this research: %v Create a comprehensive research report with: 1. Executive summary 2. Key findings with sources 3. Detailed information 4. Implications and insights`, toolResults) finalReport, _ := a.llm.Generate(ctx, reportPrompt) state.Set(\"research_report\", finalReport) return agentflow.AgentResult{Result: finalReport, State: state}, nil } return agentflow.AgentResult{Result: response, State: state}, nil }\rTechnical Operations Pattern func (a *DevOpsAgent) Run(ctx context.Context, event agentflow.Event, state agentflow.State) (agentflow.AgentResult, error) { task := event.GetData()[\"message\"] systemPrompt := `You are a DevOps agent with access to Docker and system tools. For technical tasks: 1. Analyze what needs to be done 2. Use docker commands for container operations 3. Use filesystem tools for file operations 4. Provide clear explanations of actions taken 5. Include any warnings or important notes Be careful with destructive operations and always explain what you're doing.` toolPrompt := agentflow.FormatToolsForPrompt(ctx, a.mcpManager) prompt := fmt.Sprintf(\"%s\\n%s\\nTask: %s\", systemPrompt, toolPrompt, task) response, err := a.llm.Generate(ctx, prompt) if err != nil { return agentflow.AgentResult{}, err } // Execute technical tools with logging logger := agentflow.Logger() toolResults := agentflow.ParseAndExecuteToolCalls(ctx, a.mcpManager, response) if len(toolResults) \u003e 0 { logger.Info().Interface(\"results\", toolResults).Msg(\"Technical operations completed\") // Provide detailed summary of actions summaryPrompt := fmt.Sprintf(`Technical operations completed: %v Provide a summary that includes: 1. What actions were taken 2. Results of each operation 3. Current state 4. Any follow-up recommendations`, toolResults) summary, _ := a.llm.Generate(ctx, summaryPrompt) state.Set(\"operations_summary\", summary) return agentflow.AgentResult{Result: summary, State: state}, nil } return agentflow.AgentResult{Result: response, State: state}, nil }\rError Handling for Tools Graceful Tool Failure Handling func (a *Agent) handleToolExecution(ctx context.Context, response string) (string, error) { logger := agentflow.Logger() // Attempt tool execution toolResults := agentflow.ParseAndExecuteToolCalls(ctx, a.mcpManager, response) // Check if tools were expected but failed if strings.Contains(response, \"\u003ctool_call\u003e\") \u0026\u0026 len(toolResults) == 0 { logger.Warn().Msg(\"Tool calls were attempted but none succeeded\") // Generate fallback response fallbackPrompt := fmt.Sprintf(`The following response contained tool calls that failed to execute: %s Please provide a helpful response based on your knowledge instead, and mention that some real-time information couldn't be retrieved.`, response) fallbackResponse, err := a.llm.Generate(ctx, fallbackPrompt) if err != nil { // If even fallback fails, return original response return response, nil } return fallbackResponse, nil } // If tools succeeded, synthesize results if len(toolResults) \u003e 0 { synthesisPrompt := fmt.Sprintf(\"Response: %s\\nTool results: %v\\nFinal answer:\", response, toolResults) finalResponse, err := a.llm.Generate(ctx, synthesisPrompt) if err != nil { // Fallback to original response return response, nil } return finalResponse, nil } // No tools were called, return original response return response, nil }\rCustom MCP Servers Building a Custom Tool You can create custom MCP servers for domain-specific tools. Here’s a conceptual example:\n// custom-mcp-server.js const { MCPServer } = require('@modelcontextprotocol/server'); const server = new MCPServer({ name: \"custom-tools\", version: \"1.0.0\" }); // Define custom tool server.registerTool({ name: \"analyze_data\", description: \"Analyze CSV data and return insights\", parameters: { type: \"object\", properties: { data: { type: \"string\", description: \"CSV data to analyze\" }, analysis_type: { type: \"string\", description: \"Type of analysis: summary, trends, outliers\" } }, required: [\"data\", \"analysis_type\"] } }, async (params) =\u003e { // Custom analysis logic here const { data, analysis_type } = params; switch(analysis_type) { case \"summary\": return { result: \"Data summary: ...\" }; case \"trends\": return { result: \"Trend analysis: ...\" }; default: return { error: \"Unknown analysis type\" }; } }); server.start();\rConfigure in agentflow.toml:\n[mcp.servers.custom] command = \"node\" args = [\"custom-mcp-server.js\"] transport = \"stdio\"\rTesting Tool Integration Mock MCP Manager for Testing type MockMCPManager struct { tools []agentflow.ToolSchema toolResults map[string]interface{} } func NewMockMCPManager() *MockMCPManager { return \u0026MockMCPManager{ tools: []agentflow.ToolSchema{ { Name: \"search\", Description: \"Search for information\", Parameters: map[string]interface{}{ \"query\": map[string]interface{}{ \"type\": \"string\", \"description\": \"Search query\", }, }, }, }, toolResults: map[string]interface{}{ \"search\": map[string]interface{}{ \"results\": []string{\"Mock search result 1\", \"Mock search result 2\"}, }, }, } } func (m *MockMCPManager) ListTools(ctx context.Context) ([]agentflow.ToolSchema, error) { return m.tools, nil } func (m *MockMCPManager) CallTool(ctx context.Context, name string, args map[string]interface{}) (interface{}, error) { if result, exists := m.toolResults[name]; exists { return result, nil } return nil, fmt.Errorf(\"tool not found: %s\", name) } // Test with mock func TestAgentWithTools(t *testing.T) { mockLLM := \u0026MockModelProvider{} mockMCP := NewMockMCPManager() agent := NewToolEnabledAgent(mockLLM, mockMCP) // Test tool integration eventData := agentflow.EventData{\"message\": \"Search for Go tutorials\"} event := agentflow.NewEvent(\"test\", eventData, nil) state := agentflow.NewState() result, err := agent.Run(context.Background(), event, state) assert.NoError(t, err) assert.NotEmpty(t, result.Result) // Verify tools were used toolsUsed, _ := result.State.Get(\"tools_used\") assert.True(t, toolsUsed.(bool)) }\rPerformance Considerations Tool Execution Optimization Cache Tool Schemas: Tool discovery is cached automatically Parallel Execution: Multiple tool calls execute concurrently Timeout Management: Tools have configurable timeouts Connection Pooling: MCP connections are reused // Production MCP configuration config := agentflow.MCPConfig{ CacheEnabled: true, CacheTTL: 5 * time.Minute, ConnectionTimeout: 30 * time.Second, MaxRetries: 3, MaxConcurrentTools: 5, } mcpManager, err := agentflow.InitializeProductionMCP(ctx, config)\rNext Steps LLM Providers - Configure different LLM providers Configuration - Advanced configuration options Custom Tools - Build your own MCP servers Production Deployment - Deploy with tool integration",
    "description": "Tool Integration Dynamic Tool Discovery and Execution with MCP Protocol\nAgentFlow uses the Model Context Protocol (MCP) to provide agents with dynamic tool discovery and execution capabilities. This guide covers everything from basic tool usage to building custom MCP servers.\nOverview The MCP integration in AgentFlow provides:\nDynamic Discovery: Tools are discovered at runtime, not hard-coded Schema-Based: Tools provide their own descriptions and parameters LLM-Driven: The LLM decides which tools to use based on context Extensible: Add new tools by connecting MCP servers Core MCP Functions AgentFlow provides two key functions that handle all MCP complexity:",
    "tags": [],
    "title": "ToolIntegration",
    "uri": "/AgenticGoKitDocs/guides/toolintegration/index.html"
  },
  {
    "breadcrumb": "content \u003e guides",
    "content": "PostgreSQL + pgvector Setup Guide Complete guide for setting up PostgreSQL with pgvector for AgentFlow memory\nThis guide provides detailed instructions for setting up PostgreSQL with the pgvector extension for production AgentFlow deployments.\n🎯 Overview PostgreSQL with pgvector provides:\nPersistent storage for agent memories and knowledge base Vector similarity search with excellent performance (~45ms queries) ACID transactions for data consistency Production-ready scalability and reliability Advanced indexing for optimal query performance 📋 Prerequisites Docker (recommended) OR PostgreSQL 12+ 2GB+ RAM available Network access to PostgreSQL port (5432) Basic command line knowledge 🚀 Quick Start (Docker) Step 1: Run PostgreSQL with pgvector # Create and start PostgreSQL container with pgvector docker run -d \\ --name agentflow-postgres \\ -e POSTGRES_DB=agentflow \\ -e POSTGRES_USER=agentflow \\ -e POSTGRES_PASSWORD=password \\ -p 5432:5432 \\ -v postgres_data:/var/lib/postgresql/data \\ pgvector/pgvector:pg16 # Wait for container to start echo \"Waiting for PostgreSQL to start...\" sleep 10 # Verify container is running docker ps | grep agentflow-postgres\rStep 2: Initialize Database # Connect to database and enable pgvector docker exec -it agentflow-postgres psql -U agentflow -d agentflow -c \" CREATE EXTENSION IF NOT EXISTS vector; SELECT extname, extversion FROM pg_extension WHERE extname = 'vector'; \" # Expected output: vector | 0.5.1 (or similar)\rStep 3: Test Connection # Test connection string docker exec -it agentflow-postgres psql -U agentflow -d agentflow -c \" SELECT version(); SELECT * FROM pg_extension WHERE extname = 'vector'; \"\rStep 4: Configure AgentFlow Create agentflow.toml:\n[memory] enabled = true provider = \"pgvector\" max_results = 10 dimensions = 1536 auto_embed = true [memory.pgvector] connection = \"postgres://agentflow:password@localhost:5432/agentflow?sslmode=disable\" table_name = \"agent_memory\" [memory.embedding] provider = \"openai\" model = \"text-embedding-3-small\" [memory.rag] enabled = true chunk_size = 1000 overlap = 100 top_k = 5 score_threshold = 0.7\rSet environment variables:\nexport DATABASE_URL=\"postgres://agentflow:password@localhost:5432/agentflow?sslmode=disable\" export OPENAI_API_KEY=\"your-openai-api-key\"\r✅ You’re ready to use AgentFlow with pgvector!\n🔧 Detailed Setup Options Option 1: Docker Compose (Recommended for Development) Create docker-compose.yml:\nversion: '3.8' services: postgres: image: pgvector/pgvector:pg16 container_name: agentflow-postgres environment: POSTGRES_DB: agentflow POSTGRES_USER: agentflow POSTGRES_PASSWORD: password ports: - \"5432:5432\" volumes: - postgres_data:/var/lib/postgresql/data - ./init-db.sql:/docker-entrypoint-initdb.d/init-db.sql healthcheck: test: [\"CMD-SHELL\", \"pg_isready -U agentflow -d agentflow\"] interval: 10s timeout: 5s retries: 5 restart: unless-stopped volumes: postgres_data: driver: local\rCreate init-db.sql:\n-- AgentFlow Database Initialization Script for pgvector -- This script sets up the database schema for AgentFlow memory system -- Enable the pgvector extension CREATE EXTENSION IF NOT EXISTS vector; -- Create the agent_memory table for storing embeddings and content CREATE TABLE IF NOT EXISTS agent_memory ( id SERIAL PRIMARY KEY, content TEXT NOT NULL, embedding vector(1536), tags TEXT[], metadata JSONB, created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(), updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW() ); -- Create indexes for better performance CREATE INDEX IF NOT EXISTS idx_agent_memory_embedding ON agent_memory USING ivfflat (embedding vector_cosine_ops); CREATE INDEX IF NOT EXISTS idx_agent_memory_tags ON agent_memory USING GIN (tags); CREATE INDEX IF NOT EXISTS idx_agent_memory_created_at ON agent_memory (created_at); CREATE INDEX IF NOT EXISTS idx_agent_memory_metadata ON agent_memory USING GIN (metadata); -- Create the chat_history table for storing conversation history CREATE TABLE IF NOT EXISTS chat_history ( id SERIAL PRIMARY KEY, session_id VARCHAR(255) NOT NULL, role VARCHAR(50) NOT NULL CHECK (role IN ('user', 'assistant', 'system')), content TEXT NOT NULL, metadata JSONB, created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW() ); -- Create indexes for chat history CREATE INDEX IF NOT EXISTS idx_chat_history_session_id ON chat_history (session_id); CREATE INDEX IF NOT EXISTS idx_chat_history_created_at ON chat_history (created_at); -- Create the documents table for RAG document storage CREATE TABLE IF NOT EXISTS documents ( id SERIAL PRIMARY KEY, title VARCHAR(500), content TEXT NOT NULL, source VARCHAR(1000), document_type VARCHAR(50), chunk_index INTEGER DEFAULT 0, chunk_total INTEGER DEFAULT 1, embedding vector(1536), metadata JSONB, tags TEXT[], created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(), updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW() ); -- Create indexes for documents CREATE INDEX IF NOT EXISTS idx_documents_embedding ON documents USING ivfflat (embedding vector_cosine_ops); CREATE INDEX IF NOT EXISTS idx_documents_source ON documents (source); CREATE INDEX IF NOT EXISTS idx_documents_type ON documents (document_type); CREATE INDEX IF NOT EXISTS idx_documents_tags ON documents USING GIN (tags); CREATE INDEX IF NOT EXISTS idx_documents_created_at ON documents (created_at); -- Grant permissions to the user GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO agentflow; GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public TO agentflow; -- Insert a test record to verify the setup INSERT INTO agent_memory (content, tags, metadata) VALUES ('AgentFlow memory system initialized successfully', ARRAY['system', 'initialization'], '{\"source\": \"init_script\", \"version\": \"1.0\"}') ON CONFLICT DO NOTHING;\rStart the services:\n# Start PostgreSQL docker-compose up -d # Check logs docker-compose logs postgres # Test connection docker-compose exec postgres psql -U agentflow -d agentflow -c \"SELECT version();\"\rOption 2: Native Installation Ubuntu/Debian # Update package list sudo apt update # Install PostgreSQL sudo apt install postgresql postgresql-contrib # Install pgvector sudo apt install postgresql-15-pgvector # Start and enable PostgreSQL sudo systemctl start postgresql sudo systemctl enable postgresql # Create database and user sudo -u postgres createuser agentflow sudo -u postgres createdb agentflow -O agentflow sudo -u postgres psql -c \"ALTER USER agentflow PASSWORD 'password';\" # Enable pgvector extension sudo -u postgres psql -d agentflow -c \"CREATE EXTENSION IF NOT EXISTS vector;\" # Test connection psql \"postgres://agentflow:password@localhost:5432/agentflow\" -c \"SELECT version();\"\rCentOS/RHEL/Fedora # Install PostgreSQL sudo dnf install postgresql postgresql-server postgresql-contrib # Install pgvector (may need to compile from source) git clone https://github.com/pgvector/pgvector.git cd pgvector make sudo make install # Initialize database sudo postgresql-setup --initdb # Start and enable PostgreSQL sudo systemctl start postgresql sudo systemctl enable postgresql # Create database and user sudo -u postgres createuser agentflow sudo -u postgres createdb agentflow -O agentflow sudo -u postgres psql -c \"ALTER USER agentflow PASSWORD 'password';\" # Enable pgvector sudo -u postgres psql -d agentflow -c \"CREATE EXTENSION IF NOT EXISTS vector;\"\rmacOS (Homebrew) # Install PostgreSQL brew install postgresql # Install pgvector brew install pgvector # Start PostgreSQL service brew services start postgresql # Create database createdb agentflow # Enable pgvector psql agentflow -c \"CREATE EXTENSION IF NOT EXISTS vector;\" # Create user (optional) psql agentflow -c \"CREATE USER agentflow WITH PASSWORD 'password';\" psql agentflow -c \"GRANT ALL PRIVILEGES ON DATABASE agentflow TO agentflow;\"\r⚙️ Configuration Connection String Options # Basic connection postgres://username:password@host:port/database # With SSL (production) postgres://username:password@host:port/database?sslmode=require # With connection pooling postgres://username:password@host:port/database?pool_max_conns=25\u0026pool_min_conns=5 # Complete example postgres://agentflow:password@localhost:5432/agentflow?sslmode=disable\u0026pool_max_conns=25\u0026pool_min_conns=5\u0026pool_max_conn_lifetime=1h\rAgentFlow Configuration Basic Configuration [memory] enabled = true provider = \"pgvector\" max_results = 10 dimensions = 1536 auto_embed = true [memory.pgvector] connection = \"postgres://agentflow:password@localhost:5432/agentflow?sslmode=disable\" table_name = \"agent_memory\" [memory.embedding] provider = \"openai\" model = \"text-embedding-3-small\"\rProduction Configuration [memory] enabled = true provider = \"pgvector\" max_results = 10 dimensions = 1536 auto_embed = true [memory.pgvector] connection = \"postgres://agentflow:password@localhost:5432/agentflow?sslmode=require\u0026pool_max_conns=25\u0026pool_min_conns=5\" table_name = \"agent_memory\" [memory.embedding] provider = \"openai\" model = \"text-embedding-3-small\" cache_embeddings = true max_batch_size = 50 timeout_seconds = 30 [memory.rag] enabled = true chunk_size = 1000 overlap = 100 top_k = 5 score_threshold = 0.7 hybrid_search = true session_memory = true [memory.advanced] retry_max_attempts = 3 retry_base_delay = \"100ms\" retry_max_delay = \"5s\" connection_pool_size = 25 health_check_interval = \"1m\"\rEnvironment Variables # Database connection export DATABASE_URL=\"postgres://agentflow:password@localhost:5432/agentflow?sslmode=disable\" # OpenAI API (if using OpenAI embeddings) export OPENAI_API_KEY=\"your-openai-api-key\" # Azure OpenAI (alternative) export AZURE_OPENAI_API_KEY=\"your-azure-api-key\" export AZURE_OPENAI_ENDPOINT=\"https://your-resource.openai.azure.com/\" # Ollama (for local embeddings) export OLLAMA_BASE_URL=\"http://localhost:11434\" export OLLAMA_MODEL=\"mxbai-embed-large\"\r🚀 Performance Optimization PostgreSQL Configuration Add to postgresql.conf:\n# Memory settings shared_buffers = 256MB # 25% of RAM effective_cache_size = 1GB # 75% of RAM work_mem = 64MB # For sorting and hashing maintenance_work_mem = 256MB # For maintenance operations # Vector-specific settings max_parallel_workers_per_gather = 2 # Parallel query execution max_parallel_workers = 8 # Total parallel workers # Connection settings max_connections = 100 # Adjust based on your needs shared_preload_libraries = 'pg_stat_statements' # Query statistics # Logging (for debugging) log_statement = 'all' # Log all statements (development only) log_duration = on # Log query duration log_min_duration_statement = 1000 # Log slow queries (\u003e1s)\rIndex Optimization -- Create optimal indexes for vector operations CREATE INDEX CONCURRENTLY idx_agent_memory_embedding_cosine ON agent_memory USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100); CREATE INDEX CONCURRENTLY idx_agent_memory_embedding_l2 ON agent_memory USING ivfflat (embedding vector_l2_ops) WITH (lists = 100); -- Indexes for filtering CREATE INDEX CONCURRENTLY idx_agent_memory_session_tags ON agent_memory (session_id, tags); CREATE INDEX CONCURRENTLY idx_agent_memory_created_at_desc ON agent_memory (created_at DESC); -- Composite indexes for common queries CREATE INDEX CONCURRENTLY idx_agent_memory_session_created ON agent_memory (session_id, created_at DESC);\rQuery Optimization -- Analyze query performance EXPLAIN ANALYZE SELECT content, embedding \u003c-\u003e '[0.1,0.2,...]'::vector AS distance FROM agent_memory WHERE session_id = 'user-123' ORDER BY embedding \u003c-\u003e '[0.1,0.2,...]'::vector LIMIT 10; -- Update table statistics ANALYZE agent_memory; -- Vacuum regularly VACUUM ANALYZE agent_memory;\rConnection Pooling Use connection pooling for production:\n# Install pgbouncer sudo apt install pgbouncer # Configure pgbouncer # /etc/pgbouncer/pgbouncer.ini [databases] agentflow = host=localhost port=5432 dbname=agentflow [pgbouncer] listen_port = 6432 listen_addr = localhost auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 100 default_pool_size = 25\r📊 Monitoring and Maintenance Health Checks #!/bin/bash # health-check.sh DB_URL=\"postgres://agentflow:password@localhost:5432/agentflow\" echo \"🔍 Checking PostgreSQL health...\" # Test connection if psql \"$DB_URL\" -c \"SELECT 1;\" \u003e /dev/null 2\u003e\u00261; then echo \"✅ Database connection successful\" else echo \"❌ Database connection failed\" exit 1 fi # Check pgvector extension if psql \"$DB_URL\" -c \"SELECT * FROM pg_extension WHERE extname = 'vector';\" | grep -q vector; then echo \"✅ pgvector extension installed\" else echo \"❌ pgvector extension missing\" exit 1 fi # Check table existence TABLES=(\"agent_memory\" \"chat_history\" \"documents\") for table in \"${TABLES[@]}\"; do if psql \"$DB_URL\" -c \"\\dt $table\" | grep -q \"$table\"; then echo \"✅ Table $table exists\" else echo \"⚠️ Table $table missing (will be created automatically)\" fi done # Check disk space DISK_USAGE=$(df -h /var/lib/postgresql/data | awk 'NR==2 {print $5}' | sed 's/%//') if [ \"$DISK_USAGE\" -gt 80 ]; then echo \"⚠️ Disk usage high: ${DISK_USAGE}%\" else echo \"✅ Disk usage OK: ${DISK_USAGE}%\" fi echo \"✅ All health checks passed\"\rPerformance Monitoring -- Monitor query performance SELECT query, calls, total_time, mean_time, rows FROM pg_stat_statements WHERE query LIKE '%agent_memory%' ORDER BY total_time DESC LIMIT 10; -- Monitor table statistics SELECT schemaname, tablename, n_tup_ins as inserts, n_tup_upd as updates, n_tup_del as deletes, n_tup_hot_upd as hot_updates, n_live_tup as live_tuples, n_dead_tup as dead_tuples FROM pg_stat_user_tables WHERE tablename IN ('agent_memory', 'chat_history', 'documents'); -- Monitor index usage SELECT indexrelname, idx_scan, idx_tup_read, idx_tup_fetch FROM pg_stat_user_indexes WHERE indexrelname LIKE '%memory%' ORDER BY idx_scan DESC;\rBackup and Recovery #!/bin/bash # backup.sh DB_URL=\"postgres://agentflow:password@localhost:5432/agentflow\" BACKUP_DIR=\"/backups/agentflow\" DATE=$(date +%Y%m%d_%H%M%S) # Create backup directory mkdir -p \"$BACKUP_DIR\" # Full database backup echo \"Creating full backup...\" pg_dump \"$DB_URL\" \u003e \"$BACKUP_DIR/agentflow_full_$DATE.sql\" # Compressed backup echo \"Creating compressed backup...\" pg_dump \"$DB_URL\" | gzip \u003e \"$BACKUP_DIR/agentflow_compressed_$DATE.sql.gz\" # Schema-only backup echo \"Creating schema backup...\" pg_dump --schema-only \"$DB_URL\" \u003e \"$BACKUP_DIR/agentflow_schema_$DATE.sql\" # Data-only backup echo \"Creating data backup...\" pg_dump --data-only \"$DB_URL\" \u003e \"$BACKUP_DIR/agentflow_data_$DATE.sql\" # Cleanup old backups (keep last 7 days) find \"$BACKUP_DIR\" -name \"*.sql*\" -mtime +7 -delete echo \"✅ Backup completed: $BACKUP_DIR\"\rMaintenance Tasks #!/bin/bash # maintenance.sh DB_URL=\"postgres://agentflow:password@localhost:5432/agentflow\" echo \"🔧 Running maintenance tasks...\" # Update statistics echo \"Updating table statistics...\" psql \"$DB_URL\" -c \"ANALYZE;\" # Vacuum tables echo \"Vacuuming tables...\" psql \"$DB_URL\" -c \"VACUUM ANALYZE agent_memory;\" psql \"$DB_URL\" -c \"VACUUM ANALYZE chat_history;\" psql \"$DB_URL\" -c \"VACUUM ANALYZE documents;\" # Reindex if needed (run during low traffic) echo \"Checking index bloat...\" psql \"$DB_URL\" -c \" SELECT schemaname, tablename, indexname, pg_size_pretty(pg_relation_size(indexrelid)) as size FROM pg_stat_user_indexes WHERE indexrelname LIKE '%memory%' ORDER BY pg_relation_size(indexrelid) DESC; \" echo \"✅ Maintenance completed\"\r🔧 Troubleshooting Common Issues 1. Connection Refused # Check if PostgreSQL is running docker ps | grep postgres # or for native installation sudo systemctl status postgresql # Check port availability netstat -tlnp | grep 5432 # Test connection telnet localhost 5432\r2. pgvector Extension Missing -- Check if extension is available SELECT * FROM pg_available_extensions WHERE name = 'vector'; -- Install extension CREATE EXTENSION IF NOT EXISTS vector; -- Verify installation SELECT * FROM pg_extension WHERE extname = 'vector';\r3. Permission Denied -- Grant necessary permissions GRANT ALL PRIVILEGES ON DATABASE agentflow TO agentflow; GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO agentflow; GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public TO agentflow;\r4. Slow Queries -- Check for missing indexes SELECT schemaname, tablename, attname, n_distinct, correlation FROM pg_stats WHERE tablename = 'agent_memory'; -- Create missing indexes CREATE INDEX CONCURRENTLY idx_agent_memory_session ON agent_memory (session_id);\r5. High Memory Usage # Check PostgreSQL memory usage ps aux | grep postgres # Monitor with htop htop # Adjust PostgreSQL settings # Edit postgresql.conf: shared_buffers = 128MB # Reduce if needed work_mem = 32MB # Reduce if needed\rDebug Mode Enable detailed logging:\n-- Enable query logging ALTER SYSTEM SET log_statement = 'all'; ALTER SYSTEM SET log_duration = on; ALTER SYSTEM SET log_min_duration_statement = 0; -- Reload configuration SELECT pg_reload_conf(); -- Check logs -- Docker: docker logs agentflow-postgres -- Native: tail -f /var/log/postgresql/postgresql-*.log\rRecovery Procedures Restore from Backup # Stop applications using the database # Restore full backup psql \"$DB_URL\" \u003c /backups/agentflow/agentflow_full_20240101_120000.sql # Or restore compressed backup gunzip -c /backups/agentflow/agentflow_compressed_20240101_120000.sql.gz | psql \"$DB_URL\"\rReset Database # ⚠️ WARNING: This will delete all data! # Drop and recreate database docker exec -it agentflow-postgres psql -U agentflow -c \" DROP DATABASE IF EXISTS agentflow; CREATE DATABASE agentflow; \" # Reconnect and setup docker exec -it agentflow-postgres psql -U agentflow -d agentflow -c \" CREATE EXTENSION IF NOT EXISTS vector; \" # Tables will be recreated automatically by AgentFlow\r🎯 Production Deployment Docker Production Setup Create docker-compose.prod.yml:\nversion: '3.8' services: postgres: image: pgvector/pgvector:pg16 container_name: agentflow-postgres-prod environment: POSTGRES_DB: agentflow POSTGRES_USER: agentflow POSTGRES_PASSWORD_FILE: /run/secrets/postgres_password ports: - \"5432:5432\" volumes: - postgres_data:/var/lib/postgresql/data - ./postgresql.conf:/etc/postgresql/postgresql.conf - ./init-db.sql:/docker-entrypoint-initdb.d/init-db.sql command: postgres -c config_file=/etc/postgresql/postgresql.conf healthcheck: test: [\"CMD-SHELL\", \"pg_isready -U agentflow -d agentflow\"] interval: 30s timeout: 10s retries: 3 start_period: 60s restart: unless-stopped secrets: - postgres_password deploy: resources: limits: memory: 2G cpus: '1.0' reservations: memory: 1G cpus: '0.5' secrets: postgres_password: file: ./secrets/postgres_password.txt volumes: postgres_data: driver: local\rSecurity Considerations # Use strong passwords openssl rand -base64 32 \u003e ./secrets/postgres_password.txt # Restrict file permissions chmod 600 ./secrets/postgres_password.txt # Use SSL in production # Add to postgresql.conf: ssl = on ssl_cert_file = '/etc/ssl/certs/server.crt' ssl_key_file = '/etc/ssl/private/server.key'\rMonitoring Setup # Add to docker-compose.prod.yml postgres-exporter: image: prometheuscommunity/postgres-exporter environment: DATA_SOURCE_NAME: \"postgresql://agentflow:password@postgres:5432/agentflow?sslmode=disable\" ports: - \"9187:9187\" depends_on: - postgres\r🎉 Summary You now have a complete PostgreSQL + pgvector setup for AgentFlow:\n✅ Database Setup: PostgreSQL with pgvector extension\n✅ Performance Optimization: Indexes, connection pooling, configuration tuning\n✅ Monitoring: Health checks, performance monitoring, maintenance tasks\n✅ Production Ready: Security, backups, recovery procedures\nNext Steps Test your setup with the provided health check script Configure AgentFlow with your database connection Set up monitoring and backup procedures Optimize performance based on your workload For more information:\nMemory System Guide - Complete API reference Memory Provider Setup - All provider options Configuration Guide - Advanced configuration",
    "description": "PostgreSQL + pgvector Setup Guide Complete guide for setting up PostgreSQL with pgvector for AgentFlow memory\nThis guide provides detailed instructions for setting up PostgreSQL with the pgvector extension for production AgentFlow deployments.\n🎯 Overview PostgreSQL with pgvector provides:\nPersistent storage for agent memories and knowledge base Vector similarity search with excellent performance (~45ms queries) ACID transactions for data consistency Production-ready scalability and reliability Advanced indexing for optimal query performance 📋 Prerequisites Docker (recommended) OR PostgreSQL 12+ 2GB+ RAM available Network access to PostgreSQL port (5432) Basic command line knowledge 🚀 Quick Start (Docker) Step 1: Run PostgreSQL with pgvector # Create and start PostgreSQL container with pgvector docker run -d \\ --name agentflow-postgres \\ -e POSTGRES_DB=agentflow \\ -e POSTGRES_USER=agentflow \\ -e POSTGRES_PASSWORD=password \\ -p 5432:5432 \\ -v postgres_data:/var/lib/postgresql/data \\ pgvector/pgvector:pg16 # Wait for container to start echo \"Waiting for PostgreSQL to start...\" sleep 10 # Verify container is running docker ps | grep agentflow-postgres\rStep 2: Initialize Database # Connect to database and enable pgvector docker exec -it agentflow-postgres psql -U agentflow -d agentflow -c \" CREATE EXTENSION IF NOT EXISTS vector; SELECT extname, extversion FROM pg_extension WHERE extname = 'vector'; \" # Expected output: vector | 0.5.1 (or similar)\rStep 3: Test Connection # Test connection string docker exec -it agentflow-postgres psql -U agentflow -d agentflow -c \" SELECT version(); SELECT * FROM pg_extension WHERE extname = 'vector'; \"\rStep 4: Configure AgentFlow Create agentflow.toml:",
    "tags": [],
    "title": "PgVectorSetup",
    "uri": "/AgenticGoKitDocs/guides/pgvectorsetup/index.html"
  },
  {
    "breadcrumb": "content \u003e guides",
    "content": "Weaviate Setup Guide Complete guide for setting up Weaviate vector database for AgentFlow memory\nThis guide provides detailed instructions for setting up Weaviate as a vector database backend for AgentFlow’s memory system, from development to production deployment.\n🎯 Overview Weaviate provides:\nPurpose-built vector database optimized for similarity search GraphQL API for flexible queries and data management Built-in clustering for horizontal scalability Advanced search features including hybrid search and filtering Rich ecosystem with integrations and modules 📋 Prerequisites Docker (recommended) OR Kubernetes 4GB+ RAM available (8GB+ recommended for production) Network access to Weaviate port (8080) Basic command line knowledge 🚀 Quick Start (Docker) Step 1: Run Weaviate # Create and start Weaviate container docker run -d \\ --name agentflow-weaviate \\ -p 8080:8080 \\ -e QUERY_DEFAULTS_LIMIT=25 \\ -e AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED='true' \\ -e PERSISTENCE_DATA_PATH='/var/lib/weaviate' \\ -e DEFAULT_VECTORIZER_MODULE='none' \\ -e CLUSTER_HOSTNAME='node1' \\ -e ENABLE_MODULES='text2vec-openai,text2vec-cohere,text2vec-huggingface,ref2vec-centroid,generative-openai,qna-openai' \\ -v weaviate_data:/var/lib/weaviate \\ semitechnologies/weaviate:1.22.4 # Wait for startup echo \"Waiting for Weaviate to start...\" sleep 15 # Verify Weaviate is running curl http://localhost:8080/v1/.well-known/ready\rStep 2: Test Connection # Check Weaviate status curl http://localhost:8080/v1/meta # Expected response: JSON with version and modules info\rStep 3: Configure AgentFlow Create agentflow.toml:\n[memory] enabled = true provider = \"weaviate\" max_results = 10 dimensions = 1536 auto_embed = true [memory.weaviate] connection = \"http://localhost:8080\" class_name = \"AgentMemory\" [memory.embedding] provider = \"openai\" model = \"text-embedding-3-small\" [memory.rag] enabled = true chunk_size = 1000 overlap = 100 top_k = 5 score_threshold = 0.7\rSet environment variables:\nexport WEAVIATE_URL=\"http://localhost:8080\" export OPENAI_API_KEY=\"your-openai-api-key\"\r✅ You’re ready to use AgentFlow with Weaviate!\n🔧 Detailed Setup Options Option 1: Docker Compose (Recommended) Create docker-compose.yml:\nversion: '3.8' services: weaviate: image: semitechnologies/weaviate:1.22.4 container_name: agentflow-weaviate ports: - \"8080:8080\" environment: QUERY_DEFAULTS_LIMIT: 25 AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true' PERSISTENCE_DATA_PATH: '/var/lib/weaviate' DEFAULT_VECTORIZER_MODULE: 'none' CLUSTER_HOSTNAME: 'node1' ENABLE_MODULES: 'text2vec-openai,text2vec-cohere,text2vec-huggingface,ref2vec-centroid,generative-openai,qna-openai' volumes: - weaviate_data:/var/lib/weaviate healthcheck: test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8080/v1/.well-known/ready\"] interval: 10s timeout: 5s retries: 5 start_period: 30s restart: unless-stopped volumes: weaviate_data: driver: local\rStart Weaviate:\n# Start Weaviate docker-compose up -d # Check logs docker-compose logs weaviate # Test connection curl http://localhost:8080/v1/.well-known/ready # Stop when needed docker-compose down\rOption 2: Production Docker Compose Create docker-compose.prod.yml:\nversion: '3.8' services: weaviate: image: semitechnologies/weaviate:1.22.4 container_name: agentflow-weaviate-prod ports: - \"8080:8080\" environment: # Core settings QUERY_DEFAULTS_LIMIT: 25 PERSISTENCE_DATA_PATH: '/var/lib/weaviate' DEFAULT_VECTORIZER_MODULE: 'none' CLUSTER_HOSTNAME: 'node1' # Authentication (production) AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'false' AUTHENTICATION_APIKEY_ENABLED: 'true' AUTHENTICATION_APIKEY_ALLOWED_KEYS: 'your-secret-api-key' AUTHENTICATION_APIKEY_USERS: 'admin' # Modules ENABLE_MODULES: 'text2vec-openai,text2vec-cohere,text2vec-huggingface,ref2vec-centroid,generative-openai,qna-openai' # Performance settings LIMIT_RESOURCES: 'true' GOMEMLIMIT: '4GiB' # Backup settings BACKUP_FILESYSTEM_PATH: '/var/lib/weaviate/backups' volumes: - weaviate_data:/var/lib/weaviate - weaviate_backups:/var/lib/weaviate/backups healthcheck: test: [\"CMD\", \"curl\", \"-f\", \"-H\", \"Authorization: Bearer your-secret-api-key\", \"http://localhost:8080/v1/.well-known/ready\"] interval: 30s timeout: 10s retries: 3 start_period: 60s restart: unless-stopped deploy: resources: limits: memory: 4G cpus: '2.0' reservations: memory: 2G cpus: '1.0' volumes: weaviate_data: driver: local weaviate_backups: driver: local\rOption 3: Kubernetes Deployment Create weaviate-k8s.yaml:\napiVersion: apps/v1 kind: Deployment metadata: name: weaviate labels: app: weaviate spec: replicas: 1 selector: matchLabels: app: weaviate template: metadata: labels: app: weaviate spec: containers: - name: weaviate image: semitechnologies/weaviate:1.22.4 ports: - containerPort: 8080 env: - name: QUERY_DEFAULTS_LIMIT value: \"25\" - name: AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED value: \"true\" - name: PERSISTENCE_DATA_PATH value: \"/var/lib/weaviate\" - name: DEFAULT_VECTORIZER_MODULE value: \"none\" - name: CLUSTER_HOSTNAME value: \"node1\" - name: ENABLE_MODULES value: \"text2vec-openai,text2vec-cohere,text2vec-huggingface,ref2vec-centroid,generative-openai,qna-openai\" volumeMounts: - name: weaviate-storage mountPath: /var/lib/weaviate resources: requests: memory: \"2Gi\" cpu: \"1\" limits: memory: \"4Gi\" cpu: \"2\" livenessProbe: httpGet: path: /v1/.well-known/ready port: 8080 initialDelaySeconds: 30 periodSeconds: 30 readinessProbe: httpGet: path: /v1/.well-known/ready port: 8080 initialDelaySeconds: 10 periodSeconds: 10 volumes: - name: weaviate-storage persistentVolumeClaim: claimName: weaviate-pvc --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: weaviate-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi --- apiVersion: v1 kind: Service metadata: name: weaviate-service spec: selector: app: weaviate ports: - protocol: TCP port: 8080 targetPort: 8080 type: LoadBalancer\rDeploy to Kubernetes:\n# Apply the configuration kubectl apply -f weaviate-k8s.yaml # Check deployment status kubectl get pods -l app=weaviate kubectl get services weaviate-service # Get external IP kubectl get service weaviate-service\r⚙️ Configuration Connection Options # Basic connection http://localhost:8080 # With authentication http://localhost:8080 (with API key header) # Remote connection http://your-weaviate-host:8080 # HTTPS (production) https://your-weaviate-host:8080\rAgentFlow Configuration Basic Configuration [memory] enabled = true provider = \"weaviate\" max_results = 10 dimensions = 1536 auto_embed = true [memory.weaviate] connection = \"http://localhost:8080\" class_name = \"AgentMemory\" [memory.embedding] provider = \"openai\" model = \"text-embedding-3-small\"\rProduction Configuration [memory] enabled = true provider = \"weaviate\" max_results = 10 dimensions = 1536 auto_embed = true [memory.weaviate] connection = \"https://your-weaviate-host:8080\" api_key = \"${WEAVIATE_API_KEY}\" class_name = \"AgentMemory\" timeout = \"30s\" max_retries = 3 [memory.embedding] provider = \"openai\" model = \"text-embedding-3-small\" cache_embeddings = true max_batch_size = 50 timeout_seconds = 30 [memory.rag] enabled = true chunk_size = 1000 overlap = 100 top_k = 5 score_threshold = 0.7 hybrid_search = true session_memory = true [memory.advanced] retry_max_attempts = 3 retry_base_delay = \"100ms\" retry_max_delay = \"5s\" health_check_interval = \"1m\"\rEnvironment Variables # Weaviate connection export WEAVIATE_URL=\"http://localhost:8080\" export WEAVIATE_API_KEY=\"your-secret-api-key\" # For production export WEAVIATE_CLASS_NAME=\"AgentMemory\" # OpenAI API (if using OpenAI embeddings) export OPENAI_API_KEY=\"your-openai-api-key\" # Azure OpenAI (alternative) export AZURE_OPENAI_API_KEY=\"your-azure-api-key\" export AZURE_OPENAI_ENDPOINT=\"https://your-resource.openai.azure.com/\" # Ollama (for local embeddings) export OLLAMA_BASE_URL=\"http://localhost:11434\" export OLLAMA_MODEL=\"mxbai-embed-large\"\r🚀 Advanced Features Authentication Setup API Key Authentication # docker-compose.yml with authentication services: weaviate: environment: AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'false' AUTHENTICATION_APIKEY_ENABLED: 'true' AUTHENTICATION_APIKEY_ALLOWED_KEYS: 'your-secret-key,another-key' AUTHENTICATION_APIKEY_USERS: 'admin,user'\r# agentflow.toml with authentication [memory.weaviate] connection = \"http://localhost:8080\" api_key = \"your-secret-key\" class_name = \"AgentMemory\"\rOIDC Authentication services: weaviate: environment: AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'false' AUTHENTICATION_OIDC_ENABLED: 'true' AUTHENTICATION_OIDC_ISSUER: 'https://your-oidc-provider.com' AUTHENTICATION_OIDC_CLIENT_ID: 'your-client-id' AUTHENTICATION_OIDC_USERNAME_CLAIM: 'email' AUTHENTICATION_OIDC_GROUPS_CLAIM: 'groups'\rClustering Setup Multi-Node Cluster # docker-compose.cluster.yml version: '3.8' services: weaviate-node1: image: semitechnologies/weaviate:1.22.4 container_name: weaviate-node1 ports: - \"8080:8080\" environment: CLUSTER_HOSTNAME: 'node1' CLUSTER_GOSSIP_BIND_PORT: '7100' CLUSTER_DATA_BIND_PORT: '7101' PERSISTENCE_DATA_PATH: '/var/lib/weaviate' DEFAULT_VECTORIZER_MODULE: 'none' ENABLE_MODULES: 'text2vec-openai,text2vec-cohere,text2vec-huggingface' volumes: - weaviate_node1_data:/var/lib/weaviate networks: - weaviate-cluster weaviate-node2: image: semitechnologies/weaviate:1.22.4 container_name: weaviate-node2 ports: - \"8081:8080\" environment: CLUSTER_HOSTNAME: 'node2' CLUSTER_GOSSIP_BIND_PORT: '7102' CLUSTER_DATA_BIND_PORT: '7103' CLUSTER_JOIN: 'node1:7100' PERSISTENCE_DATA_PATH: '/var/lib/weaviate' DEFAULT_VECTORIZER_MODULE: 'none' ENABLE_MODULES: 'text2vec-openai,text2vec-cohere,text2vec-huggingface' volumes: - weaviate_node2_data:/var/lib/weaviate networks: - weaviate-cluster depends_on: - weaviate-node1 weaviate-node3: image: semitechnologies/weaviate:1.22.4 container_name: weaviate-node3 ports: - \"8082:8080\" environment: CLUSTER_HOSTNAME: 'node3' CLUSTER_GOSSIP_BIND_PORT: '7104' CLUSTER_DATA_BIND_PORT: '7105' CLUSTER_JOIN: 'node1:7100' PERSISTENCE_DATA_PATH: '/var/lib/weaviate' DEFAULT_VECTORIZER_MODULE: 'none' ENABLE_MODULES: 'text2vec-openai,text2vec-cohere,text2vec-huggingface' volumes: - weaviate_node3_data:/var/lib/weaviate networks: - weaviate-cluster depends_on: - weaviate-node1 volumes: weaviate_node1_data: weaviate_node2_data: weaviate_node3_data: networks: weaviate-cluster: driver: bridge\rBackup and Restore Automated Backups # Add backup service to docker-compose.yml services: weaviate-backup: image: curlimages/curl:latest container_name: weaviate-backup volumes: - ./backups:/backups - ./scripts:/scripts command: /scripts/backup.sh depends_on: - weaviate restart: \"no\"\rCreate scripts/backup.sh:\n#!/bin/sh # backup.sh WEAVIATE_URL=\"http://weaviate:8080\" BACKUP_DIR=\"/backups\" DATE=$(date +%Y%m%d_%H%M%S) echo \"Creating Weaviate backup...\" # Create backup curl -X POST \\ \"$WEAVIATE_URL/v1/backups/filesystem\" \\ -H \"Content-Type: application/json\" \\ -d \"{ \\\"id\\\": \\\"backup_$DATE\\\", \\\"include\\\": [\\\"AgentMemory\\\", \\\"Documents\\\", \\\"ChatHistory\\\"] }\" # Wait for backup to complete sleep 30 # Check backup status curl \"$WEAVIATE_URL/v1/backups/filesystem/backup_$DATE\" echo \"Backup completed: backup_$DATE\"\rManual Backup # Create backup curl -X POST \\ \"http://localhost:8080/v1/backups/filesystem\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"id\": \"my-backup-2024\", \"include\": [\"AgentMemory\"] }' # Check backup status curl \"http://localhost:8080/v1/backups/filesystem/my-backup-2024\" # List all backups curl \"http://localhost:8080/v1/backups/filesystem\"\rRestore from Backup # Restore backup curl -X POST \\ \"http://localhost:8080/v1/backups/filesystem/my-backup-2024/restore\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"include\": [\"AgentMemory\"] }' # Check restore status curl \"http://localhost:8080/v1/backups/filesystem/my-backup-2024/restore\"\r📊 Monitoring and Maintenance Health Checks #!/bin/bash # health-check.sh WEAVIATE_URL=\"http://localhost:8080\" API_KEY=\"\" # Set if using authentication echo \"🔍 Checking Weaviate health...\" # Test connection if curl -f \"$WEAVIATE_URL/v1/.well-known/ready\" \u003e /dev/null 2\u003e\u00261; then echo \"✅ Weaviate connection successful\" else echo \"❌ Weaviate connection failed\" exit 1 fi # Check cluster status NODES=$(curl -s \"$WEAVIATE_URL/v1/nodes\" | jq -r '.nodes | length') if [ \"$NODES\" -gt 0 ]; then echo \"✅ Cluster has $NODES node(s)\" else echo \"⚠️ No cluster nodes found\" fi # Check schema CLASSES=$(curl -s \"$WEAVIATE_URL/v1/schema\" | jq -r '.classes | length') echo \"ℹ️ Schema has $CLASSES class(es)\" # Check disk space (Docker) if command -v docker \u0026\u003e /dev/null; then CONTAINER_ID=$(docker ps -q -f name=weaviate) if [ -n \"$CONTAINER_ID\" ]; then DISK_USAGE=$(docker exec \"$CONTAINER_ID\" df -h /var/lib/weaviate | awk 'NR==2 {print $5}' | sed 's/%//') if [ \"$DISK_USAGE\" -gt 80 ]; then echo \"⚠️ Disk usage high: ${DISK_USAGE}%\" else echo \"✅ Disk usage OK: ${DISK_USAGE}%\" fi fi fi echo \"✅ All health checks passed\"\rPerformance Monitoring # Monitor Weaviate metrics curl \"http://localhost:8080/v1/meta\" | jq '.' # Check cluster statistics curl \"http://localhost:8080/v1/nodes\" | jq '.nodes[] | {name: .name, status: .status, stats: .stats}' # Monitor specific class curl \"http://localhost:8080/v1/schema/AgentMemory\" | jq '.' # Get object count curl \"http://localhost:8080/v1/objects?class=AgentMemory\u0026limit=0\" | jq '.totalResults'\rGraphQL Queries # Query objects with GraphQL curl -X POST \\ \"http://localhost:8080/v1/graphql\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"query\": \"{ Get { AgentMemory(limit: 10) { content tags _additional { id creationTimeUnix } } } }\" }' # Aggregate queries curl -X POST \\ \"http://localhost:8080/v1/graphql\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"query\": \"{ Aggregate { AgentMemory { meta { count } } } }\" }' # Vector search with GraphQL curl -X POST \\ \"http://localhost:8080/v1/graphql\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"query\": \"{ Get { AgentMemory(nearVector: {vector: [0.1, 0.2, 0.3]}, limit: 5) { content _additional { distance } } } }\" }'\rMaintenance Tasks #!/bin/bash # maintenance.sh WEAVIATE_URL=\"http://localhost:8080\" echo \"🔧 Running Weaviate maintenance tasks...\" # Check cluster health echo \"Checking cluster health...\" curl -s \"$WEAVIATE_URL/v1/nodes\" | jq '.nodes[] | {name: .name, status: .status}' # Optimize indexes (if needed) echo \"Checking index status...\" curl -s \"$WEAVIATE_URL/v1/schema\" | jq '.classes[] | {class: .class, vectorIndexType: .vectorIndexType}' # Clean up old backups (keep last 7) echo \"Cleaning up old backups...\" BACKUPS=$(curl -s \"$WEAVIATE_URL/v1/backups/filesystem\" | jq -r '.[] | select(.status == \"SUCCESS\") | .id' | sort -r | tail -n +8) for backup in $BACKUPS; do echo \"Deleting old backup: $backup\" curl -X DELETE \"$WEAVIATE_URL/v1/backups/filesystem/$backup\" done echo \"✅ Maintenance completed\"\r🔧 Troubleshooting Common Issues 1. Connection Refused # Check if Weaviate is running docker ps | grep weaviate # Check logs docker logs agentflow-weaviate # Check port availability netstat -tlnp | grep 8080 # Test connection curl http://localhost:8080/v1/.well-known/ready\r2. Authentication Errors # Test with API key curl -H \"Authorization: Bearer your-api-key\" \\ \"http://localhost:8080/v1/.well-known/ready\" # Check authentication configuration docker exec agentflow-weaviate env | grep AUTH\r3. Schema Issues # Check current schema curl \"http://localhost:8080/v1/schema\" | jq '.' # Delete class (careful!) curl -X DELETE \"http://localhost:8080/v1/schema/AgentMemory\" # Schema will be recreated automatically by AgentFlow\r4. Performance Issues # Check resource usage docker stats agentflow-weaviate # Monitor memory usage curl \"http://localhost:8080/v1/meta\" | jq '.hostname, .version' # Check for memory leaks docker exec agentflow-weaviate ps aux\r5. Cluster Issues # Check cluster status curl \"http://localhost:8080/v1/nodes\" | jq '.nodes[] | {name: .name, status: .status}' # Check gossip protocol docker logs agentflow-weaviate | grep -i gossip # Restart problematic nodes docker restart weaviate-node2\rDebug Mode Enable debug logging:\n# docker-compose.yml services: weaviate: environment: LOG_LEVEL: 'debug' PROMETHEUS_MONITORING_ENABLED: 'true'\rRecovery Procedures Reset Weaviate # ⚠️ WARNING: This will delete all data! # Stop Weaviate docker-compose down # Remove data volume docker volume rm $(docker volume ls -q | grep weaviate) # Restart Weaviate docker-compose up -d # Schema and data will be recreated by AgentFlow\rCluster Recovery # If cluster is in split-brain state: # Stop all nodes docker-compose down # Start nodes one by one docker-compose up -d weaviate-node1 sleep 30 docker-compose up -d weaviate-node2 sleep 30 docker-compose up -d weaviate-node3 # Check cluster status curl \"http://localhost:8080/v1/nodes\"\r🎯 Production Deployment Security Checklist # ✅ Enable authentication AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'false' AUTHENTICATION_APIKEY_ENABLED: 'true' # ✅ Use HTTPS in production # Configure reverse proxy (nginx, traefik, etc.) # ✅ Restrict network access # Use firewall rules or security groups # ✅ Regular backups # Automated backup schedule # ✅ Monitor resource usage # Set up alerts for CPU, memory, disk # ✅ Update regularly # Keep Weaviate version up to date\rLoad Balancer Setup # nginx.conf upstream weaviate_cluster { server weaviate-node1:8080; server weaviate-node2:8080; server weaviate-node3:8080; } server { listen 80; server_name your-weaviate-domain.com; location / { proxy_pass http://weaviate_cluster; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; } }\rMonitoring Setup # Add Prometheus monitoring services: weaviate: environment: PROMETHEUS_MONITORING_ENABLED: 'true' PROMETHEUS_MONITORING_PORT: '2112' ports: - \"2112:2112\" # Prometheus metrics prometheus: image: prom/prometheus ports: - \"9090:9090\" volumes: - ./prometheus.yml:/etc/prometheus/prometheus.yml\r🎉 Summary You now have a complete Weaviate setup for AgentFlow:\n✅ Vector Database: Purpose-built Weaviate vector database\n✅ Scalability: Clustering and load balancing support\n✅ Security: Authentication and access control\n✅ Monitoring: Health checks, metrics, and maintenance\n✅ Production Ready: Backup, recovery, and deployment procedures\nNext Steps Test your setup with the provided health check script Configure AgentFlow with your Weaviate connection Set up monitoring and backup procedures Scale horizontally with clustering if needed For more information:\nMemory System Guide - Complete API reference Memory Provider Setup - All provider options Configuration Guide - Advanced configuration",
    "description": "Weaviate Setup Guide Complete guide for setting up Weaviate vector database for AgentFlow memory\nThis guide provides detailed instructions for setting up Weaviate as a vector database backend for AgentFlow’s memory system, from development to production deployment.\n🎯 Overview Weaviate provides:\nPurpose-built vector database optimized for similarity search GraphQL API for flexible queries and data management Built-in clustering for horizontal scalability Advanced search features including hybrid search and filtering Rich ecosystem with integrations and modules 📋 Prerequisites Docker (recommended) OR Kubernetes 4GB+ RAM available (8GB+ recommended for production) Network access to Weaviate port (8080) Basic command line knowledge 🚀 Quick Start (Docker) Step 1: Run Weaviate # Create and start Weaviate container docker run -d \\ --name agentflow-weaviate \\ -p 8080:8080 \\ -e QUERY_DEFAULTS_LIMIT=25 \\ -e AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED='true' \\ -e PERSISTENCE_DATA_PATH='/var/lib/weaviate' \\ -e DEFAULT_VECTORIZER_MODULE='none' \\ -e CLUSTER_HOSTNAME='node1' \\ -e ENABLE_MODULES='text2vec-openai,text2vec-cohere,text2vec-huggingface,ref2vec-centroid,generative-openai,qna-openai' \\ -v weaviate_data:/var/lib/weaviate \\ semitechnologies/weaviate:1.22.4 # Wait for startup echo \"Waiting for Weaviate to start...\" sleep 15 # Verify Weaviate is running curl http://localhost:8080/v1/.well-known/ready\rStep 2: Test Connection # Check Weaviate status curl http://localhost:8080/v1/meta # Expected response: JSON with version and modules info\rStep 3: Configure AgentFlow Create agentflow.toml:",
    "tags": [],
    "title": "WeaviateSetup",
    "uri": "/AgenticGoKitDocs/guides/weaviatesetup/index.html"
  },
  {
    "breadcrumb": "content \u003e guides",
    "content": "Performance Tuning Guide This guide covers optimization techniques, benchmarking, and performance best practices for AgentFlow applications. Learn how to build high-throughput, low-latency agent systems.\n🎯 Performance Overview AgentFlow is designed for high performance with:\nGo’s Concurrency: Native goroutines and channels for parallel processing Event-Driven Architecture: Non-blocking event processing Connection Pooling: Efficient resource utilization Streaming Support: Low-memory processing of large datasets Intelligent Caching: Reduced redundant computations 📊 Benchmarking Basics Built-in Benchmarking AgentFlow includes benchmarking tools for measuring performance:\n# Run standard benchmarks agentcli benchmark --duration 60s --concurrent-users 10 # Benchmark specific agents agentcli benchmark --agent myagent --requests 1000 # Memory profiling agentcli benchmark --profile memory --output profile.mem # CPU profiling agentcli benchmark --profile cpu --output profile.cpu\rCustom Benchmark Setup func BenchmarkAgentExecution(b *testing.B) { // Setup config := \u0026core.Config{ LLM: core.LLMConfig{ Provider: \"azure\", Azure: core.AzureConfig{ Endpoint: os.Getenv(\"AZURE_OPENAI_ENDPOINT\"), APIKey: os.Getenv(\"AZURE_OPENAI_API_KEY\"), Deployment: \"gpt-4o\", }, }, } runner, err := core.NewRunner(config) require.NoError(b, err) agent := \u0026TestAgent{} runner.RegisterAgent(\"test\", agent) ctx := context.Background() runner.Start(ctx) defer runner.Stop() // Create test event event := core.NewEvent(\"test_query\", map[string]interface{}{ \"query\": \"What is the capital of France?\", }) // Reset timer before benchmarking b.ResetTimer() // Run benchmark b.RunParallel(func(pb *testing.PB) { for pb.Next() { err := runner.Emit(event) if err != nil { b.Error(err) } } }) }\rPerformance Metrics Collection type PerformanceMetrics struct { RequestsPerSecond float64 AverageLatency time.Duration P95Latency time.Duration P99Latency time.Duration ErrorRate float64 MemoryUsage uint64 GCPauses []time.Duration ActiveGoroutines int } func CollectMetrics(duration time.Duration) *PerformanceMetrics { var metrics PerformanceMetrics // Collect runtime stats var m runtime.MemStats runtime.ReadMemStats(\u0026m) metrics.MemoryUsage = m.Alloc metrics.ActiveGoroutines = runtime.NumGoroutine() // Collect latency percentiles latencies := collectLatencies(duration) sort.Slice(latencies, func(i, j int) bool { return latencies[i] \u003c latencies[j] }) if len(latencies) \u003e 0 { metrics.P95Latency = latencies[int(float64(len(latencies))*0.95)] metrics.P99Latency = latencies[int(float64(len(latencies))*0.99)] } return \u0026metrics }\r⚡ Agent Performance Optimization 1. Efficient State Management Minimize state copying and mutations:\ntype OptimizedAgent struct { cache *sync.Map // Thread-safe cache config AgentConfig } func (a *OptimizedAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Use read-only state access when possible query := state.GetString(\"query\") // Check cache before expensive operations if cached, ok := a.cache.Load(query); ok { return cached.(core.AgentResult), nil } // Only clone state when necessary workingState := state.CloneIfNeeded() result := a.processQuery(ctx, query, workingState) // Cache successful results if result.Success { a.cache.Store(query, result) } return result, nil } // Implement efficient state cloning func (s *State) CloneIfNeeded() *State { if s.IsReadOnly() { return s // No need to clone read-only state } return s.Clone() }\r2. Parallel Processing Patterns Leverage goroutines for concurrent operations:\nfunc (a *ParallelAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { query := event.GetData()[\"query\"].(string) // Parallel tool execution toolCalls := a.identifyToolCalls(query) results := make(chan ToolResult, len(toolCalls)) // Launch goroutines for each tool call var wg sync.WaitGroup for _, toolCall := range toolCalls { wg.Add(1) go func(tc ToolCall) { defer wg.Done() result := a.executeToolCall(ctx, tc) results \u003c- result }(toolCall) } // Close channel when all goroutines complete go func() { wg.Wait() close(results) }() // Collect results var toolResults []ToolResult for result := range results { toolResults = append(toolResults, result) } return a.synthesizeResults(toolResults), nil }\r3. Memory Pool Usage Reduce garbage collection pressure:\ntype Agent struct { bufferPool sync.Pool eventPool sync.Pool } func NewOptimizedAgent() *Agent { return \u0026Agent{ bufferPool: sync.Pool{ New: func() interface{} { return make([]byte, 0, 4096) // Pre-allocate 4KB }, }, eventPool: sync.Pool{ New: func() interface{} { return \u0026ProcessingEvent{} }, }, } } func (a *Agent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Get buffer from pool buffer := a.bufferPool.Get().([]byte) defer a.bufferPool.Put(buffer[:0]) // Reset and return to pool // Get event object from pool procEvent := a.eventPool.Get().(*ProcessingEvent) defer func() { procEvent.Reset() a.eventPool.Put(procEvent) }() // Use pooled objects for processing result := a.processWithBuffer(ctx, event, state, buffer, procEvent) return result, nil }\r🚀 LLM Provider Optimization 1. Connection Pooling Configure optimal connection pools:\n[llm.azure] max_connections = 50 min_connections = 5 connection_timeout = \"10s\" idle_timeout = \"300s\" max_connection_lifetime = \"3600s\" [llm.openai] max_connections = 30 request_timeout = \"30s\" retry_max_attempts = 3 retry_initial_interval = \"1s\"\rtype OptimizedLLMClient struct { client *http.Client pool *ConnectionPool } func NewOptimizedLLMClient(config *LLMConfig) *OptimizedLLMClient { transport := \u0026http.Transport{ MaxIdleConns: config.MaxConnections, MaxIdleConnsPerHost: config.MaxConnectionsPerHost, IdleConnTimeout: config.IdleTimeout, DisableKeepAlives: false, MaxConnsPerHost: config.MaxConnectionsPerHost, } client := \u0026http.Client{ Transport: transport, Timeout: config.RequestTimeout, } return \u0026OptimizedLLMClient{ client: client, pool: NewConnectionPool(config), } }\r2. Request Batching Batch multiple requests when supported:\ntype BatchingLLMClient struct { client LLMClient batcher *RequestBatcher maxBatch int batchTime time.Duration } func (c *BatchingLLMClient) ProcessRequests(requests []*LLMRequest) ([]*LLMResponse, error) { if len(requests) == 1 { // Single request - process immediately response, err := c.client.SendRequest(requests[0]) return []*LLMResponse{response}, err } // Batch multiple requests batches := c.createBatches(requests, c.maxBatch) responses := make([]*LLMResponse, 0, len(requests)) for _, batch := range batches { batchResponses, err := c.client.SendBatchRequest(batch) if err != nil { return nil, err } responses = append(responses, batchResponses...) } return responses, nil }\r3. Smart Caching Implement intelligent LLM response caching:\ntype CachedLLMClient struct { client LLMClient cache Cache hasher ContentHasher } func (c *CachedLLMClient) SendRequest(req *LLMRequest) (*LLMResponse, error) { // Generate cache key from request content key := c.hasher.Hash(req) // Check cache first if cached, found := c.cache.Get(key); found { return cached.(*LLMResponse), nil } // Send request to provider response, err := c.client.SendRequest(req) if err != nil { return nil, err } // Cache successful responses if response.IsSuccessful() { c.cache.Set(key, response, c.getTTL(req)) } return response, nil } func (c *CachedLLMClient) getTTL(req *LLMRequest) time.Duration { // Dynamic TTL based on request type if req.IsFactual() { return 24 * time.Hour // Cache factual queries longer } if req.IsCreative() { return 5 * time.Minute // Cache creative requests briefly } return time.Hour // Default TTL }\r🔧 MCP Tool Optimization 1. Tool Connection Management Optimize MCP server connections:\ntype OptimizedMCPManager struct { connections map[string]*MCPConnectionPool healthCheck *HealthChecker loadBalancer *LoadBalancer } type MCPConnectionPool struct { servers []*MCPConnection current int mutex sync.RWMutex } func (p *MCPConnectionPool) GetConnection() *MCPConnection { p.mutex.RLock() defer p.mutex.RUnlock() // Round-robin load balancing conn := p.servers[p.current] p.current = (p.current + 1) % len(p.servers) return conn } func (m *OptimizedMCPManager) ExecuteTool(ctx context.Context, tool string, params map[string]interface{}) (*ToolResult, error) { pool := m.connections[tool] if pool == nil { return nil, fmt.Errorf(\"no connections available for tool %s\", tool) } // Get healthy connection conn := pool.GetConnection() if !m.healthCheck.IsHealthy(conn) { conn = m.loadBalancer.GetHealthyConnection(pool) } return conn.ExecuteTool(ctx, tool, params) }\r2. MCP Server Tool Optimization Note: This pattern applies to external MCP server tools, not AgentFlow-native tools using the FunctionTool interface.\nHandle large tool results efficiently in MCP servers:\n// This is an example for an external MCP server implementation func (t *MCPServerTool) ExecuteStreaming(ctx context.Context, params map[string]interface{}) (\u003c-chan *ToolResult, error) { resultChan := make(chan *ToolResult, 100) // Buffered channel go func() { defer close(resultChan) // Use streaming database query rows, err := t.db.QueryContext(ctx, t.buildQuery(params)) if err != nil { resultChan \u003c- \u0026ToolResult{Error: err.Error()} return } defer rows.Close() batch := make([]map[string]interface{}, 0, 100) for rows.Next() { row := make(map[string]interface{}) err := rows.MapScan(row) if err != nil { resultChan \u003c- \u0026ToolResult{Error: err.Error()} return } batch = append(batch, row) // Send batch when full if len(batch) \u003e= 100 { resultChan \u003c- \u0026ToolResult{ Success: true, Data: batch, } batch = batch[:0] // Reset slice } } // Send remaining items if len(batch) \u003e 0 { resultChan \u003c- \u0026ToolResult{ Success: true, Data: batch, } } }() return resultChan, nil }\rFor AgentFlow-native tools, use pagination patterns instead:\n// AgentFlow-native tool with pagination func (t *NativeDataTool) Call(ctx context.Context, args map[string]any) (map[string]any, error) { pageSize := getIntParam(args, \"page_size\", 100) pageToken := getStringParam(args, \"page_token\", \"\") results, nextToken, err := t.fetchPage(ctx, pageToken, pageSize) if err != nil { return nil, err } response := map[string]any{ \"results\": results, \"page_size\": pageSize, \"has_more\": nextToken != \"\", } if nextToken != \"\" { response[\"next_page_token\"] = nextToken } return response, nil }\r📈 Concurrency Optimization 1. Worker Pool Pattern Limit concurrent operations with worker pools:\ntype WorkerPool struct { workers int jobQueue chan Job wg sync.WaitGroup ctx context.Context cancel context.CancelFunc } type Job struct { ID string Event core.Event State core.State Result chan\u003c- JobResult } func NewWorkerPool(workers int) *WorkerPool { ctx, cancel := context.WithCancel(context.Background()) return \u0026WorkerPool{ workers: workers, jobQueue: make(chan Job, workers*2), ctx: ctx, cancel: cancel, } } func (p *WorkerPool) Start() { for i := 0; i \u003c p.workers; i++ { p.wg.Add(1) go p.worker(i) } } func (p *WorkerPool) worker(id int) { defer p.wg.Done() for { select { case job := \u003c-p.jobQueue: result := p.processJob(job) job.Result \u003c- result case \u003c-p.ctx.Done(): return } } } func (p *WorkerPool) Submit(job Job) { select { case p.jobQueue \u003c- job: // Job queued successfully case \u003c-p.ctx.Done(): job.Result \u003c- JobResult{Error: \"Worker pool is shutting down\"} } }\r2. Circuit Breaker for High Load Protect against cascade failures:\ntype CircuitBreaker struct { state CircuitState failureCount int64 successCount int64 lastFailTime time.Time timeout time.Duration maxFailures int mutex sync.RWMutex } func (cb *CircuitBreaker) Execute(fn func() (interface{}, error)) (interface{}, error) { cb.mutex.Lock() state := cb.state cb.mutex.Unlock() switch state { case CircuitOpen: if time.Since(cb.lastFailTime) \u003e cb.timeout { cb.setState(CircuitHalfOpen) } else { return nil, ErrCircuitOpen } case CircuitHalfOpen: // Allow limited requests through } result, err := fn() if err != nil { cb.onFailure() return nil, err } cb.onSuccess() return result, nil }\r🔍 Profiling and Monitoring 1. Runtime Profiling Enable Go’s built-in profiling:\nimport ( _ \"net/http/pprof\" \"net/http\" \"log\" ) func main() { // Start pprof server go func() { log.Println(http.ListenAndServe(\"localhost:6060\", nil)) }() // Your AgentFlow application runAgentFlow() } // Profile CPU usage // go tool pprof http://localhost:6060/debug/pprof/profile // Profile memory usage // go tool pprof http://localhost:6060/debug/pprof/heap // Profile goroutines // go tool pprof http://localhost:6060/debug/pprof/goroutine\r2. Custom Metrics Track application-specific metrics:\ntype PerformanceTracker struct { requestDuration prometheus.HistogramVec requestCount prometheus.CounterVec activeRequests prometheus.GaugeVec errorRate prometheus.CounterVec } func NewPerformanceTracker() *PerformanceTracker { return \u0026PerformanceTracker{ requestDuration: prometheus.NewHistogramVec( prometheus.HistogramOpts{ Name: \"agent_request_duration_seconds\", Help: \"Agent request duration in seconds\", Buckets: []float64{0.1, 0.5, 1.0, 2.0, 5.0, 10.0}, }, []string{\"agent_name\", \"status\"}, ), requestCount: prometheus.NewCounterVec( prometheus.CounterOpts{ Name: \"agent_requests_total\", Help: \"Total agent requests\", }, []string{\"agent_name\", \"status\"}, ), } } func (pt *PerformanceTracker) TrackRequest(agentName string, duration time.Duration, success bool) { status := \"success\" if !success { status = \"error\" } pt.requestDuration.WithLabelValues(agentName, status).Observe(duration.Seconds()) pt.requestCount.WithLabelValues(agentName, status).Inc() }\r🎛️ Configuration Tuning 1. Runtime Optimization Tune Go runtime parameters:\nimport \"runtime\" func init() { // Set GOMAXPROCS to number of CPU cores runtime.GOMAXPROCS(runtime.NumCPU()) // Tune GC target percentage debug.SetGCPercent(100) // Default is 100 // Set memory limit (Go 1.19+) debug.SetMemoryLimit(8 \u003c\u003c 30) // 8GB limit }\r2. Application Configuration Optimize AgentFlow settings:\n[runner] max_concurrent_events = 100 event_buffer_size = 1000 worker_pool_size = 50 event_timeout = \"30s\" [memory] state_cache_size = 10000 state_cache_ttl = \"1h\" enable_state_compression = true [performance] enable_request_batching = true batch_size = 10 batch_timeout = \"100ms\" enable_connection_pooling = true pool_size = 50 [monitoring] enable_metrics = true metrics_interval = \"10s\" enable_profiling = true profile_port = 6060\r📊 Performance Benchmarks Typical Performance Characteristics Metric Single Agent Multi-Agent Streaming Throughput 1000 req/s 500 req/s 10k items/s Latency (P95) 50ms 200ms 5ms Memory Usage 50MB 200MB 30MB CPU Usage 20% 60% 15% Optimization Targets Latency: \u003c 100ms for simple queries, \u003c 500ms for complex multi-tool operations Throughput: \u003e 500 requests/second per instance Memory: \u003c 512MB per instance under normal load Error Rate: \u003c 0.1% for well-configured systems Load Testing Example # Install k6 load testing tool # https://k6.io/docs/getting-started/installation/ # Basic load test agentcli loadtest --script loadtest.js --duration 5m --vus 100 # Stress test agentcli loadtest --script stress.js --stages \"5m:100,10m:500,5m:100\" # Spike test agentcli loadtest --script spike.js --stages \"2m:10,1m:1000,2m:10\"\rThis performance guide provides the tools and techniques needed to build high-performance AgentFlow applications that scale efficiently under load.",
    "description": "Performance Tuning Guide This guide covers optimization techniques, benchmarking, and performance best practices for AgentFlow applications. Learn how to build high-throughput, low-latency agent systems.\n🎯 Performance Overview AgentFlow is designed for high performance with:\nGo’s Concurrency: Native goroutines and channels for parallel processing Event-Driven Architecture: Non-blocking event processing Connection Pooling: Efficient resource utilization Streaming Support: Low-memory processing of large datasets Intelligent Caching: Reduced redundant computations 📊 Benchmarking Basics Built-in Benchmarking AgentFlow includes benchmarking tools for measuring performance:",
    "tags": [],
    "title": "Performance",
    "uri": "/AgenticGoKitDocs/guides/performance/index.html"
  },
  {
    "breadcrumb": "content \u003e guides",
    "content": "Error Handling Guide AgentFlow provides comprehensive error handling mechanisms to build resilient agent systems. This guide covers error patterns, recovery strategies, and best practices for production-ready error handling.\n🎯 Error Handling Philosophy AgentFlow follows these principles for error handling:\nGraceful Degradation: Agents should continue functioning with reduced capabilities when possible Context Preservation: Error information should include enough context for debugging Recovery Strategies: Automatic retry and fallback mechanisms for transient failures User-Friendly Messages: End users should receive helpful, non-technical error messages 🔧 Core Error Types Agent Execution Errors Errors that occur during agent processing:\ntype AgentError struct { AgentName string Operation string Err error Context map[string]interface{} } func (e *AgentError) Error() string { return fmt.Sprintf(\"agent %s failed in %s: %v\", e.AgentName, e.Operation, e.Err) }\rTool Execution Errors Errors from MCP tool calls:\ntype ToolError struct { ToolName string ServerName string Method string Args map[string]interface{} Err error } func (e *ToolError) Error() string { return fmt.Sprintf(\"tool %s/%s failed: %v\", e.ServerName, e.ToolName, e.Err) }\rProvider Errors LLM provider specific errors:\ntype ProviderError struct { Provider string Type ProviderErrorType Err error Retryable bool } type ProviderErrorType string const ( ProviderRateLimit ProviderErrorType = \"rate_limit\" ProviderTimeout ProviderErrorType = \"timeout\" ProviderUnauthorized ProviderErrorType = \"unauthorized\" ProviderQuotaExceeded ProviderErrorType = \"quota_exceeded\" ProviderServiceError ProviderErrorType = \"service_error\" )\r🛡️ Error Handling Strategies 1. Graceful Error Recovery Implement agents that can continue with partial failures:\nfunc (h *ResilientHandler) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { result := core.AgentResult{ Data: make(map[string]interface{}), Errors: []error{}, } // Try primary tool first primaryResult, err := h.tryPrimaryTool(ctx, event, state) if err != nil { result.Errors = append(result.Errors, fmt.Errorf(\"primary tool failed: %w\", err)) // Fall back to secondary tool fallbackResult, fallbackErr := h.tryFallbackTool(ctx, event, state) if fallbackErr != nil { result.Errors = append(result.Errors, fmt.Errorf(\"fallback tool failed: %w\", fallbackErr)) return result, fmt.Errorf(\"all tools failed\") } result.Data[\"source\"] = \"fallback\" result.Data[\"result\"] = fallbackResult result.Data[\"warnings\"] = []string{\"Primary tool unavailable, used fallback\"} } else { result.Data[\"source\"] = \"primary\" result.Data[\"result\"] = primaryResult } return result, nil }\r2. Retry Logic with Exponential Backoff AgentFlow provides built-in retry mechanisms:\nfunc (h *RetryableHandler) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { retryConfig := \u0026core.RetryConfig{ MaxAttempts: 3, InitialInterval: time.Second, MaxInterval: 10 * time.Second, Multiplier: 2.0, RetryableErrors: []core.RetryableErrorChecker{ \u0026core.ProviderTimeoutChecker{}, \u0026core.RateLimitChecker{}, \u0026core.NetworkErrorChecker{}, }, } return core.WithRetry(ctx, retryConfig, func(ctx context.Context) (core.AgentResult, error) { return h.executeWithPossibleFailure(ctx, event, state) }) }\r3. Circuit Breaker Pattern Prevent cascading failures with circuit breakers:\nfunc NewCircuitBreakerHandler(handler core.AgentHandler) *CircuitBreakerHandler { return \u0026CircuitBreakerHandler{ handler: handler, breaker: core.NewCircuitBreaker(\u0026core.CircuitBreakerConfig{ FailureThreshold: 5, RecoveryTimeout: 30 * time.Second, SuccessThreshold: 3, Timeout: 10 * time.Second, }), } } func (h *CircuitBreakerHandler) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { result, err := h.breaker.Execute(ctx, func(ctx context.Context) (interface{}, error) { return h.handler.Run(ctx, event, state) }) if err != nil { if core.IsCircuitBreakerOpen(err) { return core.AgentResult{ Data: map[string]interface{}{ \"error\": \"Service temporarily unavailable\", \"retry_after\": 30, }, }, nil // Return graceful degradation instead of error } return core.AgentResult{}, err } return result.(core.AgentResult), nil }\r4. Input Validation and Sanitization Validate inputs before processing:\nfunc (h *ValidatedHandler) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Validate event data if err := h.validateEvent(event); err != nil { return core.AgentResult{}, \u0026core.ValidationError{ Field: \"event\", Message: \"Invalid event data\", Err: err, } } // Validate state if err := h.validateState(state); err != nil { return core.AgentResult{}, \u0026core.ValidationError{ Field: \"state\", Message: \"Invalid state data\", Err: err, } } // Sanitize input sanitizedEvent := h.sanitizeEvent(event) return h.processValidatedEvent(ctx, sanitizedEvent, state) } func (h *ValidatedHandler) validateEvent(event core.Event) error { data := event.GetData() // Check required fields if query, ok := data[\"query\"].(string); !ok || strings.TrimSpace(query) == \"\" { return fmt.Errorf(\"query field is required and cannot be empty\") } // Check data size if len(fmt.Sprintf(\"%v\", data)) \u003e h.maxEventSize { return fmt.Errorf(\"event data exceeds maximum size of %d bytes\", h.maxEventSize) } // Content safety check if !h.isContentSafe(data) { return fmt.Errorf(\"event contains unsafe content\") } return nil }\r🚨 Error Routing and Handling Automatic Error Routing AgentFlow can automatically route errors to specialized handlers:\nfunc setupErrorRouting(runner *core.Runner) error { // Register error handlers for different error types runner.RegisterErrorHandler(core.ValidationErrorType, \u0026ValidationErrorHandler{}) runner.RegisterErrorHandler(core.TimeoutErrorType, \u0026TimeoutErrorHandler{}) runner.RegisterErrorHandler(core.RateLimitErrorType, \u0026RateLimitErrorHandler{}) runner.RegisterErrorHandler(core.CriticalErrorType, \u0026CriticalErrorHandler{}) return nil } type ValidationErrorHandler struct{} func (h *ValidationErrorHandler) HandleError(ctx context.Context, err error, event core.Event) core.AgentResult { validationErr, ok := err.(*core.ValidationError) if !ok { return core.AgentResult{ Data: map[string]interface{}{ \"error\": \"Internal validation error\", }, } } return core.AgentResult{ Data: map[string]interface{}{ \"error\": \"Invalid input\", \"field\": validationErr.Field, \"message\": validationErr.GetUserFriendlyMessage(), \"suggestions\": validationErr.GetSuggestions(), }, } }\rCustom Error Middleware Create middleware for consistent error handling:\nfunc ErrorHandlingMiddleware(next core.AgentHandler) core.AgentHandler { return core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { defer func() { if r := recover(); r != nil { log.Printf(\"Agent panic recovered: %v\\nStack: %s\", r, debug.Stack()) // Could emit error event or alert here } }() // Add request ID for tracing ctx = core.WithRequestID(ctx, core.GenerateRequestID()) // Execute handler with timeout timeoutCtx, cancel := context.WithTimeout(ctx, 30*time.Second) defer cancel() result, err := next.Run(timeoutCtx, event, state) if err != nil { // Log error with context log.Printf(\"Agent error [%s]: %v\", core.GetRequestID(ctx), err) // Transform error for user if userErr := transformErrorForUser(err); userErr != nil { return core.AgentResult{ Data: map[string]interface{}{ \"error\": userErr.Error(), \"request_id\": core.GetRequestID(ctx), }, }, nil } return result, err } return result, nil }) }\r📊 Error Monitoring and Observability Error Metrics Track error patterns and frequencies:\nfunc setupErrorMetrics() { // Prometheus metrics for error tracking errorCounter := prometheus.NewCounterVec( prometheus.CounterOpts{ Name: \"agent_errors_total\", Help: \"Total number of agent errors by type and agent\", }, []string{\"agent_name\", \"error_type\", \"severity\"}, ) errorDuration := prometheus.NewHistogramVec( prometheus.HistogramOpts{ Name: \"agent_error_recovery_duration_seconds\", Help: \"Time taken to recover from errors\", Buckets: prometheus.DefBuckets, }, []string{\"agent_name\", \"recovery_strategy\"}, ) prometheus.MustRegister(errorCounter, errorDuration) } func recordError(agentName string, err error) { errorType := classifyError(err) severity := determineSeverity(err) errorCounter.WithLabelValues(agentName, errorType, severity).Inc() // Also log structured error data log.WithFields(log.Fields{ \"agent_name\": agentName, \"error_type\": errorType, \"severity\": severity, \"error\": err.Error(), \"timestamp\": time.Now(), }).Error(\"Agent execution error\") }\rError Context Collection Gather comprehensive context for debugging:\nfunc enrichErrorContext(ctx context.Context, err error, event core.Event, state core.State) *ErrorReport { return \u0026ErrorReport{ Error: err, RequestID: core.GetRequestID(ctx), Timestamp: time.Now(), AgentName: core.GetAgentName(ctx), SessionID: core.GetSessionID(ctx), Event: ErrorEventContext{ Type: event.GetType(), Data: sanitizeForLogging(event.GetData()), UserID: event.GetUserID(), }, State: ErrorStateContext{ Keys: state.Keys(), Size: len(state.String()), LastUpdate: state.GetLastModified(), }, Environment: ErrorEnvironmentContext{ Version: core.Version(), GoVersion: runtime.Version(), Platform: runtime.GOOS + \"/\" + runtime.GOARCH, MemStats: getMemoryStats(), }, Stacktrace: string(debug.Stack()), } }\r🔄 Recovery Patterns Stateful Recovery For agents that maintain state across calls:\nfunc (h *StatefulHandler) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Create checkpoint before risky operation checkpoint := state.CreateCheckpoint() defer func() { if r := recover(); r != nil { // Restore from checkpoint on panic state.RestoreFromCheckpoint(checkpoint) log.Printf(\"Restored state from checkpoint due to panic: %v\", r) } }() result, err := h.riskyOperation(ctx, event, state) if err != nil { // Restore state on error state.RestoreFromCheckpoint(checkpoint) // Try alternative approach with clean state return h.fallbackOperation(ctx, event, state) } return result, nil }\rMulti-Agent Error Recovery Coordinate error handling across multiple agents:\nfunc (o *FaultTolerantOrchestrator) ProcessEvent(ctx context.Context, event core.Event) error { agents := o.getActiveAgents() results := make(chan agentResult, len(agents)) // Run agents in parallel for _, agent := range agents { go func(a core.Agent) { result, err := a.Run(ctx, event.Clone(), o.state.Clone()) results \u003c- agentResult{agent: a.Name(), result: result, err: err} }(agent) } // Collect results and handle failures var successful []agentResult var failed []agentResult for i := 0; i \u003c len(agents); i++ { result := \u003c-results if result.err != nil { failed = append(failed, result) } else { successful = append(successful, result) } } // Determine if we can proceed with partial success if len(successful) \u003e= o.minSuccessfulAgents { return o.consolidateResults(successful, failed) } // All critical agents failed - trigger recovery return o.initiateRecovery(ctx, event, failed) }\r🧪 Testing Error Scenarios Error Injection for Testing Test error handling with controlled failures:\nfunc TestAgentErrorHandling(t *testing.T) { tests := []struct { name string injectedError error expectedResult string shouldRecover bool }{ { name: \"Network timeout\", injectedError: \u0026core.TimeoutError{Operation: \"llm_call\", Duration: time.Second * 30}, expectedResult: \"retry_scheduled\", shouldRecover: true, }, { name: \"Rate limit exceeded\", injectedError: \u0026core.RateLimitError{Provider: \"azure\", RetryAfter: time.Minute}, expectedResult: \"rate_limited\", shouldRecover: true, }, { name: \"Invalid input\", injectedError: \u0026core.ValidationError{Field: \"query\", Message: \"empty query\"}, expectedResult: \"validation_failed\", shouldRecover: false, }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { // Create agent with error injection handler := \u0026TestHandler{ injectedError: tt.injectedError, } agent := core.NewAgent(\"test-agent\", handler) // Run and verify error handling result, err := agent.Run(context.Background(), createTestEvent(), createTestState()) if tt.shouldRecover { assert.NoError(t, err) assert.Contains(t, result.Data[\"status\"], tt.expectedResult) } else { assert.Error(t, err) } }) } }\rChaos Engineering Implement chaos testing for production resilience:\nfunc ChaosTestingMiddleware(chaosConfig *ChaosConfig) func(core.AgentHandler) core.AgentHandler { return func(next core.AgentHandler) core.AgentHandler { return core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Randomly inject failures based on configuration if chaosConfig.ShouldInjectFailure() { return core.AgentResult{}, chaosConfig.GetRandomError() } // Introduce artificial latency if delay := chaosConfig.GetRandomDelay(); delay \u003e 0 { time.Sleep(delay) } return next.Run(ctx, event, state) }) } }\r📋 Error Handling Best Practices 1. Error Classification Transient: Network timeouts, rate limits, temporary service outages Permanent: Authentication failures, malformed requests, invalid configurations Critical: System panics, out-of-memory, disk full 2. User Experience Provide clear, actionable error messages Avoid exposing internal implementation details Offer alternative actions when possible Include request IDs for support purposes 3. Logging and Monitoring Log errors with sufficient context for debugging Use structured logging for easy parsing Set up alerts for error rate thresholds Track error patterns and trends 4. Recovery Strategies Implement graceful degradation for non-critical failures Use circuit breakers to prevent cascade failures Provide fallback mechanisms for critical operations Cache successful responses for replay during failures This comprehensive error handling approach ensures your AgentFlow applications remain resilient and user-friendly even when things go wrong.",
    "description": "Error Handling Guide AgentFlow provides comprehensive error handling mechanisms to build resilient agent systems. This guide covers error patterns, recovery strategies, and best practices for production-ready error handling.\n🎯 Error Handling Philosophy AgentFlow follows these principles for error handling:\nGraceful Degradation: Agents should continue functioning with reduced capabilities when possible Context Preservation: Error information should include enough context for debugging Recovery Strategies: Automatic retry and fallback mechanisms for transient failures User-Friendly Messages: End users should receive helpful, non-technical error messages 🔧 Core Error Types Agent Execution Errors Errors that occur during agent processing:",
    "tags": [],
    "title": "ErrorHandling",
    "uri": "/AgenticGoKitDocs/guides/errorhandling/index.html"
  },
  {
    "breadcrumb": "content",
    "content": "Contributor Documentation Navigation: Documentation Home → Contributors\nFor developers contributing to AgenticGoKit\nThis section contains documentation specifically for contributors to the AgenticGoKit project. If you’re looking to use AgenticGoKit in your projects, see the main documentation.\n🚀 Getting Started Essential Reading Contributor Guide - Start here! Development setup and contribution workflow Core vs Internal - Understanding the public API vs implementation details Code Style - Go standards and project conventions Development Process Adding Features - How to extend AgenticGoKit with new features Testing Strategy - Unit tests, integration tests, and benchmarks Documentation Standards - Writing user-focused documentation Project Management Release Process - How releases are managed and versioned 🏗️ Architecture Overview AgenticGoKit is designed with a clear separation between public APIs and internal implementation:\ncore/ package: Public API that users interact with internal/ package: Implementation details not exposed to users cmd/ package: CLI tools and utilities examples/ directory: Working examples and tutorials 🧪 Development Workflow Fork and Clone: Fork the repository and clone your fork Create Branch: Create a feature branch for your changes Develop: Make your changes following our code style Test: Run tests and add new tests for your changes Document: Update documentation as needed Submit PR: Create a pull request with a clear description 📋 Contribution Guidelines Code Quality Follow Go best practices and our Code Style Write comprehensive tests for new features Ensure all tests pass before submitting Use meaningful commit messages Documentation Update user documentation for new features Follow our Documentation Standards Include code examples in documentation Keep contributor docs up to date Communication Use GitHub Issues for bug reports and feature requests Use GitHub Discussions for questions and community interaction Be respectful and constructive in all interactions 🔧 Development Tools Required Tools # Go (1.21+) go version # Linting go install github.com/golangci/golangci-lint/cmd/golangci-lint@latest # Testing go test ./... # Documentation generation (if applicable) go run tools/docgen/main.go\rRecommended IDE Setup VS Code with Go extension GoLand by JetBrains Vim/Neovim with Go plugins 📚 Additional Resources GitHub Repository GitHub Issues GitHub Discussions Main Documentation - For users of AgenticGoKit Thank you for contributing to AgenticGoKit! 🎉",
    "description": "Contributor Documentation Navigation: Documentation Home → Contributors\nFor developers contributing to AgenticGoKit\nThis section contains documentation specifically for contributors to the AgenticGoKit project. If you’re looking to use AgenticGoKit in your projects, see the main documentation.\n🚀 Getting Started Essential Reading Contributor Guide - Start here! Development setup and contribution workflow Core vs Internal - Understanding the public API vs implementation details Code Style - Go standards and project conventions Development Process Adding Features - How to extend AgenticGoKit with new features Testing Strategy - Unit tests, integration tests, and benchmarks Documentation Standards - Writing user-focused documentation Project Management Release Process - How releases are managed and versioned 🏗️ Architecture Overview AgenticGoKit is designed with a clear separation between public APIs and internal implementation:",
    "tags": [],
    "title": "contributors",
    "uri": "/AgenticGoKitDocs/contributors/index.html"
  },
  {
    "breadcrumb": "content \u003e reference",
    "content": "CLI Reference Complete reference for the AgenticGoKit command-line interface\nThis document provides comprehensive reference for AgenticGoKit’s command-line interface (agentcli), covering all commands, options, and usage patterns.\n🏗️ Installation and Setup Installation # Install from source go install github.com/kunalkushwaha/agenticgokit/cmd/agentcli@latest # Or download binary from releases curl -L https://github.com/kunalkushwaha/agenticgokit/releases/latest/download/agentcli-${OS}-${ARCH}.tar.gz | tar xz\r📋 Command Structure agentcli [global options] command [command options] [arguments...]\rGlobal Options Option Short Description Default --config -c Path to configuration file agentflow.toml --verbose -v Enable verbose output false --quiet -q Suppress non-error output false --help -h Show help information --version Show version information 🚀 Available Commands Based on the actual codebase, the following commands are available:\ntrace View execution traces with detailed agent flows\n# View a basic trace with all details agentcli trace \u003csession-id\u003e # View only the agent flow for a session agentcli trace --flow-only \u003csession-id\u003e # Filter trace to see only a specific agent's activity agentcli trace --filter \u003cagent-name\u003e \u003csession-id\u003e\rmcp Manage Model Context Protocol servers and tools\n# List connected MCP servers agentcli mcp servers # View MCP server status agentcli mcp status\rcache Monitor and optimize MCP tool result caches\n# View cache statistics agentcli cache stats # Clear specific caches agentcli cache clear --server web-service\rcreate Create new AgenticGoKit projects\n# Create a new project agentcli create my-project\rlist List various resources\n# List available resources agentcli list\rmemory Memory operations and management\n# Memory operations agentcli memory\r📚 Usage Examples Tracing and Debugging # View execution traces agentcli trace session-123 # Filter traces by agent agentcli trace --filter chat-agent session-123 # View agent flow only agentcli trace --flow-only session-123\rMCP Management # List MCP servers agentcli mcp servers # Check MCP status agentcli mcp status\rCache Management # View cache statistics agentcli cache stats # Clear cache for specific server agentcli cache clear --server search-service\r🔧 Configuration The CLI uses the same agentflow.toml configuration file as the main framework. See the Configuration API for complete configuration options.\n📝 Exit Codes Code Description 0 Success 1 General error 2 Configuration error 3 Network/connectivity error For complete CLI documentation and all available options, run:\nagentcli --help agentcli \u003ccommand\u003e --help",
    "description": "CLI Reference Complete reference for the AgenticGoKit command-line interface\nThis document provides comprehensive reference for AgenticGoKit’s command-line interface (agentcli), covering all commands, options, and usage patterns.\n🏗️ Installation and Setup Installation # Install from source go install github.com/kunalkushwaha/agenticgokit/cmd/agentcli@latest # Or download binary from releases curl -L https://github.com/kunalkushwaha/agenticgokit/releases/latest/download/agentcli-${OS}-${ARCH}.tar.gz | tar xz\r📋 Command Structure agentcli [global options] command [command options] [arguments...]\rGlobal Options Option Short Description Default --config -c Path to configuration file agentflow.toml --verbose -v Enable verbose output false --quiet -q Suppress non-error output false --help -h Show help information --version Show version information 🚀 Available Commands Based on the actual codebase, the following commands are available:",
    "tags": [],
    "title": "cli",
    "uri": "/AgenticGoKitDocs/reference/cli/index.html"
  },
  {
    "breadcrumb": "content \u003e reference",
    "content": "MCP Integration API Tool integration via Model Context Protocol\nThis document covers AgenticGoKit’s MCP (Model Context Protocol) integration API, which enables agents to discover, connect to, and use external tools and services. MCP provides a standardized way to integrate with various tools, from web search to database operations.\n📋 Core Concepts MCP Overview MCP (Model Context Protocol) is a protocol for connecting AI agents with external tools and services. AgenticGoKit provides comprehensive MCP integration with three levels of complexity:\nBasic MCP: Simple tool discovery and execution Enhanced MCP: Caching and performance optimization Production MCP: Enterprise-grade features with monitoring and scaling 🚀 Basic Usage Quick Start with MCP // Initialize MCP with automatic discovery err := core.QuickStartMCP() if err != nil { panic(fmt.Sprintf(\"Failed to initialize MCP: %v\", err)) } // Create an MCP-aware agent llmProvider, err := core.NewOpenAIProvider() if err != nil { panic(fmt.Sprintf(\"Failed to create LLM provider: %v\", err)) } agent, err := core.NewMCPAgent(\"assistant\", llmProvider) if err != nil { panic(fmt.Sprintf(\"Failed to create MCP agent: %v\", err)) }\rFor complete documentation including server discovery, caching, production deployment, and custom tool development, see the Agent API reference.",
    "description": "MCP Integration API Tool integration via Model Context Protocol\nThis document covers AgenticGoKit’s MCP (Model Context Protocol) integration API, which enables agents to discover, connect to, and use external tools and services. MCP provides a standardized way to integrate with various tools, from web search to database operations.\n📋 Core Concepts MCP Overview MCP (Model Context Protocol) is a protocol for connecting AI agents with external tools and services. AgenticGoKit provides comprehensive MCP integration with three levels of complexity:",
    "tags": [],
    "title": "mcp",
    "uri": "/AgenticGoKitDocs/reference/api/mcp/index.html"
  },
  {
    "breadcrumb": "content \u003e contributors",
    "content": "Contributor Guide Navigation: Documentation Home → Contributors → Contributor Guide\nContributing to AgenticGoKit Development\nNote: This is contributor documentation. If you’re looking to use AgenticGoKit in your projects, see the main documentation.\nWelcome to AgenticGoKit! This guide will help you get started with contributing to the project, understanding the codebase, and following our development practices.\nQuick Start for Contributors 1. Development Setup # Clone the repository git clone https://github.com/kunalkushwaha/agenticgokit.git cd agenticgokit # Install dependencies go mod tidy # Run tests to ensure everything works go test ./... # Install development tools go install github.com/golangci/golangci-lint/cmd/golangci-lint@latest\r2. Project Structure agenticgokit/\r├── cmd/ # CLI tools\r│ └── agentcli/ # AgentFlow CLI\r├── core/ # Public API (this is what users import)\r│ ├── agent.go # Agent interfaces\r│ ├── mcp.go # MCP integration public API\r│ ├── factory.go # Factory functions\r│ └── ...\r├── internal/ # Private implementation (not importable)\r│ ├── agents/ # Agent implementations\r│ ├── mcp/ # MCP client management\r│ ├── llm/ # LLM provider implementations\r│ ├── orchestrator/ # Workflow orchestration\r│ └── scaffold/ # CLI project generation\r├── examples/ # Example projects and demos\r├── docs/ # Documentation\r└── benchmarks/ # Performance benchmarks\r3. Core vs Internal Architecture core/ Package (Public API):\nThis is what users import: import \"github.com/kunalkushwaha/agenticgokit/core\" Contains interfaces, types, and factory functions Must maintain backward compatibility All functions here should be well-documented and tested internal/ Package (Private Implementation):\nImplementation details that users don’t need to know about Can change without breaking user code Contains concrete implementations of core interfaces Business logic and complex algorithms Example:\n// core/agent.go - Public interface type AgentHandler interface { Run(ctx context.Context, event Event, state State) (AgentResult, error) } // internal/agents/basic_agent.go - Private implementation type basicAgent struct { llm llm.Provider mcpManager mcp.Manager } // core/factory.go - Public factory func NewMCPAgent(name string, llm ModelProvider) AgentHandler { // Creates internal implementation but returns public interface return internal.NewBasicAgent(name, llm) }\rDevelopment Workflow 1. Feature Development Process Create Issue: Discuss the feature/bug on GitHub Issues Fork \u0026 Branch: Create a feature branch from main Develop: Write code following our standards (see below) Test: Add comprehensive tests for your changes Document: Update documentation and examples PR: Submit a pull request with clear description 2. Branch Naming feature/mcp-server-discovery # New features fix/agent-error-handling # Bug fixes docs/api-reference-update # Documentation refactor/internal-mcp-client # Code improvements\r3. Commit Convention git commit -m \"feat(mcp): add server discovery caching\" git commit -m \"fix(agent): handle nil state gracefully\" git commit -m \"docs(api): update MCP integration examples\" git commit -m \"test(core): add agent factory unit tests\"\rTypes: feat, fix, docs, test, refactor, perf, ci\nCode Standards 1. Go Code Style Follow standard Go conventions plus AgenticGoKit-specific patterns:\n// Good: Clear interface with documentation // AgentHandler processes events and manages agent state. type AgentHandler interface { // Run executes the agent logic for the given event and state. // It returns the result and potentially modified state. Run(ctx context.Context, event Event, state State) (AgentResult, error) } // Good: Factory function with clear naming func NewMCPAgent(name string, llm ModelProvider, mcp MCPManager) AgentHandler { return \u0026mcpAgent{ name: name, llm: llm, mcpManager: mcp, } } // Good: Error handling with context func (a *mcpAgent) Run(ctx context.Context, event Event, state State) (AgentResult, error) { if event == nil { return AgentResult{}, fmt.Errorf(\"event cannot be nil\") } message, ok := event.GetData()[\"message\"] if !ok { return AgentResult{}, fmt.Errorf(\"missing required field: message\") } // ... implementation return AgentResult{Result: response, State: state}, nil }\r2. Public API Guidelines Do:\nUse clear, descriptive names Provide comprehensive documentation Include usage examples in godoc Return errors instead of panicking Use context.Context for cancellation Design for testability Don’t:\nExpose internal implementation details Break backward compatibility Use package-level global state Ignore errors Use interface{} unnecessarily 3. Testing Standards Every public function needs tests:\nfunc TestNewMCPAgent(t *testing.T) { // Test successful creation llm := \u0026MockModelProvider{} mcp := \u0026MockMCPManager{} agent := core.NewMCPAgent(\"test-agent\", llm, mcp) assert.NotNil(t, agent) // Test with nil parameters nilAgent := core.NewMCPAgent(\"\", nil, nil) assert.NotNil(t, nilAgent) // Should handle gracefully } func TestMCPAgent_Run(t *testing.T) { tests := []struct { name string event core.Event expectedErr bool setup func() (core.ModelProvider, core.MCPManager) }{ { name: \"successful execution\", event: core.NewEvent(\"test\", core.EventData{\"message\": \"hello\"}, nil), expectedErr: false, setup: func() (core.ModelProvider, core.MCPManager) { llm := \u0026MockModelProvider{response: \"Hello response\"} mcp := \u0026MockMCPManager{} return llm, mcp }, }, { name: \"missing message\", event: core.NewEvent(\"test\", core.EventData{}, nil), expectedErr: true, setup: func() (core.ModelProvider, core.MCPManager) { return \u0026MockModelProvider{}, \u0026MockMCPManager{} }, }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { llm, mcp := tt.setup() agent := core.NewMCPAgent(\"test\", llm, mcp) state := core.NewState() result, err := agent.Run(context.Background(), tt.event, state) if tt.expectedErr { assert.Error(t, err) } else { assert.NoError(t, err) assert.NotEmpty(t, result.Result) } }) } }\rArchitecture Decisions 1. Why core/ and internal/ ? This separation provides:\nClear Public API: Users know exactly what they can import Implementation Freedom: We can refactor internal code without breaking users Better Testing: Public API has different testing requirements than internal code Documentation Focus: We document the public API extensively 2. Interface-First Design We define interfaces in core/ and implementations in internal/:\n// core/mcp.go type MCPManager interface { ListTools(ctx context.Context) ([]ToolSchema, error) CallTool(ctx context.Context, name string, args map[string]interface{}) (interface{}, error) // ... more methods } // internal/mcp/manager.go type manager struct { clients map[string]*client.MCPClient cache *toolCache } func (m *manager) ListTools(ctx context.Context) ([]ToolSchema, error) { // Implementation details here } // core/factory.go func InitializeProductionMCP(ctx context.Context, config MCPConfig) (MCPManager, error) { return mcp.NewManager(config), nil // Returns internal implementation as interface }\r3. Factory Pattern Usage We use factories to:\nHide complex initialization Provide sensible defaults Allow easy testing with mocks Support configuration-driven setup // Simple factory func NewMCPAgent(name string, llm ModelProvider) AgentHandler // Configuration-driven factory func NewMCPAgentWithConfig(config AgentConfig) AgentHandler // Auto-configuration factory func NewMCPAgentFromWorkingDir() (AgentHandler, error)\rAdding New Features 1. Adding a New Agent Type Define Interface (if needed) in core/agent.go Implement in internal/agents/your_agent.go Add Factory in core/factory.go Write Tests for both interface and implementation Update Documentation with examples Example:\n// core/agent.go - Add to existing interfaces or create new one type SpecializedAgent interface { AgentHandler GetSpecialization() string } // internal/agents/research_agent.go type researchAgent struct { name string llm llm.Provider mcpManager mcp.Manager specialization string } func (r *researchAgent) GetSpecialization() string { return r.specialization } func (r *researchAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Implementation } // core/factory.go func NewResearchAgent(name string, llm ModelProvider, mcp MCPManager) SpecializedAgent { return \u0026agents.researchAgent{ name: name, llm: llm, mcpManager: mcp, specialization: \"research\", } }\r2. Adding a New LLM Provider Implement Interface in internal/llm/your_provider.go Add Configuration types in core/llm.go Add Factory in core/factory.go Update CLI scaffolding to support new provider Add Tests and documentation 3. Adding MCP Features Update Interface in core/mcp.go Implement in internal/mcp/ Update Helper Functions like FormatToolsForPrompt Test Integration with various MCP servers Testing Strategy 1. Test Categories Unit Tests: Test individual functions and methods\ngo test ./core/... # Test public API go test ./internal/... # Test implementations\rIntegration Tests: Test component interactions\ngo test -tags=integration ./...\rEnd-to-End Tests: Test complete workflows\ngo test -tags=e2e ./...\r2. Mock Strategy We provide mocks for testing:\n// tests/mocks/mcp.go type MockMCPManager struct { tools []core.ToolSchema toolResults map[string]interface{} } func (m *MockMCPManager) ListTools(ctx context.Context) ([]core.ToolSchema, error) { return m.tools, nil } // Usage in tests func TestAgentWithMCP(t *testing.T) { mockMCP := \u0026mocks.MockMCPManager{ tools: []core.ToolSchema{{Name: \"search\", Description: \"Search tool\"}}, toolResults: map[string]interface{}{\"search\": \"Mock results\"}, } agent := core.NewMCPAgent(\"test\", mockLLM, mockMCP) // ... test logic }\r3. Benchmark Tests For performance-critical code:\nfunc BenchmarkAgentRun(b *testing.B) { agent := core.NewMCPAgent(\"bench\", mockLLM, mockMCP) event := core.NewEvent(\"test\", core.EventData{\"message\": \"benchmark\"}, nil) state := core.NewState() b.ResetTimer() for i := 0; i \u003c b.N; i++ { _, err := agent.Run(context.Background(), event, state) if err != nil { b.Fatal(err) } } }\rDocumentation Standards 1. Code Documentation Every public function needs comprehensive godoc:\n// NewMCPAgent creates a new agent with MCP tool integration capabilities. // // The agent will automatically discover available tools from the MCP manager // and include them in LLM prompts. Tool calls in LLM responses will be // parsed and executed automatically. // // Parameters: // - name: Unique identifier for the agent // - llm: Model provider for generating responses // - mcp: MCP manager for tool discovery and execution // // Returns an AgentHandler that can process events with tool integration. // // Example: // provider, _ := core.NewAzureProvider(config) // mcpManager, _ := core.InitializeProductionMCP(ctx, mcpConfig) // agent := core.NewMCPAgent(\"research-agent\", provider, mcpManager) // // result, err := agent.Run(ctx, event, state) func NewMCPAgent(name string, llm ModelProvider, mcp MCPManager) AgentHandler { // Implementation }\r2. User Documentation When adding features, also update:\nGuides in docs/guides/ API reference in docs/api/ Examples in examples/ Main README if it’s a major feature 3. Contributing to Documentation # Add user-focused guides docs/guides/YourNewFeature.md # Add API documentation docs/api/your_package.md # Add examples examples/your-feature/\rRelease Process 1. Version Management We use semantic versioning:\nv1.0.0 - Major release (breaking changes) v1.1.0 - Minor release (new features) v1.0.1 - Patch release (bug fixes) 2. Release Checklist Before releasing:\nAll tests pass Documentation is updated Examples work with new version Breaking changes are documented Migration guide is provided (if needed) 3. Backward Compatibility We maintain backward compatibility in the core/ package. When making breaking changes:\nDeprecate old functions (add // Deprecated: comment) Provide new functions alongside old ones Update documentation and examples Remove deprecated functions only in major releases Getting Help 1. Development Questions GitHub Discussions: General questions about contributing GitHub Issues: Bug reports and feature requests Code Reviews: Ask questions in PR comments 2. Architecture Decisions For major architectural changes:\nCreate GitHub Issue with [RFC] prefix Discuss with maintainers Create design document if needed Implement after consensus 3. Code Style Questions Follow existing patterns in the codebase. When in doubt:\nLook at similar functions in the same package Check go fmt and golangci-lint outputs Ask in PR comments Next Steps Architecture Deep Dive - Understand the internal structure Testing Strategy - Learn our testing approaches Adding Features - Detailed guide for extending AgenticGoKit Release Process - How we manage releases",
    "description": "Contributor Guide Navigation: Documentation Home → Contributors → Contributor Guide\nContributing to AgenticGoKit Development\nNote: This is contributor documentation. If you’re looking to use AgenticGoKit in your projects, see the main documentation.\nWelcome to AgenticGoKit! This guide will help you get started with contributing to the project, understanding the codebase, and following our development practices.\nQuick Start for Contributors 1. Development Setup # Clone the repository git clone https://github.com/kunalkushwaha/agenticgokit.git cd agenticgokit # Install dependencies go mod tidy # Run tests to ensure everything works go test ./... # Install development tools go install github.com/golangci/golangci-lint/cmd/golangci-lint@latest\r2. Project Structure agenticgokit/\r├── cmd/ # CLI tools\r│ └── agentcli/ # AgentFlow CLI\r├── core/ # Public API (this is what users import)\r│ ├── agent.go # Agent interfaces\r│ ├── mcp.go # MCP integration public API\r│ ├── factory.go # Factory functions\r│ └── ...\r├── internal/ # Private implementation (not importable)\r│ ├── agents/ # Agent implementations\r│ ├── mcp/ # MCP client management\r│ ├── llm/ # LLM provider implementations\r│ ├── orchestrator/ # Workflow orchestration\r│ └── scaffold/ # CLI project generation\r├── examples/ # Example projects and demos\r├── docs/ # Documentation\r└── benchmarks/ # Performance benchmarks\r3. Core vs Internal Architecture core/ Package (Public API):",
    "tags": [],
    "title": "ContributorGuide",
    "uri": "/AgenticGoKitDocs/contributors/contributorguide/index.html"
  },
  {
    "breadcrumb": "content \u003e contributors",
    "content": "Code Style Guide This document defines the coding standards and conventions for AgenticGoKit to ensure consistency, readability, and maintainability across the codebase.\n🎯 Core Principles Clarity over Cleverness: Write code that is easy to understand Consistency: Follow established patterns throughout the codebase Simplicity: Prefer simple solutions over complex ones Performance: Be mindful of performance implications Documentation: Code should be self-documenting with helpful comments 🏗️ Go Language Standards Follow Standard Go Conventions AgenticGoKit adheres to all standard Go conventions:\nEffective Go Go Code Review Comments Go Best Practices Formatting and Tools Use the standard Go toolchain for consistent formatting:\n# Format code go fmt ./... # Run linter golangci-lint run # Check for unused code go mod tidy # Vet for common mistakes go vet ./...\rRequired Tools Configuration .golangci.yml run: timeout: 5m modules-download-mode: readonly linters-settings: gocyclo: min-complexity: 15 goconst: min-len: 3 min-occurrences: 3 goimports: local-prefixes: github.com/kunalkushwaha/agenticgokit govet: check-shadowing: true misspell: locale: US linters: enable: - gofmt - goimports - govet - gocyclo - goconst - misspell - ineffassign - staticcheck - unused - errcheck - gosimple - deadcode - varcheck - typecheck issues: exclude-rules: - path: _test\\.go linters: - gocyclo - errcheck - dupl - gosec\r📁 Package Organization Directory Structure agenticgokit/\r├── cmd/ # Main applications\r│ └── agentcli/ # CLI application\r├── core/ # Public API\r│ ├── agent.go # Core interfaces\r│ ├── runner.go # Public runner interface\r│ └── *.go # Other public APIs\r├── internal/ # Private implementation\r│ ├── agents/ # Agent implementations\r│ ├── mcp/ # MCP implementation\r│ ├── llm/ # LLM provider implementations\r│ └── */ # Other internal packages\r├── pkg/ # Utility packages (if needed)\r├── examples/ # Example applications\r├── docs/ # Documentation\r└── scripts/ # Build and development scripts\rPackage Naming Use lowercase, single-word package names Avoid underscores or mixed case Package names should be descriptive but concise Avoid generic names like util, common, base // Good package mcp package agents package llm // Bad package mcpUtils package agent_handlers package LLMProviders\rImport Organization Group imports in this order with blank lines between groups:\npackage core import ( // Standard library \"context\" \"fmt\" \"time\" // Third-party dependencies \"github.com/spf13/cobra\" \"go.opentelemetry.io/otel/trace\" // Local imports \"github.com/kunalkushwaha/agenticgokit/internal/mcp\" \"github.com/kunalkushwaha/agenticgokit/pkg/utils\" )\r🏷️ Naming Conventions Variables and Functions Use camelCase for variables and functions:\n// Good var agentCount int var lastExecutionTime time.Time func executeAgent() error func getAgentByName(name string) Agent // Bad var agent_count int var LastExecutionTime time.Time func execute_agent() error func GetAgentByName(name string) Agent\rConstants Use camelCase for unexported constants, PascalCase for exported:\n// Good const defaultTimeout = 30 * time.Second const MaxRetryAttempts = 3 // Bad const DEFAULT_TIMEOUT = 30 * time.Second const max_retry_attempts = 3\rTypes and Interfaces Use PascalCase for exported types, camelCase for unexported:\n// Good type Agent interface {} type AgentHandler interface {} type llmProvider struct {} // Bad type agent interface {} type agentHandler interface {} type LLMProvider struct {}\rInterface Naming Use noun or adjective forms Single-method interfaces often end with “-er” Avoid “I” prefix // Good type Runner interface {} type AgentHandler interface {} type Executor interface {} // Bad type IRunner interface {} type AgentHandlerInterface interface {} type ExecutorImpl interface {}\r📝 Documentation Standards Package Documentation Every package should have a package comment:\n// Package core provides the public API for AgenticGoKit. // It defines the primary interfaces and types used to build // AI agent systems with dynamic tool integration. // // The core package follows a clear separation between interfaces // (defined here) and implementations (in internal packages). // // Example usage: // //\tconfig := \u0026core.Config{...} //\trunner, err := core.NewRunner(config) //\tif err != nil { //\tlog.Fatal(err) //\t} //\t//\thandler := \u0026MyAgent{} //\trunner.RegisterAgent(\"my-agent\", handler) package core\rFunction Documentation Document all exported functions with their purpose, parameters, return values, and any side effects:\n// NewRunner creates a new agent runner with the provided configuration. // It initializes all configured LLM providers and MCP servers. // // The runner will not start processing events until Start() is called. // Configuration errors will be returned immediately, while connection // errors to external services may be retried automatically. // // Parameters: // - config: Configuration for the runner and its dependencies // // Returns: // - *Runner: Configured runner instance // - error: Configuration or initialization error func NewRunner(config *Config) (*Runner, error) { // Implementation... }\rType Documentation Document types, especially interfaces:\n// AgentHandler defines the interface for implementing agent logic. // // Implementations should be stateless and thread-safe, as the same // handler may be called concurrently by multiple goroutines. // // The Run method should process the input event and state, perform // any necessary operations (including calling tools or LLMs), and // return the result with any state changes. type AgentHandler interface { // Run processes an event and returns the result. // // The context may include deadlines, cancellation, and tracing // information. Implementations should respect context cancellation. // // The event contains the input data and metadata. The state // represents the current session state and may be modified. // // Returns AgentResult with response data and updated state, // or an error if processing fails. Run(ctx context.Context, event Event, state State) (AgentResult, error) }\rComment Guidelines Use complete sentences with proper capitalization and punctuation Explain “why” not just “what” Include examples for complex functionality Document any limitations or gotchas // validateConfig checks the configuration for common errors and // provides helpful suggestions for fixes. // // This validation is performed at startup to catch configuration // issues early, before attempting to connect to external services. // Some validations (like network connectivity) are performed lazily. func validateConfig(config *Config) error { // Check required fields first to provide clear error messages if config.LLM.Provider == \"\" { return fmt.Errorf(\"llm.provider is required\") } // Validate provider-specific configuration switch config.LLM.Provider { case \"azure\": return validateAzureConfig(\u0026config.LLM.Azure) case \"openai\": return validateOpenAIConfig(\u0026config.LLM.OpenAI) default: return fmt.Errorf(\"unsupported llm provider: %s\", config.LLM.Provider) } }\r🔧 Error Handling Error Types Define specific error types for different categories:\n// ValidationError represents a configuration or input validation error type ValidationError struct { Field string Value interface{} Message string } func (e *ValidationError) Error() string { return fmt.Sprintf(\"validation failed for field %s: %s\", e.Field, e.Message) } // TimeoutError represents an operation timeout type TimeoutError struct { Operation string Duration time.Duration } func (e *TimeoutError) Error() string { return fmt.Sprintf(\"operation %s timed out after %v\", e.Operation, e.Duration) }\rError Wrapping Use error wrapping to provide context:\nfunc executeAgent(ctx context.Context, agent AgentHandler, event Event, state State) (AgentResult, error) { result, err := agent.Run(ctx, event, state) if err != nil { return AgentResult{}, fmt.Errorf(\"failed to execute agent %s: %w\", agent.Name(), err) } return result, nil }\rError Messages Start with lowercase letter (Go convention) Be specific and actionable Include relevant context Avoid implementation details in user-facing errors // Good return fmt.Errorf(\"failed to connect to MCP server %s: %w\", serverName, err) return ValidationError{Field: \"timeout\", Message: \"must be positive\"} // Bad return fmt.Errorf(\"Connection failed\") return fmt.Errorf(\"Error in line 42 of mcp.go\")\r🧪 Testing Standards Test File Organization Test files should be in the same package as the code they test Use _test.go suffix Group related tests in the same file // agent_test.go package core import ( \"context\" \"testing\" \"time\" \"github.com/stretchr/testify/assert\" \"github.com/stretchr/testify/require\" )\rTest Function Naming Use descriptive test names that explain the scenario:\n// Good func TestAgent_Run_WithValidInput_ReturnsSuccess(t *testing.T) {} func TestRunner_RegisterAgent_WithNilHandler_ReturnsError(t *testing.T) {} func TestMCPManager_ExecuteTool_WhenServerUnavailable_RetriesAndFails(t *testing.T) {} // Bad func TestAgent(t *testing.T) {} func TestRunner1(t *testing.T) {} func TestError(t *testing.T) {}\rTest Structure Use the Arrange-Act-Assert pattern:\nfunc TestAgent_Run_WithValidInput_ReturnsSuccess(t *testing.T) { // Arrange agent := \u0026TestAgent{name: \"test-agent\"} event := NewEvent(\"test\", map[string]interface{}{ \"query\": \"Hello world\", }) state := NewState() ctx := context.Background() // Act result, err := agent.Run(ctx, event, state) // Assert require.NoError(t, err) assert.True(t, result.Success) assert.Equal(t, \"Hello world\", result.Data[\"processed_query\"]) }\rTable-Driven Tests Use table-driven tests for multiple scenarios:\nfunc TestValidateConfig(t *testing.T) { tests := []struct { name string config Config wantErr bool errMsg string }{ { name: \"valid azure config\", config: Config{ LLM: LLMConfig{ Provider: \"azure\", Azure: AzureConfig{ Endpoint: \"https://test.openai.azure.com\", APIKey: \"test-key\", }, }, }, wantErr: false, }, { name: \"missing provider\", config: Config{ LLM: LLMConfig{}, }, wantErr: true, errMsg: \"llm.provider is required\", }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { err := validateConfig(\u0026tt.config) if tt.wantErr { require.Error(t, err) assert.Contains(t, err.Error(), tt.errMsg) } else { require.NoError(t, err) } }) } }\r🚀 Performance Guidelines Memory Allocation Minimize allocations in hot paths:\n// Good - reuse slice capacity func processEvents(events []Event) []Result { results := make([]Result, 0, len(events)) for _, event := range events { result := processEvent(event) results = append(results, result) } return results } // Bad - repeated allocations func processEvents(events []Event) []Result { var results []Result for _, event := range events { result := processEvent(event) results = append(results, result) } return results }\rString Building Use strings.Builder for efficient string concatenation:\n// Good func buildPrompt(parts []string) string { var builder strings.Builder builder.Grow(len(parts) * 50) // Pre-allocate if size is known for i, part := range parts { if i \u003e 0 { builder.WriteString(\"\\n\") } builder.WriteString(part) } return builder.String() } // Bad func buildPrompt(parts []string) string { result := \"\" for i, part := range parts { if i \u003e 0 { result += \"\\n\" } result += part } return result }\rContext Usage Always pass context and respect cancellation:\nfunc processWithTimeout(ctx context.Context, data []byte) error { // Create timeout context timeoutCtx, cancel := context.WithTimeout(ctx, 30*time.Second) defer cancel() // Check for cancellation in loops for i, item := range data { select { case \u003c-timeoutCtx.Done(): return timeoutCtx.Err() default: } if err := processItem(timeoutCtx, item); err != nil { return fmt.Errorf(\"failed to process item %d: %w\", i, err) } } return nil }\r🔒 Security Guidelines Input Validation Validate all external inputs:\nfunc processUserQuery(query string) error { // Validate input length if len(query) \u003e maxQueryLength { return ValidationError{ Field: \"query\", Message: fmt.Sprintf(\"exceeds maximum length of %d characters\", maxQueryLength), } } // Check for malicious content if containsSQLInjection(query) { return ValidationError{ Field: \"query\", Message: \"contains potentially malicious content\", } } return nil }\rSecrets Handling Never log or expose secrets:\n// Good func logConfig(config *Config) { log.Printf(\"LLM Provider: %s\", config.LLM.Provider) log.Printf(\"Endpoint: %s\", config.LLM.Azure.Endpoint) // Don't log API key } // Bad func logConfig(config *Config) { log.Printf(\"Config: %+v\", config) // This might expose secrets }\rResource Limits Implement appropriate limits:\nconst ( maxConcurrentRequests = 100 maxRequestSize = 10 * 1024 * 1024 // 10MB maxExecutionTime = 5 * time.Minute ) func processRequest(ctx context.Context, request Request) error { // Check size limits if len(request.Data) \u003e maxRequestSize { return fmt.Errorf(\"request too large: %d bytes\", len(request.Data)) } // Set timeout timeoutCtx, cancel := context.WithTimeout(ctx, maxExecutionTime) defer cancel() return doProcessRequest(timeoutCtx, request) }\r📋 Code Review Checklist Before Submitting Code follows Go formatting standards (go fmt) All linters pass (golangci-lint run) Tests are written and passing Documentation is updated Error handling is appropriate Performance implications considered Security implications considered During Review Code is readable and well-structured Variable and function names are clear Comments explain complex logic Error messages are helpful Tests cover edge cases No obvious performance issues Follows established patterns This code style guide ensures consistency and quality across the AgenticGoKit codebase, making it easier for contributors to understand, maintain, and extend the system.",
    "description": "Code Style Guide This document defines the coding standards and conventions for AgenticGoKit to ensure consistency, readability, and maintainability across the codebase.\n🎯 Core Principles Clarity over Cleverness: Write code that is easy to understand Consistency: Follow established patterns throughout the codebase Simplicity: Prefer simple solutions over complex ones Performance: Be mindful of performance implications Documentation: Code should be self-documenting with helpful comments 🏗️ Go Language Standards Follow Standard Go Conventions AgenticGoKit adheres to all standard Go conventions:",
    "tags": [],
    "title": "CodeStyle",
    "uri": "/AgenticGoKitDocs/contributors/codestyle/index.html"
  },
  {
    "breadcrumb": "content \u003e contributors",
    "content": "Core vs Internal Architecture Understanding AgenticGoKit’s Package Structure\nAgenticGoKit uses a clear separation between public API (core/) and private implementation (internal/) to provide a stable, developer-friendly interface while maintaining implementation flexibility.\nOverview agenticgokit/\r├── core/ # Public API - what users import\r│ ├── agent.go # Agent interfaces and types\r│ ├── mcp.go # MCP integration public API │ ├── factory.go # Factory functions for creating components\r│ ├── llm.go # LLM provider interfaces\r│ └── ... # Other public interfaces\r└── internal/ # Private implementation - not importable\r├── agents/ # Concrete agent implementations\r├── mcp/ # MCP client and server management\r├── llm/ # LLM provider implementations\r├── orchestrator/ # Workflow orchestration logic\r└── ... # Other implementation packages\rDesign Principles 1. Interface Segregation Public interfaces are defined in core/:\n// core/agent.go type AgentHandler interface { Run(ctx context.Context, event Event, state State) (AgentResult, error) } type ModelProvider interface { Generate(ctx context.Context, prompt string) (string, error) GenerateWithHistory(ctx context.Context, messages []Message) (string, error) Name() string } type MCPManager interface { ListTools(ctx context.Context) ([]ToolSchema, error) CallTool(ctx context.Context, name string, args map[string]interface{}) (interface{}, error) // ... more methods }\rImplementations are in internal/:\n// internal/agents/mcp_agent.go type mcpAgent struct { name string llm llm.Provider // internal interface mcpManager mcp.Manager // internal interface } func (a *mcpAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Implementation details hidden from users }\r2. Factory Pattern Factories in core/ create internal implementations:\n// core/factory.go func NewMCPAgent(name string, llm ModelProvider, mcp MCPManager) AgentHandler { // Create internal implementation return agents.NewMCPAgent(name, llm, mcp) } func InitializeProductionMCP(ctx context.Context, config MCPConfig) (MCPManager, error) { // Create internal MCP manager return mcp.NewProductionManager(ctx, config) }\rThis pattern allows users to work with interfaces while we manage complex implementations internally.\nCore Package Structure agent.go - Agent System // Primary interfaces for agent development type AgentHandler interface { Run(ctx context.Context, event Event, state State) (AgentResult, error) } type Agent interface { Run(ctx context.Context, inputState State) (State, error) Name() string } // Supporting types type Event interface { GetID() string GetData() EventData GetMetadata() map[string]string // ... } type State interface { Get(key string) (any, bool) Set(key string, value any) Clone() State // ... }\rmcp.go - MCP Integration // Complete MCP public API type MCPManager interface { // Server management Connect(ctx context.Context, serverName string) error Disconnect(serverName string) error // Tool discovery and execution ListTools(ctx context.Context) ([]ToolSchema, error) CallTool(ctx context.Context, name string, args map[string]interface{}) (interface{}, error) // Cache and configuration RefreshTools(ctx context.Context) error GetCacheStats() CacheStats } // Configuration types type MCPConfig struct { Servers map[string]MCPServerConfig CacheEnabled bool CacheTTL time.Duration ConnectionTimeout time.Duration MaxRetries int } // Helper functions for agent development func FormatToolsForPrompt(ctx context.Context, mgr MCPManager) string func ParseAndExecuteToolCalls(ctx context.Context, mgr MCPManager, response string) []ToolResult\rllm.go - LLM Providers // Unified LLM interface type ModelProvider interface { Generate(ctx context.Context, prompt string) (string, error) GenerateWithHistory(ctx context.Context, messages []Message) (string, error) Name() string } // Provider-specific configuration types type AzureConfig struct { APIKey string Endpoint string Deployment string APIVersion string MaxTokens int Temperature float64 } type OpenAIConfig struct { APIKey string Model string MaxTokens int Temperature float64 } // And so on for other providers...\rfactory.go - Creation Functions // Agent factories func NewMCPAgent(name string, llm ModelProvider, mcp MCPManager) AgentHandler func NewBasicAgent(name string, llm ModelProvider) AgentHandler // Provider factories func NewAzureProvider(config AzureConfig) (ModelProvider, error) func NewOpenAIProvider(config OpenAIConfig) (ModelProvider, error) func NewOllamaProvider(config OllamaConfig) (ModelProvider, error) // Configuration-driven factories func NewProviderFromWorkingDir() (ModelProvider, error) func NewProviderFromConfig(configPath string) (ModelProvider, error) // MCP factories func InitializeProductionMCP(ctx context.Context, config MCPConfig) (MCPManager, error) func QuickStartMCP() (MCPManager, error)\rInternal Package Structure internal/agents/ - Agent Implementations // internal/agents/mcp_agent.go package agents type mcpAgent struct { name string llm llm.Provider // Internal LLM interface mcpManager mcp.Manager // Internal MCP interface logger zerolog.Logger } func NewMCPAgent(name string, llm llm.Provider, mcp mcp.Manager) core.AgentHandler { return \u0026mcpAgent{ name: name, llm: llm, mcpManager: mcp, logger: log.With().Str(\"agent\", name).Logger(), } } func (a *mcpAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Complex implementation details // Error handling, logging, tool execution, etc. }\rinternal/mcp/ - MCP Implementation // internal/mcp/manager.go package mcp type Manager struct { clients map[string]*client.MCPClient config core.MCPConfig cache *toolCache metrics *mcpMetrics logger zerolog.Logger } func NewProductionManager(ctx context.Context, config core.MCPConfig) core.MCPManager { mgr := \u0026Manager{ clients: make(map[string]*client.MCPClient), config: config, cache: newToolCache(config.CacheTTL), metrics: newMCPMetrics(), logger: log.With().Str(\"component\", \"mcp-manager\").Logger(), } // Initialize servers, set up monitoring, etc. return mgr } func (m *Manager) ListTools(ctx context.Context) ([]core.ToolSchema, error) { // Implementation with caching, error handling, metrics }\rinternal/llm/ - LLM Implementations // internal/llm/azure.go package llm type azureProvider struct { client *openai.Client config core.AzureConfig metrics *llmMetrics rateLimiter *rate.Limiter } func NewAzureProvider(config core.AzureConfig) core.ModelProvider { return \u0026azureProvider{ client: createAzureClient(config), config: config, metrics: newLLMMetrics(\"azure\"), rateLimiter: rate.NewLimiter(rate.Limit(config.RequestsPerSecond), 1), } } func (p *azureProvider) Generate(ctx context.Context, prompt string) (string, error) { // Rate limiting, retries, error handling, metrics }\rBenefits of This Architecture 1. Stable Public API Users import only from core/:\nimport agentflow \"github.com/kunalkushwaha/agenticgokit/core\" // All user code works with interfaces var agent agentflow.AgentHandler = agentflow.NewMCPAgent(\"my-agent\", llm, mcp) var provider agentflow.ModelProvider = agentflow.NewAzureProvider(config)\rWe can refactor internal/ without breaking user code.\n2. Implementation Flexibility We can:\nOptimize internal algorithms Change data structures Add new features to implementations Fix bugs in internal logic Without affecting users.\n3. Testing Boundaries Public API Testing (what users care about):\nfunc TestMCPAgent_PublicBehavior(t *testing.T) { // Test through public interfaces only agent := core.NewMCPAgent(\"test\", mockLLM, mockMCP) result, err := agent.Run(ctx, event, state) // Assert on public behavior assert.NoError(t, err) assert.NotEmpty(t, result.Result) }\rInternal Testing (implementation details):\nfunc TestMCPManager_CacheLogic(t *testing.T) { // Test internal implementation details mgr := mcp.NewManager(config) // Test caching behavior, error handling, etc. }\r4. Clear Documentation Focus User Documentation: Focus on core/ package Developer Documentation: Cover internal/ architecture\nGuidelines for Development When to Add to core/ Add to core/ when:\nUsers need to interact with the functionality It’s part of the public contract It needs to be stable across versions It’s a configuration type or interface // Good: User-facing interface type AgentHandler interface { Run(ctx context.Context, event Event, state State) (AgentResult, error) } // Good: Configuration type type MCPConfig struct { Servers map[string]MCPServerConfig CacheEnabled bool CacheTTL time.Duration } // Good: Factory function func NewMCPAgent(name string, llm ModelProvider, mcp MCPManager) AgentHandler\rWhen to Keep in internal/ Keep in internal/ when:\nIt’s implementation detail It might change frequently Users don’t need direct access It’s complex business logic // Good: Implementation detail type mcpClient struct { conn net.Conn encoder *json.Encoder decoder *json.Decoder msgID int64 pending map[int64]chan\u003c- mcpResponse } // Good: Internal algorithm func (c *mcpClient) sendRequest(req mcpRequest) (mcpResponse, error) { // Complex protocol handling }\rInterface Design Patterns Do:\n// Small, focused interfaces type ToolExecutor interface { ExecuteTool(ctx context.Context, name string, args map[string]interface{}) (interface{}, error) } // Composition of interfaces type MCPManager interface { ToolDiscoverer ToolExecutor ConnectionManager }\rDon’t:\n// Large, monolithic interfaces type EverythingManager interface { // 20+ methods that do different things }\rMigration and Versioning Adding New Features Add interface to core/ Implement in internal/ Add factory in core/ Maintain backward compatibility // core/agent.go - Add new interface type StreamingAgent interface { AgentHandler RunStreaming(ctx context.Context, event Event, state State) (\u003c-chan AgentResult, error) } // internal/agents/streaming_agent.go - Implement type streamingAgent struct { // Implementation } // core/factory.go - Add factory func NewStreamingAgent(name string, llm ModelProvider) StreamingAgent { return agents.NewStreamingAgent(name, llm) }\rDeprecating Features Mark as deprecated in core/ Keep implementation working Provide migration path Remove in next major version // Deprecated: Use NewMCPAgent instead. func NewBasicAgent(name string) AgentHandler { return NewMCPAgent(name, nil, nil) }\rPerformance Considerations Interface Overhead Go interfaces have minimal overhead:\nMethod calls through interfaces are fast Interface conversions are optimized The separation doesn’t impact performance Memory Management Interfaces don’t increase memory usage significantly Internal implementations can be optimized independently Factory functions don’t add overhead Compilation Benefits Users only compile against core/ interfaces internal/ changes don’t trigger user recompilation Faster development iteration Next Steps Contributor Guide - Get started with development Adding Features - Learn how to extend AgenticGoKit Testing Strategy - Understand our testing approach",
    "description": "Core vs Internal Architecture Understanding AgenticGoKit’s Package Structure\nAgenticGoKit uses a clear separation between public API (core/) and private implementation (internal/) to provide a stable, developer-friendly interface while maintaining implementation flexibility.\nOverview agenticgokit/\r├── core/ # Public API - what users import\r│ ├── agent.go # Agent interfaces and types\r│ ├── mcp.go # MCP integration public API │ ├── factory.go # Factory functions for creating components\r│ ├── llm.go # LLM provider interfaces\r│ └── ... # Other public interfaces\r└── internal/ # Private implementation - not importable\r├── agents/ # Concrete agent implementations\r├── mcp/ # MCP client and server management\r├── llm/ # LLM provider implementations\r├── orchestrator/ # Workflow orchestration logic\r└── ... # Other implementation packages\rDesign Principles 1. Interface Segregation Public interfaces are defined in core/:",
    "tags": [],
    "title": "CoreVsInternal",
    "uri": "/AgenticGoKitDocs/contributors/corevsinternal/index.html"
  },
  {
    "breadcrumb": "content \u003e contributors",
    "content": "Adding Features to AgenticGoKit This guide walks through the process of adding new features to AgenticGoKit, from design to implementation to testing and documentation.\n🎯 Feature Development Philosophy AgenticGoKit follows these principles for feature development:\nUser-Centric: Features should solve real user problems API-First: Design public APIs before implementation Backward Compatibility: Maintain compatibility when possible Performance-Aware: Consider performance implications Test-Driven: Write tests alongside code Documentation-Complete: Include comprehensive documentation 📋 Feature Development Process 1. Feature Proposal Phase Create Feature Request Start with a GitHub issue using the feature request template:\n## Feature Request: Advanced Agent Chaining ### Problem Statement Users need to chain multiple agents in complex workflows where the output of one agent becomes the input of the next, with conditional branching and error handling. ### Proposed Solution Implement an `AgentChain` component that allows: - Sequential agent execution - Conditional branching based on agent results - Error handling and fallback strategies - State passing between agents ### Use Cases 1. Research workflow: Search → Analyze → Summarize 2. Content workflow: Generate → Review → Publish 3. Data workflow: Extract → Transform → Load ### Success Criteria - Agents can be chained declaratively - Conditional logic can be expressed clearly - Error handling is robust - Performance is comparable to manual orchestration\rInitial Design Discussion Post in GitHub Discussions for community feedback Discuss with core maintainers Consider alternatives and trade-offs Define scope and non-goals 2. Design Phase API Design Document Create a detailed design document:\n# Agent Chaining Feature Design ## Public API ### AgentChain Interface ```go type AgentChain interface { // AddStep adds an agent to the chain AddStep(step ChainStep) AgentChain // AddConditionalStep adds a conditional step AddConditionalStep(condition Condition, step ChainStep) AgentChain // Execute runs the entire chain Execute(ctx context.Context, input ChainInput) (ChainResult, error) // SetErrorHandler sets global error handling strategy SetErrorHandler(handler ErrorHandler) AgentChain } type ChainStep struct { Name string Agent AgentHandler InputMapper InputMapper Condition Condition OnError ErrorAction } type ChainInput struct { Event Event State State } type ChainResult struct { Steps []StepResult FinalState State Success bool }\rCore Implementation Structure core/agent_chain.go - Public interface internal/chain/ - Implementation details internal/chain/executor.go - Chain execution logic internal/chain/condition.go - Conditional logic internal/chain/mapper.go - Input/output mapping Configuration Integration [agent_chain] max_steps = 50 step_timeout = \"30s\" enable_parallel_execution = true\r### 3. Implementation Phase\r#### Step 1: Create Public Interface\rStart with the public API in `core/`:\r```go\r// core/agent_chain.go\rpackage core\rimport (\r\"context\"\r\"time\"\r)\r// AgentChain defines the interface for chaining multiple agents\rtype AgentChain interface {\rAddStep(step ChainStep) AgentChain\rAddConditionalStep(condition Condition, step ChainStep) AgentChain\rExecute(ctx context.Context, input ChainInput) (ChainResult, error)\rSetErrorHandler(handler ErrorHandler) AgentChain\r}\r// ChainStep represents a single step in an agent chain\rtype ChainStep struct {\rName string\rAgent AgentHandler\rInputMapper InputMapper\rCondition Condition\rOnError ErrorAction\rTimeout time.Duration\r}\r// NewAgentChain creates a new agent chain\rfunc NewAgentChain(name string) AgentChain {\rreturn internal.NewAgentChain(name)\r}\rStep 2: Implement Core Logic Create the implementation in internal/:\n// internal/chain/agent_chain.go package chain import ( \"context\" \"fmt\" \"sync\" \"time\" \"github.com/kunalkushwaha/agenticgokit/core\" ) type agentChain struct { name string steps []core.ChainStep errorHandler core.ErrorHandler config ChainConfig mu sync.RWMutex } type ChainConfig struct { MaxSteps int StepTimeout time.Duration EnableParallelExecution bool } func NewAgentChain(name string) core.AgentChain { return \u0026agentChain{ name: name, steps: make([]core.ChainStep, 0), config: DefaultChainConfig(), } } func (c *agentChain) AddStep(step core.ChainStep) core.AgentChain { c.mu.Lock() defer c.mu.Unlock() c.steps = append(c.steps, step) return c } func (c *agentChain) Execute(ctx context.Context, input core.ChainInput) (core.ChainResult, error) { executor := NewChainExecutor(c.config) return executor.Execute(ctx, c.steps, input, c.errorHandler) }\rStep 3: Add Execution Logic // internal/chain/executor.go package chain import ( \"context\" \"fmt\" \"time\" \"github.com/kunalkushwaha/agenticgokit/core\" ) type ChainExecutor struct { config ChainConfig } func NewChainExecutor(config ChainConfig) *ChainExecutor { return \u0026ChainExecutor{config: config} } func (e *ChainExecutor) Execute(ctx context.Context, steps []core.ChainStep, input core.ChainInput, errorHandler core.ErrorHandler) (core.ChainResult, error) { result := core.ChainResult{ Steps: make([]core.StepResult, 0, len(steps)), FinalState: input.State.Clone(), Success: true, } currentState := input.State.Clone() for i, step := range steps { // Check context cancellation select { case \u003c-ctx.Done(): return result, ctx.Err() default: } // Evaluate condition if present if step.Condition != nil \u0026\u0026 !step.Condition.Evaluate(currentState) { continue } // Execute step with timeout stepCtx, cancel := context.WithTimeout(ctx, e.getStepTimeout(step)) stepResult, err := e.executeStep(stepCtx, step, input.Event, currentState) cancel() // Handle step result result.Steps = append(result.Steps, stepResult) if err != nil { if errorHandler != nil { action := errorHandler.HandleError(err, step, currentState) if action == core.ErrorActionStop { result.Success = false return result, fmt.Errorf(\"chain stopped at step %d: %w\", i, err) } // Continue with ErrorActionContinue } else { result.Success = false return result, fmt.Errorf(\"step %d failed: %w\", i, err) } } else { // Update state with step result if stepResult.State != nil { currentState = stepResult.State } } } result.FinalState = currentState return result, nil } func (e *ChainExecutor) executeStep(ctx context.Context, step core.ChainStep, event core.Event, state core.State) (core.StepResult, error) { // Map input if mapper is provided stepEvent := event stepState := state if step.InputMapper != nil { var err error stepEvent, stepState, err = step.InputMapper.Map(event, state) if err != nil { return core.StepResult{}, fmt.Errorf(\"input mapping failed: %w\", err) } } // Execute agent agentResult, err := step.Agent.Run(ctx, stepEvent, stepState) if err != nil { return core.StepResult{ StepName: step.Name, Success: false, Error: err, }, err } return core.StepResult{ StepName: step.Name, Success: true, Data: agentResult.Data, State: agentResult.State, }, nil }\rStep 4: Add Builder Pattern Support // core/agent_chain_builder.go package core // AgentChainBuilder provides a fluent interface for building agent chains type AgentChainBuilder struct { chain AgentChain } // NewAgentChainBuilder creates a new builder func NewAgentChainBuilder(name string) *AgentChainBuilder { return \u0026AgentChainBuilder{ chain: NewAgentChain(name), } } // Step adds a simple step to the chain func (b *AgentChainBuilder) Step(name string, agent AgentHandler) *AgentChainBuilder { b.chain.AddStep(ChainStep{ Name: name, Agent: agent, }) return b } // ConditionalStep adds a conditional step func (b *AgentChainBuilder) ConditionalStep(name string, agent AgentHandler, condition Condition) *AgentChainBuilder { b.chain.AddConditionalStep(condition, ChainStep{ Name: name, Agent: agent, }) return b } // WithErrorHandler sets the error handling strategy func (b *AgentChainBuilder) WithErrorHandler(handler ErrorHandler) *AgentChainBuilder { b.chain.SetErrorHandler(handler) return b } // Build returns the configured chain func (b *AgentChainBuilder) Build() AgentChain { return b.chain }\r4. Testing Phase Unit Tests // core/agent_chain_test.go package core import ( \"context\" \"testing\" \"time\" \"github.com/stretchr/testify/assert\" \"github.com/stretchr/testify/require\" ) func TestAgentChain_Execute(t *testing.T) { tests := []struct { name string steps []ChainStep input ChainInput expected ChainResult wantErr bool }{ { name: \"Simple two-step chain\", steps: []ChainStep{ { Name: \"step1\", Agent: \u0026mockAgent{response: \"result1\"}, }, { Name: \"step2\", Agent: \u0026mockAgent{response: \"result2\"}, }, }, input: ChainInput{ Event: NewEvent(\"test\", map[string]interface{}{\"query\": \"test\"}), State: NewState(), }, expected: ChainResult{ Success: true, }, wantErr: false, }, { name: \"Chain with error in middle step\", steps: []ChainStep{ { Name: \"step1\", Agent: \u0026mockAgent{response: \"result1\"}, }, { Name: \"step2\", Agent: \u0026mockAgent{err: fmt.Errorf(\"step error\")}, }, }, input: ChainInput{ Event: NewEvent(\"test\", map[string]interface{}{\"query\": \"test\"}), State: NewState(), }, wantErr: true, }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { chain := NewAgentChain(\"test-chain\") for _, step := range tt.steps { chain.AddStep(step) } result, err := chain.Execute(context.Background(), tt.input) if tt.wantErr { assert.Error(t, err) return } require.NoError(t, err) assert.Equal(t, tt.expected.Success, result.Success) }) } } func TestAgentChainBuilder(t *testing.T) { agent1 := \u0026mockAgent{response: \"result1\"} agent2 := \u0026mockAgent{response: \"result2\"} chain := NewAgentChainBuilder(\"test-chain\"). Step(\"step1\", agent1). Step(\"step2\", agent2). WithErrorHandler(\u0026mockErrorHandler{}). Build() assert.NotNil(t, chain) input := ChainInput{ Event: NewEvent(\"test\", map[string]interface{}{\"query\": \"test\"}), State: NewState(), } result, err := chain.Execute(context.Background(), input) require.NoError(t, err) assert.True(t, result.Success) assert.Len(t, result.Steps, 2) } // Mock implementations for testing type mockAgent struct { response string err error } func (m *mockAgent) Run(ctx context.Context, event Event, state State) (AgentResult, error) { if m.err != nil { return AgentResult{}, m.err } return AgentResult{ Data: map[string]interface{}{ \"result\": m.response, }, Success: true, State: state, }, nil }\rIntegration Tests // integration/agent_chain_integration_test.go //go:build integration // +build integration package integration import ( \"context\" \"testing\" \"github.com/stretchr/testify/assert\" \"github.com/stretchr/testify/require\" \"github.com/kunalkushwaha/agenticgokit/core\" ) func TestAgentChain_RealWorkflow(t *testing.T) { // Setup real agents searchAgent := \u0026SearchAgent{ provider: newTestSearchProvider(), } analysisAgent := \u0026AnalysisAgent{ llm: newTestLLMProvider(), } summaryAgent := \u0026SummaryAgent{ llm: newTestLLMProvider(), } // Create chain chain := core.NewAgentChainBuilder(\"research-workflow\"). Step(\"search\", searchAgent). Step(\"analyze\", analysisAgent). Step(\"summarize\", summaryAgent). Build() // Execute chain input := core.ChainInput{ Event: core.NewEvent(\"research\", map[string]interface{}{ \"topic\": \"artificial intelligence trends 2024\", }), State: core.NewState(), } result, err := chain.Execute(context.Background(), input) require.NoError(t, err) assert.True(t, result.Success) assert.Len(t, result.Steps, 3) assert.Contains(t, result.FinalState.GetString(\"summary\"), \"artificial intelligence\") }\r5. Documentation Phase API Documentation # Agent Chaining Agent chaining allows you to create sophisticated workflows by connecting multiple agents in sequence, with conditional logic and error handling. ## Basic Usage ```go import \"github.com/kunalkushwaha/agenticgokit/core\" // Create agents searchAgent := \u0026SearchAgent{} analysisAgent := \u0026AnalysisAgent{} summaryAgent := \u0026SummaryAgent{} // Build chain chain := core.NewAgentChainBuilder(\"research-workflow\"). Step(\"search\", searchAgent). Step(\"analyze\", analysisAgent). Step(\"summarize\", summaryAgent). Build() // Execute chain input := core.ChainInput{ Event: core.NewEvent(\"research\", map[string]interface{}{ \"topic\": \"AI trends\", }), State: core.NewState(), } result, err := chain.Execute(context.Background(), input) if err != nil { log.Fatal(err) } fmt.Printf(\"Final result: %v\\n\", result.FinalState)\rAdvanced Features Conditional Steps chain := core.NewAgentChainBuilder(\"conditional-workflow\"). Step(\"initial\", initialAgent). ConditionalStep(\"optional\", optionalAgent, core.StateCondition(\"need_extra_processing\", true)). Step(\"final\", finalAgent). Build()\rError Handling errorHandler := \u0026core.ContinueOnErrorHandler{ MaxErrors: 2, LogErrors: true, } chain := core.NewAgentChainBuilder(\"resilient-workflow\"). Step(\"step1\", agent1). Step(\"step2\", agent2). WithErrorHandler(errorHandler). Build()\r#### User Guide Update\rAdd section to `docs/guides/AgentBasics.md`:\r```markdown\r## Agent Chaining\rFor complex workflows requiring multiple agents, use AgentChain:\r### Creating a Chain\r```go\rchain := core.NewAgentChainBuilder(\"my-workflow\").\rStep(\"search\", searchAgent).\rStep(\"analyze\", analysisAgent).\rStep(\"summarize\", summaryAgent).\rBuild()\rExecuting the Chain result, err := chain.Execute(ctx, core.ChainInput{ Event: event, State: state, })\rThe chain will execute each step in sequence, passing state between steps.\n### 6. Configuration Integration\r#### Add Configuration Support\r```go\r// core/config.go - Add to existing config\rtype Config struct {\r// ... existing fields ...\rAgentChain AgentChainConfig `toml:\"agent_chain\"`\r}\rtype AgentChainConfig struct {\rMaxSteps int `toml:\"max_steps\"`\rStepTimeout time.Duration `toml:\"step_timeout\"`\rEnableParallelSteps bool `toml:\"enable_parallel_steps\"`\rDefaultErrorStrategy string `toml:\"default_error_strategy\"`\r}\rfunc (c AgentChainConfig) Validate() error {\rif c.MaxSteps \u003c 1 {\rreturn fmt.Errorf(\"max_steps must be at least 1\")\r}\rif c.StepTimeout \u003c time.Second {\rreturn fmt.Errorf(\"step_timeout must be at least 1 second\")\r}\rreturn nil\r}\rUpdate Default Configuration # Default agentflow.toml additions [agent_chain] max_steps = 50 step_timeout = \"30s\" enable_parallel_steps = false default_error_strategy = \"stop\"\r7. CLI Integration Add CLI Commands // cmd/agentcli/cmd/chain.go package cmd import ( \"github.com/spf13/cobra\" ) var chainCmd = \u0026cobra.Command{ Use: \"chain\", Short: \"Manage agent chains\", Long: \"Commands for creating and managing agent chains\", } var chainCreateCmd = \u0026cobra.Command{ Use: \"create \u003cname\u003e\", Short: \"Create a new agent chain\", Args: cobra.ExactArgs(1), RunE: runChainCreate, } var chainRunCmd = \u0026cobra.Command{ Use: \"run \u003cchain-name\u003e\", Short: \"Execute an agent chain\", Args: cobra.ExactArgs(1), RunE: runChainRun, } func init() { chainCmd.AddCommand(chainCreateCmd) chainCmd.AddCommand(chainRunCmd) rootCmd.AddCommand(chainCmd) }\r🔄 Feature Integration Checklist Pre-Implementation Feature request created and discussed Design document written and reviewed API design approved by maintainers Breaking changes identified and documented Implementation Public API implemented in core/ Implementation details in internal/ Configuration integration added Error handling implemented Performance considerations addressed Testing Unit tests written and passing Integration tests written and passing Benchmarks created for performance-critical paths Mock implementations created for testing Documentation API documentation written User guide updated Examples created and tested Migration guide written (if breaking changes) CLI Integration CLI commands added (if applicable) Help text and examples provided Shell completion updated Quality Assurance Code review completed Security review completed (if applicable) Performance testing completed Manual testing completed Release Preparation CHANGELOG.md updated Version compatibility documented Release notes drafted Deprecation notices added (if applicable) 📚 Feature Examples Simple Feature: Add Timeout to Agent Execution // 1. Extend AgentHandler interface type AgentHandler interface { Run(ctx context.Context, event Event, state State) (AgentResult, error) GetTimeout() time.Duration // New method } // 2. Update Runner to use timeout func (r *Runner) executeAgent(ctx context.Context, agent AgentHandler, event Event, state State) (AgentResult, error) { timeout := agent.GetTimeout() if timeout \u003e 0 { var cancel context.CancelFunc ctx, cancel = context.WithTimeout(ctx, timeout) defer cancel() } return agent.Run(ctx, event, state) } // 3. Provide default implementation type BaseAgent struct { timeout time.Duration } func (a *BaseAgent) GetTimeout() time.Duration { if a.timeout \u003e 0 { return a.timeout } return 30 * time.Second // default }\rComplex Feature: Agent Middleware System // 1. Define middleware interface type Middleware interface { Handle(next AgentHandler) AgentHandler } // 2. Create middleware chain type MiddlewareChain struct { middlewares []Middleware } func (c *MiddlewareChain) Then(handler AgentHandler) AgentHandler { for i := len(c.middlewares) - 1; i \u003e= 0; i-- { handler = c.middlewares[i].Handle(handler) } return handler } // 3. Integrate with Runner func (r *Runner) RegisterAgentWithMiddleware(name string, handler AgentHandler, middlewares ...Middleware) { chain := \u0026MiddlewareChain{middlewares: middlewares} wrappedHandler := chain.Then(handler) r.RegisterAgent(name, wrappedHandler) }\rThis comprehensive guide provides the framework for adding any feature to AgenticGoKit while maintaining code quality, performance, and user experience standards.",
    "description": "Adding Features to AgenticGoKit This guide walks through the process of adding new features to AgenticGoKit, from design to implementation to testing and documentation.\n🎯 Feature Development Philosophy AgenticGoKit follows these principles for feature development:\nUser-Centric: Features should solve real user problems API-First: Design public APIs before implementation Backward Compatibility: Maintain compatibility when possible Performance-Aware: Consider performance implications Test-Driven: Write tests alongside code Documentation-Complete: Include comprehensive documentation 📋 Feature Development Process 1. Feature Proposal Phase Create Feature Request Start with a GitHub issue using the feature request template:",
    "tags": [],
    "title": "AddingFeatures",
    "uri": "/AgenticGoKitDocs/contributors/addingfeatures/index.html"
  },
  {
    "breadcrumb": "content \u003e contributors",
    "content": "Testing Strategy This document outlines the comprehensive testing approach for AgenticGoKit, covering unit tests, integration tests, benchmarks, and quality assurance practices.\n🎯 Testing Philosophy AgenticGoKit follows a multi-layered testing strategy:\nUnit Tests: Test individual components in isolation Integration Tests: Test component interactions and workflows End-to-End Tests: Test complete user scenarios Performance Tests: Validate performance characteristics Chaos Tests: Test resilience under failure conditions 🏗️ Test Organization Directory Structure agenticgokit/\r├── core/ # Public API tests\r│ ├── agent_test.go\r│ ├── runner_test.go\r│ ├── mcp_test.go\r│ └── *_test.go\r├── internal/ # Implementation tests\r│ ├── agents/\r│ │ └── *_test.go\r│ ├── mcp/\r│ │ └── *_test.go\r│ └── */\r│ └── *_test.go\r├── integration/ # Integration tests\r│ ├── mcp_integration_test.go\r│ ├── workflow_integration_test.go\r│ └── *_integration_test.go\r├── benchmarks/ # Performance benchmarks\r│ ├── agent_benchmark_test.go\r│ ├── mcp_benchmark_test.go\r│ └── *_benchmark_test.go\r└── testdata/ # Test fixtures and data\r├── configs/\r├── fixtures/\r└── mocks/\rTest File Naming Conventions Pattern Purpose Example *_test.go Unit tests agent_test.go *_integration_test.go Integration tests mcp_integration_test.go *_benchmark_test.go Benchmarks runner_benchmark_test.go mock_*.go Mock implementations mock_llm_provider.go test_*.go Test utilities test_helpers.go 🧪 Unit Testing Test Structure Follow the AAA (Arrange, Act, Assert) pattern:\nfunc TestAgentRun(t *testing.T) { // Arrange agent := NewTestAgent(\"test-agent\") event := core.NewEvent(\"test\", map[string]interface{}{ \"query\": \"Hello world\", }) state := core.NewState() // Act result, err := agent.Run(context.Background(), event, state) // Assert assert.NoError(t, err) assert.True(t, result.Success) assert.Equal(t, \"Hello world\", result.Data[\"processed_query\"]) }\rTable-Driven Tests Use table-driven tests for multiple scenarios:\nfunc TestLLMProviderComplete(t *testing.T) { tests := []struct { name string input string expected string wantErr bool }{ { name: \"Simple query\", input: \"What is 2+2?\", expected: \"4\", wantErr: false, }, { name: \"Empty input\", input: \"\", expected: \"\", wantErr: true, }, { name: \"Complex query\", input: \"Explain quantum computing\", expected: \"Quantum computing is...\", wantErr: false, }, } provider := NewMockLLMProvider() for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { result, err := provider.Complete(context.Background(), tt.input) if tt.wantErr { assert.Error(t, err) return } assert.NoError(t, err) assert.Contains(t, result, tt.expected) }) } }\rMock Usage Create focused mocks for external dependencies:\ntype MockLLMProvider struct { responses map[string]string errors map[string]error callCount int mu sync.Mutex } func NewMockLLMProvider() *MockLLMProvider { return \u0026MockLLMProvider{ responses: make(map[string]string), errors: make(map[string]error), } } func (m *MockLLMProvider) SetResponse(input, output string) { m.mu.Lock() defer m.mu.Unlock() m.responses[input] = output } func (m *MockLLMProvider) SetError(input string, err error) { m.mu.Lock() defer m.mu.Unlock() m.errors[input] = err } func (m *MockLLMProvider) Complete(ctx context.Context, prompt string) (string, error) { m.mu.Lock() defer m.mu.Unlock() m.callCount++ if err, exists := m.errors[prompt]; exists { return \"\", err } if response, exists := m.responses[prompt]; exists { return response, nil } return \"default response\", nil } func (m *MockLLMProvider) GetCallCount() int { m.mu.Lock() defer m.mu.Unlock() return m.callCount }\rTest Utilities Create reusable test utilities:\n// test_helpers.go package core import ( \"context\" \"testing\" \"time\" ) // TestConfig returns a configuration suitable for testing func TestConfig() *Config { return \u0026Config{ LLM: LLMConfig{ Provider: \"mock\", }, MCP: MCPConfig{ Enabled: false, // Disable for unit tests }, Runner: RunnerConfig{ MaxConcurrentEvents: 1, EventTimeout: time.Second * 5, }, } } // WithTimeout creates a context with timeout for tests func WithTimeout(t *testing.T, timeout time.Duration) context.Context { ctx, cancel := context.WithTimeout(context.Background(), timeout) t.Cleanup(cancel) return ctx } // AssertEventually retries assertion until it passes or times out func AssertEventually(t *testing.T, assertion func() bool, timeout time.Duration, interval time.Duration) { ctx, cancel := context.WithTimeout(context.Background(), timeout) defer cancel() ticker := time.NewTicker(interval) defer ticker.Stop() for { if assertion() { return } select { case \u003c-ctx.Done(): t.Fatal(\"Assertion timed out\") case \u003c-ticker.C: continue } } }\r🔗 Integration Testing MCP Integration Tests Test MCP server interactions:\nfunc TestMCPIntegration(t *testing.T) { if testing.Short() { t.Skip(\"Skipping integration test in short mode\") } // Start test MCP server server := startTestMCPServer(t) defer server.Stop() // Configure AgenticGoKit with test server config := \u0026core.Config{ MCP: core.MCPConfig{ Enabled: true, Servers: []core.MCPServerConfig{ { Name: \"test-server\", Address: server.Address(), }, }, }, } runner, err := core.NewRunner(config) require.NoError(t, err) defer runner.Stop() // Test tool discovery tools, err := runner.GetMCPManager().ListTools(context.Background()) require.NoError(t, err) assert.NotEmpty(t, tools) // Test tool execution result, err := runner.GetMCPManager().ExecuteTool(context.Background(), \"test_tool\", map[string]interface{}{ \"input\": \"test data\", }) require.NoError(t, err) assert.True(t, result.Success) }\rMulti-Agent Workflow Tests Test complex agent interactions:\nfunc TestMultiAgentWorkflow(t *testing.T) { // Setup agents searchAgent := \u0026SearchAgent{} analysisAgent := \u0026AnalysisAgent{} summaryAgent := \u0026SummaryAgent{} // Create orchestrator orchestrator := core.NewOrchestrator(core.OrchestrationModeCollaborate) orchestrator.RegisterAgent(\"search\", searchAgent) orchestrator.RegisterAgent(\"analysis\", analysisAgent) orchestrator.RegisterAgent(\"summary\", summaryAgent) // Define workflow workflow := \u0026core.Workflow{ Steps: []core.WorkflowStep{ {AgentName: \"search\", Dependencies: []string{}}, {AgentName: \"analysis\", Dependencies: []string{\"search\"}}, {AgentName: \"summary\", Dependencies: []string{\"analysis\"}}, }, } // Execute workflow event := core.NewEvent(\"research\", map[string]interface{}{ \"topic\": \"AI advancements in 2024\", }) result, err := orchestrator.ExecuteWorkflow(context.Background(), workflow, event) require.NoError(t, err) // Verify workflow execution assert.Contains(t, result.Data, \"search_results\") assert.Contains(t, result.Data, \"analysis\") assert.Contains(t, result.Data, \"summary\") }\rDatabase Integration Tests Test persistent storage:\nfunc TestStatePeristenceIntegration(t *testing.T) { // Setup test database db := setupTestDB(t) defer cleanupTestDB(t, db) // Create session service with DB sessionService := memory.NewDatabaseSessionService(db) // Test session creation and retrieval session := core.NewSession(\"test-user\", \"test-session\") session.GetState().Set(\"key\", \"value\") err := sessionService.SaveSession(context.Background(), session) require.NoError(t, err) retrieved, err := sessionService.GetSession(context.Background(), \"test-session\") require.NoError(t, err) assert.Equal(t, \"value\", retrieved.GetState().GetString(\"key\")) }\r🚀 Performance Testing Benchmarks Create comprehensive benchmarks:\nfunc BenchmarkAgentExecution(b *testing.B) { agent := \u0026TestAgent{} event := core.NewEvent(\"benchmark\", map[string]interface{}{ \"query\": \"test query\", }) state := core.NewState() ctx := context.Background() b.ResetTimer() for i := 0; i \u003c b.N; i++ { _, err := agent.Run(ctx, event, state) if err != nil { b.Fatal(err) } } } func BenchmarkConcurrentAgentExecution(b *testing.B) { agent := \u0026TestAgent{} event := core.NewEvent(\"benchmark\", map[string]interface{}{ \"query\": \"test query\", }) state := core.NewState() b.ResetTimer() b.RunParallel(func(pb *testing.PB) { for pb.Next() { _, err := agent.Run(context.Background(), event, state.Clone()) if err != nil { b.Error(err) } } }) } func BenchmarkMemoryAllocation(b *testing.B) { b.ReportAllocs() for i := 0; i \u003c b.N; i++ { state := core.NewState() state.Set(\"key\", \"value\") _ = state.Clone() } }\rLoad Testing Use external tools for load testing:\n// k6 load test script import http from 'k6/http'; import { check } from 'k6'; export let options = { stages: [ { duration: '30s', target: 20 }, { duration: '1m', target: 20 }, { duration: '20s', target: 0 }, ], }; export default function() { let response = http.post('http://localhost:8080/api/chat', JSON.stringify({ query: 'Hello, how are you?', session_id: `session_${__VU}_${__ITER}`, }), { headers: { 'Content-Type': 'application/json' }, }); check(response, { 'status is 200': (r) =\u003e r.status === 200, 'response time \u003c 500ms': (r) =\u003e r.timings.duration \u003c 500, }); }\r🔥 Chaos Testing Failure Injection Test system resilience:\nfunc TestChaosFailureRecovery(t *testing.T) { // Create chaos injector chaos := \u0026ChaosInjector{ FailureRate: 0.3, // 30% failure rate FailureTypes: []FailureType{ NetworkTimeout, ServiceUnavailable, RateLimitExceeded, }, } // Wrap agent with chaos injection agent := NewChaosAgent(\u0026TestAgent{}, chaos) // Run multiple iterations successCount := 0 totalRuns := 100 for i := 0; i \u003c totalRuns; i++ { result, err := agent.Run(context.Background(), testEvent, testState) if err == nil \u0026\u0026 result.Success { successCount++ } } // Verify system remains functional despite failures successRate := float64(successCount) / float64(totalRuns) assert.Greater(t, successRate, 0.6, \"System should maintain \u003e60% success rate under chaos\") }\rResource Exhaustion Tests func TestMemoryPressure(t *testing.T) { // Create memory pressure var memoryHog [][]byte defer func() { memoryHog = nil runtime.GC() }() // Allocate significant memory for i := 0; i \u003c 100; i++ { memoryHog = append(memoryHog, make([]byte, 1024*1024)) // 1MB chunks } // Test agent behavior under memory pressure agent := \u0026TestAgent{} result, err := agent.Run(context.Background(), testEvent, testState) assert.NoError(t, err) assert.True(t, result.Success) }\r📊 Test Coverage Coverage Requirements Minimum Overall Coverage: 80% Critical Path Coverage: 95% Public API Coverage: 90% Error Path Coverage: 70% Measuring Coverage # Run tests with coverage go test -coverprofile=coverage.out ./... # Generate HTML coverage report go tool cover -html=coverage.out -o coverage.html # Check coverage percentage go tool cover -func=coverage.out | grep total # Fail if coverage below threshold go test -coverprofile=coverage.out ./... \u0026\u0026 \\ go tool cover -func=coverage.out | \\ awk '/total:/ {print $3}' | \\ sed 's/%//' | \\ awk '{if($1 \u003c 80) exit 1}'\rCoverage Analysis //go:build coverage // +build coverage package main import ( \"encoding/json\" \"fmt\" \"go/ast\" \"go/parser\" \"go/token\" \"os\" \"testing\" ) func TestCoverageAnalysis(t *testing.T) { // Parse coverage profile coverage := parseCoverageProfile(\"coverage.out\") // Analyze critical functions criticalFunctions := []string{ \"Agent.Run\", \"Runner.Emit\", \"MCPManager.ExecuteTool\", } for _, fn := range criticalFunctions { if coverage[fn] \u003c 95.0 { t.Errorf(\"Critical function %s has insufficient coverage: %.1f%%\", fn, coverage[fn]) } } }\r🚦 Continuous Integration GitHub Actions Workflow name: Test Suite on: push: branches: [ main, develop ] pull_request: branches: [ main ] jobs: test: runs-on: ubuntu-latest strategy: matrix: go-version: [1.21, 1.22] steps: - uses: actions/checkout@v4 - name: Set up Go uses: actions/setup-go@v4 with: go-version: ${{ matrix.go-version }} - name: Cache dependencies uses: actions/cache@v3 with: path: ~/go/pkg/mod key: ${{ runner.os }}-go-${{ hashFiles('**/go.sum') }} - name: Install dependencies run: go mod download - name: Run unit tests run: go test -v -short ./... - name: Run integration tests run: go test -v -tags=integration ./integration/... env: AZURE_OPENAI_ENDPOINT: ${{ secrets.AZURE_OPENAI_ENDPOINT }} AZURE_OPENAI_API_KEY: ${{ secrets.AZURE_OPENAI_API_KEY }} - name: Run benchmarks run: go test -bench=. -benchmem ./benchmarks/... - name: Generate coverage run: go test -coverprofile=coverage.out ./... - name: Upload coverage uses: codecov/codecov-action@v3 with: file: ./coverage.out - name: Check coverage threshold run: | COVERAGE=$(go tool cover -func=coverage.out | grep total | awk '{print $3}' | sed 's/%//') if (( $(echo \"$COVERAGE \u003c 80\" | bc -l) )); then echo \"Coverage $COVERAGE% is below threshold 80%\" exit 1 fi\r🛠️ Test Utilities and Helpers Test Server Setup // testserver.go type TestServer struct { httpServer *httptest.Server mcpServer *TestMCPServer cleanup []func() } func NewTestServer(t *testing.T) *TestServer { ts := \u0026TestServer{} // Setup HTTP server mux := http.NewServeMux() mux.HandleFunc(\"/health\", ts.healthHandler) mux.HandleFunc(\"/api/chat\", ts.chatHandler) ts.httpServer = httptest.NewServer(mux) ts.cleanup = append(ts.cleanup, ts.httpServer.Close) // Setup MCP server ts.mcpServer = NewTestMCPServer(t) ts.cleanup = append(ts.cleanup, ts.mcpServer.Stop) t.Cleanup(func() { for i := len(ts.cleanup) - 1; i \u003e= 0; i-- { ts.cleanup[i]() } }) return ts } func (ts *TestServer) URL() string { return ts.httpServer.URL }\rTest Data Management // testdata.go type TestDataManager struct { baseDir string } func NewTestDataManager(t *testing.T) *TestDataManager { return \u0026TestDataManager{ baseDir: filepath.Join(\"testdata\", t.Name()), } } func (tdm *TestDataManager) LoadJSON(filename string, v interface{}) error { data, err := os.ReadFile(filepath.Join(tdm.baseDir, filename)) if err != nil { return err } return json.Unmarshal(data, v) } func (tdm *TestDataManager) SaveJSON(filename string, v interface{}) error { data, err := json.MarshalIndent(v, \"\", \" \") if err != nil { return err } dir := filepath.Dir(filepath.Join(tdm.baseDir, filename)) if err := os.MkdirAll(dir, 0755); err != nil { return err } return os.WriteFile(filepath.Join(tdm.baseDir, filename), data, 0644) }\r📝 Testing Best Practices Do’s ✅ Write tests before or alongside code (TDD/BDD) ✅ Use descriptive test names that explain behavior ✅ Test both happy path and error conditions ✅ Use table-driven tests for multiple scenarios ✅ Mock external dependencies appropriately ✅ Keep tests independent and idempotent ✅ Use proper setup and teardown ✅ Measure and maintain high test coverage Don’ts ❌ Test implementation details instead of behavior ❌ Write tests that depend on external services in unit tests ❌ Create tests that depend on execution order ❌ Use overly complex test setups ❌ Ignore test failures or flaky tests ❌ Write tests without assertions ❌ Mock everything (test real integrations when appropriate) ❌ Skip testing error conditions This comprehensive testing strategy ensures AgenticGoKit maintains high quality, reliability, and performance across all components and use cases.",
    "description": "Testing Strategy This document outlines the comprehensive testing approach for AgenticGoKit, covering unit tests, integration tests, benchmarks, and quality assurance practices.\n🎯 Testing Philosophy AgenticGoKit follows a multi-layered testing strategy:\nUnit Tests: Test individual components in isolation Integration Tests: Test component interactions and workflows End-to-End Tests: Test complete user scenarios Performance Tests: Validate performance characteristics Chaos Tests: Test resilience under failure conditions 🏗️ Test Organization Directory Structure agenticgokit/\r├── core/ # Public API tests\r│ ├── agent_test.go\r│ ├── runner_test.go\r│ ├── mcp_test.go\r│ └── *_test.go\r├── internal/ # Implementation tests\r│ ├── agents/\r│ │ └── *_test.go\r│ ├── mcp/\r│ │ └── *_test.go\r│ └── */\r│ └── *_test.go\r├── integration/ # Integration tests\r│ ├── mcp_integration_test.go\r│ ├── workflow_integration_test.go\r│ └── *_integration_test.go\r├── benchmarks/ # Performance benchmarks\r│ ├── agent_benchmark_test.go\r│ ├── mcp_benchmark_test.go\r│ └── *_benchmark_test.go\r└── testdata/ # Test fixtures and data\r├── configs/\r├── fixtures/\r└── mocks/\rTest File Naming Conventions Pattern Purpose Example *_test.go Unit tests agent_test.go *_integration_test.go Integration tests mcp_integration_test.go *_benchmark_test.go Benchmarks runner_benchmark_test.go mock_*.go Mock implementations mock_llm_provider.go test_*.go Test utilities test_helpers.go 🧪 Unit Testing Test Structure Follow the AAA (Arrange, Act, Assert) pattern:",
    "tags": [],
    "title": "Testing",
    "uri": "/AgenticGoKitDocs/contributors/testing/index.html"
  },
  {
    "breadcrumb": "content \u003e contributors",
    "content": "Documentation Standards This guide outlines the standards and best practices for writing and maintaining AgenticGoKit documentation.\nTable of Contents Documentation Philosophy Structure and Organization Writing Guidelines Code Examples File Naming and Organization Maintenance and Updates Documentation Philosophy AgenticGoKit documentation follows these core principles:\n1. User-Centric Approach Start with the user’s goal - What are they trying to accomplish? Provide immediate value - Get users to success quickly Explain the “why” not just the “how” Include real-world context and use cases 2. Clarity and Accessibility Write for beginners while providing depth for experts Use clear, concise language - avoid jargon when possible Structure content logically with clear headings and flow Make content scannable with bullet points, code blocks, and visual breaks 3. Accuracy and Completeness Keep examples working - test all code samples Stay current with the codebase - documentation should never lag behind features Provide complete context - don’t assume prior knowledge Link between related concepts to build understanding 4. Separation of Concerns User docs focus on building with AgenticGoKit Contributor docs focus on extending AgenticGoKit API reference provides comprehensive technical details Examples demonstrate practical applications Structure and Organization Documentation Hierarchy docs/\r├── README.md # Main documentation index\r├── Architecture.md # High-level system overview\r├── ROADMAP.md # Project roadmap (maintained separately)\r├── guides/ # User-focused tutorials and guides\r│ ├── AgentBasics.md # Getting started with agents\r│ ├── Examples.md # Practical code examples\r│ ├── ToolIntegration.md # MCP and tool usage\r│ ├── Providers.md # LLM provider setup\r│ ├── Configuration.md # Project configuration\r│ ├── Production.md # Deployment and scaling\r│ ├── ErrorHandling.md # Error handling patterns\r│ ├── CustomTools.md # Building MCP servers\r│ └── Performance.md # Optimization guide\r├── api/ # Technical API reference\r│ ├── core.md # Core package API\r│ ├── agents.md # Agent interfaces\r│ ├── mcp.md # MCP integration API\r│ └── cli.md # CLI command reference\r├── contributors/ # Contributor-focused documentation\r│ ├── ContributorGuide.md # Getting started contributing\r│ ├── CoreVsInternal.md # Codebase architecture\r│ ├── Testing.md # Testing strategy\r│ ├── ReleaseProcess.md # Release management\r│ ├── AddingFeatures.md # Feature development\r│ ├── CodeStyle.md # Code standards\r│ └── DocsStandards.md # This document\r└── archive/ # Archived/outdated documents\r└── ... # Migration docs, old plans, etc.\rCross-References Always provide clear navigation paths:\nForward references: Link to related concepts users will need Backward references: Link to prerequisite knowledge Lateral references: Link to alternative approaches or related topics Example:\n## Configuration Before configuring agents, make sure you've completed the [basic setup](../guides/AgentBasics.md#setup). For production deployments, see the [Production Guide](Production.md) for advanced configuration options. Related: [LLM Providers](../guides/Providers.md) | [Error Handling](../guides/ErrorHandling.md)\rWriting Guidelines Voice and Tone For User Documentation:\nEncouraging and supportive - “You can easily…” Direct and action-oriented - Use imperative mood (“Create an agent…”) Confident but not arrogant - “This approach works well” vs “This is the only way” For Contributor Documentation:\nTechnical but approachable - Assume programming knowledge but explain AgenticGoKit-specific concepts Collaborative - “We use this pattern because…” Detailed and precise - Include implementation details and reasoning Structure Templates Guide Template:\n# Guide Title Brief description of what this guide covers and who it's for. ## Table of Contents - [Section 1](#section-1) - [Section 2](#section-2) ## Prerequisites - What users need to know/have before starting - Links to required setup ## Main Content ### Step-by-step sections with: - Clear headings - Code examples - Expected output - Common pitfalls ## Next Steps - Where to go from here - Related guides\rAPI Reference Template:\n# Package/Interface Name Overview of the package/interface purpose. ## Types ### TypeName Description of the type and its purpose. ```go type TypeName struct { Field1 string // Description Field2 int // Description }\rFields:\nField1: Detailed description, constraints, examples Field2: Detailed description, constraints, examples Functions FunctionName Brief description.\nfunc FunctionName(param1 Type1, param2 Type2) (ReturnType, error)\rParameters:\nparam1: Description and constraints param2: Description and constraints Returns:\nReturnType: Description of return value error: When and why errors occur Example:\n// Working example with context\r### Headings and Structure\r- **Use consistent heading levels**\r- H1 (`#`) for document title\r- H2 (`##`) for major sections\r- H3 (`###`) for subsections\r- H4+ for detailed breakdowns if needed\r- **Make headings descriptive**\r- Good: \"Creating Your First Agent\"\r- Bad: \"Getting Started\"\r- **Use parallel structure** in lists and headings\r- \"Creating agents\", \"Configuring tools\", \"Running workflows\"\r- Not: \"Create agents\", \"Tool configuration\", \"How to run workflows\"\r### Language and Style\r**Do:**\r- Use active voice: \"The agent processes the request\"\r- Use present tense: \"The function returns a result\"\r- Use specific verbs: \"configure\", \"initialize\", \"execute\"\r- Define acronyms on first use: \"Model Context Protocol (MCP)\"\r- Include units and constraints: \"timeout (30 seconds max)\", \"1-100 agents\"\r**Don't:**\r- Use passive voice unnecessarily: \"The request is processed by the agent\"\r- Use future tense unless discussing roadmap: \"The function will return\"\r- Use vague language: \"some\", \"various\", \"might\"\r- Assume knowledge of other systems without context\r## Code Examples\r### Principles\r1. **All examples must work** - Test every code sample\r2. **Show complete context** - Include imports, setup, error handling\r3. **Focus on the concept** - Don't include unrelated complexity\r4. **Include expected output** when relevant\r### Code Block Standards\r**Complete, runnable examples:**\r```go\rpackage main\rimport (\r\"context\"\r\"fmt\"\r\"log\"\r\"github.com/kunalkushwaha/agenticgokit/core\"\r)\rfunc main() {\r// Initialize MCP for tool discovery\rcore.QuickStartMCP()\r// Create agent with Azure OpenAI\rconfig := core.LLMConfig{\rProvider: \"azure-openai\",\rAPIKey: \"your-api-key\",\rBaseURL: \"https://your-resource.openai.azure.com\",\r}\rllm := core.NewAzureOpenAIAdapter(config)\ragent, err := core.NewMCPAgent(\"helper\", llm)\rif err != nil {\rlog.Fatal(err)\r}\r// Create state and run agent\rstate := core.NewState()\rstate.Set(\"query\", \"What is the capital of France?\")\rresult, err := agent.Run(context.Background(), state)\rif err != nil {\rlog.Fatal(err)\r}\rfmt.Println(\"Response:\", result.GetResult())\r}\r// Expected output:\r// Response: The capital of France is Paris.\rCode snippets (when full context isn’t needed):\n// Create an agent agent, err := core.NewMCPAgent(\"helper\", llmProvider) if err != nil { return err }\rError Handling in Examples Always show proper error handling:\nGood:\nresult, err := agent.Run(ctx, state) if err != nil { log.Printf(\"Agent failed: %v\", err) return err }\rBad:\nresult, _ := agent.Run(ctx, state) // Don't ignore errors in docs\rConfiguration Examples Use realistic but safe configuration values:\n// Good - Shows structure with placeholder values config := core.LLMConfig{ Provider: \"azure-openai\", APIKey: \"your-api-key\", // Clear placeholder BaseURL: \"https://your-resource.openai.azure.com\", Model: \"gpt-4\", Timeout: 30 * time.Second, } // Bad - Fake values that might be confusing config := core.LLMConfig{ APIKey: \"sk-abc123\", // Looks real but fake BaseURL: \"https://api.openai.com\", // Wrong for Azure }\rFile Naming and Organization File Naming Conventions Use PascalCase for multi-word concepts: ErrorHandling.md, CustomTools.md Be descriptive but concise: AgentBasics.md not Agents.md Avoid abbreviations unless they’re widely known: Configuration.md not Config.md Group related files in subdirectories Cross-File Dependencies Minimize dependencies between guides:\nEach guide should be mostly self-contained Link to prerequisites clearly Don’t duplicate content - reference other guides instead Good dependency pattern:\nAgentBasics.md (foundation)\r↓\rExamples.md (builds on basics)\r↓\rProduction.md (builds on examples)\rBad dependency pattern:\nAgentBasics.md ←→ ToolIntegration.md ←→ Configuration.md\r(circular dependencies make docs hard to follow)\rMaintenance and Updates Keeping Documentation Current Update docs with code changes\nDocumentation PRs should accompany feature PRs Breaking changes require documentation updates Deprecations need clear migration paths Regular audits\nQuarterly review of all user guides Annual review of contributor documentation Continuous monitoring of code examples Version compatibility\nSpecify which version examples target Archive old documentation when appropriate Maintain migration guides for major changes Documentation Review Process For new documentation:\nTechnical accuracy review (by code owner) Clarity review (by someone unfamiliar with the feature) Structure review (by documentation maintainer) For updates:\nVerify all code examples still work Check links are still valid Ensure consistent voice and style Measuring Documentation Quality Quantitative metrics:\nUser completion rates on tutorials Time spent on documentation pages Support questions that are answered in docs Qualitative feedback:\nUser feedback on clarity and usefulness Contributor feedback on development documentation Regular surveys and interviews Common Maintenance Tasks Monthly:\nTest all code examples in user guides Check for broken links Update any references to specific versions Quarterly:\nReview and update API documentation Audit cross-references and navigation Update screenshots and visual elements Annually:\nMajor reorganization if needed Archive outdated documentation Review and update writing standards Style Reference Formatting Standards Code references in text:\nUse backticks for: function names, package names, file names Use code blocks for: multi-line code, configuration files Emphasis:\nBold for UI elements, important concepts, section headers in lists Italic for emphasis, new terms on first use Code formatting for technical terms, values, commands Lists:\nUse bullet points for unordered concepts Use numbers for sequential steps Use consistent parallel structure Links:\nUse descriptive link text: “Agent Basics guide” Not: “Click here for agent basics” Common Terminology Consistent terms:\n“AgenticGoKit” (not “agenticgokit” or “Agentic Go Kit”) - the framework “agent” (lowercase) - an instance of an agent “MCP” - Model Context Protocol (define on first use) “LLM” - Large Language Model (define on first use) Preferred usage:\n“configuration” not “config” (in formal documentation) “initialize” not “init” (in explanatory text) “function” not “func” (except in code) Templates and Tools Documentation Templates See the structure templates in Structure Templates above.\nUseful Tools For writing:\nUse VS Code with Markdown preview Check spelling and grammar with tools like Grammarly Validate links with link checkers For code examples:\nTest all Go code with go run or go test Use gofmt to ensure consistent formatting Validate JSON and YAML configuration examples For maintenance:\nUse GitHub Issues to track documentation debt Create checklists for common update tasks Use automation for link checking and code validation Contributing to Documentation Getting Started Read this standards guide Look at existing documentation for examples Start with small improvements to get familiar with the style Ask questions in GitHub Discussions if unsure Making Changes Small fixes (typos, broken links): Submit PR directly Content updates: Open issue first to discuss approach Major reorganization: Discuss in GitHub Discussions Review Process All documentation changes go through the same review process as code:\nTechnical accuracy Adherence to these standards User experience and clarity This standards guide is a living document. Updates and improvements are welcome through the standard PR process.",
    "description": "Documentation Standards This guide outlines the standards and best practices for writing and maintaining AgenticGoKit documentation.\nTable of Contents Documentation Philosophy Structure and Organization Writing Guidelines Code Examples File Naming and Organization Maintenance and Updates Documentation Philosophy AgenticGoKit documentation follows these core principles:\n1. User-Centric Approach Start with the user’s goal - What are they trying to accomplish? Provide immediate value - Get users to success quickly Explain the “why” not just the “how” Include real-world context and use cases 2. Clarity and Accessibility Write for beginners while providing depth for experts Use clear, concise language - avoid jargon when possible Structure content logically with clear headings and flow Make content scannable with bullet points, code blocks, and visual breaks 3. Accuracy and Completeness Keep examples working - test all code samples Stay current with the codebase - documentation should never lag behind features Provide complete context - don’t assume prior knowledge Link between related concepts to build understanding 4. Separation of Concerns User docs focus on building with AgenticGoKit Contributor docs focus on extending AgenticGoKit API reference provides comprehensive technical details Examples demonstrate practical applications Structure and Organization Documentation Hierarchy docs/\r├── README.md # Main documentation index\r├── Architecture.md # High-level system overview\r├── ROADMAP.md # Project roadmap (maintained separately)\r├── guides/ # User-focused tutorials and guides\r│ ├── AgentBasics.md # Getting started with agents\r│ ├── Examples.md # Practical code examples\r│ ├── ToolIntegration.md # MCP and tool usage\r│ ├── Providers.md # LLM provider setup\r│ ├── Configuration.md # Project configuration\r│ ├── Production.md # Deployment and scaling\r│ ├── ErrorHandling.md # Error handling patterns\r│ ├── CustomTools.md # Building MCP servers\r│ └── Performance.md # Optimization guide\r├── api/ # Technical API reference\r│ ├── core.md # Core package API\r│ ├── agents.md # Agent interfaces\r│ ├── mcp.md # MCP integration API\r│ └── cli.md # CLI command reference\r├── contributors/ # Contributor-focused documentation\r│ ├── ContributorGuide.md # Getting started contributing\r│ ├── CoreVsInternal.md # Codebase architecture\r│ ├── Testing.md # Testing strategy\r│ ├── ReleaseProcess.md # Release management\r│ ├── AddingFeatures.md # Feature development\r│ ├── CodeStyle.md # Code standards\r│ └── DocsStandards.md # This document\r└── archive/ # Archived/outdated documents\r└── ... # Migration docs, old plans, etc.\rCross-References Always provide clear navigation paths:",
    "tags": [],
    "title": "DocsStandards",
    "uri": "/AgenticGoKitDocs/contributors/docsstandards/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials",
    "content": "Advanced Tutorials Production patterns and optimization techniques\nThis section covers advanced topics for building production-ready agent systems with AgenticGoKit. These tutorials assume familiarity with basic AgenticGoKit concepts.\nAdvanced Topics Circuit Breaker Patterns - Implement resilient agent systems Retry Policies - Handle failures gracefully Testing Strategies - Test multi-agent systems effectively Load Balancing and Scaling - Scale agents horizontally Prerequisites Before starting these tutorials, you should:\nComplete the Getting Started tutorials Understand Core Concepts Have experience with Go testing and production deployment Production Focus These tutorials emphasize:\nReliability: Building fault-tolerant systems Performance: Optimizing for speed and efficiency Scalability: Handling increased load Maintainability: Testing and monitoring strategies Next Steps After mastering these advanced concepts:\nExplore How-To Guides for specific implementation tasks Check API Reference for detailed interface documentation Consider Contributing to the AgenticGoKit project",
    "description": "Advanced Tutorials Production patterns and optimization techniques\nThis section covers advanced topics for building production-ready agent systems with AgenticGoKit. These tutorials assume familiarity with basic AgenticGoKit concepts.\nAdvanced Topics Circuit Breaker Patterns - Implement resilient agent systems Retry Policies - Handle failures gracefully Testing Strategies - Test multi-agent systems effectively Load Balancing and Scaling - Scale agents horizontally Prerequisites Before starting these tutorials, you should:\nComplete the Getting Started tutorials Understand Core Concepts Have experience with Go testing and production deployment Production Focus These tutorials emphasize:",
    "tags": [],
    "title": "advanced",
    "uri": "/AgenticGoKitDocs/tutorials/advanced/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials \u003e mcp",
    "content": "Advanced Tool Patterns in AgenticGoKit Overview This tutorial explores advanced patterns for tool usage in AgenticGoKit, including tool composition, conditional execution, parallel tool usage, and sophisticated error handling strategies. These patterns enable you to build complex, production-ready agent systems that can handle sophisticated workflows and edge cases.\nPrerequisites Understanding of MCP Overview Completion of Tool Development Familiarity with Tool Integration Knowledge of Orchestration Patterns Tool Composition Patterns 1. Sequential Tool Chains package patterns import ( \"context\" \"fmt\" \"github.com/kunalkushwaha/agenticgokit/core\" ) // ToolChain executes tools in sequence, passing results between them type ToolChain struct { name string steps []ToolStep } type ToolStep struct { ToolName string ParamMapper func(previousResult interface{}, initialParams map[string]interface{}) map[string]interface{} } func NewToolChain(name string) *ToolChain { return \u0026ToolChain{ name: name, steps: make([]ToolStep, 0), } } func (tc *ToolChain) AddStep(toolName string, paramMapper func(interface{}, map[string]interface{}) map[string]interface{}) *ToolChain { tc.steps = append(tc.steps, ToolStep{ ToolName: toolName, ParamMapper: paramMapper, }) return tc } func (tc *ToolChain) Name() string { return tc.name } func (tc *ToolChain) Description() string { return fmt.Sprintf(\"Sequential tool chain with %d steps\", len(tc.steps)) } func (tc *ToolChain) ParameterSchema() map[string]core.ParameterDefinition { return map[string]core.ParameterDefinition{ \"initial_params\": { Type: \"object\", Description: \"Initial parameters for the tool chain\", Required: true, }, \"mcp_manager\": { Type: \"object\", Description: \"MCP manager for tool execution\", Required: true, }, } } func (tc *ToolChain) Execute(ctx context.Context, params map[string]interface{}) (interface{}, error) { initialParams, ok := params[\"initial_params\"].(map[string]interface{}) if !ok { return nil, fmt.Errorf(\"initial_params must be an object\") } mcpManager, ok := params[\"mcp_manager\"].(*core.MCPManager) if !ok { return nil, fmt.Errorf(\"mcp_manager must be provided\") } var currentResult interface{} = initialParams results := make([]interface{}, 0, len(tc.steps)) for i, step := range tc.steps { // Get tool tool, err := mcpManager.GetTool(step.ToolName) if err != nil { return nil, fmt.Errorf(\"step %d: failed to get tool %s: %w\", i+1, step.ToolName, err) } // Map parameters var toolParams map[string]interface{} if step.ParamMapper != nil { toolParams = step.ParamMapper(currentResult, initialParams) } else { toolParams = initialParams } // Execute tool result, err := tool.Execute(ctx, toolParams) if err != nil { return nil, fmt.Errorf(\"step %d: tool %s execution failed: %w\", i+1, step.ToolName, err) } currentResult = result results = append(results, result) } return map[string]interface{}{ \"final_result\": currentResult, \"all_results\": results, \"steps\": len(tc.steps), }, nil } // Example: Research and Analysis Chain func CreateResearchChain() *ToolChain { return NewToolChain(\"research_analysis\"). AddStep(\"search\", func(prev interface{}, initial map[string]interface{}) map[string]interface{} { query, _ := initial[\"query\"].(string) return map[string]interface{}{ \"query\": query, \"limit\": 5, } }). AddStep(\"summarizer\", func(prev interface{}, initial map[string]interface{}) map[string]interface{} { searchResults := prev.(map[string]interface{}) return map[string]interface{}{ \"content\": searchResults[\"results\"], \"max_length\": 500, } }). AddStep(\"analyzer\", func(prev interface{}, initial map[string]interface{}) map[string]interface{} { summary := prev.(map[string]interface{}) return map[string]interface{}{ \"text\": summary[\"summary\"], \"analysis_type\": \"sentiment_and_topics\", } }) } ```### 2. Parallel Tool Execution ```go // ParallelToolExecutor runs multiple tools concurrently type ParallelToolExecutor struct { name string tools []ParallelToolConfig aggregator ResultAggregator } type ParallelToolConfig struct { ToolName string Parameters map[string]interface{} Optional bool // If true, failure won't fail the entire execution } type ResultAggregator func(results map[string]interface{}, errors map[string]error) (interface{}, error) func NewParallelToolExecutor(name string, aggregator ResultAggregator) *ParallelToolExecutor { return \u0026ParallelToolExecutor{ name: name, tools: make([]ParallelToolConfig, 0), aggregator: aggregator, } } func (pte *ParallelToolExecutor) AddTool(toolName string, params map[string]interface{}, optional bool) *ParallelToolExecutor { pte.tools = append(pte.tools, ParallelToolConfig{ ToolName: toolName, Parameters: params, Optional: optional, }) return pte } func (pte *ParallelToolExecutor) Name() string { return pte.name } func (pte *ParallelToolExecutor) Description() string { return fmt.Sprintf(\"Parallel execution of %d tools\", len(pte.tools)) } func (pte *ParallelToolExecutor) ParameterSchema() map[string]core.ParameterDefinition { return map[string]core.ParameterDefinition{ \"mcp_manager\": { Type: \"object\", Description: \"MCP manager for tool execution\", Required: true, }, \"timeout\": { Type: \"number\", Description: \"Timeout in seconds for parallel execution\", Required: false, Default: 30, }, } } func (pte *ParallelToolExecutor) Execute(ctx context.Context, params map[string]interface{}) (interface{}, error) { mcpManager, ok := params[\"mcp_manager\"].(*core.MCPManager) if !ok { return nil, fmt.Errorf(\"mcp_manager must be provided\") } timeout := 30 * time.Second if timeoutParam, ok := params[\"timeout\"].(float64); ok { timeout = time.Duration(timeoutParam) * time.Second } // Create context with timeout execCtx, cancel := context.WithTimeout(ctx, timeout) defer cancel() // Execute tools in parallel results := make(map[string]interface{}) errors := make(map[string]error) var wg sync.WaitGroup var mu sync.Mutex for _, toolConfig := range pte.tools { wg.Add(1) go func(config ParallelToolConfig) { defer wg.Done() tool, err := mcpManager.GetTool(config.ToolName) if err != nil { mu.Lock() errors[config.ToolName] = fmt.Errorf(\"failed to get tool: %w\", err) mu.Unlock() return } result, err := tool.Execute(execCtx, config.Parameters) mu.Lock() if err != nil { errors[config.ToolName] = err } else { results[config.ToolName] = result } mu.Unlock() }(toolConfig) } wg.Wait() // Check for required tool failures for _, toolConfig := range pte.tools { if !toolConfig.Optional { if err, exists := errors[toolConfig.ToolName]; exists { return nil, fmt.Errorf(\"required tool %s failed: %w\", toolConfig.ToolName, err) } } } // Aggregate results return pte.aggregator(results, errors) } // Example: Multi-Source Information Gathering func CreateInfoGatheringTool() *ParallelToolExecutor { return NewParallelToolExecutor(\"info_gathering\", func(results map[string]interface{}, errors map[string]error) (interface{}, error) { gathered := map[string]interface{}{ \"sources\": make(map[string]interface{}), \"errors\": make(map[string]string), \"summary\": \"\", } // Collect successful results for toolName, result := range results { gathered[\"sources\"].(map[string]interface{})[toolName] = result } // Collect errors for optional tools for toolName, err := range errors { gathered[\"errors\"].(map[string]string)[toolName] = err.Error() } // Create summary sourceCount := len(results) errorCount := len(errors) gathered[\"summary\"] = fmt.Sprintf(\"Gathered information from %d sources with %d errors\", sourceCount, errorCount) return gathered, nil }). AddTool(\"web_search\", map[string]interface{}{\"query\": \"latest news\"}, false). AddTool(\"weather\", map[string]interface{}{\"location\": \"current\"}, true). AddTool(\"stock_prices\", map[string]interface{}{\"symbols\": []string{\"AAPL\", \"GOOGL\"}}, true). AddTool(\"calendar\", map[string]interface{}{\"days\": 7}, true) }\rConditional Tool Execution 1. Rule-Based Tool Selection // ConditionalToolExecutor executes tools based on conditions type ConditionalToolExecutor struct { name string rules []ExecutionRule } type ExecutionRule struct { Condition func(context.Context, map[string]interface{}) bool ToolName string Parameters func(map[string]interface{}) map[string]interface{} Description string } func NewConditionalToolExecutor(name string) *ConditionalToolExecutor { return \u0026ConditionalToolExecutor{ name: name, rules: make([]ExecutionRule, 0), } } func (cte *ConditionalToolExecutor) AddRule( condition func(context.Context, map[string]interface{}) bool, toolName string, paramMapper func(map[string]interface{}) map[string]interface{}, description string, ) *ConditionalToolExecutor { cte.rules = append(cte.rules, ExecutionRule{ Condition: condition, ToolName: toolName, Parameters: paramMapper, Description: description, }) return cte } func (cte *ConditionalToolExecutor) Name() string { return cte.name } func (cte *ConditionalToolExecutor) Description() string { return fmt.Sprintf(\"Conditional tool executor with %d rules\", len(cte.rules)) } func (cte *ConditionalToolExecutor) ParameterSchema() map[string]core.ParameterDefinition { return map[string]core.ParameterDefinition{ \"input_data\": { Type: \"object\", Description: \"Input data for condition evaluation\", Required: true, }, \"mcp_manager\": { Type: \"object\", Description: \"MCP manager for tool execution\", Required: true, }, } } func (cte *ConditionalToolExecutor) Execute(ctx context.Context, params map[string]interface{}) (interface{}, error) { inputData, ok := params[\"input_data\"].(map[string]interface{}) if !ok { return nil, fmt.Errorf(\"input_data must be an object\") } mcpManager, ok := params[\"mcp_manager\"].(*core.MCPManager) if !ok { return nil, fmt.Errorf(\"mcp_manager must be provided\") } executedRules := make([]map[string]interface{}, 0) for i, rule := range cte.rules { if rule.Condition(ctx, inputData) { // Get tool tool, err := mcpManager.GetTool(rule.ToolName) if err != nil { return nil, fmt.Errorf(\"rule %d: failed to get tool %s: %w\", i+1, rule.ToolName, err) } // Prepare parameters toolParams := rule.Parameters(inputData) // Execute tool result, err := tool.Execute(ctx, toolParams) if err != nil { return nil, fmt.Errorf(\"rule %d: tool %s execution failed: %w\", i+1, rule.ToolName, err) } executedRules = append(executedRules, map[string]interface{}{ \"rule_description\": rule.Description, \"tool_name\": rule.ToolName, \"result\": result, }) } } return map[string]interface{}{ \"executed_rules\": executedRules, \"total_rules\": len(cte.rules), \"matched_rules\": len(executedRules), }, nil } // Example: Smart Assistant Tool Selection func CreateSmartAssistantTool() *ConditionalToolExecutor { return NewConditionalToolExecutor(\"smart_assistant\"). AddRule( func(ctx context.Context, data map[string]interface{}) bool { message, ok := data[\"message\"].(string) return ok \u0026\u0026 strings.Contains(strings.ToLower(message), \"weather\") }, \"weather\", func(data map[string]interface{}) map[string]interface{} { location := \"current\" if loc, ok := data[\"location\"].(string); ok { location = loc } return map[string]interface{}{\"location\": location} }, \"Weather information requested\", ). AddRule( func(ctx context.Context, data map[string]interface{}) bool { message, ok := data[\"message\"].(string) if !ok { return false } mathKeywords := []string{\"calculate\", \"math\", \"+\", \"-\", \"*\", \"/\", \"=\"} msgLower := strings.ToLower(message) for _, keyword := range mathKeywords { if strings.Contains(msgLower, keyword) { return true } } return false }, \"calculator\", func(data map[string]interface{}) map[string]interface{} { // Simple math expression parser would go here return map[string]interface{}{ \"expression\": data[\"message\"], } }, \"Mathematical calculation requested\", ). AddRule( func(ctx context.Context, data map[string]interface{}) bool { message, ok := data[\"message\"].(string) return ok \u0026\u0026 strings.Contains(strings.ToLower(message), \"search\") }, \"web_search\", func(data map[string]interface{}) map[string]interface{} { return map[string]interface{}{ \"query\": data[\"message\"], \"limit\": 5, } }, \"Web search requested\", ) } ```## Error Handling and Recovery Patterns ### 1. Retry with Backoff ```go // RetryableTool wraps a tool with retry logic type RetryableTool struct { tool core.Tool maxRetries int baseDelay time.Duration maxDelay time.Duration backoffFunc func(attempt int, baseDelay time.Duration) time.Duration retryChecker func(error) bool } func NewRetryableTool(tool core.Tool, maxRetries int, baseDelay time.Duration) *RetryableTool { return \u0026RetryableTool{ tool: tool, maxRetries: maxRetries, baseDelay: baseDelay, maxDelay: 5 * time.Minute, backoffFunc: func(attempt int, baseDelay time.Duration) time.Duration { // Exponential backoff with jitter delay := time.Duration(float64(baseDelay) * math.Pow(2, float64(attempt))) jitter := time.Duration(rand.Float64() * float64(delay) * 0.1) return delay + jitter }, retryChecker: func(err error) bool { // Retry on network errors, timeouts, and rate limits errStr := strings.ToLower(err.Error()) return strings.Contains(errStr, \"timeout\") || strings.Contains(errStr, \"network\") || strings.Contains(errStr, \"rate limit\") || strings.Contains(errStr, \"temporary\") }, } } func (rt *RetryableTool) WithMaxDelay(maxDelay time.Duration) *RetryableTool { rt.maxDelay = maxDelay return rt } func (rt *RetryableTool) WithRetryChecker(checker func(error) bool) *RetryableTool { rt.retryChecker = checker return rt } func (rt *RetryableTool) Name() string { return rt.tool.Name() } func (rt *RetryableTool) Description() string { return rt.tool.Description() } func (rt *RetryableTool) ParameterSchema() map[string]core.ParameterDefinition { return rt.tool.ParameterSchema() } func (rt *RetryableTool) Execute(ctx context.Context, params map[string]interface{}) (interface{}, error) { var lastErr error for attempt := 0; attempt \u003c= rt.maxRetries; attempt++ { result, err := rt.tool.Execute(ctx, params) if err == nil { return result, nil } lastErr = err // Check if we should retry if !rt.retryChecker(err) { return nil, fmt.Errorf(\"non-retryable error: %w\", err) } // Don't wait after the last attempt if attempt == rt.maxRetries { break } // Calculate delay delay := rt.backoffFunc(attempt, rt.baseDelay) if delay \u003e rt.maxDelay { delay = rt.maxDelay } // Wait with context cancellation support select { case \u003c-ctx.Done(): return nil, ctx.Err() case \u003c-time.After(delay): // Continue to next attempt } } return nil, fmt.Errorf(\"max retries (%d) exceeded, last error: %w\", rt.maxRetries, lastErr) }\r2. Circuit Breaker Pattern // CircuitBreakerTool implements circuit breaker pattern for tools type CircuitBreakerTool struct { tool core.Tool failureThreshold int resetTimeout time.Duration state CircuitState failures int lastFailureTime time.Time mu sync.RWMutex } type CircuitState int const ( CircuitClosed CircuitState = iota CircuitOpen CircuitHalfOpen ) func NewCircuitBreakerTool(tool core.Tool, failureThreshold int, resetTimeout time.Duration) *CircuitBreakerTool { return \u0026CircuitBreakerTool{ tool: tool, failureThreshold: failureThreshold, resetTimeout: resetTimeout, state: CircuitClosed, } } func (cbt *CircuitBreakerTool) Name() string { return cbt.tool.Name() } func (cbt *CircuitBreakerTool) Description() string { return cbt.tool.Description() } func (cbt *CircuitBreakerTool) ParameterSchema() map[string]core.ParameterDefinition { return cbt.tool.ParameterSchema() } func (cbt *CircuitBreakerTool) Execute(ctx context.Context, params map[string]interface{}) (interface{}, error) { cbt.mu.Lock() // Check if circuit should be reset if cbt.state == CircuitOpen \u0026\u0026 time.Since(cbt.lastFailureTime) \u003e cbt.resetTimeout { cbt.state = CircuitHalfOpen cbt.failures = 0 } // Fail fast if circuit is open if cbt.state == CircuitOpen { cbt.mu.Unlock() return nil, fmt.Errorf(\"circuit breaker is open for tool %s\", cbt.tool.Name()) } cbt.mu.Unlock() // Execute tool result, err := cbt.tool.Execute(ctx, params) cbt.mu.Lock() defer cbt.mu.Unlock() if err != nil { cbt.failures++ cbt.lastFailureTime = time.Now() // Open circuit if threshold exceeded if cbt.failures \u003e= cbt.failureThreshold { cbt.state = CircuitOpen } return nil, err } // Success - reset circuit if it was half-open if cbt.state == CircuitHalfOpen { cbt.state = CircuitClosed cbt.failures = 0 } return result, nil } func (cbt *CircuitBreakerTool) GetState() (CircuitState, int, time.Time) { cbt.mu.RLock() defer cbt.mu.RUnlock() return cbt.state, cbt.failures, cbt.lastFailureTime }\r3. Fallback Tool Pattern // FallbackTool tries primary tool first, then fallbacks type FallbackTool struct { name string description string primaryTool core.Tool fallbackTools []FallbackConfig } type FallbackConfig struct { Tool core.Tool Condition func(error) bool ParamMapper func(map[string]interface{}) map[string]interface{} } func NewFallbackTool(name, description string, primaryTool core.Tool) *FallbackTool { return \u0026FallbackTool{ name: name, description: description, primaryTool: primaryTool, fallbackTools: make([]FallbackConfig, 0), } } func (ft *FallbackTool) AddFallback( tool core.Tool, condition func(error) bool, paramMapper func(map[string]interface{}) map[string]interface{}, ) *FallbackTool { ft.fallbackTools = append(ft.fallbackTools, FallbackConfig{ Tool: tool, Condition: condition, ParamMapper: paramMapper, }) return ft } func (ft *FallbackTool) Name() string { return ft.name } func (ft *FallbackTool) Description() string { return ft.description } func (ft *FallbackTool) ParameterSchema() map[string]core.ParameterDefinition { return ft.primaryTool.ParameterSchema() } func (ft *FallbackTool) Execute(ctx context.Context, params map[string]interface{}) (interface{}, error) { // Try primary tool first result, err := ft.primaryTool.Execute(ctx, params) if err == nil { return map[string]interface{}{ \"result\": result, \"tool_used\": ft.primaryTool.Name(), \"fallback\": false, }, nil } primaryError := err // Try fallback tools for i, fallback := range ft.fallbackTools { if fallback.Condition(err) { // Map parameters if needed fallbackParams := params if fallback.ParamMapper != nil { fallbackParams = fallback.ParamMapper(params) } result, err := fallback.Tool.Execute(ctx, fallbackParams) if err == nil { return map[string]interface{}{ \"result\": result, \"tool_used\": fallback.Tool.Name(), \"fallback\": true, \"fallback_index\": i, \"primary_error\": primaryError.Error(), }, nil } } } return nil, fmt.Errorf(\"primary tool and all fallbacks failed, primary error: %w\", primaryError) } // Example: Weather with Multiple Sources func CreateWeatherWithFallbacks() *FallbackTool { primaryWeather := tools.NewWeatherTool(os.Getenv(\"OPENWEATHER_API_KEY\")) return NewFallbackTool(\"weather_with_fallbacks\", \"Weather information with multiple sources\", primaryWeather). AddFallback( tools.NewWeatherAPITool(os.Getenv(\"WEATHERAPI_KEY\")), func(err error) bool { return strings.Contains(err.Error(), \"api key\") || strings.Contains(err.Error(), \"rate limit\") }, func(params map[string]interface{}) map[string]interface{} { // Convert location format if needed return params }, ). AddFallback( tools.NewMockWeatherTool(), // Returns mock data func(err error) bool { return true // Always try mock as last resort }, nil, ) }\rBest Practices and Guidelines 1. Tool Design Principles Composability: Design tools that can be easily combined Idempotency: Ensure tools can be safely retried Observability: Include comprehensive monitoring and logging Graceful Degradation: Implement fallback mechanisms Configuration: Make tools configurable for different environments 2. Performance Optimization Caching: Cache expensive operations and API calls Parallel Execution: Use parallel execution where appropriate Connection Pooling: Reuse connections for external services Batch Operations: Combine multiple operations when possible Resource Management: Properly manage resources and connections 3. Error Handling Strategy Categorize Errors: Distinguish between retryable and non-retryable errors Circuit Breakers: Protect against cascading failures Fallback Mechanisms: Provide alternative execution paths Monitoring: Track error patterns and rates User Experience: Provide meaningful error messages Conclusion Advanced tool patterns enable you to build sophisticated, production-ready agent systems that can handle complex workflows, recover from failures, and scale effectively. By combining these patterns, you can create robust tool ecosystems that enhance your agents’ capabilities while maintaining reliability and performance.\nKey takeaways:\nUse composition patterns for complex workflows Implement proper error handling and recovery mechanisms Monitor tool performance and health comprehensively Design for scalability and maintainability Follow production deployment best practices Next Steps Tool Development - Learn the basics of tool creation Tool Integration - Understand tool integration patterns Orchestration Patterns - Explore agent orchestration Production Deployment - Deploy your systems Further Reading API Reference: MCP Examples: Advanced Patterns Monitoring and Observability",
    "description": "Advanced Tool Patterns in AgenticGoKit Overview This tutorial explores advanced patterns for tool usage in AgenticGoKit, including tool composition, conditional execution, parallel tool usage, and sophisticated error handling strategies. These patterns enable you to build complex, production-ready agent systems that can handle sophisticated workflows and edge cases.\nPrerequisites Understanding of MCP Overview Completion of Tool Development Familiarity with Tool Integration Knowledge of Orchestration Patterns Tool Composition Patterns 1. Sequential Tool Chains package patterns import ( \"context\" \"fmt\" \"github.com/kunalkushwaha/agenticgokit/core\" ) // ToolChain executes tools in sequence, passing results between them type ToolChain struct { name string steps []ToolStep } type ToolStep struct { ToolName string ParamMapper func(previousResult interface{}, initialParams map[string]interface{}) map[string]interface{} } func NewToolChain(name string) *ToolChain { return \u0026ToolChain{ name: name, steps: make([]ToolStep, 0), } } func (tc *ToolChain) AddStep(toolName string, paramMapper func(interface{}, map[string]interface{}) map[string]interface{}) *ToolChain { tc.steps = append(tc.steps, ToolStep{ ToolName: toolName, ParamMapper: paramMapper, }) return tc } func (tc *ToolChain) Name() string { return tc.name } func (tc *ToolChain) Description() string { return fmt.Sprintf(\"Sequential tool chain with %d steps\", len(tc.steps)) } func (tc *ToolChain) ParameterSchema() map[string]core.ParameterDefinition { return map[string]core.ParameterDefinition{ \"initial_params\": { Type: \"object\", Description: \"Initial parameters for the tool chain\", Required: true, }, \"mcp_manager\": { Type: \"object\", Description: \"MCP manager for tool execution\", Required: true, }, } } func (tc *ToolChain) Execute(ctx context.Context, params map[string]interface{}) (interface{}, error) { initialParams, ok := params[\"initial_params\"].(map[string]interface{}) if !ok { return nil, fmt.Errorf(\"initial_params must be an object\") } mcpManager, ok := params[\"mcp_manager\"].(*core.MCPManager) if !ok { return nil, fmt.Errorf(\"mcp_manager must be provided\") } var currentResult interface{} = initialParams results := make([]interface{}, 0, len(tc.steps)) for i, step := range tc.steps { // Get tool tool, err := mcpManager.GetTool(step.ToolName) if err != nil { return nil, fmt.Errorf(\"step %d: failed to get tool %s: %w\", i+1, step.ToolName, err) } // Map parameters var toolParams map[string]interface{} if step.ParamMapper != nil { toolParams = step.ParamMapper(currentResult, initialParams) } else { toolParams = initialParams } // Execute tool result, err := tool.Execute(ctx, toolParams) if err != nil { return nil, fmt.Errorf(\"step %d: tool %s execution failed: %w\", i+1, step.ToolName, err) } currentResult = result results = append(results, result) } return map[string]interface{}{ \"final_result\": currentResult, \"all_results\": results, \"steps\": len(tc.steps), }, nil } // Example: Research and Analysis Chain func CreateResearchChain() *ToolChain { return NewToolChain(\"research_analysis\"). AddStep(\"search\", func(prev interface{}, initial map[string]interface{}) map[string]interface{} { query, _ := initial[\"query\"].(string) return map[string]interface{}{ \"query\": query, \"limit\": 5, } }). AddStep(\"summarizer\", func(prev interface{}, initial map[string]interface{}) map[string]interface{} { searchResults := prev.(map[string]interface{}) return map[string]interface{}{ \"content\": searchResults[\"results\"], \"max_length\": 500, } }). AddStep(\"analyzer\", func(prev interface{}, initial map[string]interface{}) map[string]interface{} { summary := prev.(map[string]interface{}) return map[string]interface{}{ \"text\": summary[\"summary\"], \"analysis_type\": \"sentiment_and_topics\", } }) } ```### 2. Parallel Tool Execution ```go // ParallelToolExecutor runs multiple tools concurrently type ParallelToolExecutor struct { name string tools []ParallelToolConfig aggregator ResultAggregator } type ParallelToolConfig struct { ToolName string Parameters map[string]interface{} Optional bool // If true, failure won't fail the entire execution } type ResultAggregator func(results map[string]interface{}, errors map[string]error) (interface{}, error) func NewParallelToolExecutor(name string, aggregator ResultAggregator) *ParallelToolExecutor { return \u0026ParallelToolExecutor{ name: name, tools: make([]ParallelToolConfig, 0), aggregator: aggregator, } } func (pte *ParallelToolExecutor) AddTool(toolName string, params map[string]interface{}, optional bool) *ParallelToolExecutor { pte.tools = append(pte.tools, ParallelToolConfig{ ToolName: toolName, Parameters: params, Optional: optional, }) return pte } func (pte *ParallelToolExecutor) Name() string { return pte.name } func (pte *ParallelToolExecutor) Description() string { return fmt.Sprintf(\"Parallel execution of %d tools\", len(pte.tools)) } func (pte *ParallelToolExecutor) ParameterSchema() map[string]core.ParameterDefinition { return map[string]core.ParameterDefinition{ \"mcp_manager\": { Type: \"object\", Description: \"MCP manager for tool execution\", Required: true, }, \"timeout\": { Type: \"number\", Description: \"Timeout in seconds for parallel execution\", Required: false, Default: 30, }, } } func (pte *ParallelToolExecutor) Execute(ctx context.Context, params map[string]interface{}) (interface{}, error) { mcpManager, ok := params[\"mcp_manager\"].(*core.MCPManager) if !ok { return nil, fmt.Errorf(\"mcp_manager must be provided\") } timeout := 30 * time.Second if timeoutParam, ok := params[\"timeout\"].(float64); ok { timeout = time.Duration(timeoutParam) * time.Second } // Create context with timeout execCtx, cancel := context.WithTimeout(ctx, timeout) defer cancel() // Execute tools in parallel results := make(map[string]interface{}) errors := make(map[string]error) var wg sync.WaitGroup var mu sync.Mutex for _, toolConfig := range pte.tools { wg.Add(1) go func(config ParallelToolConfig) { defer wg.Done() tool, err := mcpManager.GetTool(config.ToolName) if err != nil { mu.Lock() errors[config.ToolName] = fmt.Errorf(\"failed to get tool: %w\", err) mu.Unlock() return } result, err := tool.Execute(execCtx, config.Parameters) mu.Lock() if err != nil { errors[config.ToolName] = err } else { results[config.ToolName] = result } mu.Unlock() }(toolConfig) } wg.Wait() // Check for required tool failures for _, toolConfig := range pte.tools { if !toolConfig.Optional { if err, exists := errors[toolConfig.ToolName]; exists { return nil, fmt.Errorf(\"required tool %s failed: %w\", toolConfig.ToolName, err) } } } // Aggregate results return pte.aggregator(results, errors) } // Example: Multi-Source Information Gathering func CreateInfoGatheringTool() *ParallelToolExecutor { return NewParallelToolExecutor(\"info_gathering\", func(results map[string]interface{}, errors map[string]error) (interface{}, error) { gathered := map[string]interface{}{ \"sources\": make(map[string]interface{}), \"errors\": make(map[string]string), \"summary\": \"\", } // Collect successful results for toolName, result := range results { gathered[\"sources\"].(map[string]interface{})[toolName] = result } // Collect errors for optional tools for toolName, err := range errors { gathered[\"errors\"].(map[string]string)[toolName] = err.Error() } // Create summary sourceCount := len(results) errorCount := len(errors) gathered[\"summary\"] = fmt.Sprintf(\"Gathered information from %d sources with %d errors\", sourceCount, errorCount) return gathered, nil }). AddTool(\"web_search\", map[string]interface{}{\"query\": \"latest news\"}, false). AddTool(\"weather\", map[string]interface{}{\"location\": \"current\"}, true). AddTool(\"stock_prices\", map[string]interface{}{\"symbols\": []string{\"AAPL\", \"GOOGL\"}}, true). AddTool(\"calendar\", map[string]interface{}{\"days\": 7}, true) }\rConditional Tool Execution 1. Rule-Based Tool Selection // ConditionalToolExecutor executes tools based on conditions type ConditionalToolExecutor struct { name string rules []ExecutionRule } type ExecutionRule struct { Condition func(context.Context, map[string]interface{}) bool ToolName string Parameters func(map[string]interface{}) map[string]interface{} Description string } func NewConditionalToolExecutor(name string) *ConditionalToolExecutor { return \u0026ConditionalToolExecutor{ name: name, rules: make([]ExecutionRule, 0), } } func (cte *ConditionalToolExecutor) AddRule( condition func(context.Context, map[string]interface{}) bool, toolName string, paramMapper func(map[string]interface{}) map[string]interface{}, description string, ) *ConditionalToolExecutor { cte.rules = append(cte.rules, ExecutionRule{ Condition: condition, ToolName: toolName, Parameters: paramMapper, Description: description, }) return cte } func (cte *ConditionalToolExecutor) Name() string { return cte.name } func (cte *ConditionalToolExecutor) Description() string { return fmt.Sprintf(\"Conditional tool executor with %d rules\", len(cte.rules)) } func (cte *ConditionalToolExecutor) ParameterSchema() map[string]core.ParameterDefinition { return map[string]core.ParameterDefinition{ \"input_data\": { Type: \"object\", Description: \"Input data for condition evaluation\", Required: true, }, \"mcp_manager\": { Type: \"object\", Description: \"MCP manager for tool execution\", Required: true, }, } } func (cte *ConditionalToolExecutor) Execute(ctx context.Context, params map[string]interface{}) (interface{}, error) { inputData, ok := params[\"input_data\"].(map[string]interface{}) if !ok { return nil, fmt.Errorf(\"input_data must be an object\") } mcpManager, ok := params[\"mcp_manager\"].(*core.MCPManager) if !ok { return nil, fmt.Errorf(\"mcp_manager must be provided\") } executedRules := make([]map[string]interface{}, 0) for i, rule := range cte.rules { if rule.Condition(ctx, inputData) { // Get tool tool, err := mcpManager.GetTool(rule.ToolName) if err != nil { return nil, fmt.Errorf(\"rule %d: failed to get tool %s: %w\", i+1, rule.ToolName, err) } // Prepare parameters toolParams := rule.Parameters(inputData) // Execute tool result, err := tool.Execute(ctx, toolParams) if err != nil { return nil, fmt.Errorf(\"rule %d: tool %s execution failed: %w\", i+1, rule.ToolName, err) } executedRules = append(executedRules, map[string]interface{}{ \"rule_description\": rule.Description, \"tool_name\": rule.ToolName, \"result\": result, }) } } return map[string]interface{}{ \"executed_rules\": executedRules, \"total_rules\": len(cte.rules), \"matched_rules\": len(executedRules), }, nil } // Example: Smart Assistant Tool Selection func CreateSmartAssistantTool() *ConditionalToolExecutor { return NewConditionalToolExecutor(\"smart_assistant\"). AddRule( func(ctx context.Context, data map[string]interface{}) bool { message, ok := data[\"message\"].(string) return ok \u0026\u0026 strings.Contains(strings.ToLower(message), \"weather\") }, \"weather\", func(data map[string]interface{}) map[string]interface{} { location := \"current\" if loc, ok := data[\"location\"].(string); ok { location = loc } return map[string]interface{}{\"location\": location} }, \"Weather information requested\", ). AddRule( func(ctx context.Context, data map[string]interface{}) bool { message, ok := data[\"message\"].(string) if !ok { return false } mathKeywords := []string{\"calculate\", \"math\", \"+\", \"-\", \"*\", \"/\", \"=\"} msgLower := strings.ToLower(message) for _, keyword := range mathKeywords { if strings.Contains(msgLower, keyword) { return true } } return false }, \"calculator\", func(data map[string]interface{}) map[string]interface{} { // Simple math expression parser would go here return map[string]interface{}{ \"expression\": data[\"message\"], } }, \"Mathematical calculation requested\", ). AddRule( func(ctx context.Context, data map[string]interface{}) bool { message, ok := data[\"message\"].(string) return ok \u0026\u0026 strings.Contains(strings.ToLower(message), \"search\") }, \"web_search\", func(data map[string]interface{}) map[string]interface{} { return map[string]interface{}{ \"query\": data[\"message\"], \"limit\": 5, } }, \"Web search requested\", ) } ```## Error Handling and Recovery Patterns ### 1. Retry with Backoff ```go // RetryableTool wraps a tool with retry logic type RetryableTool struct { tool core.Tool maxRetries int baseDelay time.Duration maxDelay time.Duration backoffFunc func(attempt int, baseDelay time.Duration) time.Duration retryChecker func(error) bool } func NewRetryableTool(tool core.Tool, maxRetries int, baseDelay time.Duration) *RetryableTool { return \u0026RetryableTool{ tool: tool, maxRetries: maxRetries, baseDelay: baseDelay, maxDelay: 5 * time.Minute, backoffFunc: func(attempt int, baseDelay time.Duration) time.Duration { // Exponential backoff with jitter delay := time.Duration(float64(baseDelay) * math.Pow(2, float64(attempt))) jitter := time.Duration(rand.Float64() * float64(delay) * 0.1) return delay + jitter }, retryChecker: func(err error) bool { // Retry on network errors, timeouts, and rate limits errStr := strings.ToLower(err.Error()) return strings.Contains(errStr, \"timeout\") || strings.Contains(errStr, \"network\") || strings.Contains(errStr, \"rate limit\") || strings.Contains(errStr, \"temporary\") }, } } func (rt *RetryableTool) WithMaxDelay(maxDelay time.Duration) *RetryableTool { rt.maxDelay = maxDelay return rt } func (rt *RetryableTool) WithRetryChecker(checker func(error) bool) *RetryableTool { rt.retryChecker = checker return rt } func (rt *RetryableTool) Name() string { return rt.tool.Name() } func (rt *RetryableTool) Description() string { return rt.tool.Description() } func (rt *RetryableTool) ParameterSchema() map[string]core.ParameterDefinition { return rt.tool.ParameterSchema() } func (rt *RetryableTool) Execute(ctx context.Context, params map[string]interface{}) (interface{}, error) { var lastErr error for attempt := 0; attempt \u003c= rt.maxRetries; attempt++ { result, err := rt.tool.Execute(ctx, params) if err == nil { return result, nil } lastErr = err // Check if we should retry if !rt.retryChecker(err) { return nil, fmt.Errorf(\"non-retryable error: %w\", err) } // Don't wait after the last attempt if attempt == rt.maxRetries { break } // Calculate delay delay := rt.backoffFunc(attempt, rt.baseDelay) if delay \u003e rt.maxDelay { delay = rt.maxDelay } // Wait with context cancellation support select { case \u003c-ctx.Done(): return nil, ctx.Err() case \u003c-time.After(delay): // Continue to next attempt } } return nil, fmt.Errorf(\"max retries (%d) exceeded, last error: %w\", rt.maxRetries, lastErr) }\r2. Circuit Breaker Pattern // CircuitBreakerTool implements circuit breaker pattern for tools type CircuitBreakerTool struct { tool core.Tool failureThreshold int resetTimeout time.Duration state CircuitState failures int lastFailureTime time.Time mu sync.RWMutex } type CircuitState int const ( CircuitClosed CircuitState = iota CircuitOpen CircuitHalfOpen ) func NewCircuitBreakerTool(tool core.Tool, failureThreshold int, resetTimeout time.Duration) *CircuitBreakerTool { return \u0026CircuitBreakerTool{ tool: tool, failureThreshold: failureThreshold, resetTimeout: resetTimeout, state: CircuitClosed, } } func (cbt *CircuitBreakerTool) Name() string { return cbt.tool.Name() } func (cbt *CircuitBreakerTool) Description() string { return cbt.tool.Description() } func (cbt *CircuitBreakerTool) ParameterSchema() map[string]core.ParameterDefinition { return cbt.tool.ParameterSchema() } func (cbt *CircuitBreakerTool) Execute(ctx context.Context, params map[string]interface{}) (interface{}, error) { cbt.mu.Lock() // Check if circuit should be reset if cbt.state == CircuitOpen \u0026\u0026 time.Since(cbt.lastFailureTime) \u003e cbt.resetTimeout { cbt.state = CircuitHalfOpen cbt.failures = 0 } // Fail fast if circuit is open if cbt.state == CircuitOpen { cbt.mu.Unlock() return nil, fmt.Errorf(\"circuit breaker is open for tool %s\", cbt.tool.Name()) } cbt.mu.Unlock() // Execute tool result, err := cbt.tool.Execute(ctx, params) cbt.mu.Lock() defer cbt.mu.Unlock() if err != nil { cbt.failures++ cbt.lastFailureTime = time.Now() // Open circuit if threshold exceeded if cbt.failures \u003e= cbt.failureThreshold { cbt.state = CircuitOpen } return nil, err } // Success - reset circuit if it was half-open if cbt.state == CircuitHalfOpen { cbt.state = CircuitClosed cbt.failures = 0 } return result, nil } func (cbt *CircuitBreakerTool) GetState() (CircuitState, int, time.Time) { cbt.mu.RLock() defer cbt.mu.RUnlock() return cbt.state, cbt.failures, cbt.lastFailureTime }\r3. Fallback Tool Pattern // FallbackTool tries primary tool first, then fallbacks type FallbackTool struct { name string description string primaryTool core.Tool fallbackTools []FallbackConfig } type FallbackConfig struct { Tool core.Tool Condition func(error) bool ParamMapper func(map[string]interface{}) map[string]interface{} } func NewFallbackTool(name, description string, primaryTool core.Tool) *FallbackTool { return \u0026FallbackTool{ name: name, description: description, primaryTool: primaryTool, fallbackTools: make([]FallbackConfig, 0), } } func (ft *FallbackTool) AddFallback( tool core.Tool, condition func(error) bool, paramMapper func(map[string]interface{}) map[string]interface{}, ) *FallbackTool { ft.fallbackTools = append(ft.fallbackTools, FallbackConfig{ Tool: tool, Condition: condition, ParamMapper: paramMapper, }) return ft } func (ft *FallbackTool) Name() string { return ft.name } func (ft *FallbackTool) Description() string { return ft.description } func (ft *FallbackTool) ParameterSchema() map[string]core.ParameterDefinition { return ft.primaryTool.ParameterSchema() } func (ft *FallbackTool) Execute(ctx context.Context, params map[string]interface{}) (interface{}, error) { // Try primary tool first result, err := ft.primaryTool.Execute(ctx, params) if err == nil { return map[string]interface{}{ \"result\": result, \"tool_used\": ft.primaryTool.Name(), \"fallback\": false, }, nil } primaryError := err // Try fallback tools for i, fallback := range ft.fallbackTools { if fallback.Condition(err) { // Map parameters if needed fallbackParams := params if fallback.ParamMapper != nil { fallbackParams = fallback.ParamMapper(params) } result, err := fallback.Tool.Execute(ctx, fallbackParams) if err == nil { return map[string]interface{}{ \"result\": result, \"tool_used\": fallback.Tool.Name(), \"fallback\": true, \"fallback_index\": i, \"primary_error\": primaryError.Error(), }, nil } } } return nil, fmt.Errorf(\"primary tool and all fallbacks failed, primary error: %w\", primaryError) } // Example: Weather with Multiple Sources func CreateWeatherWithFallbacks() *FallbackTool { primaryWeather := tools.NewWeatherTool(os.Getenv(\"OPENWEATHER_API_KEY\")) return NewFallbackTool(\"weather_with_fallbacks\", \"Weather information with multiple sources\", primaryWeather). AddFallback( tools.NewWeatherAPITool(os.Getenv(\"WEATHERAPI_KEY\")), func(err error) bool { return strings.Contains(err.Error(), \"api key\") || strings.Contains(err.Error(), \"rate limit\") }, func(params map[string]interface{}) map[string]interface{} { // Convert location format if needed return params }, ). AddFallback( tools.NewMockWeatherTool(), // Returns mock data func(err error) bool { return true // Always try mock as last resort }, nil, ) }\rBest Practices and Guidelines 1. Tool Design Principles Composability: Design tools that can be easily combined Idempotency: Ensure tools can be safely retried Observability: Include comprehensive monitoring and logging Graceful Degradation: Implement fallback mechanisms Configuration: Make tools configurable for different environments 2. Performance Optimization Caching: Cache expensive operations and API calls Parallel Execution: Use parallel execution where appropriate Connection Pooling: Reuse connections for external services Batch Operations: Combine multiple operations when possible Resource Management: Properly manage resources and connections 3. Error Handling Strategy Categorize Errors: Distinguish between retryable and non-retryable errors Circuit Breakers: Protect against cascading failures Fallback Mechanisms: Provide alternative execution paths Monitoring: Track error patterns and rates User Experience: Provide meaningful error messages Conclusion Advanced tool patterns enable you to build sophisticated, production-ready agent systems that can handle complex workflows, recover from failures, and scale effectively. By combining these patterns, you can create robust tool ecosystems that enhance your agents’ capabilities while maintaining reliability and performance.",
    "tags": [],
    "title": "advanced-tool-patterns",
    "uri": "/AgenticGoKitDocs/tutorials/mcp/advanced-tool-patterns/index.html"
  },
  {
    "breadcrumb": "content \u003e reference",
    "content": "Agent API Building individual agents and agent handlers\nThis document covers the Agent API in AgenticGoKit, which provides the foundation for creating individual agents that can process events, maintain state, and participate in multi-agent orchestrations.\n📋 Core Interfaces Agent Interface The basic Agent interface for simple state transformations:\ntype Agent interface { // Run processes the input State and returns an output State or an error Run(ctx context.Context, inputState State) (State, error) // Name returns the unique identifier name of the agent Name() string }\rAgentHandler Interface The enhanced AgentHandler interface for event-driven processing:\ntype AgentHandler interface { Run(ctx context.Context, event Event, state State) (AgentResult, error) }\rAgentHandlerFunc A function type that implements AgentHandler:\ntype AgentHandlerFunc func(ctx context.Context, event Event, state State) (AgentResult, error) func (f AgentHandlerFunc) Run(ctx context.Context, event Event, state State) (AgentResult, error) { return f(ctx, event, state) }\r🚀 Basic Usage Creating a Simple Agent package main import ( \"context\" \"fmt\" \"github.com/kunalkushwaha/agenticgokit/core\" ) // Method 1: Using AgentHandlerFunc func main() { agent := core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Get data from event message, ok := event.Data[\"message\"].(string) if !ok { return core.AgentResult{}, fmt.Errorf(\"missing message in event\") } // Process the message response := fmt.Sprintf(\"Hello, %s!\", message) // Return result return core.AgentResult{ Data: map[string]interface{}{ \"response\": response, \"processed_at\": time.Now().Unix(), }, }, nil }) // Test the agent event := core.NewEvent(\"greeting\", map[string]interface{}{ \"message\": \"World\", }) state := core.NewState() result, err := agent.Run(context.Background(), event, state) if err != nil { panic(err) } fmt.Printf(\"Response: %s\\n\", result.Data[\"response\"]) }\r🏗️ Agent Implementation Patterns Struct-Based Agent type ChatAgent struct { llm core.LLMProvider name string } func NewChatAgent(name string, llm core.LLMProvider) *ChatAgent { return \u0026ChatAgent{ name: name, llm: llm, } } func (a *ChatAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Extract query from event query, ok := event.Data[\"query\"].(string) if !ok { return core.AgentResult{}, fmt.Errorf(\"missing query in event data\") } // Get conversation history from state history, _ := state.Get(\"history\") var messages []string if history != nil { messages = history.([]string) } // Build prompt with context prompt := buildPrompt(query, messages) // Call LLM response, err := a.llm.Complete(ctx, prompt) if err != nil { return core.AgentResult{}, fmt.Errorf(\"LLM error: %w\", err) } // Update conversation history updatedHistory := append(messages, query, response) state.Set(\"history\", updatedHistory) return core.AgentResult{ Data: map[string]interface{}{ \"response\": response, \"query\": query, }, }, nil } func buildPrompt(query string, history []string) string { var prompt strings.Builder prompt.WriteString(\"You are a helpful assistant.\\n\\n\") // Add conversation history for i := 0; i \u003c len(history); i += 2 { if i+1 \u003c len(history) { prompt.WriteString(fmt.Sprintf(\"User: %s\\n\", history[i])) prompt.WriteString(fmt.Sprintf(\"Assistant: %s\\n\\n\", history[i+1])) } } prompt.WriteString(fmt.Sprintf(\"User: %s\\n\", query)) return prompt.String() }\rStateless Processing Agent type DataProcessorAgent struct{} func (a *DataProcessorAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Extract data from event data, ok := event.Data[\"data\"].([]interface{}) if !ok { return core.AgentResult{}, fmt.Errorf(\"missing data array in event\") } // Process data (example: calculate sum) var sum float64 var count int for _, item := range data { if num, ok := item.(float64); ok { sum += num count++ } } var average float64 if count \u003e 0 { average = sum / float64(count) } return core.AgentResult{ Data: map[string]interface{}{ \"sum\": sum, \"count\": count, \"average\": average, }, }, nil }\rTool-Using Agent type ResearchAgent struct { llm core.LLMProvider mcp core.MCPManager } func NewResearchAgent(llm core.LLMProvider, mcp core.MCPManager) *ResearchAgent { return \u0026ResearchAgent{ llm: llm, mcp: mcp, } } func (a *ResearchAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { query, ok := event.GetData()[\"query\"].(string) if !ok { return core.AgentResult{}, fmt.Errorf(\"missing query in event data\") } // Get available tools tools, err := a.mcp.ListTools(ctx) if err != nil { return core.AgentResult{}, fmt.Errorf(\"failed to list tools: %w\", err) } // Build prompt with tool information toolPrompt := core.FormatToolsForPrompt(ctx, a.mcp) prompt := fmt.Sprintf(`You are a research assistant with access to tools. Available tools: %s Research query: %s Use the appropriate tools to research this query and provide a comprehensive answer.`, toolPrompt, query) // Generate response response, err := a.llm.Complete(ctx, prompt) if err != nil { return core.AgentResult{}, fmt.Errorf(\"LLM generation failed: %w\", err) } // Execute any tool calls found in the response toolResults := core.ParseAndExecuteToolCalls(ctx, a.mcp, response) // If tools were used, synthesize final response var finalResponse string if len(toolResults) \u003e 0 { synthesisPrompt := fmt.Sprintf(`Based on the research findings below, provide a comprehensive answer to: %s Research findings: %v Please synthesize this information into a clear, well-structured response.`, query, toolResults) finalResponse, err = a.llm.Complete(ctx, synthesisPrompt) if err != nil { finalResponse = response // Fallback to original response } } else { finalResponse = response } // Store results in state state.Set(\"response\", finalResponse) state.Set(\"tools_used\", len(toolResults) \u003e 0) state.Set(\"tool_count\", len(toolResults)) return core.AgentResult{ OutputState: state, }, nil }\r📊 Agent Result Structure AgentResult Type type AgentResult struct { // OutputState contains the updated state after agent execution OutputState State `json:\"output_state\"` // Error contains any error message (empty string if successful) Error string `json:\"error,omitempty\"` // StartTime and EndTime track execution timing StartTime time.Time `json:\"start_time\"` EndTime time.Time `json:\"end_time\"` // Duration tracks how long the agent took to execute Duration time.Duration `json:\"duration\"` }\rResult Examples Simple Response return core.AgentResult{ Data: map[string]interface{}{ \"answer\": \"Paris is the capital of France\", \"confidence\": 0.95, }, Success: true, }, nil\rResponse with State Updates // Update conversation state state.Set(\"last_query\", query) state.Set(\"query_count\", state.GetInt(\"query_count\")+1) return core.AgentResult{ Data: map[string]interface{}{ \"response\": answer, }, State: state, Success: true, }, nil\rResponse with Metadata return core.AgentResult{ Data: map[string]interface{}{ \"response\": answer, }, Metadata: map[string]interface{}{ \"execution_time\": time.Since(start), \"tokens_used\": tokenCount, \"model\": \"gpt-4\", \"tools_called\": []string{\"search\", \"calculator\"}, }, Success: true, }, nil\rPartial Success with Errors return core.AgentResult{ Data: map[string]interface{}{ \"response\": partialAnswer, }, Errors: []error{ fmt.Errorf(\"tool 'advanced_search' failed: %w\", searchErr), fmt.Errorf(\"cache miss for query: %s\", query), }, Success: true, // Still successful despite errors }, nil\r🔧 Agent Builder Pattern AgentBuilder Interface type AgentBuilder interface { WithName(name string) AgentBuilder WithLLM(provider LLMProvider) AgentBuilder WithMCP(config MCPConfig) AgentBuilder WithCapabilities(caps ...Capability) AgentBuilder WithMiddleware(middleware ...MiddlewareFunc) AgentBuilder Build() (Agent, error) }\rUsage Example agent, err := core.NewAgentBuilder(). WithName(\"research-assistant\"). WithLLM(azureLLM). WithMCP(mcpConfig). WithCapabilities( core.SearchCapability, core.CalculationCapability, core.MemoryCapability, ). WithMiddleware( LoggingMiddleware, AuthenticationMiddleware, RateLimitMiddleware, ). Build() if err != nil { log.Fatal(\"Failed to build agent:\", err) }\r🎭 Agent Capabilities Built-in Capabilities SearchCapability agent := core.NewAgentBuilder(). WithCapabilities(core.SearchCapability). Build()\rCalculationCapability agent := core.NewAgentBuilder(). WithCapabilities(core.CalculationCapability). Build()\rMemoryCapability agent := core.NewAgentBuilder(). WithCapabilities(core.MemoryCapability). Build()\rFileCapability agent := core.NewAgentBuilder(). WithCapabilities(core.FileCapability). Build()\rCustom Capabilities type CustomCapability struct { name string } func (c *CustomCapability) Name() string { return c.name } func (c *CustomCapability) Configure(agent Agent) error { // Configure the agent with custom functionality return nil } func (c *CustomCapability) Dependencies() []string { return []string{\"SearchCapability\"} // Depends on search }\r🔄 Agent Middleware Middleware Function Type type MiddlewareFunc func(next AgentHandler) AgentHandler\rCommon Middleware Examples Logging Middleware func LoggingMiddleware(next core.AgentHandler) core.AgentHandler { return core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { start := time.Now() log.Printf(\"Agent request started: %s\", event.GetType()) result, err := next.Run(ctx, event, state) duration := time.Since(start) if err != nil { log.Printf(\"Agent request failed: %s (duration: %v, error: %v)\", event.GetType(), duration, err) } else { log.Printf(\"Agent request completed: %s (duration: %v)\", event.GetType(), duration) } return result, err }) }\rAuthentication Middleware func AuthenticationMiddleware(next core.AgentHandler) core.AgentHandler { return core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Check authentication userID := event.GetUserID() if !isAuthenticated(userID) { return core.AgentResult{}, fmt.Errorf(\"unauthorized\") } return next.Run(ctx, event, state) }) }\rRate Limiting Middleware func RateLimitMiddleware(limiter *rate.Limiter) func(core.AgentHandler) core.AgentHandler { return func(next core.AgentHandler) core.AgentHandler { return core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { if !limiter.Allow() { return core.AgentResult{}, fmt.Errorf(\"rate limit exceeded\") } return next.Run(ctx, event, state) }) } }\rTimeout Middleware func TimeoutMiddleware(timeout time.Duration) func(core.AgentHandler) core.AgentHandler { return func(next core.AgentHandler) core.AgentHandler { return core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { timeoutCtx, cancel := context.WithTimeout(ctx, timeout) defer cancel() type result struct { res core.AgentResult err error } resultChan := make(chan result, 1) go func() { res, err := next.Run(timeoutCtx, event, state) resultChan \u003c- result{res, err} }() select { case r := \u003c-resultChan: return r.res, r.err case \u003c-timeoutCtx.Done(): return core.AgentResult{}, fmt.Errorf(\"agent timeout after %v\", timeout) } }) } }\r🧪 Testing Agents Test Utilities func TestChatAgent(t *testing.T) { // Create mock LLM mockLLM := \u0026MockLLMProvider{ responses: map[string]string{ \"Hello\": \"Hi there! How can I help you?\", }, } // Create agent agent := NewChatAgent(\"test-agent\", mockLLM) // Create test event and state event := core.NewEvent(\"chat\", map[string]interface{}{ \"query\": \"Hello\", }) state := core.NewState() // Run agent result, err := agent.Run(context.Background(), event, state) // Assertions assert.NoError(t, err) assert.True(t, result.Success) assert.Equal(t, \"Hi there! How can I help you?\", result.Data[\"response\"]) // Verify state was updated history, exists := result.State.Get(\"history\") assert.True(t, exists) assert.Equal(t, []string{\"Hello\", \"Hi there! How can I help you?\"}, history) }\rMock LLM Provider type MockLLMProvider struct { responses map[string]string } func (m *MockLLMProvider) Complete(ctx context.Context, prompt string) (string, error) { for key, response := range m.responses { if strings.Contains(prompt, key) { return response, nil } } return \"I don't understand\", nil } func (m *MockLLMProvider) Name() string { return \"mock\" }\r📚 Best Practices Error Handling func (a *MyAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Validate input query, ok := event.Data[\"query\"].(string) if !ok { return core.AgentResult{}, fmt.Errorf(\"missing or invalid query in event data\") } if strings.TrimSpace(query) == \"\" { return core.AgentResult{}, fmt.Errorf(\"query cannot be empty\") } // Process with error handling result, err := a.processQuery(ctx, query) if err != nil { return core.AgentResult{}, fmt.Errorf(\"failed to process query: %w\", err) } return core.AgentResult{ Data: map[string]interface{}{ \"response\": result, }, Success: true, }, nil }\rContext Handling func (a *MyAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Check if context is cancelled select { case \u003c-ctx.Done(): return core.AgentResult{}, ctx.Err() default: } // Pass context to all operations result, err := a.llm.Complete(ctx, prompt) if err != nil { return core.AgentResult{}, err } // Check context again for long operations select { case \u003c-ctx.Done(): return core.AgentResult{}, ctx.Err() default: } return core.AgentResult{ Data: map[string]interface{}{ \"response\": result, }, Success: true, }, nil }\rState Management func (a *MyAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Read from state safely conversationHistory, _ := state.Get(\"conversation\") var history []string if conversationHistory != nil { if h, ok := conversationHistory.([]string); ok { history = h } } // Process query query := event.Data[\"query\"].(string) response, err := a.processWithHistory(ctx, query, history) if err != nil { return core.AgentResult{}, err } // Update state updatedHistory := append(history, query, response) state.Set(\"conversation\", updatedHistory) state.Set(\"last_interaction\", time.Now().Unix()) return core.AgentResult{ Data: map[string]interface{}{ \"response\": response, }, Success: true, }, nil }\rThis comprehensive Agent API reference covers all aspects of building and using agents in AgenticGoKit, from basic implementations to advanced patterns and best practices.",
    "description": "Agent API Building individual agents and agent handlers\nThis document covers the Agent API in AgenticGoKit, which provides the foundation for creating individual agents that can process events, maintain state, and participate in multi-agent orchestrations.\n📋 Core Interfaces Agent Interface The basic Agent interface for simple state transformations:\ntype Agent interface { // Run processes the input State and returns an output State or an error Run(ctx context.Context, inputState State) (State, error) // Name returns the unique identifier name of the agent Name() string }\rAgentHandler Interface The enhanced AgentHandler interface for event-driven processing:",
    "tags": [],
    "title": "agent",
    "uri": "/AgenticGoKitDocs/reference/api/agent/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials \u003e core-concepts",
    "content": "Agent Lifecycle in AgenticGoKit Overview Understanding the agent lifecycle is fundamental to building effective multi-agent systems. This tutorial explores how agents are created, initialized, executed, and cleaned up in AgenticGoKit, along with best practices for managing agent resources and state.\nThe agent lifecycle encompasses everything from agent creation and configuration to execution patterns and resource cleanup.\nPrerequisites Basic understanding of Go programming Familiarity with Message Passing and Event Flow Knowledge of State Management Agent Lifecycle Phases 1. Creation and Configuration Agents go through several phases during their lifecycle:\n┌─────────────┐ ┌──────────────┐ ┌─────────────┐ ┌─────────────┐\r│ Creation │───▶│Configuration │───▶│Initialization│───▶│ Ready │\r└─────────────┘ └──────────────┘ └─────────────┘ └─────────────┘\r│\r┌─────────────┐ ┌──────────────┐ ┌─────────────┐ │\r│ Cleanup │◀───│ Shutdown │◀───│ Execution │◀──────────┘\r└─────────────┘ └──────────────┘ └─────────────┘\rAgent Creation Patterns 1. Builder Pattern AgenticGoKit uses the builder pattern for agent creation:\n// Create an agent using the builder pattern agent, err := core.NewAgent(\"my-agent\"). WithLLMAndConfig(provider, core.LLMConfig{ SystemPrompt: \"You are a helpful assistant.\", Temperature: 0.7, MaxTokens: 1000, }). WithMemory(memorySystem). WithMCP(mcpManager). WithMetrics(). Build() if err != nil { log.Fatalf(\"Failed to create agent: %v\", err) }\r2. Factory Pattern For complex agent creation scenarios:\ntype AgentFactory struct { defaultConfig AgentConfig providers map[string]LLMProvider memory Memory } func (f *AgentFactory) CreateAgent(agentType string, config AgentConfig) (Agent, error) { switch agentType { case \"research\": return f.createResearchAgent(config) case \"analysis\": return f.createAnalysisAgent(config) case \"writing\": return f.createWritingAgent(config) default: return nil, fmt.Errorf(\"unknown agent type: %s\", agentType) } } func (f *AgentFactory) createResearchAgent(config AgentConfig) (Agent, error) { return core.NewAgent(\"research-agent\"). WithLLMAndConfig(f.providers[\"research\"], core.LLMConfig{ SystemPrompt: \"You are a research specialist...\", Temperature: 0.3, }). WithMemory(f.memory). WithMCP(f.getMCPForResearch()). Build() }\r3. Configuration-Based Creation Create agents from configuration files:\ntype AgentSpec struct { Name string `yaml:\"name\"` Type string `yaml:\"type\"` LLM LLMConfig `yaml:\"llm\"` Memory MemoryConfig `yaml:\"memory\"` MCP MCPConfig `yaml:\"mcp\"` Metadata map[string]string `yaml:\"metadata\"` } func CreateAgentFromSpec(spec AgentSpec) (Agent, error) { builder := core.NewAgent(spec.Name) // Configure LLM if spec.LLM.Provider != \"\" { provider, err := createLLMProvider(spec.LLM) if err != nil { return nil, err } builder = builder.WithLLMAndConfig(provider, spec.LLM) } // Configure memory if spec.Memory.Enabled { memory, err := createMemorySystem(spec.Memory) if err != nil { return nil, err } builder = builder.WithMemory(memory) } // Configure MCP if len(spec.MCP.Tools) \u003e 0 { mcpManager, err := createMCPManager(spec.MCP) if err != nil { return nil, err } builder = builder.WithMCP(mcpManager) } return builder.Build() }\rAgent Initialization 1. Initialization Hooks Agents can implement initialization logic:\ntype InitializableAgent interface { Agent Initialize(ctx context.Context) error IsInitialized() bool } type MyAgent struct { name string llm LLMProvider initialized bool resources []Resource } func (a *MyAgent) Initialize(ctx context.Context) error { if a.initialized { return nil } // Initialize resources for _, resource := range a.resources { if err := resource.Initialize(ctx); err != nil { return fmt.Errorf(\"failed to initialize resource: %w\", err) } } // Perform any setup tasks if err := a.setupTasks(ctx); err != nil { return fmt.Errorf(\"setup tasks failed: %w\", err) } a.initialized = true return nil } func (a *MyAgent) IsInitialized() bool { return a.initialized }\r2. Lazy Initialization Initialize resources only when needed:\ntype LazyAgent struct { name string config AgentConfig llm LLMProvider initOnce sync.Once initError error } func (a *LazyAgent) ensureInitialized(ctx context.Context) error { a.initOnce.Do(func() { a.initError = a.initialize(ctx) }) return a.initError } func (a *LazyAgent) Run(ctx context.Context, event Event, state State) (AgentResult, error) { if err := a.ensureInitialized(ctx); err != nil { return AgentResult{}, fmt.Errorf(\"initialization failed: %w\", err) } // Normal execution return a.execute(ctx, event, state) }\rAgent Execution Lifecycle 1. Pre-Execution Phase Before each agent execution:\nfunc (a *MyAgent) Run(ctx context.Context, event Event, state State) (AgentResult, error) { // Pre-execution setup executionID := generateExecutionID() startTime := time.Now() // Log execution start a.logger.Info(\"Agent execution started\", \"agent\", a.name, \"execution_id\", executionID, \"event_id\", event.GetID()) // Validate inputs if err := a.validateInputs(event, state); err != nil { return AgentResult{}, fmt.Errorf(\"input validation failed: %w\", err) } // Setup execution context execCtx := a.setupExecutionContext(ctx, executionID) // Execute main logic result, err := a.execute(execCtx, event, state) // Post-execution cleanup duration := time.Since(startTime) a.recordMetrics(executionID, duration, err) return result, err }\r2. Execution Context Management Manage execution-specific context:\ntype ExecutionContext struct { ExecutionID string StartTime time.Time Metadata map[string]interface{} Resources map[string]interface{} Cleanup []func() } func (a *MyAgent) setupExecutionContext(ctx context.Context, executionID string) context.Context { execCtx := \u0026ExecutionContext{ ExecutionID: executionID, StartTime: time.Now(), Metadata: make(map[string]interface{}), Resources: make(map[string]interface{}), Cleanup: make([]func(), 0), } return context.WithValue(ctx, \"execution_context\", execCtx) } func (a *MyAgent) execute(ctx context.Context, event Event, state State) (AgentResult, error) { execCtx := ctx.Value(\"execution_context\").(*ExecutionContext) defer a.cleanup(execCtx) // Main execution logic return a.processEvent(ctx, event, state) } func (a *MyAgent) cleanup(execCtx *ExecutionContext) { for _, cleanupFunc := range execCtx.Cleanup { cleanupFunc() } }\r3. Resource Management During Execution Manage resources throughout execution:\ntype ResourceManager struct { resources map[string]Resource mu sync.RWMutex } func (rm *ResourceManager) AcquireResource(ctx context.Context, name string) (Resource, error) { rm.mu.Lock() defer rm.mu.Unlock() resource, exists := rm.resources[name] if !exists { return nil, fmt.Errorf(\"resource not found: %s\", name) } if err := resource.Acquire(ctx); err != nil { return nil, fmt.Errorf(\"failed to acquire resource %s: %w\", name, err) } return resource, nil } func (rm *ResourceManager) ReleaseResource(name string) error { rm.mu.RLock() resource, exists := rm.resources[name] rm.mu.RUnlock() if !exists { return fmt.Errorf(\"resource not found: %s\", name) } return resource.Release() }\rAgent State Management 1. Agent Internal State Manage agent’s internal state across executions:\ntype StatefulAgent struct { name string internalState map[string]interface{} stateMutex sync.RWMutex persistence StatePersistence } func (a *StatefulAgent) GetInternalState(key string) (interface{}, bool) { a.stateMutex.RLock() defer a.stateMutex.RUnlock() value, exists := a.internalState[key] return value, exists } func (a *StatefulAgent) SetInternalState(key string, value interface{}) { a.stateMutex.Lock() defer a.stateMutex.Unlock() a.internalState[key] = value // Persist state if configured if a.persistence != nil { a.persistence.SaveState(a.name, a.internalState) } } func (a *StatefulAgent) Run(ctx context.Context, event Event, state State) (AgentResult, error) { // Load persisted state on first run if len(a.internalState) == 0 \u0026\u0026 a.persistence != nil { if persistedState, err := a.persistence.LoadState(a.name); err == nil { a.internalState = persistedState } } // Use internal state in processing return a.processWithInternalState(ctx, event, state) }\r2. State Persistence Persist agent state across restarts:\ntype StatePersistence interface { SaveState(agentName string, state map[string]interface{}) error LoadState(agentName string) (map[string]interface{}, error) DeleteState(agentName string) error } type FileStatePersistence struct { baseDir string } func (fsp *FileStatePersistence) SaveState(agentName string, state map[string]interface{}) error { filename := filepath.Join(fsp.baseDir, agentName+\".json\") data, err := json.Marshal(state) if err != nil { return err } return os.WriteFile(filename, data, 0644) } func (fsp *FileStatePersistence) LoadState(agentName string) (map[string]interface{}, error) { filename := filepath.Join(fsp.baseDir, agentName+\".json\") data, err := os.ReadFile(filename) if err != nil { return nil, err } var state map[string]interface{} err = json.Unmarshal(data, \u0026state) return state, err }\rAgent Health and Monitoring 1. Health Checks Implement health monitoring for agents:\ntype HealthStatus int const ( HealthStatusHealthy HealthStatus = iota HealthStatusDegraded HealthStatusUnhealthy ) type HealthCheckable interface { HealthCheck(ctx context.Context) HealthStatus GetHealthDetails() map[string]interface{} } type MyAgent struct { name string lastExecution time.Time errorCount int64 successCount int64 resources []HealthCheckable } func (a *MyAgent) HealthCheck(ctx context.Context) HealthStatus { // Check if agent has been executing recently if time.Since(a.lastExecution) \u003e 5*time.Minute { return HealthStatusDegraded } // Check error rate totalExecutions := a.errorCount + a.successCount if totalExecutions \u003e 0 { errorRate := float64(a.errorCount) / float64(totalExecutions) if errorRate \u003e 0.5 { return HealthStatusUnhealthy } else if errorRate \u003e 0.2 { return HealthStatusDegraded } } // Check resource health for _, resource := range a.resources { if resource.HealthCheck(ctx) == HealthStatusUnhealthy { return HealthStatusDegraded } } return HealthStatusHealthy } func (a *MyAgent) GetHealthDetails() map[string]interface{} { return map[string]interface{}{ \"name\": a.name, \"last_execution\": a.lastExecution, \"error_count\": a.errorCount, \"success_count\": a.successCount, \"error_rate\": float64(a.errorCount) / float64(a.errorCount + a.successCount), } }\r2. Performance Monitoring Monitor agent performance metrics:\ntype PerformanceMonitor struct { agentName string executionTimes []time.Duration memoryUsage []int64 maxHistorySize int mu sync.RWMutex } func (pm *PerformanceMonitor) RecordExecution(duration time.Duration, memoryUsed int64) { pm.mu.Lock() defer pm.mu.Unlock() pm.executionTimes = append(pm.executionTimes, duration) pm.memoryUsage = append(pm.memoryUsage, memoryUsed) // Keep only recent history if len(pm.executionTimes) \u003e pm.maxHistorySize { pm.executionTimes = pm.executionTimes[1:] pm.memoryUsage = pm.memoryUsage[1:] } } func (pm *PerformanceMonitor) GetAverageExecutionTime() time.Duration { pm.mu.RLock() defer pm.mu.RUnlock() if len(pm.executionTimes) == 0 { return 0 } var total time.Duration for _, duration := range pm.executionTimes { total += duration } return total / time.Duration(len(pm.executionTimes)) }\rAgent Cleanup and Shutdown 1. Graceful Shutdown Implement graceful shutdown for agents:\ntype GracefulAgent interface { Agent Shutdown(ctx context.Context) error } type MyAgent struct { name string resources []Resource shutdown chan struct{} wg sync.WaitGroup } func (a *MyAgent) Shutdown(ctx context.Context) error { // Signal shutdown close(a.shutdown) // Wait for ongoing operations with timeout done := make(chan struct{}) go func() { a.wg.Wait() close(done) }() select { case \u003c-done: // All operations completed case \u003c-ctx.Done(): return ctx.Err() } // Cleanup resources var errors []error for _, resource := range a.resources { if err := resource.Close(); err != nil { errors = append(errors, err) } } if len(errors) \u003e 0 { return fmt.Errorf(\"cleanup errors: %v\", errors) } return nil } func (a *MyAgent) Run(ctx context.Context, event Event, state State) (AgentResult, error) { // Check if shutting down select { case \u003c-a.shutdown: return AgentResult{}, errors.New(\"agent is shutting down\") default: } // Track ongoing operation a.wg.Add(1) defer a.wg.Done() // Execute normally return a.execute(ctx, event, state) }\r2. Resource Cleanup Ensure proper resource cleanup:\ntype ResourceCleanup struct { resources []CleanupFunc mu sync.Mutex } type CleanupFunc func() error func (rc *ResourceCleanup) AddCleanup(cleanup CleanupFunc) { rc.mu.Lock() defer rc.mu.Unlock() rc.resources = append(rc.resources, cleanup) } func (rc *ResourceCleanup) Cleanup() error { rc.mu.Lock() defer rc.mu.Unlock() var errors []error // Cleanup in reverse order for i := len(rc.resources) - 1; i \u003e= 0; i-- { if err := rc.resources[i](); err != nil { errors = append(errors, err) } } if len(errors) \u003e 0 { return fmt.Errorf(\"cleanup errors: %v\", errors) } return nil }\rAgent Lifecycle Management 1. Agent Manager Centralized agent lifecycle management:\ntype AgentManager struct { agents map[string]Agent lifecycle map[string]*AgentLifecycle mu sync.RWMutex } type AgentLifecycle struct { Agent Agent Status AgentStatus CreatedAt time.Time StartedAt *time.Time StoppedAt *time.Time HealthCheck HealthCheckable Monitor *PerformanceMonitor } type AgentStatus int const ( AgentStatusCreated AgentStatus = iota AgentStatusStarted AgentStatusStopped AgentStatusError ) func (am *AgentManager) RegisterAgent(name string, agent Agent) error { am.mu.Lock() defer am.mu.Unlock() if _, exists := am.agents[name]; exists { return fmt.Errorf(\"agent already registered: %s\", name) } am.agents[name] = agent am.lifecycle[name] = \u0026AgentLifecycle{ Agent: agent, Status: AgentStatusCreated, CreatedAt: time.Now(), Monitor: \u0026PerformanceMonitor{agentName: name, maxHistorySize: 100}, } return nil } func (am *AgentManager) StartAgent(ctx context.Context, name string) error { am.mu.Lock() lifecycle, exists := am.lifecycle[name] am.mu.Unlock() if !exists { return fmt.Errorf(\"agent not found: %s\", name) } // Initialize if needed if initializable, ok := lifecycle.Agent.(InitializableAgent); ok { if err := initializable.Initialize(ctx); err != nil { lifecycle.Status = AgentStatusError return fmt.Errorf(\"agent initialization failed: %w\", err) } } now := time.Now() lifecycle.StartedAt = \u0026now lifecycle.Status = AgentStatusStarted return nil } func (am *AgentManager) StopAgent(ctx context.Context, name string) error { am.mu.Lock() lifecycle, exists := am.lifecycle[name] am.mu.Unlock() if !exists { return fmt.Errorf(\"agent not found: %s\", name) } // Graceful shutdown if supported if graceful, ok := lifecycle.Agent.(GracefulAgent); ok { if err := graceful.Shutdown(ctx); err != nil { return fmt.Errorf(\"graceful shutdown failed: %w\", err) } } now := time.Now() lifecycle.StoppedAt = \u0026now lifecycle.Status = AgentStatusStopped return nil }\r2. Lifecycle Events Emit events during lifecycle transitions:\ntype LifecycleEvent struct { AgentName string Event LifecycleEventType Timestamp time.Time Data map[string]interface{} } type LifecycleEventType string const ( LifecycleEventCreated LifecycleEventType = \"created\" LifecycleEventInitialized LifecycleEventType = \"initialized\" LifecycleEventStarted LifecycleEventType = \"started\" LifecycleEventStopped LifecycleEventType = \"stopped\" LifecycleEventError LifecycleEventType = \"error\" ) type LifecycleEventEmitter struct { listeners []LifecycleEventListener mu sync.RWMutex } type LifecycleEventListener func(event LifecycleEvent) func (lee *LifecycleEventEmitter) AddListener(listener LifecycleEventListener) { lee.mu.Lock() defer lee.mu.Unlock() lee.listeners = append(lee.listeners, listener) } func (lee *LifecycleEventEmitter) EmitEvent(event LifecycleEvent) { lee.mu.RLock() listeners := make([]LifecycleEventListener, len(lee.listeners)) copy(listeners, lee.listeners) lee.mu.RUnlock() for _, listener := range listeners { go listener(event) } }\rBest Practices 1. Resource Management // Always use defer for cleanup func (a *MyAgent) Run(ctx context.Context, event Event, state State) (AgentResult, error) { resource, err := a.acquireResource(ctx) if err != nil { return AgentResult{}, err } defer resource.Release() // Use resource... return a.processWithResource(ctx, event, state, resource) }\r2. Error Handling in Lifecycle // Handle initialization errors gracefully func (a *MyAgent) Initialize(ctx context.Context) error { var errors []error for _, initializer := range a.initializers { if err := initializer.Initialize(ctx); err != nil { errors = append(errors, err) } } if len(errors) \u003e 0 { // Cleanup any successful initializations a.cleanup() return fmt.Errorf(\"initialization failed: %v\", errors) } return nil }\r3. Monitoring Integration // Integrate monitoring throughout lifecycle func (a *MyAgent) Run(ctx context.Context, event Event, state State) (AgentResult, error) { start := time.Now() defer func() { duration := time.Since(start) a.monitor.RecordExecution(duration, getCurrentMemoryUsage()) }() return a.execute(ctx, event, state) }\rConclusion Understanding the agent lifecycle is crucial for building robust multi-agent systems. Proper lifecycle management ensures agents are created correctly, execute reliably, and clean up resources appropriately.\nKey takeaways:\nUse builder patterns for flexible agent creation Implement proper initialization and cleanup Monitor agent health and performance Handle lifecycle transitions gracefully Manage resources carefully throughout the lifecycle Emit lifecycle events for observability Next Steps Error Handling - Learn robust error management Memory Systems - Understand persistent storage Debugging Guide - Debug agent issues Production Deployment - Deploy agents in production Further Reading API Reference: Agent Interfaces Examples: Agent Lifecycle Patterns Configuration Guide: Agent Settings",
    "description": "Agent Lifecycle in AgenticGoKit Overview Understanding the agent lifecycle is fundamental to building effective multi-agent systems. This tutorial explores how agents are created, initialized, executed, and cleaned up in AgenticGoKit, along with best practices for managing agent resources and state.\nThe agent lifecycle encompasses everything from agent creation and configuration to execution patterns and resource cleanup.\nPrerequisites Basic understanding of Go programming Familiarity with Message Passing and Event Flow Knowledge of State Management Agent Lifecycle Phases 1. Creation and Configuration Agents go through several phases during their lifecycle:",
    "tags": [],
    "title": "agent-lifecycle",
    "uri": "/AgenticGoKitDocs/tutorials/core-concepts/agent-lifecycle/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials \u003e memory-systems",
    "content": "Basic Memory Operations in AgenticGoKit Overview This tutorial covers the fundamentals of memory operations in AgenticGoKit, including storing information, retrieving data, and managing conversation history. We’ll start with simple in-memory storage and progress to more advanced concepts.\nBasic memory operations form the foundation for all memory-enabled agents, providing the essential building blocks for more sophisticated memory systems.\nPrerequisites Understanding of Core Concepts Basic knowledge of Go programming Familiarity with key-value storage concepts Setting Up Basic Memory 1. In-Memory Storage The simplest memory provider stores data in RAM:\npackage main import ( \"context\" \"fmt\" \"log\" \"github.com/kunalkushwaha/agenticgokit/core\" ) func main() { // Create in-memory storage memory, err := core.NewMemory(core.AgentMemoryConfig{ Provider: \"memory\", Connection: \"memory\", MaxResults: 10, }) if err != nil { log.Fatalf(\"Failed to create memory: %v\", err) } ctx := context.Background() // Store some information with metadata err = memory.Store(ctx, \"Paris is the capital of France\", map[string]interface{}{ \"type\": \"fact\", \"category\": \"geography\", \"source\": \"general_knowledge\", }) if err != nil { log.Fatalf(\"Failed to store: %v\", err) } // Search for information results, err := memory.Search(ctx, \"capital France\", core.WithLimit(5)) if err != nil { log.Fatalf(\"Failed to search: %v\", err) } for _, result := range results { fmt.Printf(\"Found: %s (Score: %.3f)\\n\", result.Content, result.Score) if result.Metadata != nil { fmt.Printf(\" Type: %v, Category: %v\\n\", result.Metadata[\"type\"], result.Metadata[\"category\"]) } } }\r2. Memory Configuration Options // Basic configuration config := core.AgentMemoryConfig{ Provider: \"memory\", Connection: \"memory\", MaxResults: 10, Dimensions: 1536, AutoEmbed: true, // Enable knowledge base features EnableKnowledgeBase: true, KnowledgeMaxResults: 20, KnowledgeScoreThreshold: 0.7, // Document processing settings Documents: core.DocumentConfig{ AutoChunk: true, SupportedTypes: []string{\"txt\", \"md\", \"pdf\"}, MaxFileSize: \"10MB\", EnableMetadataExtraction: true, }, // Embedding configuration Embedding: core.EmbeddingConfig{ Provider: \"dummy\", // Use \"openai\" for production Model: \"text-embedding-3-small\", CacheEmbeddings: true, MaxBatchSize: 100, TimeoutSeconds: 30, }, } memory, err := core.NewMemory(config)\rBasic Storage Operations 1. Storing Simple Information func storeBasicInformation(memory core.Memory) error { ctx := context.Background() // Store facts facts := []struct { content string contentType string }{ {\"The Earth orbits the Sun\", \"scientific-fact\"}, {\"Water boils at 100°C at sea level\", \"scientific-fact\"}, {\"Shakespeare wrote Hamlet\", \"literary-fact\"}, {\"The Great Wall of China is visible from space\", \"myth\"}, // Actually false! } for _, fact := range facts { err := memory.Store(ctx, fact.content, fact.contentType) if err != nil { return fmt.Errorf(\"failed to store fact: %w\", err) } } fmt.Println(\"Stored\", len(facts), \"facts\") return nil }\r2. Storing with Metadata func storeWithMetadata(memory core.Memory) error { ctx := context.Background() // Store with additional metadata err := memory.Store(ctx, \"The user prefers detailed technical explanations\", \"user-preference\", core.WithMetadata(map[string]string{ \"user_id\": \"user-123\", \"category\": \"communication-style\", \"priority\": \"high\", \"source\": \"conversation-analysis\", }), core.WithSession(\"session-456\"), core.WithTimestamp(time.Now()), ) if err != nil { return fmt.Errorf(\"failed to store with metadata: %w\", err) } return nil }\r3. Storing Structured Data type UserProfile struct { Name string `json:\"name\"` Interests []string `json:\"interests\"` Preferences struct { Language string `json:\"language\"` Style string `json:\"style\"` } `json:\"preferences\"` } func storeStructuredData(memory core.Memory) error { ctx := context.Background() profile := UserProfile{ Name: \"Alice Johnson\", Interests: []string{\"AI\", \"Machine Learning\", \"Go Programming\"}, } profile.Preferences.Language = \"English\" profile.Preferences.Style = \"Technical\" // Store structured data err := memory.StoreStructured(ctx, profile, core.WithContentType(\"user-profile\"), core.WithSession(\"user-123\"), core.WithMetadata(map[string]string{ \"version\": \"1.0\", \"source\": \"onboarding\", }), ) if err != nil { return fmt.Errorf(\"failed to store structured data: %w\", err) } return nil }\rBasic Retrieval Operations 1. Simple Search func performBasicSearch(memory core.Memory) error { ctx := context.Background() // Simple text search results, err := memory.Search(ctx, \"Earth Sun orbit\") if err != nil { return fmt.Errorf(\"search failed: %w\", err) } fmt.Printf(\"Found %d results:\\n\", len(results)) for i, result := range results { fmt.Printf(\"%d. %s (Score: %.3f)\\n\", i+1, result.Content, result.Score) } return nil }\r2. Search with Filters func performFilteredSearch(memory core.Memory) error { ctx := context.Background() // Search with various filters results, err := memory.Search(ctx, \"technical explanation\", core.WithLimit(5), // Limit results core.WithScoreThreshold(0.7), // Minimum relevance score core.WithContentType(\"user-preference\"), // Filter by content type core.WithSession(\"session-456\"), // Filter by session core.WithMetadataFilter(map[string]string{ \"priority\": \"high\", }), ) if err != nil { return fmt.Errorf(\"filtered search failed: %w\", err) } fmt.Printf(\"Filtered search found %d results:\\n\", len(results)) for _, result := range results { fmt.Printf(\"- %s\\n\", result.Content) fmt.Printf(\" Type: %s, Score: %.3f\\n\", result.ContentType, result.Score) fmt.Printf(\" Metadata: %+v\\n\", result.Metadata) } return nil }\r3. Retrieving by ID func retrieveById(memory core.Memory, id string) error { ctx := context.Background() // Get specific item by ID result, err := memory.GetByID(ctx, id) if err != nil { return fmt.Errorf(\"failed to retrieve by ID: %w\", err) } fmt.Printf(\"Retrieved item:\\n\") fmt.Printf(\"ID: %s\\n\", result.ID) fmt.Printf(\"Content: %s\\n\", result.Content) fmt.Printf(\"Type: %s\\n\", result.ContentType) fmt.Printf(\"Created: %s\\n\", result.CreatedAt.Format(time.RFC3339)) return nil }\rConversation History Management 1. Storing Conversation Messages func storeConversation(memory core.Memory, sessionID string) error { ctx := context.Background() // Simulate a conversation conversation := []struct { role string content string }{ {\"user\", \"What is machine learning?\"}, {\"assistant\", \"Machine learning is a subset of AI that enables computers to learn from data...\"}, {\"user\", \"Can you give me an example?\"}, {\"assistant\", \"Sure! A common example is email spam detection...\"}, {\"user\", \"How does it work technically?\"}, {\"assistant\", \"Technically, spam detection uses features like sender reputation, keywords...\"}, } for i, msg := range conversation { err := memory.Store(ctx, msg.content, msg.role+\"-message\", core.WithSession(sessionID), core.WithTimestamp(time.Now().Add(time.Duration(i)*time.Minute)), core.WithMetadata(map[string]string{ \"role\": msg.role, \"sequence\": fmt.Sprintf(\"%d\", i+1), }), ) if err != nil { return fmt.Errorf(\"failed to store message: %w\", err) } } return nil }\r2. Retrieving Conversation History func getConversationHistory(memory core.Memory, sessionID string) error { ctx := context.Background() // Get recent conversation history messages, err := memory.GetHistory(ctx, 10, core.WithSession(sessionID), core.WithTimeRange( time.Now().Add(-24*time.Hour), // Last 24 hours time.Now(), ), ) if err != nil { return fmt.Errorf(\"failed to get history: %w\", err) } fmt.Printf(\"Conversation history (%d messages):\\n\", len(messages)) for _, msg := range messages { fmt.Printf(\"[%s] %s: %s\\n\", msg.Timestamp.Format(\"15:04\"), msg.Role, msg.Content, ) } return nil }\r3. Conversation Context Building func buildConversationContext(memory core.Memory, sessionID string, currentMessage string) (string, error) { ctx := context.Background() // Get recent history for context history, err := memory.GetHistory(ctx, 5, core.WithSession(sessionID), ) if err != nil { return \"\", fmt.Errorf(\"failed to get history: %w\", err) } // Build context string var contextBuilder strings.Builder contextBuilder.WriteString(\"Recent conversation:\\n\") for _, msg := range history { contextBuilder.WriteString(fmt.Sprintf(\"%s: %s\\n\", strings.Title(msg.Role), msg.Content)) } contextBuilder.WriteString(fmt.Sprintf(\"\\nCurrent message: %s\\n\", currentMessage)) return contextBuilder.String(), nil }\rMemory-Enabled Agent Example 1. Simple Memory Agent type MemoryAgent struct { name string memory core.Memory llm core.LLMProvider } func NewMemoryAgent(name string, memory core.Memory, llm core.LLMProvider) *MemoryAgent { return \u0026MemoryAgent{ name: name, memory: memory, llm: llm, } } func (m *MemoryAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Extract message from state message, ok := state.Get(\"message\") if !ok { return core.AgentResult{}, errors.New(\"no message in state\") } messageStr := message.(string) sessionID := event.GetSessionID() // Store the user message err := m.memory.Store(ctx, messageStr, \"user-message\", core.WithSession(sessionID), core.WithTimestamp(time.Now()), ) if err != nil { return core.AgentResult{}, fmt.Errorf(\"failed to store message: %w\", err) } // Get conversation context context, err := m.buildContext(ctx, sessionID, messageStr) if err != nil { return core.AgentResult{}, fmt.Errorf(\"failed to build context: %w\", err) } // Generate response with context response, err := m.llm.Generate(ctx, context) if err != nil { return core.AgentResult{}, fmt.Errorf(\"failed to generate response: %w\", err) } // Store the assistant response err = m.memory.Store(ctx, response, \"assistant-message\", core.WithSession(sessionID), core.WithTimestamp(time.Now()), ) if err != nil { return core.AgentResult{}, fmt.Errorf(\"failed to store response: %w\", err) } // Return result outputState := state.Clone() outputState.Set(\"response\", response) return core.AgentResult{OutputState: outputState}, nil } func (m *MemoryAgent) buildContext(ctx context.Context, sessionID, currentMessage string) (string, error) { // Get recent conversation history history, err := m.memory.GetHistory(ctx, 5, core.WithSession(sessionID), ) if err != nil { return \"\", err } // Search for relevant information relevant, err := m.memory.Search(ctx, currentMessage, core.WithLimit(3), core.WithScoreThreshold(0.7), core.WithContentType(\"fact\"), ) if err != nil { return \"\", err } // Build enhanced context var contextBuilder strings.Builder // Add relevant facts if len(relevant) \u003e 0 { contextBuilder.WriteString(\"Relevant information:\\n\") for _, item := range relevant { contextBuilder.WriteString(fmt.Sprintf(\"- %s\\n\", item.Content)) } contextBuilder.WriteString(\"\\n\") } // Add conversation history if len(history) \u003e 0 { contextBuilder.WriteString(\"Recent conversation:\\n\") for _, msg := range history { contextBuilder.WriteString(fmt.Sprintf(\"%s: %s\\n\", strings.Title(msg.Role), msg.Content)) } contextBuilder.WriteString(\"\\n\") } contextBuilder.WriteString(fmt.Sprintf(\"Current question: %s\\n\", currentMessage)) contextBuilder.WriteString(\"Please provide a helpful response based on the context above.\") return contextBuilder.String(), nil }\r2. Using the Memory Agent func main() { // Create memory system memory, err := core.NewMemory(core.AgentMemoryConfig{ Provider: \"memory\", MaxSize: 1000, }) if err != nil { log.Fatalf(\"Failed to create memory: %v\", err) } // Create LLM provider llm, err := core.NewOpenAIAdapter( os.Getenv(\"OPENAI_API_KEY\"), \"gpt-3.5-turbo\", 1000, 0.7, ) if err != nil { log.Fatalf(\"Failed to create LLM: %v\", err) } // Create memory-enabled agent agent := NewMemoryAgent(\"memory-assistant\", memory, llm) // Store some initial facts ctx := context.Background() facts := []string{ \"Go is a programming language developed by Google\", \"AgenticGoKit is a framework for building multi-agent systems in Go\", \"Vector databases are used for similarity search\", } for _, fact := range facts { memory.Store(ctx, fact, \"fact\") } // Create runner and register agent runner := core.NewRunner(100) orchestrator := core.NewRouteOrchestrator(runner.GetCallbackRegistry()) runner.SetOrchestrator(orchestrator) agentHandler := core.ConvertAgentToHandler(agent) runner.RegisterAgent(\"memory-assistant\", agentHandler) // Start runner runner.Start(ctx) defer runner.Stop() // Simulate conversation sessionID := \"user-session-123\" messages := []string{ \"What is Go?\", \"How is it related to AgenticGoKit?\", \"What are vector databases used for?\", } for _, msg := range messages { event := core.NewEvent( \"memory-assistant\", core.EventData{\"message\": msg}, map[string]string{ \"session_id\": sessionID, \"route\": \"memory-assistant\", }, ) runner.Emit(event) time.Sleep(2 * time.Second) // Wait for processing } }\rMemory Management 1. Updating Stored Information func updateMemoryContent(memory core.Memory) error { ctx := context.Background() // First, find the item to update results, err := memory.Search(ctx, \"Great Wall China visible space\") if err != nil { return err } if len(results) \u003e 0 { // Update the incorrect information err = memory.Update(ctx, results[0].ID, \"The Great Wall of China is NOT visible from space with the naked eye\", core.WithMetadata(map[string]string{ \"corrected\": \"true\", \"updated_at\": time.Now().Format(time.RFC3339), }), ) if err != nil { return fmt.Errorf(\"failed to update: %w\", err) } fmt.Println(\"Corrected misinformation about the Great Wall\") } return nil }\r2. Deleting Information func cleanupOldMemories(memory core.Memory) error { ctx := context.Background() // Get memory statistics stats, err := memory.GetStats(ctx) if err != nil { return err } fmt.Printf(\"Memory stats: %d items, %d MB used\\n\", stats.ItemCount, stats.SizeBytes/1024/1024) // Delete specific items results, err := memory.Search(ctx, \"temporary data\", core.WithContentType(\"temporary\"), ) if err != nil { return err } for _, result := range results { err = memory.Delete(ctx, result.ID) if err != nil { fmt.Printf(\"Failed to delete %s: %v\\n\", result.ID, err) } else { fmt.Printf(\"Deleted temporary item: %s\\n\", result.ID) } } return nil }\r3. Memory Cleanup Strategies type MemoryManager struct { memory core.Memory maxAge time.Duration maxItems int cleanupInterval time.Duration } func NewMemoryManager(memory core.Memory) *MemoryManager { return \u0026MemoryManager{ memory: memory, maxAge: 24 * time.Hour, maxItems: 1000, cleanupInterval: time.Hour, } } func (mm *MemoryManager) StartCleanup(ctx context.Context) { ticker := time.NewTicker(mm.cleanupInterval) defer ticker.Stop() for { select { case \u003c-ctx.Done(): return case \u003c-ticker.C: mm.performCleanup(ctx) } } } func (mm *MemoryManager) performCleanup(ctx context.Context) { // Get memory statistics stats, err := mm.memory.GetStats(ctx) if err != nil { fmt.Printf(\"Failed to get memory stats: %v\\n\", err) return } // Clean up old items if over limit if stats.ItemCount \u003e mm.maxItems { mm.cleanupOldItems(ctx, stats.ItemCount-mm.maxItems) } // Clean up expired items mm.cleanupExpiredItems(ctx) } func (mm *MemoryManager) cleanupOldItems(ctx context.Context, itemsToRemove int) { // Implementation would search for oldest items and remove them fmt.Printf(\"Cleaning up %d old items\\n\", itemsToRemove) } func (mm *MemoryManager) cleanupExpiredItems(ctx context.Context) { // Implementation would find and remove expired items cutoff := time.Now().Add(-mm.maxAge) fmt.Printf(\"Cleaning up items older than %s\\n\", cutoff.Format(time.RFC3339)) }\rBest Practices for Basic Memory 1. Content Organization // Use consistent content types const ( ContentTypeFact = \"fact\" ContentTypePreference = \"user-preference\" ContentTypeMessage = \"message\" ContentTypeKnowledge = \"knowledge\" ContentTypeTemporary = \"temporary\" ) // Use structured metadata func storeWithConsistentMetadata(memory core.Memory, content, contentType string) error { return memory.Store(context.Background(), content, contentType, core.WithMetadata(map[string]string{ \"version\": \"1.0\", \"source\": \"user-input\", \"confidence\": \"high\", \"language\": \"en\", }), core.WithTimestamp(time.Now()), ) }\r2. Session Management func manageUserSessions(memory core.Memory) { // Use consistent session IDs sessionID := fmt.Sprintf(\"user-%s-%d\", userID, time.Now().Unix()) // Store session metadata memory.Store(context.Background(), \"Session started\", \"session-event\", core.WithSession(sessionID), core.WithMetadata(map[string]string{ \"event_type\": \"session_start\", \"user_id\": userID, \"ip_address\": clientIP, }), ) }\r3. Error Handling func robustMemoryOperations(memory core.Memory) { ctx := context.Background() // Store with retry logic maxRetries := 3 for i := 0; i \u003c maxRetries; i++ { err := memory.Store(ctx, \"important data\", \"critical\") if err == nil { break } if i == maxRetries-1 { log.Printf(\"Failed to store after %d attempts: %v\", maxRetries, err) // Handle permanent failure } else { log.Printf(\"Store attempt %d failed, retrying: %v\", i+1, err) time.Sleep(time.Duration(i+1) * time.Second) } } // Search with fallback results, err := memory.Search(ctx, \"query\") if err != nil { log.Printf(\"Search failed, using fallback: %v\", err) // Use cached results or default response results = getFallbackResults() } }\rCommon Patterns 1. Context-Aware Responses func generateContextAwareResponse(memory core.Memory, sessionID, message string) (string, error) { ctx := context.Background() // Get user preferences preferences, err := memory.Search(ctx, \"user preference\", core.WithSession(sessionID), core.WithContentType(\"user-preference\"), ) if err != nil { return \"\", err } // Get relevant facts facts, err := memory.Search(ctx, message, core.WithContentType(\"fact\"), core.WithLimit(3), ) if err != nil { return \"\", err } // Build context-aware prompt prompt := buildPromptWithContext(message, preferences, facts) // Generate response (would use LLM here) return generateResponse(prompt), nil }\r2. Learning from Interactions func learnFromInteraction(memory core.Memory, sessionID, userMessage, response string, feedback string) error { ctx := context.Background() // Store the interaction interaction := fmt.Sprintf(\"Q: %s\\nA: %s\\nFeedback: %s\", userMessage, response, feedback) err := memory.Store(ctx, interaction, \"interaction\", core.WithSession(sessionID), core.WithMetadata(map[string]string{ \"feedback_type\": classifyFeedback(feedback), \"quality\": scoreFeedback(feedback), }), ) if err != nil { return err } // Extract learnings if feedback is positive if feedback == \"helpful\" || feedback == \"correct\" { // Store successful patterns pattern := extractPattern(userMessage, response) memory.Store(ctx, pattern, \"successful-pattern\", core.WithMetadata(map[string]string{ \"pattern_type\": \"response-pattern\", \"success_rate\": \"high\", }), ) } return nil }\rConclusion Basic memory operations provide the foundation for building intelligent agents that can remember, learn, and improve over time. The key concepts covered include:\nSetting up in-memory storage Storing and retrieving information Managing conversation history Building context-aware responses Memory management and cleanup These fundamentals prepare you for more advanced memory systems including vector databases and RAG implementations.\nNext Steps Vector Databases - Learn about production-ready storage RAG Implementation - Build retrieval-augmented systems Knowledge Bases - Create comprehensive knowledge systems Memory Optimization - Optimize performance and scaling Further Reading API Reference: Memory Interface Examples: Basic Memory Usage Configuration Guide: Memory Settings",
    "description": "Basic Memory Operations in AgenticGoKit Overview This tutorial covers the fundamentals of memory operations in AgenticGoKit, including storing information, retrieving data, and managing conversation history. We’ll start with simple in-memory storage and progress to more advanced concepts.\nBasic memory operations form the foundation for all memory-enabled agents, providing the essential building blocks for more sophisticated memory systems.\nPrerequisites Understanding of Core Concepts Basic knowledge of Go programming Familiarity with key-value storage concepts Setting Up Basic Memory 1. In-Memory Storage The simplest memory provider stores data in RAM:",
    "tags": [],
    "title": "basic-memory",
    "uri": "/AgenticGoKitDocs/tutorials/memory-systems/basic-memory/index.html"
  },
  {
    "breadcrumb": "content \u003e guides \u003e development",
    "content": "Best Practices Development best practices for building robust AgenticGoKit applications\nThis guide covers essential best practices for developing, deploying, and maintaining AgenticGoKit applications. Follow these guidelines to build reliable, scalable, and maintainable multi-agent systems.\nAgent Design Principles Single Responsibility Principle Each agent should have one clear, well-defined purpose:\n// Good: Focused agent with single responsibility type EmailValidatorAgent struct { name string } func (a *EmailValidatorAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { email := event.Data[\"email\"].(string) if !isValidEmail(email) { return core.AgentResult{ Data: map[string]interface{}{ \"valid\": false, \"error\": \"Invalid email format\", }, }, nil } return core.AgentResult{ Data: map[string]interface{}{ \"valid\": true, \"email\": email, }, }, nil } // Bad: Agent trying to do too many things type EmailProcessorAgent struct { // Validates, sends, logs, and analyzes emails - too many responsibilities }\rStateless Design When Possible Prefer stateless agents for better scalability and testability:\n// Good: Stateless agent type TextAnalyzerAgent struct { llmProvider core.ModelProvider } func (a *TextAnalyzerAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { text := event.Data[\"text\"].(string) // All state comes from event and state parameters analysis, err := a.analyzeText(ctx, text) if err != nil { return core.AgentResult{}, err } return core.AgentResult{ Data: map[string]interface{}{ \"analysis\": analysis, }, }, nil } // Use stateful agents only when necessary type ConversationAgent struct { llmProvider core.ModelProvider memory core.Memory // Stateful for conversation history }\rRobust Error Handling Always handle errors gracefully and provide meaningful feedback:\nfunc (a *MyAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Validate inputs input, ok := event.Data[\"input\"].(string) if !ok { return core.AgentResult{}, fmt.Errorf(\"invalid input type: expected string, got %T\", event.Data[\"input\"]) } if len(input) == 0 { return core.AgentResult{}, fmt.Errorf(\"input cannot be empty\") } // Process with timeout ctx, cancel := context.WithTimeout(ctx, 30*time.Second) defer cancel() result, err := a.processInput(ctx, input) if err != nil { // Wrap errors with context return core.AgentResult{}, fmt.Errorf(\"processing input %q failed: %w\", input, err) } return core.AgentResult{ Data: map[string]interface{}{ \"result\": result, }, }, nil }\rConfiguration Management Environment-Based Configuration Use environment variables for deployment-specific settings:\n# agentflow.toml [llm] provider = \"${LLM_PROVIDER:azure}\" api_key = \"${LLM_API_KEY}\" endpoint = \"${LLM_ENDPOINT}\" [memory] provider = \"${MEMORY_PROVIDER:memory}\" connection = \"${MEMORY_CONNECTION:memory}\" [logging] level = \"${LOG_LEVEL:info}\" format = \"${LOG_FORMAT:json}\"\r// Load configuration with validation func loadConfig() (*Config, error) { config, err := core.LoadConfigFromWorkingDir() if err != nil { return nil, fmt.Errorf(\"failed to load configuration: %w\", err) } // Validate required settings if config.LLM.APIKey == \"\" { return nil, fmt.Errorf(\"LLM API key is required\") } return config, nil }\rConfiguration Profiles Use different configurations for different environments:\n# Development export AGENTFLOW_PROFILE=development export LLM_PROVIDER=ollama export MEMORY_PROVIDER=memory # Production export AGENTFLOW_PROFILE=production export LLM_PROVIDER=azure export MEMORY_PROVIDER=pgvector export MEMORY_CONNECTION=\"postgres://...\"\rPerformance Optimization Connection Pooling Configure appropriate connection pools for external services:\n[memory] provider = \"pgvector\" connection = \"postgres://user:pass@localhost:5432/db\" max_connections = 25 idle_connections = 5 connection_lifetime = \"1h\" [llm] provider = \"azure\" max_connections = 10 request_timeout = \"30s\"\rCaching Strategies Implement caching for expensive operations:\ntype CachedAgent struct { llmProvider core.ModelProvider cache map[string]string cacheMutex sync.RWMutex cacheExpiry time.Duration } func (a *CachedAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { input := event.Data[\"input\"].(string) // Check cache first a.cacheMutex.RLock() if cached, exists := a.cache[input]; exists { a.cacheMutex.RUnlock() return core.AgentResult{ Data: map[string]interface{}{ \"result\": cached, \"cached\": true, }, }, nil } a.cacheMutex.RUnlock() // Process and cache result result, err := a.llmProvider.Generate(ctx, input) if err != nil { return core.AgentResult{}, err } a.cacheMutex.Lock() a.cache[input] = result a.cacheMutex.Unlock() return core.AgentResult{ Data: map[string]interface{}{ \"result\": result, \"cached\": false, }, }, nil }\rResource Management Always clean up resources properly:\nfunc (a *MyAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Use context with timeout ctx, cancel := context.WithTimeout(ctx, 30*time.Second) defer cancel() // Clean up resources resource, err := acquireResource() if err != nil { return core.AgentResult{}, err } defer resource.Close() // Process with resource result, err := processWithResource(ctx, resource, event.Data) if err != nil { return core.AgentResult{}, err } return core.AgentResult{Data: result}, nil }\rSecurity Best Practices API Key Management Never hardcode API keys or sensitive information:\n// Good: Use environment variables func createLLMProvider() (core.ModelProvider, error) { apiKey := os.Getenv(\"OPENAI_API_KEY\") if apiKey == \"\" { return nil, fmt.Errorf(\"OPENAI_API_KEY environment variable is required\") } return core.NewOpenAIProvider(core.OpenAIConfig{ APIKey: apiKey, Model: \"gpt-4\", }) } // Bad: Hardcoded secrets func createLLMProviderBad() (core.ModelProvider, error) { return core.NewOpenAIProvider(core.OpenAIConfig{ APIKey: \"sk-hardcoded-key-here\", // Never do this! Model: \"gpt-4\", }) }\rInput Validation Always validate and sanitize inputs:\nfunc (a *MyAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Validate input types userInput, ok := event.Data[\"user_input\"].(string) if !ok { return core.AgentResult{}, fmt.Errorf(\"user_input must be a string\") } // Sanitize input userInput = strings.TrimSpace(userInput) if len(userInput) == 0 { return core.AgentResult{}, fmt.Errorf(\"user_input cannot be empty\") } // Limit input size if len(userInput) \u003e 10000 { return core.AgentResult{}, fmt.Errorf(\"user_input too long (max 10000 characters)\") } // Remove potentially dangerous content userInput = sanitizeInput(userInput) // Process sanitized input return a.processInput(ctx, userInput) } func sanitizeInput(input string) string { // Remove or escape potentially dangerous content // This is a simplified example input = strings.ReplaceAll(input, \"\u003cscript\u003e\", \"\") input = strings.ReplaceAll(input, \"\u003c/script\u003e\", \"\") return input }\rTesting Best Practices Comprehensive Test Coverage Write tests at multiple levels:\n// Unit tests for individual agents func TestAgentLogic(t *testing.T) { agent := NewMyAgent() tests := []struct { name string input map[string]interface{} expected string wantErr bool }{ { name: \"valid input\", input: map[string]interface{}{\"data\": \"test\"}, expected: \"processed: test\", wantErr: false, }, { name: \"invalid input\", input: map[string]interface{}{\"data\": 123}, wantErr: true, }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { event := core.NewEvent(\"test\", tt.input) result, err := agent.Run(context.Background(), event, core.NewState()) if tt.wantErr { assert.Error(t, err) return } require.NoError(t, err) assert.Equal(t, tt.expected, result.Data[\"result\"]) }) } } // Integration tests for agent interactions func TestAgentWorkflow(t *testing.T) { agents := map[string]core.AgentHandler{ \"step1\": NewStep1Agent(), \"step2\": NewStep2Agent(), } runner := core.CreateSequentialRunner(agents, []string{\"step1\", \"step2\"}, 30*time.Second) event := core.NewEvent(\"process\", map[string]interface{}{ \"input\": \"test data\", }) results, err := runner.ProcessEvent(context.Background(), event) require.NoError(t, err) assert.Len(t, results, 2) // Verify workflow results... }\rUse Mocks for External Dependencies Mock external services for reliable testing:\ntype MockLLMProvider struct { responses map[string]string errors map[string]error } func (m *MockLLMProvider) Generate(ctx context.Context, prompt string) (string, error) { if err, exists := m.errors[prompt]; exists { return \"\", err } if response, exists := m.responses[prompt]; exists { return response, nil } return \"default mock response\", nil } func TestAgentWithMockLLM(t *testing.T) { mockLLM := \u0026MockLLMProvider{ responses: map[string]string{ \"test prompt\": \"expected response\", }, } agent := NewMyAgent(mockLLM) // Test with predictable mock responses event := core.NewEvent(\"test\", map[string]interface{}{ \"prompt\": \"test prompt\", }) result, err := agent.Run(context.Background(), event, core.NewState()) require.NoError(t, err) assert.Equal(t, \"expected response\", result.Data[\"response\"]) }\rMonitoring and Observability Structured Logging Use structured logging for better observability:\nimport ( \"github.com/sirupsen/logrus\" ) func (a *MyAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { logger := logrus.WithFields(logrus.Fields{ \"agent\": a.name, \"event_type\": event.Type, \"trace_id\": getTraceID(ctx), }) logger.Info(\"Agent execution started\") start := time.Now() result, err := a.processEvent(ctx, event, state) duration := time.Since(start) if err != nil { logger.WithFields(logrus.Fields{ \"error\": err.Error(), \"duration\": duration, }).Error(\"Agent execution failed\") return core.AgentResult{}, err } logger.WithFields(logrus.Fields{ \"duration\": duration, \"success\": true, }).Info(\"Agent execution completed\") return result, nil }\rHealth Checks Implement health checks for monitoring:\ntype HealthChecker interface { HealthCheck(ctx context.Context) error } func (a *MyAgent) HealthCheck(ctx context.Context) error { // Check LLM provider connectivity if a.llmProvider != nil { _, err := a.llmProvider.Generate(ctx, \"health check\") if err != nil { return fmt.Errorf(\"LLM provider health check failed: %w\", err) } } // Check memory system if a.memory != nil { err := a.memory.Store(ctx, \"health check\", \"test\") if err != nil { return fmt.Errorf(\"memory system health check failed: %w\", err) } } return nil } // HTTP health endpoint func setupHealthEndpoint(agents map[string]core.AgentHandler) { http.HandleFunc(\"/health\", func(w http.ResponseWriter, r *http.Request) { ctx, cancel := context.WithTimeout(r.Context(), 10*time.Second) defer cancel() health := map[string]interface{}{ \"status\": \"healthy\", \"timestamp\": time.Now(), \"agents\": make(map[string]string), } for name, agent := range agents { if checker, ok := agent.(HealthChecker); ok { if err := checker.HealthCheck(ctx); err != nil { health[\"agents\"].(map[string]string)[name] = \"unhealthy: \" + err.Error() health[\"status\"] = \"degraded\" } else { health[\"agents\"].(map[string]string)[name] = \"healthy\" } } else { health[\"agents\"].(map[string]string)[name] = \"no health check\" } } w.Header().Set(\"Content-Type\", \"application/json\") if health[\"status\"] != \"healthy\" { w.WriteHeader(http.StatusServiceUnavailable) } json.NewEncoder(w).Encode(health) }) }\rDeployment Best Practices Containerization Use multi-stage Docker builds for efficient containers:\n# Build stage FROM golang:1.21-alpine AS builder WORKDIR /app COPY go.mod go.sum ./ RUN go mod download COPY . . RUN CGO_ENABLED=0 GOOS=linux go build -o main . # Runtime stage FROM alpine:latest RUN apk --no-cache add ca-certificates WORKDIR /root/ COPY --from=builder /app/main . COPY --from=builder /app/agentflow.toml . CMD [\"./main\"]\rConfiguration Management Use configuration management for different environments:\n# docker-compose.yml version: '3.8' services: agent: build: . environment: - LLM_PROVIDER=azure - LLM_API_KEY=${AZURE_OPENAI_API_KEY} - LLM_ENDPOINT=${AZURE_OPENAI_ENDPOINT} - MEMORY_PROVIDER=pgvector - MEMORY_CONNECTION=postgres://user:pass@postgres:5432/agentflow - LOG_LEVEL=info depends_on: - postgres postgres: image: pgvector/pgvector:pg15 environment: POSTGRES_DB: agentflow POSTGRES_USER: user POSTGRES_PASSWORD: pass volumes: - postgres_data:/var/lib/postgresql/data volumes: postgres_data:\rGraceful Shutdown Implement graceful shutdown for clean resource cleanup:\nfunc main() { // Create agents and runner agents := createAgents() runner := createRunner(agents) // Set up graceful shutdown ctx, cancel := context.WithCancel(context.Background()) defer cancel() // Handle shutdown signals sigChan := make(chan os.Signal, 1) signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM) go func() { \u003c-sigChan log.Println(\"Shutdown signal received, cleaning up...\") // Cancel context to stop all operations cancel() // Clean up resources if err := runner.Stop(); err != nil { log.Printf(\"Error stopping runner: %v\", err) } // Close memory connections for _, agent := range agents { if closer, ok := agent.(io.Closer); ok { closer.Close() } } log.Println(\"Cleanup completed\") os.Exit(0) }() // Start application if err := runner.Start(ctx); err != nil { log.Fatalf(\"Failed to start runner: %v\", err) } // Keep running until shutdown \u003c-ctx.Done() }\rCode Organization Project Structure Organize your project with clear separation of concerns:\nmy-agent-app/\r├── cmd/\r│ └── main.go # Application entry point\r├── internal/\r│ ├── agents/ # Agent implementations\r│ │ ├── analyzer.go\r│ │ ├── processor.go\r│ │ └── responder.go\r│ ├── config/ # Configuration management\r│ │ └── config.go\r│ ├── handlers/ # HTTP handlers (if applicable)\r│ │ └── api.go\r│ └── services/ # Business logic services\r│ └── business_logic.go\r├── pkg/ # Public packages (if any)\r├── configs/\r│ ├── agentflow.toml # Default configuration\r│ ├── development.toml # Development overrides\r│ └── production.toml # Production overrides\r├── deployments/\r│ ├── docker-compose.yml\r│ └── Dockerfile\r├── scripts/\r│ ├── setup.sh\r│ └── test.sh\r├── tests/\r│ ├── integration/\r│ └── fixtures/\r├── go.mod\r├── go.sum\r└── README.md\rPackage Design Keep packages focused and minimize dependencies:\n// Good: Focused package with clear interface package analyzer import ( \"context\" \"github.com/kunalkushwaha/agenticgokit/core\" ) type Analyzer interface { Analyze(ctx context.Context, text string) (*Analysis, error) } type Analysis struct { Sentiment string `json:\"sentiment\"` Keywords []string `json:\"keywords\"` Score float64 `json:\"score\"` } // Bad: Package with too many responsibilities package everything // Contains agents, configuration, HTTP handlers, database logic, etc.\rDocumentation Code Documentation Document your agents and their behavior:\n// TextAnalyzerAgent analyzes text content for sentiment, keywords, and other metrics. // It uses an LLM provider to perform the analysis and returns structured results. // // The agent expects events with the following data: // - text (string): The text content to analyze // - analysis_type (string, optional): Type of analysis (\"sentiment\", \"keywords\", \"full\") // // Returns results with: // - sentiment (string): Detected sentiment (\"positive\", \"negative\", \"neutral\") // - keywords ([]string): Extracted keywords // - confidence (float64): Confidence score (0.0-1.0) type TextAnalyzerAgent struct { llmProvider core.ModelProvider name string } // Run processes a text analysis event and returns structured analysis results. // It validates the input text and analysis type, then uses the LLM provider // to perform the requested analysis. func (a *TextAnalyzerAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Implementation... }\rREADME Documentation Provide clear setup and usage instructions:\n# My Agent Application Brief description of what your application does. ## Quick Start 1. Install dependencies: ```bash go mod download\rSet up environment:\nexport OPENAI_API_KEY=\"your-key-here\"\rRun the application:\ngo run cmd/main.go\rConfiguration The application uses agentflow.toml for configuration. Key settings:\nllm.provider: LLM provider (“openai”, “azure”, “ollama”) memory.provider: Memory provider (“memory”, “pgvector”, “weaviate”) Agents Analyzer: Analyzes text for sentiment and keywords Processor: Processes and transforms data Responder: Generates responses based on analysis Deployment See deployments/ directory for Docker and Kubernetes configurations.\n## Next Steps\r- **[Testing Agents](testing-agents.md)** - Comprehensive testing strategies\r- **[Debugging](debugging.md)** - Debug agent interactions effectively\r- **[Production Deployment](../deployment/README.md)** - Production deployment setup",
    "description": "Best Practices Development best practices for building robust AgenticGoKit applications\nThis guide covers essential best practices for developing, deploying, and maintaining AgenticGoKit applications. Follow these guidelines to build reliable, scalable, and maintainable multi-agent systems.\nAgent Design Principles Single Responsibility Principle Each agent should have one clear, well-defined purpose:\n// Good: Focused agent with single responsibility type EmailValidatorAgent struct { name string } func (a *EmailValidatorAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { email := event.Data[\"email\"].(string) if !isValidEmail(email) { return core.AgentResult{ Data: map[string]interface{}{ \"valid\": false, \"error\": \"Invalid email format\", }, }, nil } return core.AgentResult{ Data: map[string]interface{}{ \"valid\": true, \"email\": email, }, }, nil } // Bad: Agent trying to do too many things type EmailProcessorAgent struct { // Validates, sends, logs, and analyzes emails - too many responsibilities }\rStateless Design When Possible Prefer stateless agents for better scalability and testability:",
    "tags": [],
    "title": "best-practices",
    "uri": "/AgenticGoKitDocs/guides/development/best-practices/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials \u003e advanced",
    "content": "Circuit Breaker Patterns in AgenticGoKit Building fault-tolerant agent systems with automatic failure detection and recovery\nCircuit breakers are essential for building resilient multi-agent systems. They automatically detect failures and prevent cascade failures by temporarily stopping requests to failing services. This guide shows you how to implement and configure circuit breaker patterns in AgenticGoKit.\n🔌 Understanding Circuit Breakers A circuit breaker works like an electrical circuit breaker - it “opens” when it detects too many failures, preventing further requests until the service recovers.\nstateDiagram-v2\r[*] --\u003e Closed\rClosed --\u003e Open : Failure threshold reached\rOpen --\u003e HalfOpen : Timeout expires\rHalfOpen --\u003e Closed : Success threshold reached\rHalfOpen --\u003e Open : Failure detected\rstate Closed {\r[*] --\u003e Normal\rNormal --\u003e CountingFailures : Failure occurs\rCountingFailures --\u003e Normal : Success occurs\r}\rstate Open {\r[*] --\u003e Blocking\rBlocking --\u003e Waiting : Time passes\r}\rstate HalfOpen {\r[*] --\u003e Testing\rTesting --\u003e Evaluating : Request completes\r}\r🛠️ Built-in Circuit Breaker Configuration AgenticGoKit includes built-in circuit breaker support for LLM providers and MCP tools:\nLLM Provider Circuit Breakers # agentflow.toml [providers.openai.circuit_breaker] enabled = true failure_threshold = 5 # Open after 5 consecutive failures success_threshold = 3 # Close after 3 consecutive successes timeout_ms = 30000 # Stay open for 30 seconds max_concurrent_calls = 10 # Limit concurrent requests [providers.azure.circuit_breaker] enabled = true failure_threshold = 3 success_threshold = 2 timeout_ms = 60000\rMCP Tool Circuit Breakers # agentflow.toml [mcp.circuit_breaker] enabled = true failure_threshold = 10 # Higher threshold for tools success_threshold = 5 timeout_ms = 60000 # Per-server circuit breaker settings [[mcp.servers]] name = \"web-search\" type = \"stdio\" command = \"npx @modelcontextprotocol/server-web-search\" enabled = true [mcp.servers.web-search.circuit_breaker] failure_threshold = 3 # More sensitive for critical tools timeout_ms = 30000\r🔧 Custom Circuit Breaker Implementation For custom agents and services, you can implement your own circuit breakers:\nBasic Circuit Breaker package patterns import ( \"context\" \"errors\" \"sync\" \"time\" ) type CircuitState int const ( StateClosed CircuitState = iota StateOpen StateHalfOpen ) type CircuitBreaker struct { mu sync.RWMutex state CircuitState failureCount int successCount int lastFailureTime time.Time // Configuration failureThreshold int successThreshold int timeout time.Duration maxConcurrent int currentCalls int } func NewCircuitBreaker(failureThreshold, successThreshold int, timeout time.Duration) *CircuitBreaker { return \u0026CircuitBreaker{ state: StateClosed, failureThreshold: failureThreshold, successThreshold: successThreshold, timeout: timeout, maxConcurrent: 10, } } func (cb *CircuitBreaker) Execute(ctx context.Context, fn func() error) error { // Check if we can execute if !cb.canExecute() { return errors.New(\"circuit breaker is open\") } // Track concurrent calls cb.incrementCalls() defer cb.decrementCalls() // Execute the function err := fn() // Record the result cb.recordResult(err) return err } func (cb *CircuitBreaker) canExecute() bool { cb.mu.RLock() defer cb.mu.RUnlock() switch cb.state { case StateClosed: return cb.currentCalls \u003c cb.maxConcurrent case StateOpen: return time.Since(cb.lastFailureTime) \u003e= cb.timeout case StateHalfOpen: return cb.currentCalls == 0 // Only allow one test call default: return false } } func (cb *CircuitBreaker) recordResult(err error) { cb.mu.Lock() defer cb.mu.Unlock() if err != nil { cb.recordFailure() } else { cb.recordSuccess() } } func (cb *CircuitBreaker) recordFailure() { cb.failureCount++ cb.successCount = 0 cb.lastFailureTime = time.Now() switch cb.state { case StateClosed: if cb.failureCount \u003e= cb.failureThreshold { cb.state = StateOpen } case StateHalfOpen: cb.state = StateOpen } } func (cb *CircuitBreaker) recordSuccess() { cb.successCount++ cb.failureCount = 0 switch cb.state { case StateHalfOpen: if cb.successCount \u003e= cb.successThreshold { cb.state = StateClosed } case StateOpen: cb.state = StateHalfOpen cb.successCount = 1 } } func (cb *CircuitBreaker) incrementCalls() { cb.mu.Lock() cb.currentCalls++ cb.mu.Unlock() } func (cb *CircuitBreaker) decrementCalls() { cb.mu.Lock() cb.currentCalls-- cb.mu.Unlock() } func (cb *CircuitBreaker) GetState() CircuitState { cb.mu.RLock() defer cb.mu.RUnlock() return cb.state }\rAgent with Circuit Breaker package agents import ( \"context\" \"fmt\" \"time\" \"github.com/kunalkushwaha/agenticgokit/core\" \"your-project/patterns\" ) type ResilientAgent struct { name string llmProvider core.ModelProvider circuitBreaker *patterns.CircuitBreaker fallbackAgent core.AgentHandler } func NewResilientAgent(name string, provider core.ModelProvider) *ResilientAgent { return \u0026ResilientAgent{ name: name, llmProvider: provider, circuitBreaker: patterns.NewCircuitBreaker( 5, // failure threshold 3, // success threshold 30*time.Second, // timeout ), } } func (a *ResilientAgent) Execute(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { // Try primary execution with circuit breaker var result *core.AgentResult var err error cbErr := a.circuitBreaker.Execute(ctx, func() error { result, err = a.executePrimary(ctx, event, state) return err }) // If circuit breaker is open or primary fails, use fallback if cbErr != nil || err != nil { if a.fallbackAgent != nil { fmt.Printf(\"Circuit breaker open for %s, using fallback\\n\", a.name) return a.fallbackAgent.Execute(ctx, event, state) } // Return a safe fallback response return \u0026core.AgentResult{ Data: map[string]interface{}{ \"error\": \"Service temporarily unavailable\", \"message\": \"Please try again later\", \"agent\": a.name, }, }, nil } return result, nil } func (a *ResilientAgent) executePrimary(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { // Your primary agent logic here query := fmt.Sprintf(\"Process this request: %v\", event.Data) response, err := a.llmProvider.GenerateResponse(ctx, query, nil) if err != nil { return nil, fmt.Errorf(\"LLM request failed: %w\", err) } return \u0026core.AgentResult{ Data: map[string]interface{}{ \"response\": response, \"agent\": a.name, }, }, nil } func (a *ResilientAgent) SetFallback(fallback core.AgentHandler) { a.fallbackAgent = fallback } func (a *ResilientAgent) GetCircuitBreakerState() patterns.CircuitState { return a.circuitBreaker.GetState() }\r🔄 Advanced Circuit Breaker Patterns Hierarchical Circuit Breakers Implement circuit breakers at multiple levels for fine-grained control:\ntype HierarchicalCircuitBreaker struct { serviceBreaker *patterns.CircuitBreaker // Service-level operationBreakers map[string]*patterns.CircuitBreaker // Operation-level mu sync.RWMutex } func NewHierarchicalCircuitBreaker() *HierarchicalCircuitBreaker { return \u0026HierarchicalCircuitBreaker{ serviceBreaker: patterns.NewCircuitBreaker(10, 5, 60*time.Second), operationBreakers: make(map[string]*patterns.CircuitBreaker), } } func (hcb *HierarchicalCircuitBreaker) Execute(ctx context.Context, operation string, fn func() error) error { // Check service-level circuit breaker first if !hcb.serviceBreaker.canExecute() { return errors.New(\"service circuit breaker is open\") } // Get or create operation-level circuit breaker opBreaker := hcb.getOperationBreaker(operation) // Execute with both circuit breakers return opBreaker.Execute(ctx, func() error { return hcb.serviceBreaker.Execute(ctx, fn) }) } func (hcb *HierarchicalCircuitBreaker) getOperationBreaker(operation string) *patterns.CircuitBreaker { hcb.mu.RLock() breaker, exists := hcb.operationBreakers[operation] hcb.mu.RUnlock() if !exists { hcb.mu.Lock() // Double-check pattern if breaker, exists = hcb.operationBreakers[operation]; !exists { breaker = patterns.NewCircuitBreaker(3, 2, 30*time.Second) hcb.operationBreakers[operation] = breaker } hcb.mu.Unlock() } return breaker }\rAdaptive Circuit Breaker Circuit breaker that adapts its thresholds based on historical performance:\ntype AdaptiveCircuitBreaker struct { *patterns.CircuitBreaker // Adaptive parameters baseFailureThreshold int performanceHistory []float64 adaptationWindow int mu sync.RWMutex } func NewAdaptiveCircuitBreaker(baseThreshold int) *AdaptiveCircuitBreaker { return \u0026AdaptiveCircuitBreaker{ CircuitBreaker: patterns.NewCircuitBreaker(baseThreshold, 3, 30*time.Second), baseFailureThreshold: baseThreshold, performanceHistory: make([]float64, 0, 100), adaptationWindow: 20, } } func (acb *AdaptiveCircuitBreaker) Execute(ctx context.Context, fn func() error) error { start := time.Now() err := acb.CircuitBreaker.Execute(ctx, fn) duration := time.Since(start) // Record performance metrics acb.recordPerformance(duration, err) // Adapt thresholds based on recent performance acb.adaptThresholds() return err } func (acb *AdaptiveCircuitBreaker) recordPerformance(duration time.Duration, err error) { acb.mu.Lock() defer acb.mu.Unlock() // Record success rate and response time successRate := 1.0 if err != nil { successRate = 0.0 } // Combine success rate and response time into a performance score responseTimeScore := math.Max(0, 1.0-duration.Seconds()/10.0) // Normalize to 10s max performanceScore := (successRate + responseTimeScore) / 2.0 acb.performanceHistory = append(acb.performanceHistory, performanceScore) // Keep only recent history if len(acb.performanceHistory) \u003e 100 { acb.performanceHistory = acb.performanceHistory[1:] } } func (acb *AdaptiveCircuitBreaker) adaptThresholds() { acb.mu.Lock() defer acb.mu.Unlock() if len(acb.performanceHistory) \u003c acb.adaptationWindow { return } // Calculate recent average performance recent := acb.performanceHistory[len(acb.performanceHistory)-acb.adaptationWindow:] avgPerformance := 0.0 for _, score := range recent { avgPerformance += score } avgPerformance /= float64(len(recent)) // Adapt failure threshold based on performance if avgPerformance \u003e 0.8 { // Good performance - increase threshold (more tolerant) acb.failureThreshold = int(float64(acb.baseFailureThreshold) * 1.5) } else if avgPerformance \u003c 0.5 { // Poor performance - decrease threshold (more sensitive) acb.failureThreshold = int(float64(acb.baseFailureThreshold) * 0.7) } else { // Normal performance - use base threshold acb.failureThreshold = acb.baseFailureThreshold } }\r📊 Circuit Breaker Monitoring Metrics Collection type CircuitBreakerMetrics struct { totalRequests int64 successfulRequests int64 failedRequests int64 circuitOpenTime time.Duration stateChanges int64 mu sync.RWMutex } func (cbm *CircuitBreakerMetrics) RecordRequest(success bool) { cbm.mu.Lock() defer cbm.mu.Unlock() cbm.totalRequests++ if success { cbm.successfulRequests++ } else { cbm.failedRequests++ } } func (cbm *CircuitBreakerMetrics) RecordStateChange(newState patterns.CircuitState) { cbm.mu.Lock() defer cbm.mu.Unlock() cbm.stateChanges++ // Track time spent in open state if newState == patterns.StateOpen { cbm.circuitOpenTime = time.Now().Sub(time.Time{}) } } func (cbm *CircuitBreakerMetrics) GetSuccessRate() float64 { cbm.mu.RLock() defer cbm.mu.RUnlock() if cbm.totalRequests == 0 { return 0.0 } return float64(cbm.successfulRequests) / float64(cbm.totalRequests) }\rHealth Check Integration func (a *ResilientAgent) HealthCheck() map[string]interface{} { state := a.circuitBreaker.GetState() health := map[string]interface{}{ \"agent\": a.name, \"circuit_breaker\": map[string]interface{}{ \"state\": stateToString(state), \"healthy\": state == patterns.StateClosed, }, } // Add metrics if available if metrics := a.getMetrics(); metrics != nil { health[\"metrics\"] = map[string]interface{}{ \"success_rate\": metrics.GetSuccessRate(), \"total_requests\": metrics.totalRequests, \"state_changes\": metrics.stateChanges, } } return health } func stateToString(state patterns.CircuitState) string { switch state { case patterns.StateClosed: return \"closed\" case patterns.StateOpen: return \"open\" case patterns.StateHalfOpen: return \"half-open\" default: return \"unknown\" } }\r🚨 Error Handling Strategies Graceful Degradation type GracefulAgent struct { primary core.AgentHandler secondary core.AgentHandler cache map[string]*core.AgentResult mu sync.RWMutex } func (ga *GracefulAgent) Execute(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { // Try primary agent result, err := ga.primary.Execute(ctx, event, state) if err == nil { // Cache successful result ga.cacheResult(event, result) return result, nil } // Try secondary agent if ga.secondary != nil { result, err = ga.secondary.Execute(ctx, event, state) if err == nil { return result, nil } } // Try cached result if cached := ga.getCachedResult(event); cached != nil { return cached, nil } // Return safe fallback return \u0026core.AgentResult{ Data: map[string]interface{}{ \"error\": \"All services unavailable\", \"message\": \"Using fallback response\", }, }, nil } func (ga *GracefulAgent) cacheResult(event core.Event, result *core.AgentResult) { ga.mu.Lock() defer ga.mu.Unlock() key := fmt.Sprintf(\"%s:%v\", event.Type, event.Data) ga.cache[key] = result // Implement cache eviction policy if len(ga.cache) \u003e 100 { // Remove oldest entries (simple LRU) for k := range ga.cache { delete(ga.cache, k) break } } }\r🔧 Best Practices 1. Circuit Breaker Configuration # Conservative settings for critical services [critical_service.circuit_breaker] failure_threshold = 3 success_threshold = 2 timeout_ms = 60000 max_concurrent_calls = 5 # More tolerant settings for non-critical services [non_critical_service.circuit_breaker] failure_threshold = 10 success_threshold = 5 timeout_ms = 30000 max_concurrent_calls = 20\r2. Monitoring and Alerting // Set up alerts for circuit breaker state changes func (cb *CircuitBreaker) onStateChange(oldState, newState CircuitState) { if newState == StateOpen { // Alert: Circuit breaker opened sendAlert(\"Circuit breaker opened\", map[string]interface{}{ \"service\": cb.serviceName, \"timestamp\": time.Now(), \"failures\": cb.failureCount, }) } if oldState == StateOpen \u0026\u0026 newState == StateClosed { // Alert: Circuit breaker recovered sendAlert(\"Circuit breaker recovered\", map[string]interface{}{ \"service\": cb.serviceName, \"timestamp\": time.Now(), \"downtime\": time.Since(cb.lastFailureTime), }) } }\r3. Testing Circuit Breakers func TestCircuitBreakerBehavior(t *testing.T) { cb := patterns.NewCircuitBreaker(3, 2, 100*time.Millisecond) // Test closed state assert.Equal(t, patterns.StateClosed, cb.GetState()) // Trigger failures to open circuit for i := 0; i \u003c 3; i++ { err := cb.Execute(context.Background(), func() error { return errors.New(\"simulated failure\") }) assert.Error(t, err) } // Circuit should be open now assert.Equal(t, patterns.StateOpen, cb.GetState()) // Requests should be rejected err := cb.Execute(context.Background(), func() error { return nil }) assert.Error(t, err) assert.Contains(t, err.Error(), \"circuit breaker is open\") // Wait for timeout time.Sleep(150 * time.Millisecond) // Should allow test request (half-open) err = cb.Execute(context.Background(), func() error { return nil }) assert.NoError(t, err) }\r🎯 Common Patterns 1. Bulkhead Pattern with Circuit Breakers type BulkheadAgent struct { pools map[string]*ResourcePool mu sync.RWMutex } type ResourcePool struct { semaphore chan struct{} circuitBreaker *patterns.CircuitBreaker } func (ba *BulkheadAgent) Execute(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { poolName := ba.getPoolName(event) pool := ba.getPool(poolName) // Acquire resource from pool select { case pool.semaphore \u003c- struct{}{}: defer func() { \u003c-pool.semaphore }() case \u003c-ctx.Done(): return nil, ctx.Err() } // Execute with circuit breaker var result *core.AgentResult var err error cbErr := pool.circuitBreaker.Execute(ctx, func() error { result, err = ba.executeInPool(ctx, event, state, poolName) return err }) if cbErr != nil { return nil, cbErr } return result, err }\r2. Retry with Circuit Breaker func (a *ResilientAgent) ExecuteWithRetry(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { var lastErr error for attempt := 0; attempt \u003c 3; attempt++ { // Check circuit breaker before each attempt if !a.circuitBreaker.canExecute() { return nil, errors.New(\"circuit breaker is open\") } result, err := a.Execute(ctx, event, state) if err == nil { return result, nil } lastErr = err // Exponential backoff backoff := time.Duration(attempt*attempt) * 100 * time.Millisecond select { case \u003c-time.After(backoff): continue case \u003c-ctx.Done(): return nil, ctx.Err() } } return nil, fmt.Errorf(\"all retry attempts failed: %w\", lastErr) }\rCircuit breakers are essential for building production-ready agent systems. They provide automatic failure detection, prevent cascade failures, and enable graceful degradation when services are unavailable.\n🚀 Next Steps Retry Policies - Learn about intelligent retry strategies Load Balancing - Distribute load across multiple agents Testing Strategies - Test your resilient agent systems Production Monitoring - Monitor circuit breaker health",
    "description": "Circuit Breaker Patterns in AgenticGoKit Building fault-tolerant agent systems with automatic failure detection and recovery\nCircuit breakers are essential for building resilient multi-agent systems. They automatically detect failures and prevent cascade failures by temporarily stopping requests to failing services. This guide shows you how to implement and configure circuit breaker patterns in AgenticGoKit.\n🔌 Understanding Circuit Breakers A circuit breaker works like an electrical circuit breaker - it “opens” when it detects too many failures, preventing further requests until the service recovers.",
    "tags": [],
    "title": "circuit-breaker-patterns",
    "uri": "/AgenticGoKitDocs/tutorials/advanced/circuit-breaker-patterns/index.html"
  },
  {
    "breadcrumb": "",
    "content": "AgenticGoKit Documentation The Go Framework for Building Multi-Agent AI Systems\nBuild intelligent agent workflows with dynamic tool integration, multi-provider LLM support, and enterprise-grade orchestration patterns. Go-native performance meets AI agent systems.\n⚡ 5-Minute Demo Create a collaborative multi-agent system with one command:\n# Install the CLI go install github.com/kunalkushwaha/agenticgokit/cmd/agentcli@latest # Create a multi-agent research team agentcli create research-team --orchestration-mode collaborative --agents 3 --visualize cd research-team # Set your API key export AZURE_OPENAI_API_KEY=your-key-here export AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/ export AZURE_OPENAI_DEPLOYMENT=your-deployment-name # Run the collaborative system go run . -m \"Research the latest developments in AI agent frameworks\"\rWhat you get:\n✅ Complete Go project with main.go, agentflow.toml, and go.mod ✅ Three specialized agents working in parallel ✅ Automatic result synthesis and error handling ✅ Mermaid workflow diagrams generated ✅ Production-ready project structure 🚀 Why AgenticGoKit? 🏃‍♂️ For Developers Go-Native Performance: Compiled binaries, efficient memory usage Type Safety: Compile-time error checking prevents runtime issues Simple Deployment: Single binary, no complex Python environments Native Concurrency: Goroutines for true parallel agent execution 🤖 For AI Systems Multi-Agent Focus: Built specifically for agent orchestration Memory \u0026 RAG: Built-in vector databases and knowledge management Tool Integration: MCP protocol for dynamic tool discovery Production Ready: Error handling, monitoring, scaling patterns 🎯 Quick Start Paths 🏃‍♂️ 5-Minute Start Get your first agent running immediately\ngo get github.com/kunalkushwaha/agenticgokit\r→ Start Building\n🎓 Learn Step-by-Step Follow guided tutorials to master concepts\nYour First Agent Multi-Agent Collaboration Memory \u0026 RAG Tool Integration → Start Learning\n🚀 Explore Examples Run working examples and demos\ngit clone https://github.com/kunalkushwaha/agenticgokit cd examples/04-rag-knowledge-base docker-compose up -d go run main.go\r→ Browse Examples\n🏗️ What You Can Build 🔍 Research Assistants Multi-agent research teams with web search, analysis, and synthesis\nagentcli create research-team \\ --orchestration-mode collaborative \\ --agents 3 --mcp-enabled --visualize\r📊 Data Processing Pipelines Sequential workflows with error handling and monitoring\nagentcli create data-pipeline \\ --orchestration-mode sequential \\ --agents 4 --visualize\r💬 Conversational Systems Chat agents with persistent memory and context\nagentcli create chat-system \\ --agents 2 --visualize\r📚 Knowledge Bases RAG-powered Q\u0026A with document ingestion and vector search\nagentcli create knowledge-base \\ --orchestration-mode collaborative \\ --agents 3 --visualize\r📚 Documentation Structure 🚀 Learning Paths New to AgenticGoKit? Follow these guided paths:\nBeginner Path (30 minutes) 5-Minute Quickstart - Get running immediately Your First Agent - Build a simple agent Multi-Agent Collaboration - Agents working together Intermediate Path (1 hour) Memory \u0026 RAG - Add knowledge capabilities Tool Integration - Connect external tools Core Concepts - Deep dive into fundamentals Advanced Path (2+ hours) Advanced Patterns - Complex orchestration patterns Production Deployment - Deploy to production Performance Optimization - Scale your systems 📖 Documentation Sections 📚 Tutorials Learning-oriented guides to help you understand AgenticGoKit:\nGetting Started - Step-by-step beginner tutorials Core Concepts - Fundamental concepts and patterns Memory Systems - RAG and knowledge management MCP Tools - Tool integration and development Advanced Patterns - Complex orchestration patterns Debugging - Debugging and troubleshooting 🛠️ How-To Guides Task-oriented guides for specific scenarios:\nSetup - Configuration and environment setup Development - Development patterns and best practices Deployment - Production deployment and scaling Framework Comparison - vs LangChain, AutoGen, CrewAI 📋 Reference Information-oriented documentation:\nAPI Reference - Complete API documentation CLI Reference - Command-line interface documentation Configuration Reference - Configuration options 👥 Contributors For developers contributing to AgenticGoKit:\nContributor Guide - Development setup and workflow Code Style - Coding standards and conventions Testing - Testing strategies and guidelines 🧠 Core Concepts Multi-Agent Orchestration // Collaborative agents (parallel execution) agents := map[string]core.AgentHandler{ \"researcher\": NewResearchAgent(), \"analyzer\": NewAnalysisAgent(), \"validator\": NewValidationAgent(), } runner := core.CreateCollaborativeRunner(agents, 30*time.Second) result, err := runner.ProcessEvent(ctx, event)\rConfiguration-Based Setup # agentflow.toml [orchestration] mode = \"collaborative\" timeout_seconds = 30 [agent_memory] provider = \"pgvector\" enable_rag = true chunk_size = 1000 [mcp] enabled = true\rMemory \u0026 RAG Integration // Configure persistent memory with vector search memory, err := core.NewMemory(core.AgentMemoryConfig{ Provider: \"pgvector\", EnableRAG: true, EnableKnowledgeBase: true, ChunkSize: 1000, })\rTool Integration (MCP) // MCP tools are automatically discovered and integrated // Agents can use web search, file operations, and custom tools agent := agents.NewToolEnabledAgent(\"assistant\", llmProvider, toolManager)\r🌟 Current Features 🤖 Multi-Agent Orchestration: Collaborative, sequential, loop, and mixed patterns 🧠 Memory \u0026 RAG: PostgreSQL pgvector, Weaviate, and in-memory providers 🔧 Tool Integration: MCP protocol support for dynamic tool discovery ⚙️ Configuration Management: TOML-based configuration with environment overrides 📊 Workflow Visualization: Automatic Mermaid diagram generation 🎯 CLI Scaffolding: Generate complete projects with one command 📈 Production Patterns: Error handling, retry logic, and monitoring hooks 🚀 Installation \u0026 Setup Option 1: CLI Tool (Recommended) # Install the CLI go install github.com/kunalkushwaha/agenticgokit/cmd/agentcli@latest # Create your first project agentcli create my-agents --orchestration-mode collaborative --agents 3 --visualize cd my-agents\rOption 2: Go Module go mod init my-agent-project go get github.com/kunalkushwaha/agenticgokit # Create agentflow.toml configuration file # See reference/api/configuration.md for details\rEnvironment Setup # For Azure OpenAI (recommended) export AZURE_OPENAI_API_KEY=your-key-here export AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/ export AZURE_OPENAI_DEPLOYMENT=your-deployment-name # For OpenAI export OPENAI_API_KEY=your-key-here # For Ollama (local) export OLLAMA_HOST=http://localhost:11434\r🌍 Community \u0026 Support 💬 Get Help GitHub Discussions - Q\u0026A and community GitHub Issues - Bug reports and features Troubleshooting Guide - Common solutions 🤝 Contribute Contributor Guide - How to contribute Good First Issues - Start here Roadmap - Future plans 📢 Stay Updated GitHub Releases - Latest updates Star the Repo - Get notifications Follow Development - Activity 🏆 Why Choose AgenticGoKit? 🚀 Performance Compiled Go: Native performance, efficient memory usage Concurrent Processing: True parallel agent execution with goroutines Single Binary: No complex runtime dependencies Fast Startup: Instant initialization, no warm-up time 🛠️ Developer Experience Type Safety: Compile-time error checking CLI Scaffolding: Generate complete projects instantly Configuration-Driven: Change behavior without code changes Workflow Visualization: Automatic Mermaid diagrams 🤖 AI-First Design Multi-Agent Focus: Built specifically for agent orchestration Memory Integration: Built-in vector databases and RAG Tool Ecosystem: MCP protocol for dynamic capabilities Production Patterns: Error handling, retry logic, monitoring 🏭 Production Ready Error Handling: Comprehensive error routing and recovery Monitoring: Built-in logging and tracing capabilities Scalability: Designed for horizontal scaling patterns Configuration: Environment-based configuration management 🚀 Ready to Build? 🏃‍♂️ Start with 5-Minute Quickstart Build your first multi-agent system in 5 minutes\n🎓 Follow the Learning Path Master AgenticGoKit with step-by-step tutorials\n🚀 Explore Live Examples See working multi-agent systems in action\n⭐ Star us on GitHub • 📖 Read the Docs • 💬 Join Discussions\nLicense Apache 2.0 - see LICENSE for details.\nAgenticGoKit: Where Go performance meets AI agent intelligence.",
    "description": "AgenticGoKit Documentation The Go Framework for Building Multi-Agent AI Systems\nBuild intelligent agent workflows with dynamic tool integration, multi-provider LLM support, and enterprise-grade orchestration patterns. Go-native performance meets AI agent systems.",
    "tags": [],
    "title": "content",
    "uri": "/AgenticGoKitDocs/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials",
    "content": "Core Concepts Overview Navigation: Documentation Home → Tutorials → Core Concepts\nUnderstanding AgenticGoKit’s core concepts is essential for building effective multi-agent systems. This section covers the fundamental building blocks that power the framework.\nThe Big Picture AgenticGoKit is built around a few key concepts that work together to create a powerful multi-agent system:\ngraph TB\rEvent[Event] --\u003e Runner[Runner]\rRunner --\u003e Orchestrator[Orchestrator]\rOrchestrator --\u003e Agent[Agent]\rAgent --\u003e State[State]\rState --\u003e Memory[Memory]\rsubgraph \"Core Flow\"\rEvent --\u003e State\rState --\u003e Agent\rAgent --\u003e State\rend\rsubgraph \"Orchestration\"\rOrchestrator --\u003e AgentA[Agent A]\rOrchestrator --\u003e AgentB[Agent B]\rOrchestrator --\u003e AgentC[Agent C]\rend\rKey Components 1. Events - The Message System Events are the messages that flow through your agent system. They carry data, metadata, and routing information.\n// Create an event event := core.NewEvent(\"target-agent\", core.EventData{\"message\": \"Hello, world!\"}, map[string]string{\"priority\": \"high\"}) // Events have IDs, timestamps, and routing info fmt.Println(\"Event ID:\", event.GetID()) fmt.Println(\"Target:\", event.GetTargetAgentID())\r2. State - The Data Container State objects carry data between agents and persist information across interactions.\n// Create and manipulate state state := core.NewState() state.Set(\"user_input\", \"What's the weather like?\") state.SetMeta(\"session_id\", \"user-123\") // State is thread-safe and can be cloned/merged clonedState := state.Clone()\r3. Agents - The Processing Units Agents are the core processing units that transform input state into output state.\n// Agents implement a simple interface type Agent interface { Run(ctx context.Context, inputState State) (State, error) Name() string } // Create a simple agent agent := core.NewLLMAgent(\"assistant\", llmProvider) agentResult, err := agent.Run(ctx, event, state)\r4. Runner - The Event Processor The Runner manages the event processing loop, routing events to the appropriate agents.\n// Create and start a runner runner := core.NewRunner(100) // queue size runner.RegisterAgent(\"assistant\", agentHandler) runner.Start(ctx) // Emit events for processing runner.Emit(event)\r5. Orchestrator - The Coordination Engine Orchestrators determine how events are distributed to agents (single, parallel, sequential, etc.).\n// Different orchestration modes collaborativeRunner := core.CreateCollaborativeRunner(agents, 30*time.Second) sequentialRunner := core.NewOrchestrationBuilder(core.OrchestrationSequential). WithAgents(agents). Build()\rData Flow Architecture Understanding how data flows through the system is crucial:\nsequenceDiagram\rparticipant Client\rparticipant Runner\rparticipant Orchestrator\rparticipant Agent\rparticipant Memory\rClient-\u003e\u003eRunner: Emit Event\rRunner-\u003e\u003eOrchestrator: Dispatch Event\rOrchestrator-\u003e\u003eAgent: Run(ctx, event, state)\rAgent-\u003e\u003eMemory: Store/Retrieve Data\rMemory--\u003e\u003eAgent: Return Results\rAgent--\u003e\u003eOrchestrator: Return AgentResult\rOrchestrator--\u003e\u003eRunner: Return Result\rRunner-\u003e\u003eRunner: Process Result (emit new events)\rCore Patterns 1. Simple Agent Execution package main import ( \"context\" \"fmt\" \"log\" \"github.com/kunalkushwaha/agentflow/core\" ) // Simple agent implementation type SimpleAgent struct { name string llm core.ModelProvider } func (a *SimpleAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Get message from event message, ok := event.GetData()[\"message\"].(string) if !ok { return core.AgentResult{}, fmt.Errorf(\"no message found\") } // Create prompt prompt := core.Prompt{ System: \"You are a helpful assistant.\", User: message, } // Call LLM response, err := a.llm.Call(ctx, prompt) if err != nil { return core.AgentResult{}, err } // Create output state outputState := core.NewState() outputState.Set(\"response\", response.Content) return core.AgentResult{OutputState: outputState}, nil } func main() { // Create LLM provider from configuration provider, err := core.NewProviderFromWorkingDir() if err != nil { log.Fatal(err) } // Create agent agent := \u0026SimpleAgent{ name: \"assistant\", llm: provider, } // Create state with input state := core.NewState() state.Set(\"message\", \"Hello, world!\") // Create event event := core.NewEvent(\"assistant\", core.EventData{ \"message\": \"Hello, world!\", }, nil) // Run agent result, err := agent.Run(context.Background(), event, state) if err != nil { log.Fatal(err) } // Get response from output state if response, ok := result.OutputState.Get(\"response\"); ok { fmt.Println(response) } }\r2. Event-Driven Processing func main() { // Create agents agents := map[string]core.AgentHandler{ \"processor\": \u0026ProcessorAgent{}, \"responder\": \u0026ResponderAgent{}, } // Create collaborative runner runner := core.CreateCollaborativeRunner(agents, 30*time.Second) }) // Start processing runner.Start(context.Background()) // Emit event event := core.NewEvent(\"processor\", core.EventData{\"task\": \"analyze data\"}, nil) runner.Emit(event) }\r3. Multi-Agent Collaboration func main() { agents := map[string]core.AgentHandler{ \"researcher\": \u0026ResearchAgent{}, \"analyzer\": \u0026AnalysisAgent{}, \"writer\": \u0026WritingAgent{}, } // All agents work on the same input runner := core.CreateCollaborativeRunner(agents, 60*time.Second) ctx := context.Background() runner.Start(ctx) defer runner.Stop() event := core.NewEvent(\"researcher\", // Start with researcher core.EventData{\"topic\": \"AI trends\"}, map[string]string{\"route\": \"researcher\"}) err := runner.Emit(event) if err != nil { log.Fatal(err) } // Wait for processing time.Sleep(10 * time.Second) }\rMemory Integration AgenticGoKit provides powerful memory capabilities for persistent storage and RAG:\n// Configure memory memoryConfig := core.AgentMemoryConfig{ Provider: \"pgvector\", Connection: \"postgres://user:pass@localhost/db\", EnableRAG: true, } memory, err := core.NewMemory(memoryConfig) // Store information memory.Store(ctx, core.MemoryItem{ Content: \"Important information\", Tags: []string{\"important\", \"user-data\"}, }) // Search with RAG results, err := memory.Search(ctx, \"find important information\")\rError Handling Patterns AgenticGoKit provides sophisticated error handling and recovery:\n// Configure error routing errorConfig := core.DefaultErrorRouterConfig() errorConfig.MaxRetries = 3 errorConfig.BackoffFactor = 2.0 runner := core.NewRunnerWithConfig(core.RunnerConfig{ ErrorRouterConfig: errorConfig, Agents: map[string]core.AgentHandler{ \"main-agent\": mainAgent, \"error-handler\": errorHandlerAgent, }, })\rNext Steps Now that you understand the core concepts, dive deeper into specific areas:\nMessage Passing - Learn how events flow through the system State Management - Master data handling between agents Agent Lifecycle - Understand agent creation and execution Error Handling - Build robust error management Or jump to specific orchestration patterns:\nOrchestration Overview - Learn about different orchestration modes Key Takeaways Events carry messages and data through the system State objects persist data between agent interactions Agents are the core processing units that transform state Runners manage the event processing loop Orchestrators coordinate how agents work together Memory provides persistent storage and RAG capabilities The system is designed for scalability, fault tolerance, and flexibility Understanding these concepts will help you build more effective and maintainable multi-agent systems with AgenticGoKit.",
    "description": "Core Concepts Overview Navigation: Documentation Home → Tutorials → Core Concepts\nUnderstanding AgenticGoKit’s core concepts is essential for building effective multi-agent systems. This section covers the fundamental building blocks that power the framework.\nThe Big Picture AgenticGoKit is built around a few key concepts that work together to create a powerful multi-agent system:\ngraph TB\rEvent[Event] --\u003e Runner[Runner]\rRunner --\u003e Orchestrator[Orchestrator]\rOrchestrator --\u003e Agent[Agent]\rAgent --\u003e State[State]\rState --\u003e Memory[Memory]\rsubgraph \"Core Flow\"\rEvent --\u003e State\rState --\u003e Agent\rAgent --\u003e State\rend\rsubgraph \"Orchestration\"\rOrchestrator --\u003e AgentA[Agent A]\rOrchestrator --\u003e AgentB[Agent B]\rOrchestrator --\u003e AgentC[Agent C]\rend\rKey Components 1. Events - The Message System Events are the messages that flow through your agent system. They carry data, metadata, and routing information.",
    "tags": [],
    "title": "core-concepts",
    "uri": "/AgenticGoKitDocs/tutorials/core-concepts/index.html"
  },
  {
    "breadcrumb": "content \u003e guides \u003e development",
    "content": "Debugging Agent Interactions Understand and troubleshoot multi-agent workflows effectively\nThis guide teaches you how to debug complex agent interactions in AgenticGoKit. You’ll learn to use tracing, logging, and debugging tools to understand what’s happening in your multi-agent systems and resolve issues quickly.\nQuick Start (5 minutes) 1. Enable Debug Logging Update your agentflow.toml:\n[logging] level = \"debug\" # Change from \"info\" to \"debug\" format = \"json\" file = \"debug.log\" # Optional: log to file\r2. Run with Verbose Output # Run your agent with debug output go run . -m \"Test message\" --verbose # Or set environment variable export AGENTFLOW_LOG_LEVEL=debug go run . -m \"Test message\"\r3. Use Built-in Tracing # Check if tracing is available agentcli trace --help # View recent traces (if available) agentcli trace --recent\rUnderstanding Agent Flow Basic Flow Visualization AgenticGoKit processes events through this flow:\nEvent → Runner → Agent Selection → Agent Execution → Result Processing → Next Agent\rTracing Agent Execution Enable detailed tracing in your code:\nimport ( \"context\" \"log\" \"github.com/kunalkushwaha/agenticgokit/core\" ) func main() { // Enable debug logging core.SetLogLevel(\"debug\") // Create agents with tracing agents := map[string]core.AgentHandler{ \"analyzer\": NewTracedAgent(\"analyzer\", llmProvider), \"processor\": NewTracedAgent(\"processor\", llmProvider), } // Create runner with debug info runner := core.CreateSequentialRunner(agents, []string{\"analyzer\", \"processor\"}, 30*time.Second) // Process event with context ctx := context.WithValue(context.Background(), \"trace_id\", generateTraceID()) event := core.NewEvent(\"analyze\", map[string]interface{}{ \"input\": \"Debug this workflow\", }) results, err := runner.ProcessEvent(ctx, event) if err != nil { log.Printf(\"Error processing event: %v\", err) } log.Printf(\"Results: %+v\", results) }\rCreating a Traced Agent type TracedAgent struct { name string llmProvider core.ModelProvider } func NewTracedAgent(name string, provider core.ModelProvider) *TracedAgent { return \u0026TracedAgent{ name: name, llmProvider: provider, } } func (a *TracedAgent) Execute(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { traceID := ctx.Value(\"trace_id\") log.Printf(\"[TRACE:%v] Agent %s starting execution\", traceID, a.name) log.Printf(\"[TRACE:%v] Event type: %s, data: %+v\", traceID, event.Type, event.Data) log.Printf(\"[TRACE:%v] State: %+v\", traceID, state.Data) start := time.Now() // Your agent logic here prompt := fmt.Sprintf(\"Process this input: %v\", event.Data) response, err := a.llmProvider.GenerateResponse(ctx, prompt, nil) duration := time.Since(start) if err != nil { log.Printf(\"[TRACE:%v] Agent %s failed after %v: %v\", traceID, a.name, duration, err) return nil, err } result := \u0026core.AgentResult{ Data: map[string]interface{}{ \"response\": response, \"agent\": a.name, \"duration\": duration.String(), }, } log.Printf(\"[TRACE:%v] Agent %s completed in %v\", traceID, a.name, duration) log.Printf(\"[TRACE:%v] Result: %+v\", traceID, result.Data) return result, nil } func generateTraceID() string { return fmt.Sprintf(\"trace_%d\", time.Now().UnixNano()) }\rCommon Debugging Scenarios 1. Agent Not Executing Symptoms:\nNo output from specific agents Workflow stops unexpectedly Silent failures Debug Steps:\n// Add execution checks func (a *MyAgent) Execute(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { log.Printf(\"DEBUG: Agent %s received event type: %s\", a.name, event.Type) // Check if agent should handle this event if !a.shouldHandle(event) { log.Printf(\"DEBUG: Agent %s skipping event type: %s\", a.name, event.Type) return nil, nil // Return nil to pass to next agent } log.Printf(\"DEBUG: Agent %s processing event\", a.name) // Your processing logic... return result, nil } func (a *MyAgent) shouldHandle(event core.Event) bool { // Add your logic to determine if this agent should handle the event return event.Type == \"analyze\" || event.Type == \"process\" }\r2. State Management Issues Symptoms:\nData not passed between agents Unexpected state modifications Missing context Debug Steps:\nfunc debugState(ctx context.Context, state *core.State, agentName string, phase string) { log.Printf(\"DEBUG: [%s] Agent %s state %s:\", generateTraceID(), agentName, phase) log.Printf(\" Data keys: %v\", getKeys(state.Data)) log.Printf(\" Metadata keys: %v\", getKeys(state.Metadata)) // Log specific important fields if val, exists := state.Data[\"important_field\"]; exists { log.Printf(\" important_field: %v\", val) } // Check for common issues if len(state.Data) == 0 { log.Printf(\" WARNING: State data is empty\") } } func getKeys(m map[string]interface{}) []string { keys := make([]string, 0, len(m)) for k := range m { keys = append(keys, k) } return keys } // Use in your agent func (a *MyAgent) Execute(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { debugState(ctx, state, a.name, \"before\") // Your processing... debugState(ctx, state, a.name, \"after\") return result, nil }\r3. Performance Issues Symptoms:\nSlow agent execution High memory usage Timeout errors Debug Steps:\ntype PerformanceTracker struct { agentTimes map[string][]time.Duration mutex sync.RWMutex } func NewPerformanceTracker() *PerformanceTracker { return \u0026PerformanceTracker{ agentTimes: make(map[string][]time.Duration), } } func (pt *PerformanceTracker) TrackAgent(agentName string, duration time.Duration) { pt.mutex.Lock() defer pt.mutex.Unlock() pt.agentTimes[agentName] = append(pt.agentTimes[agentName], duration) } func (pt *PerformanceTracker) GetStats(agentName string) (avg, min, max time.Duration, count int) { pt.mutex.RLock() defer pt.mutex.RUnlock() times := pt.agentTimes[agentName] if len(times) == 0 { return 0, 0, 0, 0 } var total time.Duration min = times[0] max = times[0] for _, t := range times { total += t if t \u003c min { min = t } if t \u003e max { max = t } } avg = total / time.Duration(len(times)) count = len(times) return } func (pt *PerformanceTracker) PrintStats() { pt.mutex.RLock() defer pt.mutex.RUnlock() fmt.Println(\"Agent Performance Stats:\") for agent, times := range pt.agentTimes { if len(times) \u003e 0 { avg, min, max, count := pt.GetStats(agent) fmt.Printf(\" %s: avg=%v, min=%v, max=%v, count=%d\\n\", agent, avg, min, max, count) } } }\rProduction Debugging Structured Logging import ( \"github.com/sirupsen/logrus\" ) func setupProductionLogging() *logrus.Logger { logger := logrus.New() logger.SetFormatter(\u0026logrus.JSONFormatter{}) logger.SetLevel(logrus.InfoLevel) // Add common fields logger = logger.WithFields(logrus.Fields{ \"service\": \"agenticgokit\", \"version\": \"1.0.0\", }).Logger return logger } // Use in agents func (a *MyAgent) Execute(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { logger := setupProductionLogging() logger.WithFields(logrus.Fields{ \"agent\": a.name, \"event_type\": event.Type, \"trace_id\": ctx.Value(\"trace_id\"), }).Info(\"Agent execution started\") // Your processing... logger.WithFields(logrus.Fields{ \"agent\": a.name, \"duration\": time.Since(start).String(), \"success\": err == nil, }).Info(\"Agent execution completed\") return result, err }\rHealth Checks func setupHealthChecks(agents map[string]core.AgentHandler) { http.HandleFunc(\"/health\", func(w http.ResponseWriter, r *http.Request) { health := map[string]interface{}{ \"status\": \"healthy\", \"timestamp\": time.Now(), \"agents\": make(map[string]string), } // Check each agent's health for name, agent := range agents { if healthChecker, ok := agent.(HealthChecker); ok { if healthChecker.IsHealthy() { health[\"agents\"].(map[string]string)[name] = \"healthy\" } else { health[\"agents\"].(map[string]string)[name] = \"unhealthy\" health[\"status\"] = \"degraded\" } } else { health[\"agents\"].(map[string]string)[name] = \"unknown\" } } w.Header().Set(\"Content-Type\", \"application/json\") json.NewEncoder(w).Encode(health) }) log.Println(\"Health check endpoint available at /health\") go http.ListenAndServe(\":8081\", nil) } type HealthChecker interface { IsHealthy() bool }\rTroubleshooting Checklist When Agents Don’t Respond Check if the agent is registered with the runner Verify the event type matches what the agent expects Ensure the agent’s Execute method is being called Check for panics or unhandled errors Verify LLM provider credentials and connectivity When Performance is Poor Measure individual agent execution times Check for memory leaks or excessive allocations Monitor goroutine count for potential leaks Verify database connection pooling (if using memory providers) Check LLM provider response times When State is Lost Verify state is being passed correctly between agents Check for state mutations that might cause issues Ensure proper error handling doesn’t lose state Verify orchestration mode is appropriate for your use case Tools and Utilities Debug Command Line Tool # Create a simple debug script cat \u003e debug.sh \u003c\u003c 'EOF' #!/bin/bash echo \"Starting AgenticGoKit Debug Session\" echo \"==================================\" # Set debug environment export AGENTFLOW_LOG_LEVEL=debug export DEBUG_PAUSE=false # Run with debug output go run . -m \"$1\" 2\u003e\u00261 | tee debug_output.log echo \"Debug output saved to debug_output.log\" EOF chmod +x debug.sh # Usage ./debug.sh \"Debug this interaction\"\rLog Analysis # Analyze debug logs grep \"ERROR\" debug_output.log grep \"Agent.*failed\" debug_output.log grep \"duration\" debug_output.log | sort -k3 -n\rNext Steps Testing Agents - Comprehensive testing strategies Best Practices - Development best practices Production Deployment - Production deployment",
    "description": "Debugging Agent Interactions Understand and troubleshoot multi-agent workflows effectively\nThis guide teaches you how to debug complex agent interactions in AgenticGoKit. You’ll learn to use tracing, logging, and debugging tools to understand what’s happening in your multi-agent systems and resolve issues quickly.\nQuick Start (5 minutes) 1. Enable Debug Logging Update your agentflow.toml:\n[logging] level = \"debug\" # Change from \"info\" to \"debug\" format = \"json\" file = \"debug.log\" # Optional: log to file\r2. Run with Verbose Output # Run your agent with debug output go run . -m \"Test message\" --verbose # Or set environment variable export AGENTFLOW_LOG_LEVEL=debug go run . -m \"Test message\"\r3. Use Built-in Tracing # Check if tracing is available agentcli trace --help # View recent traces (if available) agentcli trace --recent\rUnderstanding Agent Flow Basic Flow Visualization AgenticGoKit processes events through this flow:",
    "tags": [],
    "title": "debugging",
    "uri": "/AgenticGoKitDocs/guides/development/debugging/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials",
    "content": "Debugging and Monitoring in AgenticGoKit Overview This section covers comprehensive debugging and monitoring strategies for AgenticGoKit applications. You’ll learn how to troubleshoot multi-agent systems, implement effective logging, set up monitoring, and optimize performance in production environments.\nDebugging multi-agent systems presents unique challenges due to their distributed nature, asynchronous execution, and complex interaction patterns. This guide provides practical tools and techniques to help you identify, diagnose, and resolve issues effectively.\nWhat You’ll Learn Debugging Multi-Agent Systems: Techniques for troubleshooting complex agent interactions Logging and Tracing: Implementing structured logging and distributed tracing Performance Monitoring: Setting up metrics, alerts, and performance optimization Production Troubleshooting: Common issues and their solutions in production environments Prerequisites Before diving into debugging and monitoring, you should be familiar with:\nAgent Lifecycle Message Passing and Event Flow Orchestration Patterns State Management Why Debugging Multi-Agent Systems is Different Multi-agent systems introduce several debugging challenges:\n1. Concurrent Execution Agents may run on different threads within the same process State changes occur across multiple components simultaneously Race conditions and timing issues are more common with concurrent execution 2. Asynchronous Communication Events flow through the system asynchronously Cause and effect relationships may be separated in time Error propagation can be complex and delayed 3. Complex State Management State is shared and modified by multiple agents State transformations may be non-deterministic Debugging requires understanding the entire state flow 4. Orchestration Complexity Different orchestration patterns have different failure modes Agent interactions can create emergent behaviors System behavior may vary based on timing and load Debugging Philosophy Effective debugging of multi-agent systems requires a systematic approach:\n1. Observability First Implement comprehensive logging before you need it Use structured logging with consistent formats Include correlation IDs to track requests across agents 2. Fail Fast and Clearly Validate inputs and state at agent boundaries Use clear, actionable error messages Implement circuit breakers to prevent cascade failures 3. Isolate and Reproduce Create minimal test cases that reproduce issues Use deterministic testing with controlled inputs Implement agent mocking for isolated testing 4. Monitor Continuously Set up metrics and alerts for key system behaviors Track performance trends over time Use health checks to detect issues early Getting Started Start with Debugging Multi-Agent Systems to learn fundamental debugging techniques, then progress through the other guides based on your specific needs.\nQuick Reference Common Debugging Commands # List available trace sessions agentcli list # View execution trace for a session agentcli trace \u003csession-id\u003e # View only agent flow without state details agentcli trace --flow-only \u003csession-id\u003e # Filter trace to specific agent agentcli trace --filter agent=\u003cagent-name\u003e \u003csession-id\u003e # View verbose trace with full state details agentcli trace --verbose \u003csession-id\u003e # Debug trace structure agentcli trace --debug \u003csession-id\u003e # Check MCP servers and tools agentcli mcp servers agentcli mcp tools # View memory system status agentcli memory --stats # Check cache statistics agentcli cache stats\rConfiguration in agentflow.toml [logging] level = \"debug\" # debug, info, warn, error format = \"json\" # json or text [agent_flow] name = \"my-agent-system\" version = \"1.0.0\" # Enable tracing (traces are saved as .trace.json files) [runtime] max_concurrent_agents = 10\rKey Metrics to Monitor Agent Response Time: Time from event receipt to result Error Rate: Percentage of failed agent executions Queue Depth: Number of pending events in orchestrator Memory Usage: Memory consumption per agent Tool Call Latency: Time for external tool executions",
    "description": "Debugging and Monitoring in AgenticGoKit Overview This section covers comprehensive debugging and monitoring strategies for AgenticGoKit applications. You’ll learn how to troubleshoot multi-agent systems, implement effective logging, set up monitoring, and optimize performance in production environments.\nDebugging multi-agent systems presents unique challenges due to their distributed nature, asynchronous execution, and complex interaction patterns. This guide provides practical tools and techniques to help you identify, diagnose, and resolve issues effectively.\nWhat You’ll Learn Debugging Multi-Agent Systems: Techniques for troubleshooting complex agent interactions Logging and Tracing: Implementing structured logging and distributed tracing Performance Monitoring: Setting up metrics, alerts, and performance optimization Production Troubleshooting: Common issues and their solutions in production environments Prerequisites Before diving into debugging and monitoring, you should be familiar with:",
    "tags": [],
    "title": "debugging",
    "uri": "/AgenticGoKitDocs/tutorials/debugging/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials \u003e debugging",
    "content": "Debugging Multi-Agent Systems in AgenticGoKit Overview Debugging multi-agent systems requires specialized techniques due to their distributed, asynchronous nature. This guide covers practical strategies for identifying, isolating, and resolving issues in complex agent interactions using AgenticGoKit’s built-in tracing and debugging capabilities.\nAgenticGoKit provides comprehensive tracing through the agentcli tool and structured logging through the zerolog library, making it easier to understand agent interactions and diagnose issues.\nPrerequisites Understanding of Agent Lifecycle Familiarity with Orchestration Patterns Basic knowledge of Go debugging tools AgenticGoKit project with agentflow.toml configuration Common Debugging Scenarios 1. Agent Not Responding When an agent stops responding to events, follow this systematic approach:\nSymptoms Events are queued but not processed Agent appears “stuck” or unresponsive Timeout errors in orchestrator Debugging Steps // 1. Check agent health status func checkAgentHealth(ctx context.Context, agent core.Agent) error { // Create a simple health check event healthEvent := core.NewEvent(\"health_check\", map[string]interface{}{ \"timestamp\": time.Now(), \"check_id\": uuid.New().String(), }) // Set a short timeout for health check healthCtx, cancel := context.WithTimeout(ctx, 5*time.Second) defer cancel() // Try to process the health check result, err := agent.Process(healthCtx, healthEvent, core.NewState()) if err != nil { return fmt.Errorf(\"agent health check failed: %w\", err) } log.Printf(\"Agent health check passed: %+v\", result) return nil } // 2. Check for deadlocks or blocking operations func debugAgentExecution(agent core.Agent) { // Enable detailed logging logger := log.With(). Str(\"component\", \"agent_debug\"). Str(\"agent_id\", agent.ID()). Logger() // Wrap agent with debugging middleware debugAgent := \u0026DebuggingAgent{ agent: agent, logger: logger, } // Use the debug-wrapped agent // This will log all method calls and their durations } type DebuggingAgent struct { agent core.Agent logger zerolog.Logger } func (d *DebuggingAgent) Process(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { start := time.Now() eventID := event.GetID() d.logger.Info(). Str(\"event_id\", eventID). Str(\"event_type\", event.GetType()). Msg(\"Agent processing started\") // Check for context cancellation select { case \u003c-ctx.Done(): d.logger.Warn(). Str(\"event_id\", eventID). Err(ctx.Err()). Msg(\"Context cancelled before processing\") return core.AgentResult{}, ctx.Err() default: } result, err := d.agent.Process(ctx, event, state) duration := time.Since(start) if err != nil { d.logger.Error(). Str(\"event_id\", eventID). Dur(\"duration\", duration). Err(err). Msg(\"Agent processing failed\") } else { d.logger.Info(). Str(\"event_id\", eventID). Dur(\"duration\", duration). Msg(\"Agent processing completed\") } return result, err }\rCommon Causes and Solutions Blocking I/O Operations\n// Problem: Blocking without timeout resp, err := http.Get(\"https://api.example.com/data\") // Solution: Use context with timeout ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second) defer cancel() req, _ := http.NewRequestWithContext(ctx, \"GET\", \"https://api.example.com/data\", nil) resp, err := http.DefaultClient.Do(req)\rResource Exhaustion\n// Monitor resource usage func monitorAgentResources(agent core.Agent) { ticker := time.NewTicker(30 * time.Second) defer ticker.Stop() for range ticker.C { var m runtime.MemStats runtime.ReadMemStats(\u0026m) log.Printf(\"Agent %s - Memory: %d KB, Goroutines: %d\", agent.ID(), m.Alloc/1024, runtime.NumGoroutine()) } }\rDeadlocks\n// Use Go's race detector and deadlock detector // go run -race -tags deadlock main.go // Add timeout to all blocking operations func safeChannelSend(ch chan\u003c- interface{}, data interface{}, timeout time.Duration) error { select { case ch \u003c- data: return nil case \u003c-time.After(timeout): return fmt.Errorf(\"channel send timeout after %v\", timeout) } }\r2. Incorrect Agent Interactions When agents don’t interact as expected, use these debugging techniques:\nEvent Flow Tracing // Create a request tracer to follow events through the system type RequestTracer struct { requestID string events []TraceEvent mu sync.Mutex } type TraceEvent struct { Timestamp time.Time AgentID string EventType string Action string // \"received\", \"processing\", \"completed\", \"failed\" Data map[string]interface{} } func NewRequestTracer(requestID string) *RequestTracer { return \u0026RequestTracer{ requestID: requestID, events: make([]TraceEvent, 0), } } func (rt *RequestTracer) TraceEvent(agentID, eventType, action string, data map[string]interface{}) { rt.mu.Lock() defer rt.mu.Unlock() rt.events = append(rt.events, TraceEvent{ Timestamp: time.Now(), AgentID: agentID, EventType: eventType, Action: action, Data: data, }) } func (rt *RequestTracer) PrintTrace() { rt.mu.Lock() defer rt.mu.Unlock() fmt.Printf(\"\\\\n=== Request Trace: %s ===\\\\n\", rt.requestID) for _, event := range rt.events { fmt.Printf(\"[%s] %s:%s - %s (%+v)\\\\n\", event.Timestamp.Format(\"15:04:05.000\"), event.AgentID, event.EventType, event.Action, event.Data) } fmt.Printf(\"=== End Trace ===\\\\n\\\\n\") } // Use the tracer in your orchestrator func debugOrchestration(orchestrator core.Orchestrator, event core.Event) { requestID := event.GetID() tracer := NewRequestTracer(requestID) // Wrap agents with tracing wrappedAgents := make([]core.Agent, 0) for _, agent := range orchestrator.GetAgents() { wrappedAgent := \u0026TracingAgent{ agent: agent, tracer: tracer, } wrappedAgents = append(wrappedAgents, wrappedAgent) } // Create new orchestrator with wrapped agents debugOrchestrator := core.NewOrchestrator(wrappedAgents...) // Process the event result, err := debugOrchestrator.Process(context.Background(), event, core.NewState()) // Print the trace tracer.PrintTrace() if err != nil { log.Printf(\"Orchestration failed: %v\", err) } else { log.Printf(\"Orchestration completed: %+v\", result) } } type TracingAgent struct { agent core.Agent tracer *RequestTracer } func (ta *TracingAgent) Process(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { agentID := ta.agent.ID() eventType := event.GetType() ta.tracer.TraceEvent(agentID, eventType, \"received\", map[string]interface{}{ \"state_keys\": state.Keys(), }) ta.tracer.TraceEvent(agentID, eventType, \"processing\", nil) result, err := ta.agent.Process(ctx, event, state) if err != nil { ta.tracer.TraceEvent(agentID, eventType, \"failed\", map[string]interface{}{ \"error\": err.Error(), }) } else { ta.tracer.TraceEvent(agentID, eventType, \"completed\", map[string]interface{}{ \"result_type\": fmt.Sprintf(\"%T\", result.Data), }) } return result, err }\rState Flow Analysis // Track state changes through the system type StateTracker struct { states []StateSnapshot mu sync.Mutex } type StateSnapshot struct { Timestamp time.Time AgentID string Action string // \"input\", \"output\" Keys []string Values map[string]interface{} } func NewStateTracker() *StateTracker { return \u0026StateTracker{ states: make([]StateSnapshot, 0), } } func (st *StateTracker) TrackState(agentID, action string, state core.State) { st.mu.Lock() defer st.mu.Unlock() // Create a snapshot of the state snapshot := StateSnapshot{ Timestamp: time.Now(), AgentID: agentID, Action: action, Keys: state.Keys(), Values: make(map[string]interface{}), } // Copy state values (be careful with large objects) for _, key := range state.Keys() { if value, exists := state.Get(key); exists { // Only copy serializable values for debugging if isSerializable(value) { snapshot.Values[key] = value } else { snapshot.Values[key] = fmt.Sprintf(\"\u003c%T\u003e\", value) } } } st.states = append(st.states, snapshot) } func (st *StateTracker) PrintStateFlow() { st.mu.Lock() defer st.mu.Unlock() fmt.Printf(\"\\\\n=== State Flow Analysis ===\\\\n\") for i, snapshot := range st.states { fmt.Printf(\"[%d] %s - %s:%s\\\\n\", i, snapshot.Timestamp.Format(\"15:04:05.000\"), snapshot.AgentID, snapshot.Action) fmt.Printf(\" Keys: %v\\\\n\", snapshot.Keys) for key, value := range snapshot.Values { fmt.Printf(\" %s: %v\\\\n\", key, value) } fmt.Printf(\"\\\\n\") } fmt.Printf(\"=== End State Flow ===\\\\n\\\\n\") } func isSerializable(value interface{}) bool { switch value.(type) { case string, int, int64, float64, bool, nil: return true case []interface{}, map[string]interface{}: return true default: return false } }\r3. Performance Issues When agents are slow or consuming too many resources:\nPerformance Profiling import ( \"net/http\" _ \"net/http/pprof\" \"runtime\" \"time\" ) // Enable pprof endpoint for profiling func enableProfiling() { go func() { log.Println(\"Starting pprof server on :6060\") log.Println(http.ListenAndServe(\"localhost:6060\", nil)) }() } // Profile agent execution func profileAgentExecution(agent core.Agent, event core.Event, state core.State) { // CPU profiling start := time.Now() var m1, m2 runtime.MemStats runtime.ReadMemStats(\u0026m1) runtime.GC() // Force GC to get accurate memory measurements result, err := agent.Process(context.Background(), event, state) runtime.ReadMemStats(\u0026m2) duration := time.Since(start) // Calculate memory usage memUsed := m2.Alloc - m1.Alloc log.Printf(\"Agent Performance Profile:\") log.Printf(\" Duration: %v\", duration) log.Printf(\" Memory Used: %d bytes\", memUsed) log.Printf(\" Goroutines: %d\", runtime.NumGoroutine()) log.Printf(\" GC Runs: %d\", m2.NumGC-m1.NumGC) if err != nil { log.Printf(\" Error: %v\", err) } else { log.Printf(\" Result: %+v\", result) } } // Benchmark agent performance func benchmarkAgent(agent core.Agent, iterations int) { event := core.NewEvent(\"benchmark\", map[string]interface{}{ \"test\": \"performance\", }) state := core.NewState() durations := make([]time.Duration, iterations) var totalMemory uint64 for i := 0; i \u003c iterations; i++ { var m1, m2 runtime.MemStats runtime.ReadMemStats(\u0026m1) start := time.Now() _, err := agent.Process(context.Background(), event, state) duration := time.Since(start) runtime.ReadMemStats(\u0026m2) durations[i] = duration totalMemory += (m2.Alloc - m1.Alloc) if err != nil { log.Printf(\"Iteration %d failed: %v\", i, err) } } // Calculate statistics var totalDuration time.Duration minDuration := durations[0] maxDuration := durations[0] for _, d := range durations { totalDuration += d if d \u003c minDuration { minDuration = d } if d \u003e maxDuration { maxDuration = d } } avgDuration := totalDuration / time.Duration(iterations) avgMemory := totalMemory / uint64(iterations) log.Printf(\"Agent Benchmark Results (%d iterations):\", iterations) log.Printf(\" Average Duration: %v\", avgDuration) log.Printf(\" Min Duration: %v\", minDuration) log.Printf(\" Max Duration: %v\", maxDuration) log.Printf(\" Average Memory: %d bytes\", avgMemory) }\rResource Monitoring // Monitor agent resource usage over time type ResourceMonitor struct { agentID string metrics []ResourceMetric mu sync.Mutex stopChan chan struct{} interval time.Duration } type ResourceMetric struct { Timestamp time.Time CPUPercent float64 MemoryBytes uint64 Goroutines int GCPauses time.Duration } func NewResourceMonitor(agentID string, interval time.Duration) *ResourceMonitor { return \u0026ResourceMonitor{ agentID: agentID, metrics: make([]ResourceMetric, 0), stopChan: make(chan struct{}), interval: interval, } } func (rm *ResourceMonitor) Start() { go rm.monitor() } func (rm *ResourceMonitor) Stop() { close(rm.stopChan) } func (rm *ResourceMonitor) monitor() { ticker := time.NewTicker(rm.interval) defer ticker.Stop() var lastCPUTime time.Duration var lastTimestamp time.Time for { select { case \u003c-rm.stopChan: return case \u003c-ticker.C: rm.collectMetrics(\u0026lastCPUTime, \u0026lastTimestamp) } } } func (rm *ResourceMonitor) collectMetrics(lastCPUTime *time.Duration, lastTimestamp *time.Time) { var m runtime.MemStats runtime.ReadMemStats(\u0026m) now := time.Now() // Calculate CPU usage (simplified) var cpuPercent float64 if !lastTimestamp.IsZero() { // This is a simplified CPU calculation // In production, use proper CPU monitoring libraries timeDiff := now.Sub(*lastTimestamp) if timeDiff \u003e 0 { cpuPercent = float64(runtime.NumCPU()) * 100.0 / float64(timeDiff.Nanoseconds()) * 1000000 } } metric := ResourceMetric{ Timestamp: now, CPUPercent: cpuPercent, MemoryBytes: m.Alloc, Goroutines: runtime.NumGoroutine(), GCPauses: time.Duration(m.PauseTotalNs), } rm.mu.Lock() rm.metrics = append(rm.metrics, metric) // Keep only last 1000 metrics to prevent memory growth if len(rm.metrics) \u003e 1000 { rm.metrics = rm.metrics[len(rm.metrics)-1000:] } rm.mu.Unlock() *lastTimestamp = now } func (rm *ResourceMonitor) GetMetrics() []ResourceMetric { rm.mu.Lock() defer rm.mu.Unlock() // Return a copy to prevent race conditions metrics := make([]ResourceMetric, len(rm.metrics)) copy(metrics, rm.metrics) return metrics } func (rm *ResourceMonitor) PrintSummary() { metrics := rm.GetMetrics() if len(metrics) == 0 { log.Printf(\"No metrics collected for agent %s\", rm.agentID) return } var totalCPU, totalMemory float64 var maxMemory uint64 var maxGoroutines int for _, metric := range metrics { totalCPU += metric.CPUPercent totalMemory += float64(metric.MemoryBytes) if metric.MemoryBytes \u003e maxMemory { maxMemory = metric.MemoryBytes } if metric.Goroutines \u003e maxGoroutines { maxGoroutines = metric.Goroutines } } count := float64(len(metrics)) avgCPU := totalCPU / count avgMemory := totalMemory / count log.Printf(\"Resource Summary for Agent %s:\", rm.agentID) log.Printf(\" Samples: %d\", len(metrics)) log.Printf(\" Average CPU: %.2f%%\", avgCPU) log.Printf(\" Average Memory: %.2f MB\", avgMemory/1024/1024) log.Printf(\" Peak Memory: %.2f MB\", float64(maxMemory)/1024/1024) log.Printf(\" Max Goroutines: %d\", maxGoroutines) }\rDebugging Tools and Utilities 1. Agent Inspector // Comprehensive agent inspection utility type AgentInspector struct { agent core.Agent history []InspectionRecord mu sync.Mutex } type InspectionRecord struct { Timestamp time.Time Event core.Event State core.State Result core.AgentResult Error error Duration time.Duration } func NewAgentInspector(agent core.Agent) *AgentInspector { return \u0026AgentInspector{ agent: agent, history: make([]InspectionRecord, 0), } } func (ai *AgentInspector) Process(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { start := time.Now() result, err := ai.agent.Process(ctx, event, state) duration := time.Since(start) record := InspectionRecord{ Timestamp: start, Event: event, State: state.Clone(), // Clone to preserve state at this point Result: result, Error: err, Duration: duration, } ai.mu.Lock() ai.history = append(ai.history, record) // Keep only last 100 records if len(ai.history) \u003e 100 { ai.history = ai.history[len(ai.history)-100:] } ai.mu.Unlock() return result, err } func (ai *AgentInspector) GetHistory() []InspectionRecord { ai.mu.Lock() defer ai.mu.Unlock() history := make([]InspectionRecord, len(ai.history)) copy(history, ai.history) return history } func (ai *AgentInspector) PrintHistory() { history := ai.GetHistory() fmt.Printf(\"\\\\n=== Agent Inspection History ===\\\\n\") for i, record := range history { status := \"SUCCESS\" if record.Error != nil { status = \"ERROR\" } fmt.Printf(\"[%d] %s - %s (%v) - %s\\\\n\", i, record.Timestamp.Format(\"15:04:05.000\"), record.Event.GetType(), record.Duration, status) if record.Error != nil { fmt.Printf(\" Error: %v\\\\n\", record.Error) } } fmt.Printf(\"=== End History ===\\\\n\\\\n\") } func (ai *AgentInspector) GetStats() map[string]interface{} { history := ai.GetHistory() if len(history) == 0 { return map[string]interface{}{ \"total_requests\": 0, } } var totalDuration time.Duration var errorCount int eventTypes := make(map[string]int) for _, record := range history { totalDuration += record.Duration if record.Error != nil { errorCount++ } eventTypes[record.Event.GetType()]++ } return map[string]interface{}{ \"total_requests\": len(history), \"error_count\": errorCount, \"error_rate\": float64(errorCount) / float64(len(history)), \"average_duration\": totalDuration / time.Duration(len(history)), \"event_types\": eventTypes, } }\r2. System Health Checker // Comprehensive system health checking type HealthChecker struct { agents []core.Agent orchestrator core.Orchestrator checks []HealthCheck } type HealthCheck struct { Name string Description string CheckFunc func(context.Context) error Timeout time.Duration Critical bool } type HealthResult struct { CheckName string Status string // \"PASS\", \"FAIL\", \"TIMEOUT\" Error error Duration time.Duration Timestamp time.Time } func NewHealthChecker(orchestrator core.Orchestrator, agents ...core.Agent) *HealthChecker { hc := \u0026HealthChecker{ agents: agents, orchestrator: orchestrator, checks: make([]HealthCheck, 0), } // Add default health checks hc.AddDefaultChecks() return hc } func (hc *HealthChecker) AddDefaultChecks() { // Agent responsiveness check hc.AddCheck(HealthCheck{ Name: \"agent_responsiveness\", Description: \"Check if all agents respond to health check events\", CheckFunc: hc.checkAgentResponsiveness, Timeout: 10 * time.Second, Critical: true, }) // Memory usage check hc.AddCheck(HealthCheck{ Name: \"memory_usage\", Description: \"Check system memory usage\", CheckFunc: hc.checkMemoryUsage, Timeout: 5 * time.Second, Critical: false, }) // Goroutine leak check hc.AddCheck(HealthCheck{ Name: \"goroutine_count\", Description: \"Check for goroutine leaks\", CheckFunc: hc.checkGoroutineCount, Timeout: 5 * time.Second, Critical: false, }) } func (hc *HealthChecker) AddCheck(check HealthCheck) { hc.checks = append(hc.checks, check) } func (hc *HealthChecker) RunHealthChecks(ctx context.Context) []HealthResult { results := make([]HealthResult, 0, len(hc.checks)) for _, check := range hc.checks { result := hc.runSingleCheck(ctx, check) results = append(results, result) } return results } func (hc *HealthChecker) runSingleCheck(ctx context.Context, check HealthCheck) HealthResult { start := time.Now() // Create timeout context checkCtx, cancel := context.WithTimeout(ctx, check.Timeout) defer cancel() // Run the check err := check.CheckFunc(checkCtx) duration := time.Since(start) status := \"PASS\" if err != nil { if errors.Is(err, context.DeadlineExceeded) { status = \"TIMEOUT\" } else { status = \"FAIL\" } } return HealthResult{ CheckName: check.Name, Status: status, Error: err, Duration: duration, Timestamp: start, } } func (hc *HealthChecker) checkAgentResponsiveness(ctx context.Context) error { healthEvent := core.NewEvent(\"health_check\", map[string]interface{}{ \"timestamp\": time.Now(), \"check_id\": uuid.New().String(), }) for _, agent := range hc.agents { _, err := agent.Process(ctx, healthEvent, core.NewState()) if err != nil { return fmt.Errorf(\"agent %s failed health check: %w\", agent.ID(), err) } } return nil } func (hc *HealthChecker) checkMemoryUsage(ctx context.Context) error { var m runtime.MemStats runtime.ReadMemStats(\u0026m) // Check if memory usage is above threshold (e.g., 1GB) const maxMemoryBytes = 1024 * 1024 * 1024 if m.Alloc \u003e maxMemoryBytes { return fmt.Errorf(\"memory usage too high: %d bytes (max: %d)\", m.Alloc, maxMemoryBytes) } return nil } func (hc *HealthChecker) checkGoroutineCount(ctx context.Context) error { count := runtime.NumGoroutine() // Check if goroutine count is above threshold const maxGoroutines = 1000 if count \u003e maxGoroutines { return fmt.Errorf(\"too many goroutines: %d (max: %d)\", count, maxGoroutines) } return nil } func (hc *HealthChecker) PrintHealthReport(results []HealthResult) { fmt.Printf(\"\\\\n=== System Health Report ===\\\\n\") fmt.Printf(\"Timestamp: %s\\\\n\\\\n\", time.Now().Format(\"2006-01-02 15:04:05\")) passCount := 0 failCount := 0 timeoutCount := 0 for _, result := range results { status := result.Status switch status { case \"PASS\": passCount++ case \"FAIL\": failCount++ case \"TIMEOUT\": timeoutCount++ } fmt.Printf(\"[%s] %s (%v)\\\\n\", status, result.CheckName, result.Duration) if result.Error != nil { fmt.Printf(\" Error: %v\\\\n\", result.Error) } } fmt.Printf(\"\\\\nSummary: %d PASS, %d FAIL, %d TIMEOUT\\\\n\", passCount, failCount, timeoutCount) if failCount \u003e 0 || timeoutCount \u003e 0 { fmt.Printf(\"⚠️ System health issues detected!\\\\n\") } else { fmt.Printf(\"✅ All health checks passed\\\\n\") } fmt.Printf(\"=== End Health Report ===\\\\n\\\\n\") }\rBest Practices 1. Structured Logging Use consistent log formats across all agents Include correlation IDs to track requests Log at appropriate levels (DEBUG, INFO, WARN, ERROR) Include context information (agent ID, event type, etc.) 2. Error Handling Wrap errors with context using fmt.Errorf Use typed errors for different failure modes Implement circuit breakers for external dependencies Provide actionable error messages 3. Testing Strategies Write unit tests for individual agents Create integration tests for agent interactions Use mocking to isolate components Implement chaos testing for resilience 4. Monitoring Set up metrics for key performance indicators Use distributed tracing for complex workflows Implement health checks and alerting Monitor resource usage trends Conclusion Debugging multi-agent systems requires systematic approaches and specialized tools. By implementing comprehensive logging, tracing, and monitoring, you can effectively identify and resolve issues in complex agent interactions.\nIn the next tutorial, we’ll explore Logging and Tracing techniques for better observability.\nNext Steps Logging and Tracing Performance Monitoring Production Troubleshooting Further Reading Go Debugging Tools pprof Profiling Distributed Tracing Best Practices",
    "description": "Debugging Multi-Agent Systems in AgenticGoKit Overview Debugging multi-agent systems requires specialized techniques due to their distributed, asynchronous nature. This guide covers practical strategies for identifying, isolating, and resolving issues in complex agent interactions using AgenticGoKit’s built-in tracing and debugging capabilities.\nAgenticGoKit provides comprehensive tracing through the agentcli tool and structured logging through the zerolog library, making it easier to understand agent interactions and diagnose issues.\nPrerequisites Understanding of Agent Lifecycle Familiarity with Orchestration Patterns Basic knowledge of Go debugging tools AgenticGoKit project with agentflow.toml configuration Common Debugging Scenarios 1. Agent Not Responding When an agent stops responding to events, follow this systematic approach:",
    "tags": [],
    "title": "debugging-multi-agent-systems",
    "uri": "/AgenticGoKitDocs/tutorials/debugging/debugging-multi-agent-systems/index.html"
  },
  {
    "breadcrumb": "content \u003e guides",
    "content": "Deployment \u0026 Operations Guides Guides for deploying and operating AgenticGoKit applications in production.\nAvailable Guides Docker Deployment Containerize your AgenticGoKit applications with Docker, including multi-stage builds and optimization techniques.\nWhen to use: Deploying agents in containerized environments or cloud platforms.\nMonitoring Set up comprehensive monitoring for agent performance, including metrics, logging, and alerting.\nWhen to use: Running agents in production and need visibility into system health and performance.\nScaling Scale AgenticGoKit applications horizontally, including load balancing and distributed deployment patterns.\nWhen to use: Handling increased load or building high-availability agent systems.\nDeployment Patterns Common deployment patterns:\nSingle Instance Simple applications with low traffic Development and testing environments Proof-of-concept deployments Load Balanced Multiple instances behind a load balancer Horizontal scaling for increased throughput High availability with failover Microservices Agents deployed as separate services Independent scaling and deployment Service mesh integration Production Checklist Before deploying to production:\nConfiguration management - Externalized configuration Security - Proper authentication and authorization Monitoring - Metrics, logging, and alerting set up Backup and recovery - Data backup and disaster recovery plans Performance testing - Load testing and capacity planning Documentation - Operational runbooks and procedures Infrastructure Requirements Typical production requirements:\nCompute - CPU and memory based on agent complexity Storage - Persistent storage for state and memory systems Network - Reliable network connectivity for LLM APIs Monitoring - Observability infrastructure Next Steps For production deployment:\nStart with Docker containerization Add monitoring for observability Scale with appropriate patterns as needed Reference Best Practices for operational excellence",
    "description": "Deployment \u0026 Operations Guides Guides for deploying and operating AgenticGoKit applications in production.\nAvailable Guides Docker Deployment Containerize your AgenticGoKit applications with Docker, including multi-stage builds and optimization techniques.\nWhen to use: Deploying agents in containerized environments or cloud platforms.\nMonitoring Set up comprehensive monitoring for agent performance, including metrics, logging, and alerting.\nWhen to use: Running agents in production and need visibility into system health and performance.\nScaling Scale AgenticGoKit applications horizontally, including load balancing and distributed deployment patterns.",
    "tags": [],
    "title": "deployment",
    "uri": "/AgenticGoKitDocs/guides/deployment/index.html"
  },
  {
    "breadcrumb": "content \u003e guides",
    "content": "Development Guides Guides for developing, testing, and debugging AgenticGoKit applications.\nAvailable Guides Testing Agents Comprehensive guide to testing multi-agent systems, including unit tests, integration tests, and end-to-end testing strategies.\nWhen to use: Building reliable agent systems that need thorough testing coverage.\nDebugging Debug agent interactions, trace execution flows, and troubleshoot common issues in multi-agent systems.\nWhen to use: When agents aren’t behaving as expected or you need to understand execution flow.\nBest Practices Development best practices for building maintainable, scalable, and robust agent systems.\nWhen to use: Starting a new project or improving existing agent implementations.\nDevelopment Workflow Recommended development workflow:\nDesign your agents - Plan agent responsibilities and interactions Implement incrementally - Start with simple agents, add complexity gradually Test continuously - Use Testing Agents strategies Debug systematically - Apply Debugging techniques Follow best practices - Reference Best Practices Development Tools AgenticGoKit provides several tools to help with development:\nagentcli - Command-line tool for tracing and debugging Configuration validation - Built-in config validation Logging and tracing - Comprehensive observability features Code Quality All guides emphasize:\nError handling - Proper error management patterns Testing - Comprehensive test coverage Documentation - Clear code documentation Performance - Efficient resource usage Next Steps After development:\nDeployment Guides for production deployment Setup Guides for configuration management API Reference for detailed documentation",
    "description": "Development Guides Guides for developing, testing, and debugging AgenticGoKit applications.\nAvailable Guides Testing Agents Comprehensive guide to testing multi-agent systems, including unit tests, integration tests, and end-to-end testing strategies.\nWhen to use: Building reliable agent systems that need thorough testing coverage.\nDebugging Debug agent interactions, trace execution flows, and troubleshoot common issues in multi-agent systems.\nWhen to use: When agents aren’t behaving as expected or you need to understand execution flow.",
    "tags": [],
    "title": "development",
    "uri": "/AgenticGoKitDocs/guides/development/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials \u003e memory-systems",
    "content": "Document Ingestion and Knowledge Base Management Overview Document ingestion is a critical component of building comprehensive knowledge bases in AgenticGoKit. This tutorial covers the complete pipeline from raw documents to searchable knowledge, including document processing, chunking strategies, metadata extraction, and optimization techniques.\nEffective document ingestion enables agents to access and reason over large collections of structured and unstructured data.\nPrerequisites Understanding of Memory Systems Overview Familiarity with Vector Databases Knowledge of document formats (PDF, Markdown, HTML, etc.) Basic understanding of text processing and NLP concepts Document Ingestion Pipeline Architecture Overview ┌─────────────────┐ ┌──────────────────┐ ┌─────────────────┐\r│ Raw │───▶│ Document │───▶│ Text │\r│ Documents │ │ Parser │ │ Extraction │\r└─────────────────┘ └──────────────────┘ └─────────────────┘\r│\r▼\r┌─────────────────┐ ┌──────────────────┐ ┌─────────────────┐\r│ Vector │◀───│ Embedding │◀───│ Text │\r│ Storage │ │ Generation │ │ Chunking │\r└─────────────────┘ └──────────────────┘ └─────────────────┘\r│\r▼\r┌─────────────────┐\r│ Metadata │\r│ Extraction │\r└─────────────────┘\rDocument Types and Processing 1. Supported Document Types // Document types supported by AgenticGoKit const ( DocumentTypePDF DocumentType = \"pdf\" DocumentTypeText DocumentType = \"txt\" DocumentTypeMarkdown DocumentType = \"md\" DocumentTypeWeb DocumentType = \"web\" DocumentTypeCode DocumentType = \"code\" DocumentTypeJSON DocumentType = \"json\" ) // Document structure for ingestion type Document struct { ID string `json:\"id\"` Title string `json:\"title,omitempty\"` Content string `json:\"content\"` Source string `json:\"source,omitempty\"` // URL, file path, etc. Type DocumentType `json:\"type,omitempty\"` Metadata map[string]any `json:\"metadata,omitempty\"` Tags []string `json:\"tags,omitempty\"` CreatedAt time.Time `json:\"created_at\"` UpdatedAt time.Time `json:\"updated_at,omitempty\"` ChunkIndex int `json:\"chunk_index,omitempty\"` // For chunked documents ChunkTotal int `json:\"chunk_total,omitempty\"` }\r2. Basic Document Ingestion package main import ( \"context\" \"fmt\" \"log\" \"time\" \"github.com/kunalkushwaha/agenticgokit/core\" ) func ingestBasicDocument(memory core.Memory) error { ctx := context.Background() // Create a document doc := core.Document{ ID: \"doc-001\", Title: \"Introduction to Machine Learning\", Content: `Machine learning is a subset of artificial intelligence that enables computers to learn and make decisions from data without being explicitly programmed for every task. It involves algorithms that can identify patterns, make predictions, and improve their performance over time.`, Source: \"textbook-chapter-1.pdf\", Type: core.DocumentTypePDF, Metadata: map[string]any{ \"author\": \"Dr. Jane Smith\", \"chapter\": 1, \"subject\": \"machine-learning\", \"difficulty\": \"beginner\", \"language\": \"english\", }, Tags: []string{\"ml\", \"ai\", \"introduction\"}, CreatedAt: time.Now(), } // Ingest the document err := memory.IngestDocument(ctx, doc) if err != nil { return fmt.Errorf(\"failed to ingest document: %w\", err) } fmt.Printf(\"Successfully ingested document: %s\\n\", doc.Title) return nil }\r3. Batch Document Ingestion func ingestMultipleDocuments(memory core.Memory) error { ctx := context.Background() // Prepare multiple documents documents := []core.Document{ { ID: \"doc-002\", Title: \"Neural Networks Fundamentals\", Content: \"Neural networks are computing systems inspired by biological neural networks...\", Source: \"textbook-chapter-2.pdf\", Type: core.DocumentTypePDF, Metadata: map[string]any{ \"author\": \"Dr. Jane Smith\", \"chapter\": 2, \"subject\": \"neural-networks\", \"difficulty\": \"intermediate\", }, Tags: []string{\"neural-networks\", \"deep-learning\"}, }, { ID: \"doc-003\", Title: \"Data Preprocessing Techniques\", Content: \"Data preprocessing is a crucial step in machine learning pipelines...\", Source: \"textbook-chapter-3.pdf\", Type: core.DocumentTypePDF, Metadata: map[string]any{ \"author\": \"Dr. Jane Smith\", \"chapter\": 3, \"subject\": \"data-preprocessing\", \"difficulty\": \"beginner\", }, Tags: []string{\"data-science\", \"preprocessing\"}, }, } // Batch ingest documents err := memory.IngestDocuments(ctx, documents) if err != nil { return fmt.Errorf(\"failed to ingest documents: %w\", err) } fmt.Printf(\"Successfully ingested %d documents\\n\", len(documents)) return nil }\rText Chunking Strategies 1. Fixed-Size Chunking type FixedSizeChunker struct { ChunkSize int ChunkOverlap int } func NewFixedSizeChunker(chunkSize, overlap int) *FixedSizeChunker { return \u0026FixedSizeChunker{ ChunkSize: chunkSize, ChunkOverlap: overlap, } } func (c *FixedSizeChunker) ChunkText(text string) []string { if len(text) \u003c= c.ChunkSize { return []string{text} } var chunks []string start := 0 for start \u003c len(text) { end := start + c.ChunkSize if end \u003e len(text) { end = len(text) } chunk := text[start:end] chunks = append(chunks, chunk) // Move start position considering overlap start += c.ChunkSize - c.ChunkOverlap if start \u003e= len(text) { break } } return chunks } // Example usage func chunkLargeDocument(memory core.Memory, largeText string) error { ctx := context.Background() chunker := NewFixedSizeChunker(1000, 200) chunks := chunker.ChunkText(largeText) for i, chunk := range chunks { doc := core.Document{ ID: fmt.Sprintf(\"large-doc-chunk-%d\", i), Title: fmt.Sprintf(\"Large Document - Chunk %d\", i+1), Content: chunk, Source: \"large-document.pdf\", Type: core.DocumentTypePDF, ChunkIndex: i, ChunkTotal: len(chunks), Metadata: map[string]any{ \"chunk_method\": \"fixed-size\", \"chunk_size\": 1000, \"chunk_overlap\": 200, }, CreatedAt: time.Now(), } err := memory.IngestDocument(ctx, doc) if err != nil { return fmt.Errorf(\"failed to ingest chunk %d: %w\", i, err) } } return nil }\r2. Semantic Chunking type SemanticChunker struct { MaxChunkSize int MinChunkSize int } func NewSemanticChunker(minSize, maxSize int) *SemanticChunker { return \u0026SemanticChunker{ MinChunkSize: minSize, MaxChunkSize: maxSize, } } func (c *SemanticChunker) ChunkText(text string) []string { // Split by paragraphs first paragraphs := strings.Split(text, \"\\n\\n\") var chunks []string var currentChunk strings.Builder for _, paragraph := range paragraphs { paragraph = strings.TrimSpace(paragraph) if paragraph == \"\" { continue } // Check if adding this paragraph would exceed max size if currentChunk.Len() \u003e 0 \u0026\u0026 currentChunk.Len()+len(paragraph) \u003e c.MaxChunkSize { // Finalize current chunk if it meets minimum size if currentChunk.Len() \u003e= c.MinChunkSize { chunks = append(chunks, currentChunk.String()) currentChunk.Reset() } } // Add paragraph to current chunk if currentChunk.Len() \u003e 0 { currentChunk.WriteString(\"\\n\\n\") } currentChunk.WriteString(paragraph) } // Add final chunk if it has content if currentChunk.Len() \u003e 0 { chunks = append(chunks, currentChunk.String()) } return chunks }\r3. Sentence-Based Chunking type SentenceChunker struct { MaxSentences int Overlap int } func NewSentenceChunker(maxSentences, overlap int) *SentenceChunker { return \u0026SentenceChunker{ MaxSentences: maxSentences, Overlap: overlap, } } func (c *SentenceChunker) ChunkText(text string) []string { sentences := c.splitIntoSentences(text) if len(sentences) \u003c= c.MaxSentences { return []string{text} } var chunks []string start := 0 for start \u003c len(sentences) { end := start + c.MaxSentences if end \u003e len(sentences) { end = len(sentences) } chunk := strings.Join(sentences[start:end], \" \") chunks = append(chunks, chunk) start += c.MaxSentences - c.Overlap if start \u003e= len(sentences) { break } } return chunks } func (c *SentenceChunker) splitIntoSentences(text string) []string { // Simple sentence splitting (in production, use a proper NLP library) sentences := strings.FieldsFunc(text, func(r rune) bool { return r == '.' || r == '!' || r == '?' }) // Clean up sentences var cleanSentences []string for _, sentence := range sentences { sentence = strings.TrimSpace(sentence) if len(sentence) \u003e 10 { // Filter out very short fragments cleanSentences = append(cleanSentences, sentence) } } return cleanSentences }\rAdvanced Document Processing 1. Document Processor with Multiple Strategies type DocumentProcessor struct { memory core.Memory chunkers map[string]TextChunker extractors map[core.DocumentType]MetadataExtractor config ProcessorConfig } type TextChunker interface { ChunkText(text string) []string } type MetadataExtractor interface { ExtractMetadata(doc core.Document) (map[string]any, error) } type ProcessorConfig struct { DefaultChunkStrategy string MaxConcurrentDocs int EnableMetadataExtraction bool EnableContentCleaning bool } func NewDocumentProcessor(memory core.Memory, config ProcessorConfig) *DocumentProcessor { dp := \u0026DocumentProcessor{ memory: memory, chunkers: make(map[string]TextChunker), extractors: make(map[core.DocumentType]MetadataExtractor), config: config, } // Register default chunkers dp.chunkers[\"fixed\"] = NewFixedSizeChunker(1000, 200) dp.chunkers[\"semantic\"] = NewSemanticChunker(500, 1500) dp.chunkers[\"sentence\"] = NewSentenceChunker(10, 2) // Register metadata extractors dp.extractors[core.DocumentTypePDF] = \u0026PDFMetadataExtractor{} dp.extractors[core.DocumentTypeMarkdown] = \u0026MarkdownMetadataExtractor{} dp.extractors[core.DocumentTypeCode] = \u0026CodeMetadataExtractor{} return dp } func (dp *DocumentProcessor) ProcessDocument(ctx context.Context, doc core.Document, chunkStrategy string) error { // Clean content if enabled if dp.config.EnableContentCleaning { doc.Content = dp.cleanContent(doc.Content) } // Extract metadata if enabled if dp.config.EnableMetadataExtraction { if extractor, exists := dp.extractors[doc.Type]; exists { metadata, err := extractor.ExtractMetadata(doc) if err == nil { // Merge extracted metadata with existing if doc.Metadata == nil { doc.Metadata = make(map[string]any) } for k, v := range metadata { doc.Metadata[k] = v } } } } // Choose chunking strategy if chunkStrategy == \"\" { chunkStrategy = dp.config.DefaultChunkStrategy } chunker, exists := dp.chunkers[chunkStrategy] if !exists { return fmt.Errorf(\"unknown chunking strategy: %s\", chunkStrategy) } // Chunk the document chunks := chunker.ChunkText(doc.Content) // Process chunks if len(chunks) == 1 { // Single chunk - ingest as-is return dp.memory.IngestDocument(ctx, doc) } // Multiple chunks - create separate documents var documents []core.Document for i, chunk := range chunks { chunkDoc := doc // Copy original document chunkDoc.ID = fmt.Sprintf(\"%s-chunk-%d\", doc.ID, i) chunkDoc.Content = chunk chunkDoc.ChunkIndex = i chunkDoc.ChunkTotal = len(chunks) // Add chunking metadata if chunkDoc.Metadata == nil { chunkDoc.Metadata = make(map[string]any) } chunkDoc.Metadata[\"chunk_strategy\"] = chunkStrategy chunkDoc.Metadata[\"original_doc_id\"] = doc.ID documents = append(documents, chunkDoc) } return dp.memory.IngestDocuments(ctx, documents) } func (dp *DocumentProcessor) cleanContent(content string) string { // Remove excessive whitespace content = regexp.MustCompile(`\\s+`).ReplaceAllString(content, \" \") // Remove special characters that might interfere with processing content = regexp.MustCompile(`[^\\w\\s\\.,!?;:()\\-\"']`).ReplaceAllString(content, \"\") // Trim whitespace content = strings.TrimSpace(content) return content }\r2. Metadata Extractors // PDF Metadata Extractor type PDFMetadataExtractor struct{} func (e *PDFMetadataExtractor) ExtractMetadata(doc core.Document) (map[string]any, error) { metadata := make(map[string]any) // Extract basic statistics metadata[\"word_count\"] = len(strings.Fields(doc.Content)) metadata[\"char_count\"] = len(doc.Content) metadata[\"paragraph_count\"] = len(strings.Split(doc.Content, \"\\n\\n\")) // Extract potential headings (lines that are short and followed by longer content) lines := strings.Split(doc.Content, \"\\n\") var headings []string for i, line := range lines { line = strings.TrimSpace(line) if len(line) \u003e 0 \u0026\u0026 len(line) \u003c 100 \u0026\u0026 i+1 \u003c len(lines) { nextLine := strings.TrimSpace(lines[i+1]) if len(nextLine) \u003e len(line)*2 { headings = append(headings, line) } } } metadata[\"potential_headings\"] = headings // Detect language (simple heuristic) metadata[\"detected_language\"] = detectLanguage(doc.Content) return metadata, nil } // Markdown Metadata Extractor type MarkdownMetadataExtractor struct{} func (e *MarkdownMetadataExtractor) ExtractMetadata(doc core.Document) (map[string]any, error) { metadata := make(map[string]any) // Extract headings headings := extractMarkdownHeadings(doc.Content) metadata[\"headings\"] = headings metadata[\"heading_count\"] = len(headings) // Extract links links := extractMarkdownLinks(doc.Content) metadata[\"links\"] = links metadata[\"link_count\"] = len(links) // Extract code blocks codeBlocks := extractMarkdownCodeBlocks(doc.Content) metadata[\"code_blocks\"] = len(codeBlocks) // Extract front matter if present frontMatter := extractFrontMatter(doc.Content) if frontMatter != nil { metadata[\"front_matter\"] = frontMatter } return metadata, nil } // Code Metadata Extractor type CodeMetadataExtractor struct{} func (e *CodeMetadataExtractor) ExtractMetadata(doc core.Document) (map[string]any, error) { metadata := make(map[string]any) // Detect programming language language := detectProgrammingLanguage(doc.Source, doc.Content) metadata[\"programming_language\"] = language // Count lines of code lines := strings.Split(doc.Content, \"\\n\") metadata[\"total_lines\"] = len(lines) // Count non-empty lines nonEmptyLines := 0 commentLines := 0 for _, line := range lines { line = strings.TrimSpace(line) if line != \"\" { nonEmptyLines++ if isCommentLine(line, language) { commentLines++ } } } metadata[\"code_lines\"] = nonEmptyLines metadata[\"comment_lines\"] = commentLines // Extract functions/methods (basic pattern matching) functions := extractFunctions(doc.Content, language) metadata[\"functions\"] = functions metadata[\"function_count\"] = len(functions) return metadata, nil } // Helper functions for metadata extraction func detectLanguage(content string) string { // Simple language detection based on common words englishWords := []string{\"the\", \"and\", \"is\", \"in\", \"to\", \"of\", \"a\", \"that\", \"it\", \"with\"} words := strings.Fields(strings.ToLower(content)) englishCount := 0 for _, word := range words { for _, englishWord := range englishWords { if word == englishWord { englishCount++ break } } } if float64(englishCount)/float64(len(words)) \u003e 0.1 { return \"english\" } return \"unknown\" } func extractMarkdownHeadings(content string) []string { var headings []string lines := strings.Split(content, \"\\n\") for _, line := range lines { line = strings.TrimSpace(line) if strings.HasPrefix(line, \"#\") { headings = append(headings, line) } } return headings } func extractMarkdownLinks(content string) []string { // Simple regex for markdown links [text](url) linkRegex := regexp.MustCompile(`\\[([^\\]]+)\\]\\(([^)]+)\\)`) matches := linkRegex.FindAllStringSubmatch(content, -1) var links []string for _, match := range matches { if len(match) \u003e= 3 { links = append(links, match[2]) // URL part } } return links } func extractMarkdownCodeBlocks(content string) []string { // Simple extraction of code blocks codeBlockRegex := regexp.MustCompile(\"```[\\\\s\\\\S]*?```\") matches := codeBlockRegex.FindAllString(content, -1) return matches } func extractFrontMatter(content string) map[string]any { // Extract YAML front matter if !strings.HasPrefix(content, \"---\") { return nil } parts := strings.SplitN(content, \"---\", 3) if len(parts) \u003c 3 { return nil } // Simple key-value extraction (in production, use a YAML parser) frontMatter := make(map[string]any) lines := strings.Split(parts[1], \"\\n\") for _, line := range lines { line = strings.TrimSpace(line) if strings.Contains(line, \":\") { parts := strings.SplitN(line, \":\", 2) if len(parts) == 2 { key := strings.TrimSpace(parts[0]) value := strings.TrimSpace(parts[1]) frontMatter[key] = value } } } return frontMatter } func detectProgrammingLanguage(filename, content string) string { // Detect by file extension ext := strings.ToLower(filepath.Ext(filename)) switch ext { case \".go\": return \"go\" case \".py\": return \"python\" case \".js\": return \"javascript\" case \".ts\": return \"typescript\" case \".java\": return \"java\" case \".cpp\", \".cc\", \".cxx\": return \"cpp\" case \".c\": return \"c\" case \".rs\": return \"rust\" } // Detect by content patterns if strings.Contains(content, \"package main\") || strings.Contains(content, \"func \") { return \"go\" } if strings.Contains(content, \"def \") || strings.Contains(content, \"import \") { return \"python\" } return \"unknown\" } func isCommentLine(line, language string) bool { switch language { case \"go\", \"javascript\", \"typescript\", \"java\", \"cpp\", \"c\", \"rust\": return strings.HasPrefix(line, \"//\") || strings.HasPrefix(line, \"/*\") case \"python\": return strings.HasPrefix(line, \"#\") } return false } func extractFunctions(content, language string) []string { var functions []string switch language { case \"go\": funcRegex := regexp.MustCompile(`func\\s+(\\w+)\\s*\\(`) matches := funcRegex.FindAllStringSubmatch(content, -1) for _, match := range matches { if len(match) \u003e= 2 { functions = append(functions, match[1]) } } case \"python\": funcRegex := regexp.MustCompile(`def\\s+(\\w+)\\s*\\(`) matches := funcRegex.FindAllStringSubmatch(content, -1) for _, match := range matches { if len(match) \u003e= 2 { functions = append(functions, match[1]) } } case \"javascript\", \"typescript\": funcRegex := regexp.MustCompile(`function\\s+(\\w+)\\s*\\(`) matches := funcRegex.FindAllStringSubmatch(content, -1) for _, match := range matches { if len(match) \u003e= 2 { functions = append(functions, match[1]) } } } return functions }\rKnowledge Base Search and Retrieval 1. Advanced Search with Filters func performAdvancedKnowledgeSearch(memory core.Memory) error { ctx := context.Background() // Search with multiple filters results, err := memory.SearchKnowledge(ctx, \"machine learning algorithms\", core.WithLimit(10), core.WithScoreThreshold(0.7), core.WithSources([]string{\"textbook-chapter-1.pdf\", \"textbook-chapter-2.pdf\"}), core.WithDocumentTypes([]core.DocumentType{core.DocumentTypePDF}), core.WithTags([]string{\"ml\", \"algorithms\"}), core.WithDateRange(\u0026core.DateRange{ Start: time.Now().Add(-30 * 24 * time.Hour), End: time.Now(), }), ) if err != nil { return fmt.Errorf(\"knowledge search failed: %w\", err) } fmt.Printf(\"Found %d relevant knowledge items:\\n\", len(results)) for _, result := range results { fmt.Printf(\"- %s (Score: %.3f)\\n\", result.Title, result.Score) fmt.Printf(\" Source: %s\\n\", result.Source) fmt.Printf(\" Content: %s...\\n\", truncateString(result.Content, 100)) if result.ChunkIndex \u003e 0 { fmt.Printf(\" Chunk: %d/%d\\n\", result.ChunkIndex+1, result.ChunkTotal) } fmt.Println() } return nil } func truncateString(s string, maxLen int) string { if len(s) \u003c= maxLen { return s } return s[:maxLen] + \"...\" }\r2. Hybrid Search (Personal + Knowledge) func performHybridSearch(memory core.Memory) error { ctx := context.Context() // Perform hybrid search combining personal memory and knowledge base result, err := memory.SearchAll(ctx, \"neural network implementation\", core.WithLimit(15), core.WithScoreThreshold(0.6), core.WithIncludePersonal(true), core.WithIncludeKnowledge(true), core.WithHybridWeight(0.7), // 70% semantic, 30% keyword ) if err != nil { return fmt.Errorf(\"hybrid search failed: %w\", err) } fmt.Printf(\"Hybrid Search Results for: %s\\n\", result.Query) fmt.Printf(\"Total Results: %d (Search Time: %v)\\n\\n\", result.TotalResults, result.SearchTime) // Display personal memory results if len(result.PersonalMemory) \u003e 0 { fmt.Println(\"Personal Memory Results:\") for _, item := range result.PersonalMemory { fmt.Printf(\"- %s (Score: %.3f)\\n\", truncateString(item.Content, 80), item.Score) } fmt.Println() } // Display knowledge base results if len(result.Knowledge) \u003e 0 { fmt.Println(\"Knowledge Base Results:\") for _, item := range result.Knowledge { fmt.Printf(\"- %s (Score: %.3f)\\n\", item.Title, item.Score) fmt.Printf(\" Source: %s\\n\", item.Source) } } return nil }\r3. RAG Context Building func buildRAGContext(memory core.Memory, query string) error { ctx := context.Background() // Build comprehensive RAG context ragContext, err := memory.BuildContext(ctx, query, core.WithMaxTokens(4000), core.WithPersonalWeight(0.3), core.WithKnowledgeWeight(0.7), core.WithHistoryLimit(5), core.WithIncludeSources(true), core.WithFormatTemplate(`Context Information: Personal Memory: {{range .PersonalMemory}} - {{.Content}} {{end}} Knowledge Base: {{range .Knowledge}} - {{.Content}} (Source: {{.Source}}) {{end}} Recent Conversation: {{range .ChatHistory}} {{.Role}}: {{.Content}} {{end}} Query: {{.Query}}`), ) if err != nil { return fmt.Errorf(\"failed to build RAG context: %w\", err) } fmt.Printf(\"RAG Context for: %s\\n\", ragContext.Query) fmt.Printf(\"Token Count: %d\\n\", ragContext.TokenCount) fmt.Printf(\"Sources: %v\\n\", ragContext.Sources) fmt.Printf(\"Context Text:\\n%s\\n\", ragContext.ContextText) return nil }\rProduction Optimization 1. Batch Processing Pipeline type BatchProcessor struct { memory core.Memory processor *DocumentProcessor concurrency int batchSize int } func NewBatchProcessor(memory core.Memory, concurrency, batchSize int) *BatchProcessor { return \u0026BatchProcessor{ memory: memory, processor: NewDocumentProcessor(memory, ProcessorConfig{ DefaultChunkStrategy: \"semantic\", MaxConcurrentDocs: concurrency, EnableMetadataExtraction: true, EnableContentCleaning: true, }), concurrency: concurrency, batchSize: batchSize, } } func (bp *BatchProcessor) ProcessDocuments(ctx context.Context, documents []core.Document) error { // Process documents in batches for i := 0; i \u003c len(documents); i += bp.batchSize { end := i + bp.batchSize if end \u003e len(documents) { end = len(documents) } batch := documents[i:end] err := bp.processBatch(ctx, batch) if err != nil { return fmt.Errorf(\"failed to process batch %d-%d: %w\", i, end-1, err) } fmt.Printf(\"Processed batch %d-%d (%d documents)\\n\", i, end-1, len(batch)) } return nil } func (bp *BatchProcessor) processBatch(ctx context.Context, documents []core.Document) error { // Use worker pool for concurrent processing jobs := make(chan core.Document, len(documents)) results := make(chan error, len(documents)) // Start workers for w := 0; w \u003c bp.concurrency; w++ { go bp.worker(ctx, jobs, results) } // Send jobs for _, doc := range documents { jobs \u003c- doc } close(jobs) // Collect results var errors []error for i := 0; i \u003c len(documents); i++ { if err := \u003c-results; err != nil { errors = append(errors, err) } } if len(errors) \u003e 0 { return fmt.Errorf(\"batch processing failed with %d errors: %v\", len(errors), errors[0]) } return nil } func (bp *BatchProcessor) worker(ctx context.Context, jobs \u003c-chan core.Document, results chan\u003c- error) { for doc := range jobs { err := bp.processor.ProcessDocument(ctx, doc, \"\") results \u003c- err } }\r2. Performance Monitoring type IngestionMetrics struct { DocumentsProcessed int64 `json:\"documents_processed\"` ChunksCreated int64 `json:\"chunks_created\"` ProcessingTime time.Duration `json:\"processing_time\"` ErrorCount int64 `json:\"error_count\"` AverageChunkSize float64 `json:\"average_chunk_size\"` mu sync.RWMutex } func (m *IngestionMetrics) RecordDocument(chunkCount int, processingTime time.Duration, chunkSizes []int) { m.mu.Lock() defer m.mu.Unlock() m.DocumentsProcessed++ m.ChunksCreated += int64(chunkCount) m.ProcessingTime += processingTime // Update average chunk size if len(chunkSizes) \u003e 0 { totalSize := 0 for _, size := range chunkSizes { totalSize += size } avgSize := float64(totalSize) / float64(len(chunkSizes)) // Running average totalChunks := float64(m.ChunksCreated) m.AverageChunkSize = (m.AverageChunkSize*(totalChunks-float64(chunkCount)) + avgSize*float64(chunkCount)) / totalChunks } } func (m *IngestionMetrics) RecordError() { m.mu.Lock() defer m.mu.Unlock() m.ErrorCount++ } func (m *IngestionMetrics) GetStats() IngestionMetrics { m.mu.RLock() defer m.mu.RUnlock() return IngestionMetrics{ DocumentsProcessed: m.DocumentsProcessed, ChunksCreated: m.ChunksCreated, ProcessingTime: m.ProcessingTime, ErrorCount: m.ErrorCount, AverageChunkSize: m.AverageChunkSize, } }\rBest Practices 1. Document Ingestion Guidelines Chunk Size: Balance between context preservation and retrieval precision Overlap: Use 10-20% overlap to maintain context continuity Metadata: Extract and store rich metadata for better filtering Batch Processing: Process documents in batches for better performance Error Handling: Implement robust error handling and retry mechanisms 2. Performance Optimization Concurrent Processing: Use worker pools for parallel document processing Embedding Caching: Cache embeddings to avoid recomputation Index Optimization: Optimize vector database indexes for your query patterns Memory Management: Monitor memory usage during large batch operations 3. Quality Assurance Content Validation: Validate document content before ingestion Duplicate Detection: Implement deduplication to avoid redundant storage Quality Metrics: Track ingestion quality and search relevance Regular Maintenance: Periodically clean up and optimize the knowledge base Conclusion Document ingestion and knowledge base management are critical for building effective RAG systems. By implementing proper chunking strategies, metadata extraction, and optimization techniques, you can create knowledge bases that provide accurate and relevant information to your agents.\nKey takeaways:\nChoose appropriate chunking strategies based on your content type Extract rich metadata to enable better filtering and search Implement batch processing for handling large document collections Monitor performance and optimize based on usage patterns Follow best practices for quality and maintenance Next Steps RAG Implementation - Build complete RAG systems Memory Optimization - Optimize performance and scaling Vector Databases - Advanced database configuration Further Reading API Reference: Document Interface Examples: Document Processing Configuration Guide: Memory Settings",
    "description": "Document Ingestion and Knowledge Base Management Overview Document ingestion is a critical component of building comprehensive knowledge bases in AgenticGoKit. This tutorial covers the complete pipeline from raw documents to searchable knowledge, including document processing, chunking strategies, metadata extraction, and optimization techniques.\nEffective document ingestion enables agents to access and reason over large collections of structured and unstructured data.\nPrerequisites Understanding of Memory Systems Overview Familiarity with Vector Databases Knowledge of document formats (PDF, Markdown, HTML, etc.) Basic understanding of text processing and NLP concepts Document Ingestion Pipeline Architecture Overview ┌─────────────────┐ ┌──────────────────┐ ┌─────────────────┐\r│ Raw │───▶│ Document │───▶│ Text │\r│ Documents │ │ Parser │ │ Extraction │\r└─────────────────┘ └──────────────────┘ └─────────────────┘\r│\r▼\r┌─────────────────┐ ┌──────────────────┐ ┌─────────────────┐\r│ Vector │◀───│ Embedding │◀───│ Text │\r│ Storage │ │ Generation │ │ Chunking │\r└─────────────────┘ └──────────────────┘ └─────────────────┘\r│\r▼\r┌─────────────────┐\r│ Metadata │\r│ Extraction │\r└─────────────────┘\rDocument Types and Processing 1. Supported Document Types // Document types supported by AgenticGoKit const ( DocumentTypePDF DocumentType = \"pdf\" DocumentTypeText DocumentType = \"txt\" DocumentTypeMarkdown DocumentType = \"md\" DocumentTypeWeb DocumentType = \"web\" DocumentTypeCode DocumentType = \"code\" DocumentTypeJSON DocumentType = \"json\" ) // Document structure for ingestion type Document struct { ID string `json:\"id\"` Title string `json:\"title,omitempty\"` Content string `json:\"content\"` Source string `json:\"source,omitempty\"` // URL, file path, etc. Type DocumentType `json:\"type,omitempty\"` Metadata map[string]any `json:\"metadata,omitempty\"` Tags []string `json:\"tags,omitempty\"` CreatedAt time.Time `json:\"created_at\"` UpdatedAt time.Time `json:\"updated_at,omitempty\"` ChunkIndex int `json:\"chunk_index,omitempty\"` // For chunked documents ChunkTotal int `json:\"chunk_total,omitempty\"` }\r2. Basic Document Ingestion package main import ( \"context\" \"fmt\" \"log\" \"time\" \"github.com/kunalkushwaha/agenticgokit/core\" ) func ingestBasicDocument(memory core.Memory) error { ctx := context.Background() // Create a document doc := core.Document{ ID: \"doc-001\", Title: \"Introduction to Machine Learning\", Content: `Machine learning is a subset of artificial intelligence that enables computers to learn and make decisions from data without being explicitly programmed for every task. It involves algorithms that can identify patterns, make predictions, and improve their performance over time.`, Source: \"textbook-chapter-1.pdf\", Type: core.DocumentTypePDF, Metadata: map[string]any{ \"author\": \"Dr. Jane Smith\", \"chapter\": 1, \"subject\": \"machine-learning\", \"difficulty\": \"beginner\", \"language\": \"english\", }, Tags: []string{\"ml\", \"ai\", \"introduction\"}, CreatedAt: time.Now(), } // Ingest the document err := memory.IngestDocument(ctx, doc) if err != nil { return fmt.Errorf(\"failed to ingest document: %w\", err) } fmt.Printf(\"Successfully ingested document: %s\\n\", doc.Title) return nil }\r3. Batch Document Ingestion func ingestMultipleDocuments(memory core.Memory) error { ctx := context.Background() // Prepare multiple documents documents := []core.Document{ { ID: \"doc-002\", Title: \"Neural Networks Fundamentals\", Content: \"Neural networks are computing systems inspired by biological neural networks...\", Source: \"textbook-chapter-2.pdf\", Type: core.DocumentTypePDF, Metadata: map[string]any{ \"author\": \"Dr. Jane Smith\", \"chapter\": 2, \"subject\": \"neural-networks\", \"difficulty\": \"intermediate\", }, Tags: []string{\"neural-networks\", \"deep-learning\"}, }, { ID: \"doc-003\", Title: \"Data Preprocessing Techniques\", Content: \"Data preprocessing is a crucial step in machine learning pipelines...\", Source: \"textbook-chapter-3.pdf\", Type: core.DocumentTypePDF, Metadata: map[string]any{ \"author\": \"Dr. Jane Smith\", \"chapter\": 3, \"subject\": \"data-preprocessing\", \"difficulty\": \"beginner\", }, Tags: []string{\"data-science\", \"preprocessing\"}, }, } // Batch ingest documents err := memory.IngestDocuments(ctx, documents) if err != nil { return fmt.Errorf(\"failed to ingest documents: %w\", err) } fmt.Printf(\"Successfully ingested %d documents\\n\", len(documents)) return nil }\rText Chunking Strategies 1. Fixed-Size Chunking type FixedSizeChunker struct { ChunkSize int ChunkOverlap int } func NewFixedSizeChunker(chunkSize, overlap int) *FixedSizeChunker { return \u0026FixedSizeChunker{ ChunkSize: chunkSize, ChunkOverlap: overlap, } } func (c *FixedSizeChunker) ChunkText(text string) []string { if len(text) \u003c= c.ChunkSize { return []string{text} } var chunks []string start := 0 for start \u003c len(text) { end := start + c.ChunkSize if end \u003e len(text) { end = len(text) } chunk := text[start:end] chunks = append(chunks, chunk) // Move start position considering overlap start += c.ChunkSize - c.ChunkOverlap if start \u003e= len(text) { break } } return chunks } // Example usage func chunkLargeDocument(memory core.Memory, largeText string) error { ctx := context.Background() chunker := NewFixedSizeChunker(1000, 200) chunks := chunker.ChunkText(largeText) for i, chunk := range chunks { doc := core.Document{ ID: fmt.Sprintf(\"large-doc-chunk-%d\", i), Title: fmt.Sprintf(\"Large Document - Chunk %d\", i+1), Content: chunk, Source: \"large-document.pdf\", Type: core.DocumentTypePDF, ChunkIndex: i, ChunkTotal: len(chunks), Metadata: map[string]any{ \"chunk_method\": \"fixed-size\", \"chunk_size\": 1000, \"chunk_overlap\": 200, }, CreatedAt: time.Now(), } err := memory.IngestDocument(ctx, doc) if err != nil { return fmt.Errorf(\"failed to ingest chunk %d: %w\", i, err) } } return nil }\r2. Semantic Chunking type SemanticChunker struct { MaxChunkSize int MinChunkSize int } func NewSemanticChunker(minSize, maxSize int) *SemanticChunker { return \u0026SemanticChunker{ MinChunkSize: minSize, MaxChunkSize: maxSize, } } func (c *SemanticChunker) ChunkText(text string) []string { // Split by paragraphs first paragraphs := strings.Split(text, \"\\n\\n\") var chunks []string var currentChunk strings.Builder for _, paragraph := range paragraphs { paragraph = strings.TrimSpace(paragraph) if paragraph == \"\" { continue } // Check if adding this paragraph would exceed max size if currentChunk.Len() \u003e 0 \u0026\u0026 currentChunk.Len()+len(paragraph) \u003e c.MaxChunkSize { // Finalize current chunk if it meets minimum size if currentChunk.Len() \u003e= c.MinChunkSize { chunks = append(chunks, currentChunk.String()) currentChunk.Reset() } } // Add paragraph to current chunk if currentChunk.Len() \u003e 0 { currentChunk.WriteString(\"\\n\\n\") } currentChunk.WriteString(paragraph) } // Add final chunk if it has content if currentChunk.Len() \u003e 0 { chunks = append(chunks, currentChunk.String()) } return chunks }\r3. Sentence-Based Chunking type SentenceChunker struct { MaxSentences int Overlap int } func NewSentenceChunker(maxSentences, overlap int) *SentenceChunker { return \u0026SentenceChunker{ MaxSentences: maxSentences, Overlap: overlap, } } func (c *SentenceChunker) ChunkText(text string) []string { sentences := c.splitIntoSentences(text) if len(sentences) \u003c= c.MaxSentences { return []string{text} } var chunks []string start := 0 for start \u003c len(sentences) { end := start + c.MaxSentences if end \u003e len(sentences) { end = len(sentences) } chunk := strings.Join(sentences[start:end], \" \") chunks = append(chunks, chunk) start += c.MaxSentences - c.Overlap if start \u003e= len(sentences) { break } } return chunks } func (c *SentenceChunker) splitIntoSentences(text string) []string { // Simple sentence splitting (in production, use a proper NLP library) sentences := strings.FieldsFunc(text, func(r rune) bool { return r == '.' || r == '!' || r == '?' }) // Clean up sentences var cleanSentences []string for _, sentence := range sentences { sentence = strings.TrimSpace(sentence) if len(sentence) \u003e 10 { // Filter out very short fragments cleanSentences = append(cleanSentences, sentence) } } return cleanSentences }\rAdvanced Document Processing 1. Document Processor with Multiple Strategies type DocumentProcessor struct { memory core.Memory chunkers map[string]TextChunker extractors map[core.DocumentType]MetadataExtractor config ProcessorConfig } type TextChunker interface { ChunkText(text string) []string } type MetadataExtractor interface { ExtractMetadata(doc core.Document) (map[string]any, error) } type ProcessorConfig struct { DefaultChunkStrategy string MaxConcurrentDocs int EnableMetadataExtraction bool EnableContentCleaning bool } func NewDocumentProcessor(memory core.Memory, config ProcessorConfig) *DocumentProcessor { dp := \u0026DocumentProcessor{ memory: memory, chunkers: make(map[string]TextChunker), extractors: make(map[core.DocumentType]MetadataExtractor), config: config, } // Register default chunkers dp.chunkers[\"fixed\"] = NewFixedSizeChunker(1000, 200) dp.chunkers[\"semantic\"] = NewSemanticChunker(500, 1500) dp.chunkers[\"sentence\"] = NewSentenceChunker(10, 2) // Register metadata extractors dp.extractors[core.DocumentTypePDF] = \u0026PDFMetadataExtractor{} dp.extractors[core.DocumentTypeMarkdown] = \u0026MarkdownMetadataExtractor{} dp.extractors[core.DocumentTypeCode] = \u0026CodeMetadataExtractor{} return dp } func (dp *DocumentProcessor) ProcessDocument(ctx context.Context, doc core.Document, chunkStrategy string) error { // Clean content if enabled if dp.config.EnableContentCleaning { doc.Content = dp.cleanContent(doc.Content) } // Extract metadata if enabled if dp.config.EnableMetadataExtraction { if extractor, exists := dp.extractors[doc.Type]; exists { metadata, err := extractor.ExtractMetadata(doc) if err == nil { // Merge extracted metadata with existing if doc.Metadata == nil { doc.Metadata = make(map[string]any) } for k, v := range metadata { doc.Metadata[k] = v } } } } // Choose chunking strategy if chunkStrategy == \"\" { chunkStrategy = dp.config.DefaultChunkStrategy } chunker, exists := dp.chunkers[chunkStrategy] if !exists { return fmt.Errorf(\"unknown chunking strategy: %s\", chunkStrategy) } // Chunk the document chunks := chunker.ChunkText(doc.Content) // Process chunks if len(chunks) == 1 { // Single chunk - ingest as-is return dp.memory.IngestDocument(ctx, doc) } // Multiple chunks - create separate documents var documents []core.Document for i, chunk := range chunks { chunkDoc := doc // Copy original document chunkDoc.ID = fmt.Sprintf(\"%s-chunk-%d\", doc.ID, i) chunkDoc.Content = chunk chunkDoc.ChunkIndex = i chunkDoc.ChunkTotal = len(chunks) // Add chunking metadata if chunkDoc.Metadata == nil { chunkDoc.Metadata = make(map[string]any) } chunkDoc.Metadata[\"chunk_strategy\"] = chunkStrategy chunkDoc.Metadata[\"original_doc_id\"] = doc.ID documents = append(documents, chunkDoc) } return dp.memory.IngestDocuments(ctx, documents) } func (dp *DocumentProcessor) cleanContent(content string) string { // Remove excessive whitespace content = regexp.MustCompile(`\\s+`).ReplaceAllString(content, \" \") // Remove special characters that might interfere with processing content = regexp.MustCompile(`[^\\w\\s\\.,!?;:()\\-\"']`).ReplaceAllString(content, \"\") // Trim whitespace content = strings.TrimSpace(content) return content }\r2. Metadata Extractors // PDF Metadata Extractor type PDFMetadataExtractor struct{} func (e *PDFMetadataExtractor) ExtractMetadata(doc core.Document) (map[string]any, error) { metadata := make(map[string]any) // Extract basic statistics metadata[\"word_count\"] = len(strings.Fields(doc.Content)) metadata[\"char_count\"] = len(doc.Content) metadata[\"paragraph_count\"] = len(strings.Split(doc.Content, \"\\n\\n\")) // Extract potential headings (lines that are short and followed by longer content) lines := strings.Split(doc.Content, \"\\n\") var headings []string for i, line := range lines { line = strings.TrimSpace(line) if len(line) \u003e 0 \u0026\u0026 len(line) \u003c 100 \u0026\u0026 i+1 \u003c len(lines) { nextLine := strings.TrimSpace(lines[i+1]) if len(nextLine) \u003e len(line)*2 { headings = append(headings, line) } } } metadata[\"potential_headings\"] = headings // Detect language (simple heuristic) metadata[\"detected_language\"] = detectLanguage(doc.Content) return metadata, nil } // Markdown Metadata Extractor type MarkdownMetadataExtractor struct{} func (e *MarkdownMetadataExtractor) ExtractMetadata(doc core.Document) (map[string]any, error) { metadata := make(map[string]any) // Extract headings headings := extractMarkdownHeadings(doc.Content) metadata[\"headings\"] = headings metadata[\"heading_count\"] = len(headings) // Extract links links := extractMarkdownLinks(doc.Content) metadata[\"links\"] = links metadata[\"link_count\"] = len(links) // Extract code blocks codeBlocks := extractMarkdownCodeBlocks(doc.Content) metadata[\"code_blocks\"] = len(codeBlocks) // Extract front matter if present frontMatter := extractFrontMatter(doc.Content) if frontMatter != nil { metadata[\"front_matter\"] = frontMatter } return metadata, nil } // Code Metadata Extractor type CodeMetadataExtractor struct{} func (e *CodeMetadataExtractor) ExtractMetadata(doc core.Document) (map[string]any, error) { metadata := make(map[string]any) // Detect programming language language := detectProgrammingLanguage(doc.Source, doc.Content) metadata[\"programming_language\"] = language // Count lines of code lines := strings.Split(doc.Content, \"\\n\") metadata[\"total_lines\"] = len(lines) // Count non-empty lines nonEmptyLines := 0 commentLines := 0 for _, line := range lines { line = strings.TrimSpace(line) if line != \"\" { nonEmptyLines++ if isCommentLine(line, language) { commentLines++ } } } metadata[\"code_lines\"] = nonEmptyLines metadata[\"comment_lines\"] = commentLines // Extract functions/methods (basic pattern matching) functions := extractFunctions(doc.Content, language) metadata[\"functions\"] = functions metadata[\"function_count\"] = len(functions) return metadata, nil } // Helper functions for metadata extraction func detectLanguage(content string) string { // Simple language detection based on common words englishWords := []string{\"the\", \"and\", \"is\", \"in\", \"to\", \"of\", \"a\", \"that\", \"it\", \"with\"} words := strings.Fields(strings.ToLower(content)) englishCount := 0 for _, word := range words { for _, englishWord := range englishWords { if word == englishWord { englishCount++ break } } } if float64(englishCount)/float64(len(words)) \u003e 0.1 { return \"english\" } return \"unknown\" } func extractMarkdownHeadings(content string) []string { var headings []string lines := strings.Split(content, \"\\n\") for _, line := range lines { line = strings.TrimSpace(line) if strings.HasPrefix(line, \"#\") { headings = append(headings, line) } } return headings } func extractMarkdownLinks(content string) []string { // Simple regex for markdown links [text](url) linkRegex := regexp.MustCompile(`\\[([^\\]]+)\\]\\(([^)]+)\\)`) matches := linkRegex.FindAllStringSubmatch(content, -1) var links []string for _, match := range matches { if len(match) \u003e= 3 { links = append(links, match[2]) // URL part } } return links } func extractMarkdownCodeBlocks(content string) []string { // Simple extraction of code blocks codeBlockRegex := regexp.MustCompile(\"```[\\\\s\\\\S]*?```\") matches := codeBlockRegex.FindAllString(content, -1) return matches } func extractFrontMatter(content string) map[string]any { // Extract YAML front matter if !strings.HasPrefix(content, \"---\") { return nil } parts := strings.SplitN(content, \"---\", 3) if len(parts) \u003c 3 { return nil } // Simple key-value extraction (in production, use a YAML parser) frontMatter := make(map[string]any) lines := strings.Split(parts[1], \"\\n\") for _, line := range lines { line = strings.TrimSpace(line) if strings.Contains(line, \":\") { parts := strings.SplitN(line, \":\", 2) if len(parts) == 2 { key := strings.TrimSpace(parts[0]) value := strings.TrimSpace(parts[1]) frontMatter[key] = value } } } return frontMatter } func detectProgrammingLanguage(filename, content string) string { // Detect by file extension ext := strings.ToLower(filepath.Ext(filename)) switch ext { case \".go\": return \"go\" case \".py\": return \"python\" case \".js\": return \"javascript\" case \".ts\": return \"typescript\" case \".java\": return \"java\" case \".cpp\", \".cc\", \".cxx\": return \"cpp\" case \".c\": return \"c\" case \".rs\": return \"rust\" } // Detect by content patterns if strings.Contains(content, \"package main\") || strings.Contains(content, \"func \") { return \"go\" } if strings.Contains(content, \"def \") || strings.Contains(content, \"import \") { return \"python\" } return \"unknown\" } func isCommentLine(line, language string) bool { switch language { case \"go\", \"javascript\", \"typescript\", \"java\", \"cpp\", \"c\", \"rust\": return strings.HasPrefix(line, \"//\") || strings.HasPrefix(line, \"/*\") case \"python\": return strings.HasPrefix(line, \"#\") } return false } func extractFunctions(content, language string) []string { var functions []string switch language { case \"go\": funcRegex := regexp.MustCompile(`func\\s+(\\w+)\\s*\\(`) matches := funcRegex.FindAllStringSubmatch(content, -1) for _, match := range matches { if len(match) \u003e= 2 { functions = append(functions, match[1]) } } case \"python\": funcRegex := regexp.MustCompile(`def\\s+(\\w+)\\s*\\(`) matches := funcRegex.FindAllStringSubmatch(content, -1) for _, match := range matches { if len(match) \u003e= 2 { functions = append(functions, match[1]) } } case \"javascript\", \"typescript\": funcRegex := regexp.MustCompile(`function\\s+(\\w+)\\s*\\(`) matches := funcRegex.FindAllStringSubmatch(content, -1) for _, match := range matches { if len(match) \u003e= 2 { functions = append(functions, match[1]) } } } return functions }\rKnowledge Base Search and Retrieval 1. Advanced Search with Filters func performAdvancedKnowledgeSearch(memory core.Memory) error { ctx := context.Background() // Search with multiple filters results, err := memory.SearchKnowledge(ctx, \"machine learning algorithms\", core.WithLimit(10), core.WithScoreThreshold(0.7), core.WithSources([]string{\"textbook-chapter-1.pdf\", \"textbook-chapter-2.pdf\"}), core.WithDocumentTypes([]core.DocumentType{core.DocumentTypePDF}), core.WithTags([]string{\"ml\", \"algorithms\"}), core.WithDateRange(\u0026core.DateRange{ Start: time.Now().Add(-30 * 24 * time.Hour), End: time.Now(), }), ) if err != nil { return fmt.Errorf(\"knowledge search failed: %w\", err) } fmt.Printf(\"Found %d relevant knowledge items:\\n\", len(results)) for _, result := range results { fmt.Printf(\"- %s (Score: %.3f)\\n\", result.Title, result.Score) fmt.Printf(\" Source: %s\\n\", result.Source) fmt.Printf(\" Content: %s...\\n\", truncateString(result.Content, 100)) if result.ChunkIndex \u003e 0 { fmt.Printf(\" Chunk: %d/%d\\n\", result.ChunkIndex+1, result.ChunkTotal) } fmt.Println() } return nil } func truncateString(s string, maxLen int) string { if len(s) \u003c= maxLen { return s } return s[:maxLen] + \"...\" }\r2. Hybrid Search (Personal + Knowledge) func performHybridSearch(memory core.Memory) error { ctx := context.Context() // Perform hybrid search combining personal memory and knowledge base result, err := memory.SearchAll(ctx, \"neural network implementation\", core.WithLimit(15), core.WithScoreThreshold(0.6), core.WithIncludePersonal(true), core.WithIncludeKnowledge(true), core.WithHybridWeight(0.7), // 70% semantic, 30% keyword ) if err != nil { return fmt.Errorf(\"hybrid search failed: %w\", err) } fmt.Printf(\"Hybrid Search Results for: %s\\n\", result.Query) fmt.Printf(\"Total Results: %d (Search Time: %v)\\n\\n\", result.TotalResults, result.SearchTime) // Display personal memory results if len(result.PersonalMemory) \u003e 0 { fmt.Println(\"Personal Memory Results:\") for _, item := range result.PersonalMemory { fmt.Printf(\"- %s (Score: %.3f)\\n\", truncateString(item.Content, 80), item.Score) } fmt.Println() } // Display knowledge base results if len(result.Knowledge) \u003e 0 { fmt.Println(\"Knowledge Base Results:\") for _, item := range result.Knowledge { fmt.Printf(\"- %s (Score: %.3f)\\n\", item.Title, item.Score) fmt.Printf(\" Source: %s\\n\", item.Source) } } return nil }\r3. RAG Context Building func buildRAGContext(memory core.Memory, query string) error { ctx := context.Background() // Build comprehensive RAG context ragContext, err := memory.BuildContext(ctx, query, core.WithMaxTokens(4000), core.WithPersonalWeight(0.3), core.WithKnowledgeWeight(0.7), core.WithHistoryLimit(5), core.WithIncludeSources(true), core.WithFormatTemplate(`Context Information: Personal Memory: {{range .PersonalMemory}} - {{.Content}} {{end}} Knowledge Base: {{range .Knowledge}} - {{.Content}} (Source: {{.Source}}) {{end}} Recent Conversation: {{range .ChatHistory}} {{.Role}}: {{.Content}} {{end}} Query: {{.Query}}`), ) if err != nil { return fmt.Errorf(\"failed to build RAG context: %w\", err) } fmt.Printf(\"RAG Context for: %s\\n\", ragContext.Query) fmt.Printf(\"Token Count: %d\\n\", ragContext.TokenCount) fmt.Printf(\"Sources: %v\\n\", ragContext.Sources) fmt.Printf(\"Context Text:\\n%s\\n\", ragContext.ContextText) return nil }\rProduction Optimization 1. Batch Processing Pipeline type BatchProcessor struct { memory core.Memory processor *DocumentProcessor concurrency int batchSize int } func NewBatchProcessor(memory core.Memory, concurrency, batchSize int) *BatchProcessor { return \u0026BatchProcessor{ memory: memory, processor: NewDocumentProcessor(memory, ProcessorConfig{ DefaultChunkStrategy: \"semantic\", MaxConcurrentDocs: concurrency, EnableMetadataExtraction: true, EnableContentCleaning: true, }), concurrency: concurrency, batchSize: batchSize, } } func (bp *BatchProcessor) ProcessDocuments(ctx context.Context, documents []core.Document) error { // Process documents in batches for i := 0; i \u003c len(documents); i += bp.batchSize { end := i + bp.batchSize if end \u003e len(documents) { end = len(documents) } batch := documents[i:end] err := bp.processBatch(ctx, batch) if err != nil { return fmt.Errorf(\"failed to process batch %d-%d: %w\", i, end-1, err) } fmt.Printf(\"Processed batch %d-%d (%d documents)\\n\", i, end-1, len(batch)) } return nil } func (bp *BatchProcessor) processBatch(ctx context.Context, documents []core.Document) error { // Use worker pool for concurrent processing jobs := make(chan core.Document, len(documents)) results := make(chan error, len(documents)) // Start workers for w := 0; w \u003c bp.concurrency; w++ { go bp.worker(ctx, jobs, results) } // Send jobs for _, doc := range documents { jobs \u003c- doc } close(jobs) // Collect results var errors []error for i := 0; i \u003c len(documents); i++ { if err := \u003c-results; err != nil { errors = append(errors, err) } } if len(errors) \u003e 0 { return fmt.Errorf(\"batch processing failed with %d errors: %v\", len(errors), errors[0]) } return nil } func (bp *BatchProcessor) worker(ctx context.Context, jobs \u003c-chan core.Document, results chan\u003c- error) { for doc := range jobs { err := bp.processor.ProcessDocument(ctx, doc, \"\") results \u003c- err } }\r2. Performance Monitoring type IngestionMetrics struct { DocumentsProcessed int64 `json:\"documents_processed\"` ChunksCreated int64 `json:\"chunks_created\"` ProcessingTime time.Duration `json:\"processing_time\"` ErrorCount int64 `json:\"error_count\"` AverageChunkSize float64 `json:\"average_chunk_size\"` mu sync.RWMutex } func (m *IngestionMetrics) RecordDocument(chunkCount int, processingTime time.Duration, chunkSizes []int) { m.mu.Lock() defer m.mu.Unlock() m.DocumentsProcessed++ m.ChunksCreated += int64(chunkCount) m.ProcessingTime += processingTime // Update average chunk size if len(chunkSizes) \u003e 0 { totalSize := 0 for _, size := range chunkSizes { totalSize += size } avgSize := float64(totalSize) / float64(len(chunkSizes)) // Running average totalChunks := float64(m.ChunksCreated) m.AverageChunkSize = (m.AverageChunkSize*(totalChunks-float64(chunkCount)) + avgSize*float64(chunkCount)) / totalChunks } } func (m *IngestionMetrics) RecordError() { m.mu.Lock() defer m.mu.Unlock() m.ErrorCount++ } func (m *IngestionMetrics) GetStats() IngestionMetrics { m.mu.RLock() defer m.mu.RUnlock() return IngestionMetrics{ DocumentsProcessed: m.DocumentsProcessed, ChunksCreated: m.ChunksCreated, ProcessingTime: m.ProcessingTime, ErrorCount: m.ErrorCount, AverageChunkSize: m.AverageChunkSize, } }\rBest Practices 1. Document Ingestion Guidelines Chunk Size: Balance between context preservation and retrieval precision Overlap: Use 10-20% overlap to maintain context continuity Metadata: Extract and store rich metadata for better filtering Batch Processing: Process documents in batches for better performance Error Handling: Implement robust error handling and retry mechanisms 2. Performance Optimization Concurrent Processing: Use worker pools for parallel document processing Embedding Caching: Cache embeddings to avoid recomputation Index Optimization: Optimize vector database indexes for your query patterns Memory Management: Monitor memory usage during large batch operations 3. Quality Assurance Content Validation: Validate document content before ingestion Duplicate Detection: Implement deduplication to avoid redundant storage Quality Metrics: Track ingestion quality and search relevance Regular Maintenance: Periodically clean up and optimize the knowledge base Conclusion Document ingestion and knowledge base management are critical for building effective RAG systems. By implementing proper chunking strategies, metadata extraction, and optimization techniques, you can create knowledge bases that provide accurate and relevant information to your agents.",
    "tags": [],
    "title": "document-ingestion",
    "uri": "/AgenticGoKitDocs/tutorials/memory-systems/document-ingestion/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials \u003e core-concepts",
    "content": "Error Handling and Recovery in AgenticGoKit Overview Robust error handling is critical for building reliable multi-agent systems. This tutorial explores AgenticGoKit’s error handling mechanisms, recovery strategies, and best practices for building fault-tolerant agent workflows.\nUnderstanding error handling is essential because agent systems involve multiple components, network calls, and complex interactions where failures can occur at any point.\nPrerequisites Understanding of Message Passing and Event Flow Knowledge of State Management Basic understanding of Go error handling patterns Core Error Handling Concepts Error Types in AgenticGoKit AgenticGoKit handles several types of errors:\nAgent Execution Errors: Failures during agent processing Orchestration Errors: Issues with agent coordination Communication Errors: Problems with event routing Resource Errors: Database, API, or external service failures Validation Errors: Invalid input or state validation failures Error Flow Architecture ┌─────────┐ ┌──────────┐ ┌─────────────┐ ┌─────────────┐\r│ Event │───▶│ Runner │───▶│ Orchestrator│───▶│ Agent │\r└─────────┘ └──────────┘ └─────────────┘ └─────────────┘\r│ │ │\r▼ ▼ ▼\r┌──────────┐ ┌─────────────┐ ┌─────────────┐\r│Error Hook│ │Error Router │ │Error Result │\r└──────────┘ └─────────────┘ └─────────────┘\rError Handling Mechanisms 1. Agent-Level Error Handling Agents can return errors through the AgentResult:\nfunc (a *MyAgent) Run(ctx context.Context, event Event, state State) (AgentResult, error) { // Validate input query, ok := state.Get(\"query\") if !ok { return AgentResult{ Error: \"missing required field: query\", }, errors.New(\"query not found in state\") } // Process with error handling result, err := a.processQuery(query.(string)) if err != nil { return AgentResult{ Error: fmt.Sprintf(\"processing failed: %v\", err), OutputState: state, // Return original state }, err } // Success case outputState := state.Clone() outputState.Set(\"response\", result) return AgentResult{ OutputState: outputState, }, nil }\r2. Error Hooks and Callbacks AgenticGoKit provides hooks for intercepting and handling errors:\n// Register error handling callback runner.RegisterCallback(core.HookAgentError, \"error-handler\", func(ctx context.Context, args core.CallbackArgs) (core.State, error) { fmt.Printf(\"Agent %s failed: %v\\n\", args.AgentID, args.Error) // Log error details logError(args.AgentID, args.Error, args.Event) // Optionally modify state or trigger recovery if isRecoverableError(args.Error) { return handleRecoverableError(args) } return args.State, nil }, )\r3. Enhanced Error Routing AgenticGoKit includes sophisticated error routing capabilities:\n// Configure error routing errorConfig := \u0026core.ErrorRouterConfig{ MaxRetries: 3, RetryDelay: time.Second * 2, FallbackAgent: \"error-recovery-agent\", ErrorClassification: map[string]core.ErrorAction{ \"timeout\": core.ErrorActionRetry, \"rate_limit\": core.ErrorActionDelay, \"auth_error\": core.ErrorActionFallback, \"fatal_error\": core.ErrorActionFail, }, } runner.SetErrorRouterConfig(errorConfig)\rError Recovery Strategies 1. Retry Mechanisms Implement automatic retry for transient failures:\ntype RetryableAgent struct { baseAgent Agent maxRetries int retryDelay time.Duration } func (r *RetryableAgent) Run(ctx context.Context, event Event, state State) (AgentResult, error) { var lastErr error for attempt := 0; attempt \u003c= r.maxRetries; attempt++ { if attempt \u003e 0 { // Wait before retry select { case \u003c-time.After(r.retryDelay): case \u003c-ctx.Done(): return AgentResult{}, ctx.Err() } } result, err := r.baseAgent.Run(ctx, event, state) if err == nil { return result, nil } lastErr = err // Check if error is retryable if !isRetryableError(err) { break } fmt.Printf(\"Attempt %d failed, retrying: %v\\n\", attempt+1, err) } return AgentResult{ Error: fmt.Sprintf(\"failed after %d attempts: %v\", r.maxRetries+1, lastErr), }, lastErr } func isRetryableError(err error) bool { // Define which errors are worth retrying return strings.Contains(err.Error(), \"timeout\") || strings.Contains(err.Error(), \"connection\") || strings.Contains(err.Error(), \"rate limit\") }\r2. Circuit Breaker Pattern Prevent cascading failures with circuit breakers:\ntype CircuitBreakerAgent struct { baseAgent Agent breaker *CircuitBreaker } type CircuitBreaker struct { maxFailures int resetTimeout time.Duration state CircuitState failures int lastFailure time.Time mu sync.RWMutex } type CircuitState int const ( CircuitClosed CircuitState = iota CircuitOpen CircuitHalfOpen ) func (cb *CircuitBreaker) Call(fn func() error) error { cb.mu.Lock() defer cb.mu.Unlock() // Check if circuit should reset if cb.state == CircuitOpen \u0026\u0026 time.Since(cb.lastFailure) \u003e cb.resetTimeout { cb.state = CircuitHalfOpen cb.failures = 0 } // Reject if circuit is open if cb.state == CircuitOpen { return errors.New(\"circuit breaker is open\") } // Execute function err := fn() if err != nil { cb.failures++ cb.lastFailure = time.Now() if cb.failures \u003e= cb.maxFailures { cb.state = CircuitOpen } return err } // Success - reset circuit cb.failures = 0 cb.state = CircuitClosed return nil } func (cba *CircuitBreakerAgent) Run(ctx context.Context, event Event, state State) (AgentResult, error) { var result AgentResult var err error breakerErr := cba.breaker.Call(func() error { result, err = cba.baseAgent.Run(ctx, event, state) return err }) if breakerErr != nil { return AgentResult{ Error: fmt.Sprintf(\"circuit breaker: %v\", breakerErr), }, breakerErr } return result, err }\r3. Fallback Agents Implement fallback mechanisms for critical failures:\ntype FallbackAgent struct { primaryAgent Agent fallbackAgent Agent fallbackTrigger func(error) bool } func (f *FallbackAgent) Run(ctx context.Context, event Event, state State) (AgentResult, error) { // Try primary agent first result, err := f.primaryAgent.Run(ctx, event, state) // If primary succeeds, return result if err == nil { return result, nil } // Check if we should use fallback if !f.fallbackTrigger(err) { return result, err } fmt.Printf(\"Primary agent failed, using fallback: %v\\n\", err) // Try fallback agent fallbackResult, fallbackErr := f.fallbackAgent.Run(ctx, event, state) if fallbackErr != nil { // Both failed - return combined error return AgentResult{ Error: fmt.Sprintf(\"primary failed: %v, fallback failed: %v\", err, fallbackErr), }, fmt.Errorf(\"both primary and fallback failed: %v, %v\", err, fallbackErr) } // Mark result as from fallback if fallbackResult.OutputState != nil { fallbackResult.OutputState.SetMeta(\"fallback_used\", \"true\") fallbackResult.OutputState.SetMeta(\"primary_error\", err.Error()) } return fallbackResult, nil }\rError Handling in Different Orchestration Patterns 1. Route Orchestration Error Handling // Route orchestrator with error handling func (o *RouteOrchestrator) Dispatch(ctx context.Context, event Event) (AgentResult, error) { targetName, ok := event.GetMetadataValue(RouteMetadataKey) if !ok { return o.handleRoutingError(event, errors.New(\"missing route metadata\")) } handler, exists := o.handlers[targetName] if !exists { return o.handleRoutingError(event, fmt.Errorf(\"agent not found: %s\", targetName)) } // Execute with timeout ctx, cancel := context.WithTimeout(ctx, 30*time.Second) defer cancel() result, err := handler.Run(ctx, event, core.NewState()) if err != nil { return o.handleAgentError(event, targetName, err) } return result, nil } func (o *RouteOrchestrator) handleRoutingError(event Event, err error) (AgentResult, error) { // Create error event errorEvent := core.NewEvent( \"error-handler\", core.EventData{ \"error_type\": \"routing_error\", \"error_message\": err.Error(), \"original_event\": event, }, map[string]string{ \"route\": \"error-handler\", \"session_id\": event.GetSessionID(), }, ) // Emit error event if emitter is available if o.emitter != nil { o.emitter.Emit(errorEvent) } return AgentResult{ Error: err.Error(), }, err }\r2. Collaborative Orchestration Error Handling // Collaborative orchestrator with partial failure handling func (o *CollaborativeOrchestrator) Dispatch(ctx context.Context, event Event) (AgentResult, error) { var wg sync.WaitGroup results := make([]AgentResult, 0) errors := make([]error, 0) mu := \u0026sync.Mutex{} // Execute all agents for name, handler := range o.handlers { wg.Add(1) go func(agentName string, agentHandler AgentHandler) { defer wg.Done() result, err := agentHandler.Run(ctx, event, core.NewState()) mu.Lock() if err != nil { errors = append(errors, fmt.Errorf(\"agent %s: %w\", agentName, err)) } else { results = append(results, result) } mu.Unlock() }(name, handler) } wg.Wait() // Handle partial failures totalAgents := len(o.handlers) successCount := len(results) failureCount := len(errors) // Check if we have enough successes successThreshold := 0.5 // At least 50% must succeed if float64(successCount)/float64(totalAgents) \u003c successThreshold { return AgentResult{ Error: fmt.Sprintf(\"insufficient successes: %d/%d agents failed\", failureCount, totalAgents), }, fmt.Errorf(\"collaborative orchestration failed: %v\", errors) } // Combine successful results combinedResult := o.combineResults(results) // Add failure information to metadata if len(errors) \u003e 0 { if combinedResult.OutputState != nil { combinedResult.OutputState.SetMeta(\"partial_failures\", fmt.Sprintf(\"%d\", failureCount)) combinedResult.OutputState.SetMeta(\"success_rate\", fmt.Sprintf(\"%.2f\", float64(successCount)/float64(totalAgents))) } } return combinedResult, nil }\r3. Sequential Orchestration Error Handling // Sequential orchestrator with rollback capability func (o *SequentialOrchestrator) Dispatch(ctx context.Context, event Event) (AgentResult, error) { currentState := core.NewState() completedStages := make([]string, 0) // Merge event data for key, value := range event.GetData() { currentState.Set(key, value) } // Process through sequence for i, agentName := range o.sequence { handler, exists := o.handlers[agentName] if !exists { return o.rollback(completedStages, fmt.Errorf(\"agent %s not found\", agentName)) } // Create stage event stageEvent := core.NewEvent(agentName, currentState.GetAll(), event.GetMetadata()) // Execute with timeout stageCtx, cancel := context.WithTimeout(ctx, 60*time.Second) result, err := handler.Run(stageCtx, stageEvent, currentState) cancel() if err != nil { return o.rollback(completedStages, fmt.Errorf(\"stage %d (%s) failed: %w\", i+1, agentName, err)) } // Update state and track completion if result.OutputState != nil { currentState = result.OutputState } completedStages = append(completedStages, agentName) fmt.Printf(\"Stage %d (%s) completed successfully\\n\", i+1, agentName) } return AgentResult{OutputState: currentState}, nil } func (o *SequentialOrchestrator) rollback(completedStages []string, err error) (AgentResult, error) { fmt.Printf(\"Rolling back %d completed stages due to error: %v\\n\", len(completedStages), err) // Execute rollback in reverse order for i := len(completedStages) - 1; i \u003e= 0; i-- { stageName := completedStages[i] if rollbackHandler, exists := o.rollbackHandlers[stageName]; exists { rollbackCtx, cancel := context.WithTimeout(context.Background(), 30*time.Second) rollbackHandler.Rollback(rollbackCtx, stageName) cancel() } } return AgentResult{ Error: fmt.Sprintf(\"sequential processing failed: %v\", err), }, err }\rError Monitoring and Observability 1. Error Metrics Collection type ErrorMetrics struct { errorCounts map[string]int64 errorRates map[string]float64 lastErrors map[string]time.Time mu sync.RWMutex } func (em *ErrorMetrics) RecordError(agentID string, errorType string) { em.mu.Lock() defer em.mu.Unlock() key := fmt.Sprintf(\"%s:%s\", agentID, errorType) em.errorCounts[key]++ em.lastErrors[key] = time.Now() // Calculate error rate (errors per minute) em.calculateErrorRate(key) } func (em *ErrorMetrics) calculateErrorRate(key string) { // Implementation for calculating error rates // This would typically involve time windows and moving averages } // Register error metrics callback runner.RegisterCallback(core.HookAgentError, \"metrics-collector\", func(ctx context.Context, args core.CallbackArgs) (core.State, error) { errorType := classifyError(args.Error) errorMetrics.RecordError(args.AgentID, errorType) return args.State, nil }, )\r2. Error Alerting type ErrorAlerter struct { thresholds map[string]AlertThreshold notifier Notifier } type AlertThreshold struct { ErrorRate float64 // Errors per minute TimeWindow time.Duration // Time window for rate calculation Cooldown time.Duration // Minimum time between alerts } func (ea *ErrorAlerter) CheckThresholds(agentID string, errorRate float64) { threshold, exists := ea.thresholds[agentID] if !exists { return } if errorRate \u003e threshold.ErrorRate { alert := Alert{ AgentID: agentID, ErrorRate: errorRate, Threshold: threshold.ErrorRate, Timestamp: time.Now(), } ea.notifier.SendAlert(alert) } }\r3. Error Logging and Tracing type ErrorLogger struct { logger *log.Logger tracer trace.Tracer } func (el *ErrorLogger) LogError(ctx context.Context, agentID string, err error, event Event) { // Create span for error ctx, span := el.tracer.Start(ctx, \"agent_error\") defer span.End() // Add error attributes span.SetAttributes( attribute.String(\"agent.id\", agentID), attribute.String(\"error.message\", err.Error()), attribute.String(\"event.id\", event.GetID()), ) // Log structured error el.logger.Printf(\"AGENT_ERROR agent=%s event=%s error=%v\", agentID, event.GetID(), err) // Record error in span span.RecordError(err) }\rTesting Error Scenarios 1. Error Injection for Testing type ErrorInjectingAgent struct { baseAgent Agent errorRate float64 // 0.0 to 1.0 errorTypes []error random *rand.Rand } func (eia *ErrorInjectingAgent) Run(ctx context.Context, event Event, state State) (AgentResult, error) { // Inject errors based on configured rate if eia.random.Float64() \u003c eia.errorRate { errorIndex := eia.random.Intn(len(eia.errorTypes)) injectedError := eia.errorTypes[errorIndex] return AgentResult{ Error: fmt.Sprintf(\"injected error: %v\", injectedError), }, injectedError } // Normal execution return eia.baseAgent.Run(ctx, event, state) }\r2. Error Scenario Testing func TestErrorRecovery(t *testing.T) { // Create agent that fails first time, succeeds second time failingAgent := \u0026FailingAgent{failCount: 1} retryAgent := \u0026RetryableAgent{ baseAgent: failingAgent, maxRetries: 2, retryDelay: time.Millisecond * 100, } // Test retry mechanism event := core.NewEvent(\"test\", core.EventData{\"test\": \"data\"}, nil) result, err := retryAgent.Run(context.Background(), event, core.NewState()) assert.NoError(t, err) assert.NotEmpty(t, result.OutputState) } func TestCircuitBreaker(t *testing.T) { // Create agent that always fails alwaysFailingAgent := \u0026AlwaysFailingAgent{} circuitAgent := \u0026CircuitBreakerAgent{ baseAgent: alwaysFailingAgent, breaker: \u0026CircuitBreaker{ maxFailures: 3, resetTimeout: time.Second, }, } // Test that circuit opens after max failures for i := 0; i \u003c 5; i++ { _, err := circuitAgent.Run(context.Background(), event, core.NewState()) if i \u003c 3 { assert.Contains(t, err.Error(), \"always fails\") } else { assert.Contains(t, err.Error(), \"circuit breaker is open\") } } }\rBest Practices for Error Handling 1. Error Classification type ErrorClass int const ( ErrorClassTransient ErrorClass = iota // Temporary, retry-able ErrorClassPermanent // Permanent, don't retry ErrorClassResource // Resource exhaustion ErrorClassValidation // Input validation ErrorClassSecurity // Security/auth issues ) func ClassifyError(err error) ErrorClass { errStr := strings.ToLower(err.Error()) switch { case strings.Contains(errStr, \"timeout\"): return ErrorClassTransient case strings.Contains(errStr, \"connection\"): return ErrorClassTransient case strings.Contains(errStr, \"rate limit\"): return ErrorClassResource case strings.Contains(errStr, \"validation\"): return ErrorClassValidation case strings.Contains(errStr, \"unauthorized\"): return ErrorClassSecurity default: return ErrorClassPermanent } }\r2. Graceful Degradation type GracefulAgent struct { primaryAgent Agent degradedMode Agent healthChecker HealthChecker } func (ga *GracefulAgent) Run(ctx context.Context, event Event, state State) (AgentResult, error) { // Check if we should use degraded mode if !ga.healthChecker.IsHealthy() { fmt.Println(\"Using degraded mode due to health issues\") result, err := ga.degradedMode.Run(ctx, event, state) if err == nil \u0026\u0026 result.OutputState != nil { result.OutputState.SetMeta(\"degraded_mode\", \"true\") } return result, err } // Try primary agent result, err := ga.primaryAgent.Run(ctx, event, state) if err != nil { // Mark as unhealthy and try degraded mode ga.healthChecker.MarkUnhealthy() return ga.degradedMode.Run(ctx, event, state) } return result, nil }\r3. Error Context Preservation type ContextualError struct { Err error AgentID string EventID string SessionID string Timestamp time.Time Context map[string]interface{} } func (ce *ContextualError) Error() string { return fmt.Sprintf(\"agent=%s event=%s session=%s: %v\", ce.AgentID, ce.EventID, ce.SessionID, ce.Err) } func WrapError(err error, agentID string, event Event) error { return \u0026ContextualError{ Err: err, AgentID: agentID, EventID: event.GetID(), SessionID: event.GetSessionID(), Timestamp: time.Now(), Context: make(map[string]interface{}), } }\rCommon Error Patterns and Solutions 1. Timeout Handling func WithTimeout(agent Agent, timeout time.Duration) Agent { return AgentFunc(func(ctx context.Context, event Event, state State) (AgentResult, error) { ctx, cancel := context.WithTimeout(ctx, timeout) defer cancel() done := make(chan struct{}) var result AgentResult var err error go func() { result, err = agent.Run(ctx, event, state) close(done) }() select { case \u003c-done: return result, err case \u003c-ctx.Done(): return AgentResult{ Error: fmt.Sprintf(\"agent timeout after %v\", timeout), }, ctx.Err() } }) }\r2. Resource Exhaustion Handling type ResourceLimitedAgent struct { baseAgent Agent semaphore chan struct{} } func NewResourceLimitedAgent(agent Agent, maxConcurrency int) *ResourceLimitedAgent { return \u0026ResourceLimitedAgent{ baseAgent: agent, semaphore: make(chan struct{}, maxConcurrency), } } func (rla *ResourceLimitedAgent) Run(ctx context.Context, event Event, state State) (AgentResult, error) { // Acquire resource select { case rla.semaphore \u003c- struct{}{}: defer func() { \u003c-rla.semaphore }() case \u003c-ctx.Done(): return AgentResult{ Error: \"resource acquisition timeout\", }, ctx.Err() } return rla.baseAgent.Run(ctx, event, state) }\rConclusion Effective error handling is crucial for building robust multi-agent systems. AgenticGoKit provides comprehensive error handling mechanisms including hooks, routing, recovery strategies, and monitoring capabilities.\nKey takeaways:\nClassify errors appropriately for proper handling Implement retry mechanisms for transient failures Use circuit breakers to prevent cascading failures Provide fallback options for critical functionality Monitor and alert on error patterns Test error scenarios thoroughly Preserve error context for debugging Next Steps Memory Systems - Learn about persistent storage and RAG Debugging Guide - Advanced debugging techniques Performance Optimization - Optimize agent performance Production Deployment - Deploy with proper error handling Further Reading API Reference: Error Handling Examples: Error Recovery Patterns Configuration Guide: Error Settings",
    "description": "Error Handling and Recovery in AgenticGoKit Overview Robust error handling is critical for building reliable multi-agent systems. This tutorial explores AgenticGoKit’s error handling mechanisms, recovery strategies, and best practices for building fault-tolerant agent workflows.\nUnderstanding error handling is essential because agent systems involve multiple components, network calls, and complex interactions where failures can occur at any point.\nPrerequisites Understanding of Message Passing and Event Flow Knowledge of State Management Basic understanding of Go error handling patterns Core Error Handling Concepts Error Types in AgenticGoKit AgenticGoKit handles several types of errors:",
    "tags": [],
    "title": "error-handling",
    "uri": "/AgenticGoKitDocs/tutorials/core-concepts/error-handling/index.html"
  },
  {
    "breadcrumb": "content \u003e guides",
    "content": "Framework Comparison: AgenticGoKit vs. Alternatives Straightforward feature-driven comparison of leading agent frameworks for building multi-agent systems.\nNote: AgenticGoKit is Go-based and in preview; Comparison reflects capabilities as of July 2025.\n📊 Feature Set Comparison Feature AgenticGoKit LangChain AutoGen CrewAI Semantic Kernel Agno Language Go Python Python Python C#/Python Python Maturity Preview Stable Stable Growing Stable Early‑stage, but active Community Size ⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐ Multi-Agent Focus ⭐⭐⭐⭐ ⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐ ⭐⭐⭐⭐ Memory Systems ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐ ⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐ Tool Integration ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐ Documentation ⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐ Performance High Moderate Moderate Moderate Moderate High Monitoring Developing Manual Limited Basic Azure-native Built-in Modularity ✅ ✅ ✅ ✅ ✅ ✅ 🚀 AgenticGoKit: The Go-Native Approach Why Go for Agent Systems? // AgenticGoKit: Leverages Go's concurrency model func (r *Runner) ProcessEvents(ctx context.Context, events []core.Event) { var wg sync.WaitGroup results := make(chan *core.AgentResult, len(events)) for _, event := range events { wg.Add(1) go func(e core.Event) { defer wg.Done() result, _ := r.processEvent(ctx, e) results \u003c- result }(event) } wg.Wait() close(results) }\rGo Language Benefits:\nNative Concurrency: Go’s goroutines provide good concurrency support Memory Efficiency: Generally lower memory usage than interpreted languages Fast Startup: Compiled binaries start quickly Single Binary: Simplified deployment without dependency management Developing Features: Working on observability, metrics, and health checks Note: Performance characteristics will vary based on your specific use case, workload, and infrastructure. We recommend testing with your own requirements.\nReady to get started with AgenticGoKit?\n🚀 Quick Start Guide - Get running in 5 minutes\n📚 Tutorial Series - Learn core concepts\n💬 Community Discord - Get help and share ideas\n🔧 Migration Guide - Move from other frameworks\nAgenticGoKit: Built for production, designed for scale, optimized for Go.",
    "description": "Framework Comparison: AgenticGoKit vs. Alternatives Straightforward feature-driven comparison of leading agent frameworks for building multi-agent systems.\nNote: AgenticGoKit is Go-based and in preview; Comparison reflects capabilities as of July 2025.\n📊 Feature Set Comparison Feature AgenticGoKit LangChain AutoGen CrewAI Semantic Kernel Agno Language Go Python Python Python C#/Python Python Maturity Preview Stable Stable Growing Stable Early‑stage, but active Community Size ⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐ Multi-Agent Focus ⭐⭐⭐⭐ ⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐ ⭐⭐⭐⭐ Memory Systems ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐ ⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐ Tool Integration ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐ Documentation ⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐ Performance High Moderate Moderate Moderate Moderate High Monitoring Developing Manual Limited Basic Azure-native Built-in Modularity ✅ ✅ ✅ ✅ ✅ ✅ 🚀 AgenticGoKit: The Go-Native Approach Why Go for Agent Systems? // AgenticGoKit: Leverages Go's concurrency model func (r *Runner) ProcessEvents(ctx context.Context, events []core.Event) { var wg sync.WaitGroup results := make(chan *core.AgentResult, len(events)) for _, event := range events { wg.Add(1) go func(e core.Event) { defer wg.Done() result, _ := r.processEvent(ctx, e) results \u003c- result }(event) } wg.Wait() close(results) }\rGo Language Benefits:",
    "tags": [],
    "title": "framework-comparison",
    "uri": "/AgenticGoKitDocs/guides/framework-comparison/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials",
    "content": "Getting Started Tutorials Beginner-friendly tutorials to get you up and running with AgenticGoKit quickly.\nTutorial Series Follow these tutorials in order for the best learning experience:\n0. 5-Minute Quickstart Time: 5 minutes\nGet a multi-agent system running immediately with minimal setup.\nYou’ll learn:\nFastest way to create agents Basic orchestration patterns CLI vs code-first approaches 1. Your First Agent Time: 10 minutes\nCreate your first AgenticGoKit agent and understand the basic concepts.\nYou’ll learn:\nHow to set up a basic agent Understanding the Agent interface Running your first agent interaction 2. Multi-Agent Collaboration Time: 15 minutes\nLearn how multiple agents can work together using orchestration patterns.\nYou’ll learn:\nSequential and collaborative orchestration Agent communication patterns State management between agents 3. Memory and RAG Time: 20 minutes\nAdd memory capabilities and implement Retrieval-Augmented Generation (RAG).\nYou’ll learn:\nSetting up vector databases Document ingestion and retrieval Building knowledge-aware agents 4. Tool Integration Time: 15 minutes\nConnect your agents to external tools using the Model Context Protocol (MCP).\nYou’ll learn:\nMCP tool integration Custom tool development Tool caching and optimization 5. Production Deployment Time: 25 minutes\nDeploy your agents to production with proper monitoring and scaling.\nYou’ll learn:\nDocker containerization Configuration management Monitoring and logging Basic scaling patterns Prerequisites Go 1.21 or later installed Basic familiarity with Go programming Docker installed (for later tutorials) Next Steps After completing these tutorials, explore:\nCore Concepts for deeper understanding How-To Guides for specific tasks Advanced Tutorials for complex scenarios",
    "description": "Getting Started Tutorials Beginner-friendly tutorials to get you up and running with AgenticGoKit quickly.\nTutorial Series Follow these tutorials in order for the best learning experience:\n0. 5-Minute Quickstart Time: 5 minutes\nGet a multi-agent system running immediately with minimal setup.\nYou’ll learn:\nFastest way to create agents Basic orchestration patterns CLI vs code-first approaches 1. Your First Agent Time: 10 minutes\nCreate your first AgenticGoKit agent and understand the basic concepts.",
    "tags": [],
    "title": "getting-started",
    "uri": "/AgenticGoKitDocs/tutorials/getting-started/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials \u003e memory-systems",
    "content": "Knowledge Bases in AgenticGoKit Overview Knowledge bases are structured repositories of information that enable agents to access, search, and reason over large collections of documents and data. This tutorial covers building comprehensive knowledge bases with AgenticGoKit, including document ingestion, chunking strategies, metadata management, and search optimization.\nKnowledge bases transform raw information into accessible, searchable knowledge that agents can use to provide accurate and contextual responses.\nPrerequisites Understanding of RAG Implementation Familiarity with Vector Databases Knowledge of document processing and text extraction Basic understanding of information retrieval concepts Knowledge Base Architecture Components Overview ┌─────────────────┐ ┌──────────────────┐ ┌─────────────────┐\r│ Documents │───▶│ Ingestion │───▶│ Processing │\r│ (PDF, MD, │ │ Pipeline │ │ Pipeline │\r│ HTML, etc.) │ └──────────────────┘ └─────────────────┘\r└─────────────────┘ │\r▼\r┌─────────────────┐ ┌──────────────────┐ ┌─────────────────┐\r│ Search \u0026 │◀───│ Vector Store │◀───│ Chunking \u0026 │\r│ Retrieval │ │ (pgvector/ │ │ Embedding │\r└─────────────────┘ │ Weaviate) │ └─────────────────┘\r└──────────────────┘\rKnowledge Base Layers Storage Layer: Vector database with metadata Processing Layer: Document parsing and chunking Embedding Layer: Vector representation generation Retrieval Layer: Search and ranking algorithms Management Layer: Updates, versioning, and maintenance Document Ingestion Pipeline 1. Basic Document Processor package main import ( \"context\" \"fmt\" \"io/ioutil\" \"log\" \"path/filepath\" \"strings\" \"time\" \"github.com/kunalkushwaha/agenticgokit/core\" ) type DocumentProcessor struct { memory core.Memory parsers map[string]DocumentParser chunker *DocumentChunker config ProcessingConfig } type ProcessingConfig struct { ChunkSize int ChunkOverlap int MaxFileSize int64 SupportedFormats []string BatchSize int } type DocumentParser interface { Parse(filePath string) (*Document, error) SupportedExtensions() []string } type Document struct { ID string Title string Content string Metadata map[string]string Source string CreatedAt time.Time UpdatedAt time.Time } func NewDocumentProcessor(memory core.Memory) *DocumentProcessor { dp := \u0026DocumentProcessor{ memory: memory, parsers: make(map[string]DocumentParser), chunker: NewDocumentChunker(ChunkingConfig{ ChunkSize: 1000, ChunkOverlap: 200, Strategy: \"semantic\", }), config: ProcessingConfig{ ChunkSize: 1000, ChunkOverlap: 200, MaxFileSize: 10 * 1024 * 1024, // 10MB SupportedFormats: []string{\".txt\", \".md\", \".pdf\", \".html\"}, BatchSize: 10, }, } // Register parsers dp.registerParsers() return dp } func (dp *DocumentProcessor) registerParsers() { dp.parsers[\".txt\"] = \u0026TextParser{} dp.parsers[\".md\"] = \u0026MarkdownParser{} dp.parsers[\".pdf\"] = \u0026PDFParser{} dp.parsers[\".html\"] = \u0026HTMLParser{} } func (dp *DocumentProcessor) ProcessFile(ctx context.Context, filePath string) error { // Check file size fileInfo, err := os.Stat(filePath) if err != nil { return fmt.Errorf(\"failed to stat file: %w\", err) } if fileInfo.Size() \u003e dp.config.MaxFileSize { return fmt.Errorf(\"file too large: %d bytes\", fileInfo.Size()) } // Get parser for file extension ext := strings.ToLower(filepath.Ext(filePath)) parser, exists := dp.parsers[ext] if !exists { return fmt.Errorf(\"unsupported file format: %s\", ext) } // Parse document doc, err := parser.Parse(filePath) if err != nil { return fmt.Errorf(\"failed to parse document: %w\", err) } // Add file metadata doc.Metadata[\"file_path\"] = filePath doc.Metadata[\"file_size\"] = fmt.Sprintf(\"%d\", fileInfo.Size()) doc.Metadata[\"processed_at\"] = time.Now().Format(time.RFC3339) // Process document return dp.ProcessDocument(ctx, doc) } func (dp *DocumentProcessor) ProcessDocument(ctx context.Context, doc *Document) error { // Chunk the document chunks, err := dp.chunker.ChunkDocument(doc) if err != nil { return fmt.Errorf(\"failed to chunk document: %w\", err) } log.Printf(\"Processing document '%s' with %d chunks\", doc.Title, len(chunks)) // Store chunks in batches for i := 0; i \u003c len(chunks); i += dp.config.BatchSize { end := i + dp.config.BatchSize if end \u003e len(chunks) { end = len(chunks) } batch := chunks[i:end] err := dp.storeBatch(ctx, batch) if err != nil { return fmt.Errorf(\"failed to store batch: %w\", err) } } log.Printf(\"Successfully processed document '%s'\", doc.Title) return nil } func (dp *DocumentProcessor) storeBatch(ctx context.Context, chunks []*DocumentChunk) error { for _, chunk := range chunks { err := dp.memory.Store(ctx, chunk.Content, \"document-chunk\", core.WithMetadata(chunk.Metadata), core.WithTimestamp(chunk.CreatedAt), ) if err != nil { return fmt.Errorf(\"failed to store chunk %s: %w\", chunk.ID, err) } } return nil } // Simple text parser type TextParser struct{} func (tp *TextParser) Parse(filePath string) (*Document, error) { content, err := ioutil.ReadFile(filePath) if err != nil { return nil, err } return \u0026Document{ ID: generateDocumentID(filePath), Title: filepath.Base(filePath), Content: string(content), Source: filePath, CreatedAt: time.Now(), UpdatedAt: time.Now(), Metadata: map[string]string{ \"format\": \"text\", \"parser\": \"text\", }, }, nil } func (tp *TextParser) SupportedExtensions() []string { return []string{\".txt\"} } // Markdown parser type MarkdownParser struct{} func (mp *MarkdownParser) Parse(filePath string) (*Document, error) { content, err := ioutil.ReadFile(filePath) if err != nil { return nil, err } // Extract title from first heading lines := strings.Split(string(content), \"\\n\") title := filepath.Base(filePath) for _, line := range lines { if strings.HasPrefix(line, \"# \") { title = strings.TrimPrefix(line, \"# \") break } } return \u0026Document{ ID: generateDocumentID(filePath), Title: title, Content: string(content), Source: filePath, CreatedAt: time.Now(), UpdatedAt: time.Now(), Metadata: map[string]string{ \"format\": \"markdown\", \"parser\": \"markdown\", }, }, nil } func (mp *MarkdownParser) SupportedExtensions() []string { return []string{\".md\", \".markdown\"} } func generateDocumentID(filePath string) string { // Generate unique ID based on file path hash := sha256.Sum256([]byte(filePath + time.Now().String())) return fmt.Sprintf(\"%x\", hash)[:16] }\rDocument Chunking Strategies 1. Document Chunker type DocumentChunker struct { config ChunkingConfig } type ChunkingConfig struct { ChunkSize int ChunkOverlap int Strategy string // \"fixed\", \"semantic\", \"sentence\", \"paragraph\" MinChunkSize int MaxChunkSize int } type DocumentChunk struct { ID string DocumentID string Content string ChunkIndex int StartOffset int EndOffset int Metadata map[string]string CreatedAt time.Time } func NewDocumentChunker(config ChunkingConfig) *DocumentChunker { return \u0026DocumentChunker{config: config} } func (dc *DocumentChunker) ChunkDocument(doc *Document) ([]*DocumentChunk, error) { switch dc.config.Strategy { case \"fixed\": return dc.fixedSizeChunking(doc) case \"semantic\": return dc.semanticChunking(doc) case \"sentence\": return dc.sentenceChunking(doc) case \"paragraph\": return dc.paragraphChunking(doc) default: return dc.fixedSizeChunking(doc) } } func (dc *DocumentChunker) fixedSizeChunking(doc *Document) ([]*DocumentChunk, error) { content := doc.Content chunks := make([]*DocumentChunk, 0) for i := 0; i \u003c len(content); i += dc.config.ChunkSize - dc.config.ChunkOverlap { end := i + dc.config.ChunkSize if end \u003e len(content) { end = len(content) } chunkContent := content[i:end] // Skip chunks that are too small if len(chunkContent) \u003c dc.config.MinChunkSize { continue } chunk := \u0026DocumentChunk{ ID: fmt.Sprintf(\"%s_chunk_%d\", doc.ID, len(chunks)), DocumentID: doc.ID, Content: chunkContent, ChunkIndex: len(chunks), StartOffset: i, EndOffset: end, CreatedAt: time.Now(), Metadata: map[string]string{ \"document_id\": doc.ID, \"document_title\": doc.Title, \"document_source\": doc.Source, \"chunk_strategy\": \"fixed\", \"chunk_index\": fmt.Sprintf(\"%d\", len(chunks)), }, } chunks = append(chunks, chunk) if end \u003e= len(content) { break } } return chunks, nil } func (dc *DocumentChunker) semanticChunking(doc *Document) ([]*DocumentChunk, error) { content := doc.Content // Split by double newlines (paragraphs) paragraphs := strings.Split(content, \"\\n\\n\") chunks := make([]*DocumentChunk, 0) currentChunk := \"\" startOffset := 0 for _, paragraph := range paragraphs { paragraph = strings.TrimSpace(paragraph) if paragraph == \"\" { continue } // Check if adding this paragraph would exceed chunk size if len(currentChunk)+len(paragraph)+2 \u003e dc.config.ChunkSize \u0026\u0026 currentChunk != \"\" { // Create chunk from current content chunk := dc.createChunk(doc, currentChunk, len(chunks), startOffset, startOffset+len(currentChunk)) chunks = append(chunks, chunk) // Start new chunk with overlap overlapSize := min(dc.config.ChunkOverlap, len(currentChunk)) currentChunk = currentChunk[len(currentChunk)-overlapSize:] + \"\\n\\n\" + paragraph startOffset = startOffset + len(currentChunk) - overlapSize } else { // Add paragraph to current chunk if currentChunk != \"\" { currentChunk += \"\\n\\n\" } currentChunk += paragraph } } // Add final chunk if there's content if currentChunk != \"\" \u0026\u0026 len(currentChunk) \u003e= dc.config.MinChunkSize { chunk := dc.createChunk(doc, currentChunk, len(chunks), startOffset, startOffset+len(currentChunk)) chunks = append(chunks, chunk) } return chunks, nil } func (dc *DocumentChunker) createChunk(doc *Document, content string, index, startOffset, endOffset int) *DocumentChunk { return \u0026DocumentChunk{ ID: fmt.Sprintf(\"%s_chunk_%d\", doc.ID, index), DocumentID: doc.ID, Content: content, ChunkIndex: index, StartOffset: startOffset, EndOffset: endOffset, CreatedAt: time.Now(), Metadata: map[string]string{ \"document_id\": doc.ID, \"document_title\": doc.Title, \"document_source\": doc.Source, \"chunk_strategy\": dc.config.Strategy, \"chunk_index\": fmt.Sprintf(\"%d\", index), }, } } func min(a, b int) int { if a \u003c b { return a } return b }\rKnowledge Base Management 1. Knowledge Base Manager type KnowledgeBaseManager struct { memory core.Memory processor *DocumentProcessor config ManagerConfig } type ManagerConfig struct { AutoIndexing bool IndexingInterval time.Duration BackupEnabled bool BackupInterval time.Duration } func NewKnowledgeBaseManager(memory core.Memory) *KnowledgeBaseManager { processor := NewDocumentProcessor(memory) return \u0026KnowledgeBaseManager{ memory: memory, processor: processor, config: ManagerConfig{ AutoIndexing: true, IndexingInterval: 1 * time.Hour, BackupEnabled: true, BackupInterval: 24 * time.Hour, }, } } func (kbm *KnowledgeBaseManager) AddDocument(ctx context.Context, filePath string) error { return kbm.processor.ProcessFile(ctx, filePath) } func (kbm *KnowledgeBaseManager) AddDocumentFromContent(ctx context.Context, title, content string, metadata map[string]string) error { doc := \u0026Document{ ID: generateDocumentID(title + content), Title: title, Content: content, Source: \"direct-input\", CreatedAt: time.Now(), UpdatedAt: time.Now(), Metadata: metadata, } return kbm.processor.ProcessDocument(ctx, doc) } func (kbm *KnowledgeBaseManager) Search(ctx context.Context, query string, options ...core.SearchOption) ([]core.MemoryResult, error) { return kbm.memory.Search(ctx, query, options...) } func (kbm *KnowledgeBaseManager) GetStats(ctx context.Context) (*KnowledgeBaseStats, error) { memoryStats, err := kbm.memory.GetStats(ctx) if err != nil { return nil, err } return \u0026KnowledgeBaseStats{ TotalDocuments: memoryStats.ItemCount, TotalChunks: memoryStats.ItemCount, IndexSize: memoryStats.SizeBytes, LastUpdated: time.Now(), }, nil } type KnowledgeBaseStats struct { TotalDocuments int64 `json:\"total_documents\"` TotalChunks int64 `json:\"total_chunks\"` IndexSize int64 `json:\"index_size_bytes\"` LastUpdated time.Time `json:\"last_updated\"` }\rUsage Example Complete Knowledge Base Example func main() { // Setup memory with vector database memory, err := core.NewMemory(core.AgentMemoryConfig{ Provider: \"pgvector\", Connection: \"postgres://user:pass@localhost:5432/agentdb\", EnableRAG: true, Dimensions: 1536, Embedding: core.EmbeddingConfig{ Provider: \"openai\", Model: \"text-embedding-3-small\", APIKey: os.Getenv(\"OPENAI_API_KEY\"), Dimensions: 1536, }, }) if err != nil { log.Fatalf(\"Failed to create memory: %v\", err) } // Create knowledge base manager kbManager := NewKnowledgeBaseManager(memory) ctx := context.Background() // Add documents to knowledge base documents := []string{ \"./docs/tutorial1.md\", \"./docs/tutorial2.md\", \"./docs/api-reference.md\", } for _, docPath := range documents { err := kbManager.AddDocument(ctx, docPath) if err != nil { log.Printf(\"Failed to add document %s: %v\", docPath, err) } else { log.Printf(\"Successfully added document: %s\", docPath) } } // Add content directly err = kbManager.AddDocumentFromContent(ctx, \"AgenticGoKit Overview\", \"AgenticGoKit is a Go framework for building multi-agent systems...\", map[string]string{ \"category\": \"overview\", \"author\": \"AgenticGoKit Team\", }, ) if err != nil { log.Printf(\"Failed to add content: %v\", err) } // Search the knowledge base results, err := kbManager.Search(ctx, \"How to build multi-agent systems?\", core.WithLimit(5), core.WithScoreThreshold(0.7), ) if err != nil { log.Printf(\"Search failed: %v\", err) } else { fmt.Printf(\"Found %d results:\\n\", len(results)) for i, result := range results { fmt.Printf(\"%d. %s (Score: %.3f)\\n\", i+1, result.Content[:100]+\"...\", result.Score) } } // Get knowledge base statistics stats, err := kbManager.GetStats(ctx) if err != nil { log.Printf(\"Failed to get stats: %v\", err) } else { fmt.Printf(\"Knowledge Base Stats:\\n\") fmt.Printf(\" Documents: %d\\n\", stats.TotalDocuments) fmt.Printf(\" Chunks: %d\\n\", stats.TotalChunks) fmt.Printf(\" Index Size: %d MB\\n\", stats.IndexSize/1024/1024) } }\rBest Practices 1. Document Processing Format Support: Implement parsers for all relevant document formats Error Handling: Gracefully handle parsing errors and corrupted files Batch Processing: Process multiple documents efficiently Progress Tracking: Provide feedback on processing progress Validation: Validate documents before processing 2. Chunking Strategy Content-Aware: Use semantic chunking for better context preservation Overlap Management: Balance overlap size with storage efficiency Size Optimization: Optimize chunk size for your embedding model Metadata Preservation: Maintain document context in chunks Quality Control: Validate chunk quality and coherence 3. Search Optimization Index Tuning: Optimize vector database indexes Query Enhancement: Improve query understanding Result Ranking: Implement effective ranking algorithms Caching: Cache frequent searches Performance Monitoring: Track search performance metrics Conclusion Knowledge bases in AgenticGoKit provide the foundation for intelligent information retrieval and RAG systems. Key takeaways:\nDesign comprehensive document processing pipelines Implement appropriate chunking strategies for your content Use rich metadata to enhance search and filtering Optimize search performance through indexing and caching Monitor and maintain knowledge base quality over time Well-designed knowledge bases enable agents to access and utilize vast amounts of information effectively, making them more knowledgeable and helpful.\nNext Steps Memory Optimization - Advanced performance tuning Production Deployment - Deploy knowledge bases at scale Monitoring and Observability - Monitor knowledge base performance Further Reading Information Retrieval Fundamentals Vector Database Comparison Document Processing Libraries",
    "description": "Knowledge Bases in AgenticGoKit Overview Knowledge bases are structured repositories of information that enable agents to access, search, and reason over large collections of documents and data. This tutorial covers building comprehensive knowledge bases with AgenticGoKit, including document ingestion, chunking strategies, metadata management, and search optimization.\nKnowledge bases transform raw information into accessible, searchable knowledge that agents can use to provide accurate and contextual responses.\nPrerequisites Understanding of RAG Implementation Familiarity with Vector Databases Knowledge of document processing and text extraction Basic understanding of information retrieval concepts Knowledge Base Architecture Components Overview ┌─────────────────┐ ┌──────────────────┐ ┌─────────────────┐\r│ Documents │───▶│ Ingestion │───▶│ Processing │\r│ (PDF, MD, │ │ Pipeline │ │ Pipeline │\r│ HTML, etc.) │ └──────────────────┘ └─────────────────┘\r└─────────────────┘ │\r▼\r┌─────────────────┐ ┌──────────────────┐ ┌─────────────────┐\r│ Search \u0026 │◀───│ Vector Store │◀───│ Chunking \u0026 │\r│ Retrieval │ │ (pgvector/ │ │ Embedding │\r└─────────────────┘ │ Weaviate) │ └─────────────────┘\r└──────────────────┘\rKnowledge Base Layers Storage Layer: Vector database with metadata Processing Layer: Document parsing and chunking Embedding Layer: Vector representation generation Retrieval Layer: Search and ranking algorithms Management Layer: Updates, versioning, and maintenance Document Ingestion Pipeline 1. Basic Document Processor package main import ( \"context\" \"fmt\" \"io/ioutil\" \"log\" \"path/filepath\" \"strings\" \"time\" \"github.com/kunalkushwaha/agenticgokit/core\" ) type DocumentProcessor struct { memory core.Memory parsers map[string]DocumentParser chunker *DocumentChunker config ProcessingConfig } type ProcessingConfig struct { ChunkSize int ChunkOverlap int MaxFileSize int64 SupportedFormats []string BatchSize int } type DocumentParser interface { Parse(filePath string) (*Document, error) SupportedExtensions() []string } type Document struct { ID string Title string Content string Metadata map[string]string Source string CreatedAt time.Time UpdatedAt time.Time } func NewDocumentProcessor(memory core.Memory) *DocumentProcessor { dp := \u0026DocumentProcessor{ memory: memory, parsers: make(map[string]DocumentParser), chunker: NewDocumentChunker(ChunkingConfig{ ChunkSize: 1000, ChunkOverlap: 200, Strategy: \"semantic\", }), config: ProcessingConfig{ ChunkSize: 1000, ChunkOverlap: 200, MaxFileSize: 10 * 1024 * 1024, // 10MB SupportedFormats: []string{\".txt\", \".md\", \".pdf\", \".html\"}, BatchSize: 10, }, } // Register parsers dp.registerParsers() return dp } func (dp *DocumentProcessor) registerParsers() { dp.parsers[\".txt\"] = \u0026TextParser{} dp.parsers[\".md\"] = \u0026MarkdownParser{} dp.parsers[\".pdf\"] = \u0026PDFParser{} dp.parsers[\".html\"] = \u0026HTMLParser{} } func (dp *DocumentProcessor) ProcessFile(ctx context.Context, filePath string) error { // Check file size fileInfo, err := os.Stat(filePath) if err != nil { return fmt.Errorf(\"failed to stat file: %w\", err) } if fileInfo.Size() \u003e dp.config.MaxFileSize { return fmt.Errorf(\"file too large: %d bytes\", fileInfo.Size()) } // Get parser for file extension ext := strings.ToLower(filepath.Ext(filePath)) parser, exists := dp.parsers[ext] if !exists { return fmt.Errorf(\"unsupported file format: %s\", ext) } // Parse document doc, err := parser.Parse(filePath) if err != nil { return fmt.Errorf(\"failed to parse document: %w\", err) } // Add file metadata doc.Metadata[\"file_path\"] = filePath doc.Metadata[\"file_size\"] = fmt.Sprintf(\"%d\", fileInfo.Size()) doc.Metadata[\"processed_at\"] = time.Now().Format(time.RFC3339) // Process document return dp.ProcessDocument(ctx, doc) } func (dp *DocumentProcessor) ProcessDocument(ctx context.Context, doc *Document) error { // Chunk the document chunks, err := dp.chunker.ChunkDocument(doc) if err != nil { return fmt.Errorf(\"failed to chunk document: %w\", err) } log.Printf(\"Processing document '%s' with %d chunks\", doc.Title, len(chunks)) // Store chunks in batches for i := 0; i \u003c len(chunks); i += dp.config.BatchSize { end := i + dp.config.BatchSize if end \u003e len(chunks) { end = len(chunks) } batch := chunks[i:end] err := dp.storeBatch(ctx, batch) if err != nil { return fmt.Errorf(\"failed to store batch: %w\", err) } } log.Printf(\"Successfully processed document '%s'\", doc.Title) return nil } func (dp *DocumentProcessor) storeBatch(ctx context.Context, chunks []*DocumentChunk) error { for _, chunk := range chunks { err := dp.memory.Store(ctx, chunk.Content, \"document-chunk\", core.WithMetadata(chunk.Metadata), core.WithTimestamp(chunk.CreatedAt), ) if err != nil { return fmt.Errorf(\"failed to store chunk %s: %w\", chunk.ID, err) } } return nil } // Simple text parser type TextParser struct{} func (tp *TextParser) Parse(filePath string) (*Document, error) { content, err := ioutil.ReadFile(filePath) if err != nil { return nil, err } return \u0026Document{ ID: generateDocumentID(filePath), Title: filepath.Base(filePath), Content: string(content), Source: filePath, CreatedAt: time.Now(), UpdatedAt: time.Now(), Metadata: map[string]string{ \"format\": \"text\", \"parser\": \"text\", }, }, nil } func (tp *TextParser) SupportedExtensions() []string { return []string{\".txt\"} } // Markdown parser type MarkdownParser struct{} func (mp *MarkdownParser) Parse(filePath string) (*Document, error) { content, err := ioutil.ReadFile(filePath) if err != nil { return nil, err } // Extract title from first heading lines := strings.Split(string(content), \"\\n\") title := filepath.Base(filePath) for _, line := range lines { if strings.HasPrefix(line, \"# \") { title = strings.TrimPrefix(line, \"# \") break } } return \u0026Document{ ID: generateDocumentID(filePath), Title: title, Content: string(content), Source: filePath, CreatedAt: time.Now(), UpdatedAt: time.Now(), Metadata: map[string]string{ \"format\": \"markdown\", \"parser\": \"markdown\", }, }, nil } func (mp *MarkdownParser) SupportedExtensions() []string { return []string{\".md\", \".markdown\"} } func generateDocumentID(filePath string) string { // Generate unique ID based on file path hash := sha256.Sum256([]byte(filePath + time.Now().String())) return fmt.Sprintf(\"%x\", hash)[:16] }\rDocument Chunking Strategies 1. Document Chunker type DocumentChunker struct { config ChunkingConfig } type ChunkingConfig struct { ChunkSize int ChunkOverlap int Strategy string // \"fixed\", \"semantic\", \"sentence\", \"paragraph\" MinChunkSize int MaxChunkSize int } type DocumentChunk struct { ID string DocumentID string Content string ChunkIndex int StartOffset int EndOffset int Metadata map[string]string CreatedAt time.Time } func NewDocumentChunker(config ChunkingConfig) *DocumentChunker { return \u0026DocumentChunker{config: config} } func (dc *DocumentChunker) ChunkDocument(doc *Document) ([]*DocumentChunk, error) { switch dc.config.Strategy { case \"fixed\": return dc.fixedSizeChunking(doc) case \"semantic\": return dc.semanticChunking(doc) case \"sentence\": return dc.sentenceChunking(doc) case \"paragraph\": return dc.paragraphChunking(doc) default: return dc.fixedSizeChunking(doc) } } func (dc *DocumentChunker) fixedSizeChunking(doc *Document) ([]*DocumentChunk, error) { content := doc.Content chunks := make([]*DocumentChunk, 0) for i := 0; i \u003c len(content); i += dc.config.ChunkSize - dc.config.ChunkOverlap { end := i + dc.config.ChunkSize if end \u003e len(content) { end = len(content) } chunkContent := content[i:end] // Skip chunks that are too small if len(chunkContent) \u003c dc.config.MinChunkSize { continue } chunk := \u0026DocumentChunk{ ID: fmt.Sprintf(\"%s_chunk_%d\", doc.ID, len(chunks)), DocumentID: doc.ID, Content: chunkContent, ChunkIndex: len(chunks), StartOffset: i, EndOffset: end, CreatedAt: time.Now(), Metadata: map[string]string{ \"document_id\": doc.ID, \"document_title\": doc.Title, \"document_source\": doc.Source, \"chunk_strategy\": \"fixed\", \"chunk_index\": fmt.Sprintf(\"%d\", len(chunks)), }, } chunks = append(chunks, chunk) if end \u003e= len(content) { break } } return chunks, nil } func (dc *DocumentChunker) semanticChunking(doc *Document) ([]*DocumentChunk, error) { content := doc.Content // Split by double newlines (paragraphs) paragraphs := strings.Split(content, \"\\n\\n\") chunks := make([]*DocumentChunk, 0) currentChunk := \"\" startOffset := 0 for _, paragraph := range paragraphs { paragraph = strings.TrimSpace(paragraph) if paragraph == \"\" { continue } // Check if adding this paragraph would exceed chunk size if len(currentChunk)+len(paragraph)+2 \u003e dc.config.ChunkSize \u0026\u0026 currentChunk != \"\" { // Create chunk from current content chunk := dc.createChunk(doc, currentChunk, len(chunks), startOffset, startOffset+len(currentChunk)) chunks = append(chunks, chunk) // Start new chunk with overlap overlapSize := min(dc.config.ChunkOverlap, len(currentChunk)) currentChunk = currentChunk[len(currentChunk)-overlapSize:] + \"\\n\\n\" + paragraph startOffset = startOffset + len(currentChunk) - overlapSize } else { // Add paragraph to current chunk if currentChunk != \"\" { currentChunk += \"\\n\\n\" } currentChunk += paragraph } } // Add final chunk if there's content if currentChunk != \"\" \u0026\u0026 len(currentChunk) \u003e= dc.config.MinChunkSize { chunk := dc.createChunk(doc, currentChunk, len(chunks), startOffset, startOffset+len(currentChunk)) chunks = append(chunks, chunk) } return chunks, nil } func (dc *DocumentChunker) createChunk(doc *Document, content string, index, startOffset, endOffset int) *DocumentChunk { return \u0026DocumentChunk{ ID: fmt.Sprintf(\"%s_chunk_%d\", doc.ID, index), DocumentID: doc.ID, Content: content, ChunkIndex: index, StartOffset: startOffset, EndOffset: endOffset, CreatedAt: time.Now(), Metadata: map[string]string{ \"document_id\": doc.ID, \"document_title\": doc.Title, \"document_source\": doc.Source, \"chunk_strategy\": dc.config.Strategy, \"chunk_index\": fmt.Sprintf(\"%d\", index), }, } } func min(a, b int) int { if a \u003c b { return a } return b }\rKnowledge Base Management 1. Knowledge Base Manager type KnowledgeBaseManager struct { memory core.Memory processor *DocumentProcessor config ManagerConfig } type ManagerConfig struct { AutoIndexing bool IndexingInterval time.Duration BackupEnabled bool BackupInterval time.Duration } func NewKnowledgeBaseManager(memory core.Memory) *KnowledgeBaseManager { processor := NewDocumentProcessor(memory) return \u0026KnowledgeBaseManager{ memory: memory, processor: processor, config: ManagerConfig{ AutoIndexing: true, IndexingInterval: 1 * time.Hour, BackupEnabled: true, BackupInterval: 24 * time.Hour, }, } } func (kbm *KnowledgeBaseManager) AddDocument(ctx context.Context, filePath string) error { return kbm.processor.ProcessFile(ctx, filePath) } func (kbm *KnowledgeBaseManager) AddDocumentFromContent(ctx context.Context, title, content string, metadata map[string]string) error { doc := \u0026Document{ ID: generateDocumentID(title + content), Title: title, Content: content, Source: \"direct-input\", CreatedAt: time.Now(), UpdatedAt: time.Now(), Metadata: metadata, } return kbm.processor.ProcessDocument(ctx, doc) } func (kbm *KnowledgeBaseManager) Search(ctx context.Context, query string, options ...core.SearchOption) ([]core.MemoryResult, error) { return kbm.memory.Search(ctx, query, options...) } func (kbm *KnowledgeBaseManager) GetStats(ctx context.Context) (*KnowledgeBaseStats, error) { memoryStats, err := kbm.memory.GetStats(ctx) if err != nil { return nil, err } return \u0026KnowledgeBaseStats{ TotalDocuments: memoryStats.ItemCount, TotalChunks: memoryStats.ItemCount, IndexSize: memoryStats.SizeBytes, LastUpdated: time.Now(), }, nil } type KnowledgeBaseStats struct { TotalDocuments int64 `json:\"total_documents\"` TotalChunks int64 `json:\"total_chunks\"` IndexSize int64 `json:\"index_size_bytes\"` LastUpdated time.Time `json:\"last_updated\"` }\rUsage Example Complete Knowledge Base Example func main() { // Setup memory with vector database memory, err := core.NewMemory(core.AgentMemoryConfig{ Provider: \"pgvector\", Connection: \"postgres://user:pass@localhost:5432/agentdb\", EnableRAG: true, Dimensions: 1536, Embedding: core.EmbeddingConfig{ Provider: \"openai\", Model: \"text-embedding-3-small\", APIKey: os.Getenv(\"OPENAI_API_KEY\"), Dimensions: 1536, }, }) if err != nil { log.Fatalf(\"Failed to create memory: %v\", err) } // Create knowledge base manager kbManager := NewKnowledgeBaseManager(memory) ctx := context.Background() // Add documents to knowledge base documents := []string{ \"./docs/tutorial1.md\", \"./docs/tutorial2.md\", \"./docs/api-reference.md\", } for _, docPath := range documents { err := kbManager.AddDocument(ctx, docPath) if err != nil { log.Printf(\"Failed to add document %s: %v\", docPath, err) } else { log.Printf(\"Successfully added document: %s\", docPath) } } // Add content directly err = kbManager.AddDocumentFromContent(ctx, \"AgenticGoKit Overview\", \"AgenticGoKit is a Go framework for building multi-agent systems...\", map[string]string{ \"category\": \"overview\", \"author\": \"AgenticGoKit Team\", }, ) if err != nil { log.Printf(\"Failed to add content: %v\", err) } // Search the knowledge base results, err := kbManager.Search(ctx, \"How to build multi-agent systems?\", core.WithLimit(5), core.WithScoreThreshold(0.7), ) if err != nil { log.Printf(\"Search failed: %v\", err) } else { fmt.Printf(\"Found %d results:\\n\", len(results)) for i, result := range results { fmt.Printf(\"%d. %s (Score: %.3f)\\n\", i+1, result.Content[:100]+\"...\", result.Score) } } // Get knowledge base statistics stats, err := kbManager.GetStats(ctx) if err != nil { log.Printf(\"Failed to get stats: %v\", err) } else { fmt.Printf(\"Knowledge Base Stats:\\n\") fmt.Printf(\" Documents: %d\\n\", stats.TotalDocuments) fmt.Printf(\" Chunks: %d\\n\", stats.TotalChunks) fmt.Printf(\" Index Size: %d MB\\n\", stats.IndexSize/1024/1024) } }\rBest Practices 1. Document Processing Format Support: Implement parsers for all relevant document formats Error Handling: Gracefully handle parsing errors and corrupted files Batch Processing: Process multiple documents efficiently Progress Tracking: Provide feedback on processing progress Validation: Validate documents before processing 2. Chunking Strategy Content-Aware: Use semantic chunking for better context preservation Overlap Management: Balance overlap size with storage efficiency Size Optimization: Optimize chunk size for your embedding model Metadata Preservation: Maintain document context in chunks Quality Control: Validate chunk quality and coherence 3. Search Optimization Index Tuning: Optimize vector database indexes Query Enhancement: Improve query understanding Result Ranking: Implement effective ranking algorithms Caching: Cache frequent searches Performance Monitoring: Track search performance metrics Conclusion Knowledge bases in AgenticGoKit provide the foundation for intelligent information retrieval and RAG systems. Key takeaways:",
    "tags": [],
    "title": "knowledge-bases",
    "uri": "/AgenticGoKitDocs/tutorials/memory-systems/knowledge-bases/index.html"
  },
  {
    "breadcrumb": "content \u003e guides \u003e setup",
    "content": "LLM Providers Multi-Provider LLM Integration in AgenticGoKit\nAgenticGoKit provides a unified interface for working with different LLM providers. This guide covers configuration, usage patterns, and provider-specific features.\nProvider Overview AgenticGoKit supports multiple LLM providers through a unified ModelProvider interface:\nAzure OpenAI (Default) - Enterprise-ready with robust scaling OpenAI - Direct API access to GPT models Ollama - Local models for privacy and cost control Mock - Testing and development ModelProvider Interface All providers implement the same interface:\ntype ModelProvider interface { Generate(ctx context.Context, prompt string) (string, error) GenerateWithHistory(ctx context.Context, messages []Message) (string, error) Name() string } type Message struct { Role string // \"system\", \"user\", \"assistant\" Content string }\rConfiguration Azure OpenAI (Default) agentflow.toml:\n[provider] type = \"azure\" api_key = \"${AZURE_OPENAI_API_KEY}\" endpoint = \"https://your-resource.openai.azure.com\" deployment = \"gpt-4\" api_version = \"2024-02-15-preview\" model = \"gpt-4\" max_tokens = 2000 temperature = 0.7\rEnvironment variables:\nexport AZURE_OPENAI_API_KEY=\"your-azure-api-key\" export AZURE_OPENAI_ENDPOINT=\"https://your-resource.openai.azure.com\" export AZURE_OPENAI_DEPLOYMENT=\"gpt-4\"\rOpenAI agentflow.toml:\n[provider] type = \"openai\" api_key = \"${OPENAI_API_KEY}\" model = \"gpt-4\" max_tokens = 2000 temperature = 0.7 organization = \"your-org-id\" # Optional\rOllama (Local Models) agentflow.toml:\n[provider] type = \"ollama\" host = \"http://localhost:11434\" model = \"llama3.2:3b\" temperature = 0.7 context_window = 4096\rSetup Ollama:\n# Install Ollama curl -fsSL https://ollama.ai/install.sh | sh # Pull a model ollama pull llama3.2:3b # Start Ollama server (usually automatic) ollama serve\rUsage Patterns Basic Usage package main import ( \"context\" \"fmt\" agentflow \"github.com/kunalkushwaha/agenticgokit/core\" ) func main() { // Load provider from configuration provider, err := agentflow.NewProviderFromWorkingDir() if err != nil { panic(err) } ctx := context.Background() // Simple generation response, err := provider.Generate(ctx, \"Explain Go interfaces in simple terms\") if err != nil { panic(err) } fmt.Println(\"Response:\", response) fmt.Println(\"Provider:\", provider.Name()) }\rProvider Selection Strategies func createProvider() (agentflow.ModelProvider, error) { providerType := os.Getenv(\"LLM_PROVIDER\") switch providerType { case \"azure\": return agentflow.NewAzureProvider(agentflow.AzureConfig{ APIKey: os.Getenv(\"AZURE_OPENAI_API_KEY\"), Endpoint: os.Getenv(\"AZURE_OPENAI_ENDPOINT\"), Deployment: os.Getenv(\"AZURE_OPENAI_DEPLOYMENT\"), }) case \"openai\": return agentflow.NewOpenAIProvider(agentflow.OpenAIConfig{ APIKey: os.Getenv(\"OPENAI_API_KEY\"), Model: \"gpt-4\", }) case \"ollama\": return agentflow.NewOllamaProvider(agentflow.OllamaConfig{ Host: \"http://localhost:11434\", Model: \"llama3.2:3b\", }) default: return agentflow.NewMockProvider(agentflow.MockConfig{ Response: \"Mock response for testing\", }), nil } }\rPerformance Considerations Provider Performance Characteristics Provider Latency Throughput Cost Privacy Azure OpenAI Medium High Medium Enterprise OpenAI Medium High Medium Cloud Ollama Low Medium Free Full Mock Minimal Very High Free Full Production Deployment Environment Configuration # Production environment variables export LLM_PROVIDER=\"azure\" export AZURE_OPENAI_API_KEY=\"prod-api-key\" export AZURE_OPENAI_ENDPOINT=\"https://prod-resource.openai.azure.com\" export AZURE_OPENAI_DEPLOYMENT=\"gpt-4\" # Fallback provider export FALLBACK_PROVIDER=\"openai\" export OPENAI_API_KEY=\"fallback-api-key\" # Local development export LLM_PROVIDER=\"ollama\" export OLLAMA_HOST=\"http://localhost:11434\" export OLLAMA_MODEL=\"llama3.2:3b\"\rNext Steps Vector Databases - Set up persistent storage for RAG MCP Tools - Add external tool capabilities Configuration - Advanced configuration options",
    "description": "LLM Providers Multi-Provider LLM Integration in AgenticGoKit\nAgenticGoKit provides a unified interface for working with different LLM providers. This guide covers configuration, usage patterns, and provider-specific features.\nProvider Overview AgenticGoKit supports multiple LLM providers through a unified ModelProvider interface:\nAzure OpenAI (Default) - Enterprise-ready with robust scaling OpenAI - Direct API access to GPT models Ollama - Local models for privacy and cost control Mock - Testing and development ModelProvider Interface All providers implement the same interface:",
    "tags": [],
    "title": "llm-providers",
    "uri": "/AgenticGoKitDocs/guides/setup/llm-providers/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials \u003e advanced",
    "content": "Load Balancing and Scaling Patterns Building horizontally scalable agent systems with intelligent load distribution\nAs your agent systems grow, you need strategies to distribute load effectively and scale horizontally. This guide covers load balancing patterns, horizontal scaling strategies, and performance optimization techniques for AgenticGoKit applications.\n🏗️ Scaling Architecture Overview flowchart TB\rsubgraph \"Load Balancer Layer\"\rLB[Load Balancer]\rHEALTH[Health Checker]\rend\rsubgraph \"Agent Instances\"\rAGENT1[Agent Instance 1]\rAGENT2[Agent Instance 2]\rAGENT3[Agent Instance 3]\rAGENTN[Agent Instance N]\rend\rsubgraph \"Shared Resources\"\rREDIS[Redis Cache]\rPOSTGRES[PostgreSQL]\rVECTOR[Vector DB]\rMCP[MCP Servers]\rend\rsubgraph \"Monitoring\"\rMETRICS[Metrics Collector]\rALERTS[Alert Manager]\rend\rCLIENT[Client Requests] --\u003e LB\rLB --\u003e AGENT1\rLB --\u003e AGENT2\rLB --\u003e AGENT3\rLB --\u003e AGENTN\rHEALTH --\u003e AGENT1\rHEALTH --\u003e AGENT2\rHEALTH --\u003e AGENT3\rHEALTH --\u003e AGENTN\rAGENT1 --\u003e REDIS\rAGENT2 --\u003e REDIS\rAGENT3 --\u003e REDIS\rAGENTN --\u003e REDIS\rAGENT1 --\u003e POSTGRES\rAGENT2 --\u003e POSTGRES\rAGENT3 --\u003e POSTGRES\rAGENTN --\u003e POSTGRES\rAGENT1 --\u003e VECTOR\rAGENT2 --\u003e VECTOR\rAGENT3 --\u003e VECTOR\rAGENTN --\u003e VECTOR\rAGENT1 --\u003e MCP\rAGENT2 --\u003e MCP\rAGENT3 --\u003e MCP\rAGENTN --\u003e MCP\rAGENT1 --\u003e METRICS\rAGENT2 --\u003e METRICS\rAGENT3 --\u003e METRICS\rAGENTN --\u003e METRICS\rMETRICS --\u003e ALERTS\rclassDef lb fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\rclassDef agent fill:#e8f5e8,stroke:#388e3c,stroke-width:2px\rclassDef shared fill:#fff3e0,stroke:#f57c00,stroke-width:2px\rclassDef monitor fill:#fce4ec,stroke:#c2185b,stroke-width:2px\rclass LB,HEALTH lb\rclass AGENT1,AGENT2,AGENT3,AGENTN agent\rclass REDIS,POSTGRES,VECTOR,MCP shared\rclass METRICS,ALERTS monitor\r⚖️ Load Balancing Strategies 1. Round Robin Load Balancing package balancer import ( \"context\" \"sync\" \"sync/atomic\" \"github.com/kunalkushwaha/agenticgokit/core\" ) type RoundRobinBalancer struct { agents []core.AgentHandler counter int64 mu sync.RWMutex } func NewRoundRobinBalancer(agents []core.AgentHandler) *RoundRobinBalancer { return \u0026RoundRobinBalancer{ agents: agents, } } func (rb *RoundRobinBalancer) Execute(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { if len(rb.agents) == 0 { return nil, errors.New(\"no agents available\") } // Get next agent in round-robin fashion index := atomic.AddInt64(\u0026rb.counter, 1) % int64(len(rb.agents)) agent := rb.agents[index] return agent.Execute(ctx, event, state) } func (rb *RoundRobinBalancer) AddAgent(agent core.AgentHandler) { rb.mu.Lock() defer rb.mu.Unlock() rb.agents = append(rb.agents, agent) } func (rb *RoundRobinBalancer) RemoveAgent(index int) { rb.mu.Lock() defer rb.mu.Unlock() if index \u003e= 0 \u0026\u0026 index \u003c len(rb.agents) { rb.agents = append(rb.agents[:index], rb.agents[index+1:]...) } }\r2. Weighted Load Balancing type WeightedBalancer struct { agents []WeightedAgent totalWeight int mu sync.RWMutex } type WeightedAgent struct { Agent core.AgentHandler Weight int CurrentWeight int } func NewWeightedBalancer() *WeightedBalancer { return \u0026WeightedBalancer{ agents: make([]WeightedAgent, 0), } } func (wb *WeightedBalancer) AddAgent(agent core.AgentHandler, weight int) { wb.mu.Lock() defer wb.mu.Unlock() wb.agents = append(wb.agents, WeightedAgent{ Agent: agent, Weight: weight, CurrentWeight: 0, }) wb.totalWeight += weight } func (wb *WeightedBalancer) Execute(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { wb.mu.Lock() defer wb.mu.Unlock() if len(wb.agents) == 0 { return nil, errors.New(\"no agents available\") } // Weighted round-robin algorithm selected := wb.selectAgent() if selected == nil { return nil, errors.New(\"no agent selected\") } return selected.Execute(ctx, event, state) } func (wb *WeightedBalancer) selectAgent() core.AgentHandler { var selected *WeightedAgent for i := range wb.agents { agent := \u0026wb.agents[i] agent.CurrentWeight += agent.Weight if selected == nil || agent.CurrentWeight \u003e selected.CurrentWeight { selected = agent } } if selected != nil { selected.CurrentWeight -= wb.totalWeight return selected.Agent } return nil }\r3. Least Connections Load Balancing type LeastConnectionsBalancer struct { agents []ConnectionTrackingAgent mu sync.RWMutex } type ConnectionTrackingAgent struct { Agent core.AgentHandler Connections int64 mu sync.RWMutex } func NewLeastConnectionsBalancer(agents []core.AgentHandler) *LeastConnectionsBalancer { trackingAgents := make([]ConnectionTrackingAgent, len(agents)) for i, agent := range agents { trackingAgents[i] = ConnectionTrackingAgent{ Agent: agent, } } return \u0026LeastConnectionsBalancer{ agents: trackingAgents, } } func (lcb *LeastConnectionsBalancer) Execute(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { agent := lcb.selectLeastConnectedAgent() if agent == nil { return nil, errors.New(\"no agents available\") } // Increment connection count atomic.AddInt64(\u0026agent.Connections, 1) defer atomic.AddInt64(\u0026agent.Connections, -1) return agent.Agent.Execute(ctx, event, state) } func (lcb *LeastConnectionsBalancer) selectLeastConnectedAgent() *ConnectionTrackingAgent { lcb.mu.RLock() defer lcb.mu.RUnlock() if len(lcb.agents) == 0 { return nil } var selected *ConnectionTrackingAgent minConnections := int64(^uint64(0) \u003e\u003e 1) // Max int64 for i := range lcb.agents { agent := \u0026lcb.agents[i] connections := atomic.LoadInt64(\u0026agent.Connections) if connections \u003c minConnections { minConnections = connections selected = agent } } return selected }\r4. Performance-Based Load Balancing type PerformanceBasedBalancer struct { agents []PerformanceTrackingAgent mu sync.RWMutex } type PerformanceTrackingAgent struct { Agent core.AgentHandler ResponseTimes []time.Duration SuccessRate float64 LastUpdate time.Time mu sync.RWMutex } func NewPerformanceBasedBalancer(agents []core.AgentHandler) *PerformanceBasedBalancer { trackingAgents := make([]PerformanceTrackingAgent, len(agents)) for i, agent := range agents { trackingAgents[i] = PerformanceTrackingAgent{ Agent: agent, ResponseTimes: make([]time.Duration, 0, 100), SuccessRate: 1.0, LastUpdate: time.Now(), } } return \u0026PerformanceBasedBalancer{ agents: trackingAgents, } } func (pbb *PerformanceBasedBalancer) Execute(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { agent := pbb.selectBestPerformingAgent() if agent == nil { return nil, errors.New(\"no agents available\") } start := time.Now() result, err := agent.Agent.Execute(ctx, event, state) duration := time.Since(start) // Record performance metrics pbb.recordPerformance(agent, duration, err == nil) return result, err } func (pbb *PerformanceBasedBalancer) selectBestPerformingAgent() *PerformanceTrackingAgent { pbb.mu.RLock() defer pbb.mu.RUnlock() if len(pbb.agents) == 0 { return nil } var selected *PerformanceTrackingAgent bestScore := -1.0 for i := range pbb.agents { agent := \u0026pbb.agents[i] score := pbb.calculatePerformanceScore(agent) if score \u003e bestScore { bestScore = score selected = agent } } return selected } func (pbb *PerformanceBasedBalancer) calculatePerformanceScore(agent *PerformanceTrackingAgent) float64 { agent.mu.RLock() defer agent.mu.RUnlock() // Calculate average response time avgResponseTime := time.Duration(0) if len(agent.ResponseTimes) \u003e 0 { total := time.Duration(0) for _, rt := range agent.ResponseTimes { total += rt } avgResponseTime = total / time.Duration(len(agent.ResponseTimes)) } // Normalize response time (lower is better) responseTimeScore := 1.0 / (1.0 + avgResponseTime.Seconds()) // Combine success rate and response time return (agent.SuccessRate * 0.7) + (responseTimeScore * 0.3) } func (pbb *PerformanceBasedBalancer) recordPerformance(agent *PerformanceTrackingAgent, duration time.Duration, success bool) { agent.mu.Lock() defer agent.mu.Unlock() // Record response time agent.ResponseTimes = append(agent.ResponseTimes, duration) if len(agent.ResponseTimes) \u003e 100 { agent.ResponseTimes = agent.ResponseTimes[1:] } // Update success rate (exponential moving average) alpha := 0.1 if success { agent.SuccessRate = agent.SuccessRate*(1-alpha) + alpha } else { agent.SuccessRate = agent.SuccessRate * (1 - alpha) } agent.LastUpdate = time.Now() }\r🔄 Horizontal Scaling Patterns 1. Stateless Agent Design // Stateless agent that can be scaled horizontally type StatelessAgent struct { name string llmProvider core.ModelProvider cache CacheProvider // Shared cache memory MemoryProvider // Shared memory } func NewStatelessAgent(name string, provider core.ModelProvider, cache CacheProvider, memory MemoryProvider) *StatelessAgent { return \u0026StatelessAgent{ name: name, llmProvider: provider, cache: cache, memory: memory, } } func (sa *StatelessAgent) Execute(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { // All state is passed in or retrieved from shared resources // No local state that would prevent horizontal scaling // Get context from shared memory context := sa.memory.GetContext(state.SessionID) // Check shared cache cacheKey := sa.generateCacheKey(event) if cached := sa.cache.Get(cacheKey); cached != nil { return cached.(*core.AgentResult), nil } // Process request result, err := sa.processRequest(ctx, event, context) if err != nil { return nil, err } // Store in shared cache sa.cache.Set(cacheKey, result, 5*time.Minute) return result, nil } func (sa *StatelessAgent) generateCacheKey(event core.Event) string { return fmt.Sprintf(\"%s:%s:%x\", sa.name, event.Type, sha256.Sum256([]byte(fmt.Sprintf(\"%v\", event.Data)))) }\r2. Auto-Scaling Controller type AutoScaler struct { minInstances int maxInstances int currentInstances int targetCPU float64 targetMemory float64 scaleUpCooldown time.Duration scaleDownCooldown time.Duration lastScaleAction time.Time metrics MetricsProvider orchestrator Orchestrator mu sync.RWMutex } func NewAutoScaler(min, max int, orchestrator Orchestrator) *AutoScaler { return \u0026AutoScaler{ minInstances: min, maxInstances: max, currentInstances: min, targetCPU: 70.0, // 70% CPU utilization targetMemory: 80.0, // 80% memory utilization scaleUpCooldown: 2 * time.Minute, scaleDownCooldown: 5 * time.Minute, orchestrator: orchestrator, } } func (as *AutoScaler) Start(ctx context.Context) { ticker := time.NewTicker(30 * time.Second) defer ticker.Stop() for { select { case \u003c-ticker.C: as.evaluateScaling() case \u003c-ctx.Done(): return } } } func (as *AutoScaler) evaluateScaling() { as.mu.Lock() defer as.mu.Unlock() // Get current metrics cpuUsage := as.metrics.GetCPUUsage() memoryUsage := as.metrics.GetMemoryUsage() requestRate := as.metrics.GetRequestRate() // Determine if scaling is needed shouldScaleUp := (cpuUsage \u003e as.targetCPU || memoryUsage \u003e as.targetMemory) \u0026\u0026 as.currentInstances \u003c as.maxInstances \u0026\u0026 time.Since(as.lastScaleAction) \u003e as.scaleUpCooldown shouldScaleDown := cpuUsage \u003c as.targetCPU*0.5 \u0026\u0026 memoryUsage \u003c as.targetMemory*0.5 \u0026\u0026 as.currentInstances \u003e as.minInstances \u0026\u0026 time.Since(as.lastScaleAction) \u003e as.scaleDownCooldown if shouldScaleUp { as.scaleUp() } else if shouldScaleDown { as.scaleDown() } } func (as *AutoScaler) scaleUp() { newInstances := min(as.currentInstances+1, as.maxInstances) if newInstances \u003e as.currentInstances { err := as.orchestrator.AddInstance() if err == nil { as.currentInstances = newInstances as.lastScaleAction = time.Now() log.Printf(\"Scaled up to %d instances\", as.currentInstances) } } } func (as *AutoScaler) scaleDown() { newInstances := max(as.currentInstances-1, as.minInstances) if newInstances \u003c as.currentInstances { err := as.orchestrator.RemoveInstance() if err == nil { as.currentInstances = newInstances as.lastScaleAction = time.Now() log.Printf(\"Scaled down to %d instances\", as.currentInstances) } } }\r3. Health-Aware Load Balancing type HealthAwareBalancer struct { agents []HealthTrackingAgent healthCheck time.Duration mu sync.RWMutex } type HealthTrackingAgent struct { Agent core.AgentHandler Healthy bool LastCheck time.Time Endpoint string } func NewHealthAwareBalancer(agents []core.AgentHandler) *HealthAwareBalancer { trackingAgents := make([]HealthTrackingAgent, len(agents)) for i, agent := range agents { trackingAgents[i] = HealthTrackingAgent{ Agent: agent, Healthy: true, } } hab := \u0026HealthAwareBalancer{ agents: trackingAgents, healthCheck: 30 * time.Second, } // Start health checking go hab.startHealthChecking() return hab } func (hab *HealthAwareBalancer) Execute(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { healthyAgents := hab.getHealthyAgents() if len(healthyAgents) == 0 { return nil, errors.New(\"no healthy agents available\") } // Use round-robin among healthy agents index := rand.Intn(len(healthyAgents)) return healthyAgents[index].Execute(ctx, event, state) } func (hab *HealthAwareBalancer) getHealthyAgents() []core.AgentHandler { hab.mu.RLock() defer hab.mu.RUnlock() var healthy []core.AgentHandler for _, agent := range hab.agents { if agent.Healthy { healthy = append(healthy, agent.Agent) } } return healthy } func (hab *HealthAwareBalancer) startHealthChecking() { ticker := time.NewTicker(hab.healthCheck) defer ticker.Stop() for range ticker.C { hab.checkHealth() } } func (hab *HealthAwareBalancer) checkHealth() { hab.mu.Lock() defer hab.mu.Unlock() for i := range hab.agents { agent := \u0026hab.agents[i] // Perform health check healthy := hab.performHealthCheck(agent) if agent.Healthy != healthy { agent.Healthy = healthy if healthy { log.Printf(\"Agent %d is now healthy\", i) } else { log.Printf(\"Agent %d is now unhealthy\", i) } } agent.LastCheck = time.Now() } } func (hab *HealthAwareBalancer) performHealthCheck(agent *HealthTrackingAgent) bool { // Simple health check - try to execute a test request ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second) defer cancel() testEvent := core.NewEvent(\"health_check\", \"ping\") testState := \u0026core.State{} _, err := agent.Agent.Execute(ctx, testEvent, testState) return err == nil }\r🐳 Container Orchestration Docker Compose for Development # docker-compose.yml version: '3.8' services: agent-1: build: . environment: - INSTANCE_ID=1 - REDIS_URL=redis://redis:6379 - POSTGRES_URL=postgres://user:pass@postgres:5432/agentflow depends_on: - redis - postgres deploy: resources: limits: cpus: '0.5' memory: 512M reservations: cpus: '0.25' memory: 256M agent-2: build: . environment: - INSTANCE_ID=2 - REDIS_URL=redis://redis:6379 - POSTGRES_URL=postgres://user:pass@postgres:5432/agentflow depends_on: - redis - postgres deploy: resources: limits: cpus: '0.5' memory: 512M agent-3: build: . environment: - INSTANCE_ID=3 - REDIS_URL=redis://redis:6379 - POSTGRES_URL=postgres://user:pass@postgres:5432/agentflow depends_on: - redis - postgres nginx: image: nginx:alpine ports: - \"80:80\" volumes: - ./nginx.conf:/etc/nginx/nginx.conf depends_on: - agent-1 - agent-2 - agent-3 redis: image: redis:7-alpine volumes: - redis_data:/data postgres: image: pgvector/pgvector:pg15 environment: POSTGRES_DB: agentflow POSTGRES_USER: user POSTGRES_PASSWORD: pass volumes: - postgres_data:/var/lib/postgresql/data volumes: redis_data: postgres_data:\rKubernetes Deployment # k8s/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: agenticgokit-agents spec: replicas: 3 selector: matchLabels: app: agenticgokit-agents template: metadata: labels: app: agenticgokit-agents spec: containers: - name: agent image: agenticgokit:latest ports: - containerPort: 8080 env: - name: REDIS_URL value: \"redis://redis-service:6379\" - name: POSTGRES_URL valueFrom: secretKeyRef: name: db-secret key: postgres-url resources: requests: memory: \"256Mi\" cpu: \"250m\" limits: memory: \"512Mi\" cpu: \"500m\" livenessProbe: httpGet: path: /health port: 8080 initialDelaySeconds: 30 periodSeconds: 10 readinessProbe: httpGet: path: /ready port: 8080 initialDelaySeconds: 5 periodSeconds: 5 --- apiVersion: v1 kind: Service metadata: name: agenticgokit-service spec: selector: app: agenticgokit-agents ports: - port: 80 targetPort: 8080 type: LoadBalancer --- apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: agenticgokit-hpa spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: agenticgokit-agents minReplicas: 2 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80\r📊 Performance Monitoring Load Balancer Metrics type LoadBalancerMetrics struct { totalRequests int64 requestsPerAgent map[string]int64 responseTimesByAgent map[string][]time.Duration errorsByAgent map[string]int64 mu sync.RWMutex } func NewLoadBalancerMetrics() *LoadBalancerMetrics { return \u0026LoadBalancerMetrics{ requestsPerAgent: make(map[string]int64), responseTimesByAgent: make(map[string][]time.Duration), errorsByAgent: make(map[string]int64), } } func (lbm *LoadBalancerMetrics) RecordRequest(agentID string, duration time.Duration, success bool) { lbm.mu.Lock() defer lbm.mu.Unlock() lbm.totalRequests++ lbm.requestsPerAgent[agentID]++ // Record response time if lbm.responseTimesByAgent[agentID] == nil { lbm.responseTimesByAgent[agentID] = make([]time.Duration, 0, 100) } lbm.responseTimesByAgent[agentID] = append(lbm.responseTimesByAgent[agentID], duration) // Keep only recent response times if len(lbm.responseTimesByAgent[agentID]) \u003e 100 { lbm.responseTimesByAgent[agentID] = lbm.responseTimesByAgent[agentID][1:] } // Record errors if !success { lbm.errorsByAgent[agentID]++ } } func (lbm *LoadBalancerMetrics) GetStats() map[string]interface{} { lbm.mu.RLock() defer lbm.mu.RUnlock() stats := map[string]interface{}{ \"total_requests\": lbm.totalRequests, \"agents\": make(map[string]interface{}), } for agentID, requests := range lbm.requestsPerAgent { agentStats := map[string]interface{}{ \"requests\": requests, \"errors\": lbm.errorsByAgent[agentID], } // Calculate average response time if responseTimes := lbm.responseTimesByAgent[agentID]; len(responseTimes) \u003e 0 { total := time.Duration(0) for _, rt := range responseTimes { total += rt } agentStats[\"avg_response_time\"] = total / time.Duration(len(responseTimes)) } stats[\"agents\"].(map[string]interface{})[agentID] = agentStats } return stats }\r🎯 Best Practices 1. Configuration for Scaling # agentflow.toml - Production scaling configuration [runtime] max_concurrent_agents = 50 timeout_seconds = 30 enable_metrics = true metrics_port = 8080 [load_balancer] strategy = \"performance_based\" # round_robin, weighted, least_connections, performance_based health_check_interval = \"30s\" unhealthy_threshold = 3 healthy_threshold = 2 [scaling] min_instances = 2 max_instances = 20 target_cpu_utilization = 70 target_memory_utilization = 80 scale_up_cooldown = \"2m\" scale_down_cooldown = \"5m\" [shared_resources] redis_url = \"${REDIS_URL}\" postgres_url = \"${POSTGRES_URL}\" vector_db_url = \"${VECTOR_DB_URL}\"\r2. Health Check Implementation func (a *ScalableAgent) HealthCheck() map[string]interface{} { health := map[string]interface{}{ \"status\": \"healthy\", \"timestamp\": time.Now().Format(time.RFC3339), } // Check LLM provider if err := a.checkLLMProvider(); err != nil { health[\"status\"] = \"unhealthy\" health[\"llm_provider_error\"] = err.Error() } // Check shared cache if err := a.checkCache(); err != nil { health[\"status\"] = \"degraded\" health[\"cache_error\"] = err.Error() } // Check memory system if err := a.checkMemory(); err != nil { health[\"status\"] = \"degraded\" health[\"memory_error\"] = err.Error() } // Add performance metrics health[\"metrics\"] = map[string]interface{}{ \"requests_per_minute\": a.getRequestsPerMinute(), \"avg_response_time\": a.getAverageResponseTime(), \"error_rate\": a.getErrorRate(), } return health }\r3. Graceful Shutdown func (a *ScalableAgent) GracefulShutdown(ctx context.Context) error { log.Println(\"Starting graceful shutdown...\") // Stop accepting new requests a.stopAcceptingRequests() // Wait for ongoing requests to complete done := make(chan struct{}) go func() { a.waitForOngoingRequests() close(done) }() select { case \u003c-done: log.Println(\"All requests completed\") case \u003c-ctx.Done(): log.Println(\"Shutdown timeout reached, forcing shutdown\") } // Clean up resources a.cleanup() log.Println(\"Graceful shutdown completed\") return nil }\rLoad balancing and scaling are essential for building production-ready agent systems that can handle varying loads efficiently and maintain high availability.\n🚀 Next Steps Testing Strategies - Test your scaled agent systems Circuit Breaker Patterns - Add fault tolerance to scaled systems Production Monitoring - Monitor your scaled deployments Production Deployment - Deploy your scaled systems",
    "description": "Load Balancing and Scaling Patterns Building horizontally scalable agent systems with intelligent load distribution\nAs your agent systems grow, you need strategies to distribute load effectively and scale horizontally. This guide covers load balancing patterns, horizontal scaling strategies, and performance optimization techniques for AgenticGoKit applications.\n🏗️ Scaling Architecture Overview flowchart TB\rsubgraph \"Load Balancer Layer\"\rLB[Load Balancer]\rHEALTH[Health Checker]\rend\rsubgraph \"Agent Instances\"\rAGENT1[Agent Instance 1]\rAGENT2[Agent Instance 2]\rAGENT3[Agent Instance 3]\rAGENTN[Agent Instance N]\rend\rsubgraph \"Shared Resources\"\rREDIS[Redis Cache]\rPOSTGRES[PostgreSQL]\rVECTOR[Vector DB]\rMCP[MCP Servers]\rend\rsubgraph \"Monitoring\"\rMETRICS[Metrics Collector]\rALERTS[Alert Manager]\rend\rCLIENT[Client Requests] --\u003e LB\rLB --\u003e AGENT1\rLB --\u003e AGENT2\rLB --\u003e AGENT3\rLB --\u003e AGENTN\rHEALTH --\u003e AGENT1\rHEALTH --\u003e AGENT2\rHEALTH --\u003e AGENT3\rHEALTH --\u003e AGENTN\rAGENT1 --\u003e REDIS\rAGENT2 --\u003e REDIS\rAGENT3 --\u003e REDIS\rAGENTN --\u003e REDIS\rAGENT1 --\u003e POSTGRES\rAGENT2 --\u003e POSTGRES\rAGENT3 --\u003e POSTGRES\rAGENTN --\u003e POSTGRES\rAGENT1 --\u003e VECTOR\rAGENT2 --\u003e VECTOR\rAGENT3 --\u003e VECTOR\rAGENTN --\u003e VECTOR\rAGENT1 --\u003e MCP\rAGENT2 --\u003e MCP\rAGENT3 --\u003e MCP\rAGENTN --\u003e MCP\rAGENT1 --\u003e METRICS\rAGENT2 --\u003e METRICS\rAGENT3 --\u003e METRICS\rAGENTN --\u003e METRICS\rMETRICS --\u003e ALERTS\rclassDef lb fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\rclassDef agent fill:#e8f5e8,stroke:#388e3c,stroke-width:2px\rclassDef shared fill:#fff3e0,stroke:#f57c00,stroke-width:2px\rclassDef monitor fill:#fce4ec,stroke:#c2185b,stroke-width:2px\rclass LB,HEALTH lb\rclass AGENT1,AGENT2,AGENT3,AGENTN agent\rclass REDIS,POSTGRES,VECTOR,MCP shared\rclass METRICS,ALERTS monitor\r⚖️ Load Balancing Strategies 1. Round Robin Load Balancing package balancer import ( \"context\" \"sync\" \"sync/atomic\" \"github.com/kunalkushwaha/agenticgokit/core\" ) type RoundRobinBalancer struct { agents []core.AgentHandler counter int64 mu sync.RWMutex } func NewRoundRobinBalancer(agents []core.AgentHandler) *RoundRobinBalancer { return \u0026RoundRobinBalancer{ agents: agents, } } func (rb *RoundRobinBalancer) Execute(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { if len(rb.agents) == 0 { return nil, errors.New(\"no agents available\") } // Get next agent in round-robin fashion index := atomic.AddInt64(\u0026rb.counter, 1) % int64(len(rb.agents)) agent := rb.agents[index] return agent.Execute(ctx, event, state) } func (rb *RoundRobinBalancer) AddAgent(agent core.AgentHandler) { rb.mu.Lock() defer rb.mu.Unlock() rb.agents = append(rb.agents, agent) } func (rb *RoundRobinBalancer) RemoveAgent(index int) { rb.mu.Lock() defer rb.mu.Unlock() if index \u003e= 0 \u0026\u0026 index \u003c len(rb.agents) { rb.agents = append(rb.agents[:index], rb.agents[index+1:]...) } }\r2. Weighted Load Balancing type WeightedBalancer struct { agents []WeightedAgent totalWeight int mu sync.RWMutex } type WeightedAgent struct { Agent core.AgentHandler Weight int CurrentWeight int } func NewWeightedBalancer() *WeightedBalancer { return \u0026WeightedBalancer{ agents: make([]WeightedAgent, 0), } } func (wb *WeightedBalancer) AddAgent(agent core.AgentHandler, weight int) { wb.mu.Lock() defer wb.mu.Unlock() wb.agents = append(wb.agents, WeightedAgent{ Agent: agent, Weight: weight, CurrentWeight: 0, }) wb.totalWeight += weight } func (wb *WeightedBalancer) Execute(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { wb.mu.Lock() defer wb.mu.Unlock() if len(wb.agents) == 0 { return nil, errors.New(\"no agents available\") } // Weighted round-robin algorithm selected := wb.selectAgent() if selected == nil { return nil, errors.New(\"no agent selected\") } return selected.Execute(ctx, event, state) } func (wb *WeightedBalancer) selectAgent() core.AgentHandler { var selected *WeightedAgent for i := range wb.agents { agent := \u0026wb.agents[i] agent.CurrentWeight += agent.Weight if selected == nil || agent.CurrentWeight \u003e selected.CurrentWeight { selected = agent } } if selected != nil { selected.CurrentWeight -= wb.totalWeight return selected.Agent } return nil }\r3. Least Connections Load Balancing type LeastConnectionsBalancer struct { agents []ConnectionTrackingAgent mu sync.RWMutex } type ConnectionTrackingAgent struct { Agent core.AgentHandler Connections int64 mu sync.RWMutex } func NewLeastConnectionsBalancer(agents []core.AgentHandler) *LeastConnectionsBalancer { trackingAgents := make([]ConnectionTrackingAgent, len(agents)) for i, agent := range agents { trackingAgents[i] = ConnectionTrackingAgent{ Agent: agent, } } return \u0026LeastConnectionsBalancer{ agents: trackingAgents, } } func (lcb *LeastConnectionsBalancer) Execute(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { agent := lcb.selectLeastConnectedAgent() if agent == nil { return nil, errors.New(\"no agents available\") } // Increment connection count atomic.AddInt64(\u0026agent.Connections, 1) defer atomic.AddInt64(\u0026agent.Connections, -1) return agent.Agent.Execute(ctx, event, state) } func (lcb *LeastConnectionsBalancer) selectLeastConnectedAgent() *ConnectionTrackingAgent { lcb.mu.RLock() defer lcb.mu.RUnlock() if len(lcb.agents) == 0 { return nil } var selected *ConnectionTrackingAgent minConnections := int64(^uint64(0) \u003e\u003e 1) // Max int64 for i := range lcb.agents { agent := \u0026lcb.agents[i] connections := atomic.LoadInt64(\u0026agent.Connections) if connections \u003c minConnections { minConnections = connections selected = agent } } return selected }\r4. Performance-Based Load Balancing type PerformanceBasedBalancer struct { agents []PerformanceTrackingAgent mu sync.RWMutex } type PerformanceTrackingAgent struct { Agent core.AgentHandler ResponseTimes []time.Duration SuccessRate float64 LastUpdate time.Time mu sync.RWMutex } func NewPerformanceBasedBalancer(agents []core.AgentHandler) *PerformanceBasedBalancer { trackingAgents := make([]PerformanceTrackingAgent, len(agents)) for i, agent := range agents { trackingAgents[i] = PerformanceTrackingAgent{ Agent: agent, ResponseTimes: make([]time.Duration, 0, 100), SuccessRate: 1.0, LastUpdate: time.Now(), } } return \u0026PerformanceBasedBalancer{ agents: trackingAgents, } } func (pbb *PerformanceBasedBalancer) Execute(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { agent := pbb.selectBestPerformingAgent() if agent == nil { return nil, errors.New(\"no agents available\") } start := time.Now() result, err := agent.Agent.Execute(ctx, event, state) duration := time.Since(start) // Record performance metrics pbb.recordPerformance(agent, duration, err == nil) return result, err } func (pbb *PerformanceBasedBalancer) selectBestPerformingAgent() *PerformanceTrackingAgent { pbb.mu.RLock() defer pbb.mu.RUnlock() if len(pbb.agents) == 0 { return nil } var selected *PerformanceTrackingAgent bestScore := -1.0 for i := range pbb.agents { agent := \u0026pbb.agents[i] score := pbb.calculatePerformanceScore(agent) if score \u003e bestScore { bestScore = score selected = agent } } return selected } func (pbb *PerformanceBasedBalancer) calculatePerformanceScore(agent *PerformanceTrackingAgent) float64 { agent.mu.RLock() defer agent.mu.RUnlock() // Calculate average response time avgResponseTime := time.Duration(0) if len(agent.ResponseTimes) \u003e 0 { total := time.Duration(0) for _, rt := range agent.ResponseTimes { total += rt } avgResponseTime = total / time.Duration(len(agent.ResponseTimes)) } // Normalize response time (lower is better) responseTimeScore := 1.0 / (1.0 + avgResponseTime.Seconds()) // Combine success rate and response time return (agent.SuccessRate * 0.7) + (responseTimeScore * 0.3) } func (pbb *PerformanceBasedBalancer) recordPerformance(agent *PerformanceTrackingAgent, duration time.Duration, success bool) { agent.mu.Lock() defer agent.mu.Unlock() // Record response time agent.ResponseTimes = append(agent.ResponseTimes, duration) if len(agent.ResponseTimes) \u003e 100 { agent.ResponseTimes = agent.ResponseTimes[1:] } // Update success rate (exponential moving average) alpha := 0.1 if success { agent.SuccessRate = agent.SuccessRate*(1-alpha) + alpha } else { agent.SuccessRate = agent.SuccessRate * (1 - alpha) } agent.LastUpdate = time.Now() }\r🔄 Horizontal Scaling Patterns 1. Stateless Agent Design // Stateless agent that can be scaled horizontally type StatelessAgent struct { name string llmProvider core.ModelProvider cache CacheProvider // Shared cache memory MemoryProvider // Shared memory } func NewStatelessAgent(name string, provider core.ModelProvider, cache CacheProvider, memory MemoryProvider) *StatelessAgent { return \u0026StatelessAgent{ name: name, llmProvider: provider, cache: cache, memory: memory, } } func (sa *StatelessAgent) Execute(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { // All state is passed in or retrieved from shared resources // No local state that would prevent horizontal scaling // Get context from shared memory context := sa.memory.GetContext(state.SessionID) // Check shared cache cacheKey := sa.generateCacheKey(event) if cached := sa.cache.Get(cacheKey); cached != nil { return cached.(*core.AgentResult), nil } // Process request result, err := sa.processRequest(ctx, event, context) if err != nil { return nil, err } // Store in shared cache sa.cache.Set(cacheKey, result, 5*time.Minute) return result, nil } func (sa *StatelessAgent) generateCacheKey(event core.Event) string { return fmt.Sprintf(\"%s:%s:%x\", sa.name, event.Type, sha256.Sum256([]byte(fmt.Sprintf(\"%v\", event.Data)))) }\r2. Auto-Scaling Controller type AutoScaler struct { minInstances int maxInstances int currentInstances int targetCPU float64 targetMemory float64 scaleUpCooldown time.Duration scaleDownCooldown time.Duration lastScaleAction time.Time metrics MetricsProvider orchestrator Orchestrator mu sync.RWMutex } func NewAutoScaler(min, max int, orchestrator Orchestrator) *AutoScaler { return \u0026AutoScaler{ minInstances: min, maxInstances: max, currentInstances: min, targetCPU: 70.0, // 70% CPU utilization targetMemory: 80.0, // 80% memory utilization scaleUpCooldown: 2 * time.Minute, scaleDownCooldown: 5 * time.Minute, orchestrator: orchestrator, } } func (as *AutoScaler) Start(ctx context.Context) { ticker := time.NewTicker(30 * time.Second) defer ticker.Stop() for { select { case \u003c-ticker.C: as.evaluateScaling() case \u003c-ctx.Done(): return } } } func (as *AutoScaler) evaluateScaling() { as.mu.Lock() defer as.mu.Unlock() // Get current metrics cpuUsage := as.metrics.GetCPUUsage() memoryUsage := as.metrics.GetMemoryUsage() requestRate := as.metrics.GetRequestRate() // Determine if scaling is needed shouldScaleUp := (cpuUsage \u003e as.targetCPU || memoryUsage \u003e as.targetMemory) \u0026\u0026 as.currentInstances \u003c as.maxInstances \u0026\u0026 time.Since(as.lastScaleAction) \u003e as.scaleUpCooldown shouldScaleDown := cpuUsage \u003c as.targetCPU*0.5 \u0026\u0026 memoryUsage \u003c as.targetMemory*0.5 \u0026\u0026 as.currentInstances \u003e as.minInstances \u0026\u0026 time.Since(as.lastScaleAction) \u003e as.scaleDownCooldown if shouldScaleUp { as.scaleUp() } else if shouldScaleDown { as.scaleDown() } } func (as *AutoScaler) scaleUp() { newInstances := min(as.currentInstances+1, as.maxInstances) if newInstances \u003e as.currentInstances { err := as.orchestrator.AddInstance() if err == nil { as.currentInstances = newInstances as.lastScaleAction = time.Now() log.Printf(\"Scaled up to %d instances\", as.currentInstances) } } } func (as *AutoScaler) scaleDown() { newInstances := max(as.currentInstances-1, as.minInstances) if newInstances \u003c as.currentInstances { err := as.orchestrator.RemoveInstance() if err == nil { as.currentInstances = newInstances as.lastScaleAction = time.Now() log.Printf(\"Scaled down to %d instances\", as.currentInstances) } } }\r3. Health-Aware Load Balancing type HealthAwareBalancer struct { agents []HealthTrackingAgent healthCheck time.Duration mu sync.RWMutex } type HealthTrackingAgent struct { Agent core.AgentHandler Healthy bool LastCheck time.Time Endpoint string } func NewHealthAwareBalancer(agents []core.AgentHandler) *HealthAwareBalancer { trackingAgents := make([]HealthTrackingAgent, len(agents)) for i, agent := range agents { trackingAgents[i] = HealthTrackingAgent{ Agent: agent, Healthy: true, } } hab := \u0026HealthAwareBalancer{ agents: trackingAgents, healthCheck: 30 * time.Second, } // Start health checking go hab.startHealthChecking() return hab } func (hab *HealthAwareBalancer) Execute(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { healthyAgents := hab.getHealthyAgents() if len(healthyAgents) == 0 { return nil, errors.New(\"no healthy agents available\") } // Use round-robin among healthy agents index := rand.Intn(len(healthyAgents)) return healthyAgents[index].Execute(ctx, event, state) } func (hab *HealthAwareBalancer) getHealthyAgents() []core.AgentHandler { hab.mu.RLock() defer hab.mu.RUnlock() var healthy []core.AgentHandler for _, agent := range hab.agents { if agent.Healthy { healthy = append(healthy, agent.Agent) } } return healthy } func (hab *HealthAwareBalancer) startHealthChecking() { ticker := time.NewTicker(hab.healthCheck) defer ticker.Stop() for range ticker.C { hab.checkHealth() } } func (hab *HealthAwareBalancer) checkHealth() { hab.mu.Lock() defer hab.mu.Unlock() for i := range hab.agents { agent := \u0026hab.agents[i] // Perform health check healthy := hab.performHealthCheck(agent) if agent.Healthy != healthy { agent.Healthy = healthy if healthy { log.Printf(\"Agent %d is now healthy\", i) } else { log.Printf(\"Agent %d is now unhealthy\", i) } } agent.LastCheck = time.Now() } } func (hab *HealthAwareBalancer) performHealthCheck(agent *HealthTrackingAgent) bool { // Simple health check - try to execute a test request ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second) defer cancel() testEvent := core.NewEvent(\"health_check\", \"ping\") testState := \u0026core.State{} _, err := agent.Agent.Execute(ctx, testEvent, testState) return err == nil }\r🐳 Container Orchestration Docker Compose for Development # docker-compose.yml version: '3.8' services: agent-1: build: . environment: - INSTANCE_ID=1 - REDIS_URL=redis://redis:6379 - POSTGRES_URL=postgres://user:pass@postgres:5432/agentflow depends_on: - redis - postgres deploy: resources: limits: cpus: '0.5' memory: 512M reservations: cpus: '0.25' memory: 256M agent-2: build: . environment: - INSTANCE_ID=2 - REDIS_URL=redis://redis:6379 - POSTGRES_URL=postgres://user:pass@postgres:5432/agentflow depends_on: - redis - postgres deploy: resources: limits: cpus: '0.5' memory: 512M agent-3: build: . environment: - INSTANCE_ID=3 - REDIS_URL=redis://redis:6379 - POSTGRES_URL=postgres://user:pass@postgres:5432/agentflow depends_on: - redis - postgres nginx: image: nginx:alpine ports: - \"80:80\" volumes: - ./nginx.conf:/etc/nginx/nginx.conf depends_on: - agent-1 - agent-2 - agent-3 redis: image: redis:7-alpine volumes: - redis_data:/data postgres: image: pgvector/pgvector:pg15 environment: POSTGRES_DB: agentflow POSTGRES_USER: user POSTGRES_PASSWORD: pass volumes: - postgres_data:/var/lib/postgresql/data volumes: redis_data: postgres_data:\rKubernetes Deployment # k8s/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: agenticgokit-agents spec: replicas: 3 selector: matchLabels: app: agenticgokit-agents template: metadata: labels: app: agenticgokit-agents spec: containers: - name: agent image: agenticgokit:latest ports: - containerPort: 8080 env: - name: REDIS_URL value: \"redis://redis-service:6379\" - name: POSTGRES_URL valueFrom: secretKeyRef: name: db-secret key: postgres-url resources: requests: memory: \"256Mi\" cpu: \"250m\" limits: memory: \"512Mi\" cpu: \"500m\" livenessProbe: httpGet: path: /health port: 8080 initialDelaySeconds: 30 periodSeconds: 10 readinessProbe: httpGet: path: /ready port: 8080 initialDelaySeconds: 5 periodSeconds: 5 --- apiVersion: v1 kind: Service metadata: name: agenticgokit-service spec: selector: app: agenticgokit-agents ports: - port: 80 targetPort: 8080 type: LoadBalancer --- apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: agenticgokit-hpa spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: agenticgokit-agents minReplicas: 2 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80\r📊 Performance Monitoring Load Balancer Metrics type LoadBalancerMetrics struct { totalRequests int64 requestsPerAgent map[string]int64 responseTimesByAgent map[string][]time.Duration errorsByAgent map[string]int64 mu sync.RWMutex } func NewLoadBalancerMetrics() *LoadBalancerMetrics { return \u0026LoadBalancerMetrics{ requestsPerAgent: make(map[string]int64), responseTimesByAgent: make(map[string][]time.Duration), errorsByAgent: make(map[string]int64), } } func (lbm *LoadBalancerMetrics) RecordRequest(agentID string, duration time.Duration, success bool) { lbm.mu.Lock() defer lbm.mu.Unlock() lbm.totalRequests++ lbm.requestsPerAgent[agentID]++ // Record response time if lbm.responseTimesByAgent[agentID] == nil { lbm.responseTimesByAgent[agentID] = make([]time.Duration, 0, 100) } lbm.responseTimesByAgent[agentID] = append(lbm.responseTimesByAgent[agentID], duration) // Keep only recent response times if len(lbm.responseTimesByAgent[agentID]) \u003e 100 { lbm.responseTimesByAgent[agentID] = lbm.responseTimesByAgent[agentID][1:] } // Record errors if !success { lbm.errorsByAgent[agentID]++ } } func (lbm *LoadBalancerMetrics) GetStats() map[string]interface{} { lbm.mu.RLock() defer lbm.mu.RUnlock() stats := map[string]interface{}{ \"total_requests\": lbm.totalRequests, \"agents\": make(map[string]interface{}), } for agentID, requests := range lbm.requestsPerAgent { agentStats := map[string]interface{}{ \"requests\": requests, \"errors\": lbm.errorsByAgent[agentID], } // Calculate average response time if responseTimes := lbm.responseTimesByAgent[agentID]; len(responseTimes) \u003e 0 { total := time.Duration(0) for _, rt := range responseTimes { total += rt } agentStats[\"avg_response_time\"] = total / time.Duration(len(responseTimes)) } stats[\"agents\"].(map[string]interface{})[agentID] = agentStats } return stats }\r🎯 Best Practices 1. Configuration for Scaling # agentflow.toml - Production scaling configuration [runtime] max_concurrent_agents = 50 timeout_seconds = 30 enable_metrics = true metrics_port = 8080 [load_balancer] strategy = \"performance_based\" # round_robin, weighted, least_connections, performance_based health_check_interval = \"30s\" unhealthy_threshold = 3 healthy_threshold = 2 [scaling] min_instances = 2 max_instances = 20 target_cpu_utilization = 70 target_memory_utilization = 80 scale_up_cooldown = \"2m\" scale_down_cooldown = \"5m\" [shared_resources] redis_url = \"${REDIS_URL}\" postgres_url = \"${POSTGRES_URL}\" vector_db_url = \"${VECTOR_DB_URL}\"\r2. Health Check Implementation func (a *ScalableAgent) HealthCheck() map[string]interface{} { health := map[string]interface{}{ \"status\": \"healthy\", \"timestamp\": time.Now().Format(time.RFC3339), } // Check LLM provider if err := a.checkLLMProvider(); err != nil { health[\"status\"] = \"unhealthy\" health[\"llm_provider_error\"] = err.Error() } // Check shared cache if err := a.checkCache(); err != nil { health[\"status\"] = \"degraded\" health[\"cache_error\"] = err.Error() } // Check memory system if err := a.checkMemory(); err != nil { health[\"status\"] = \"degraded\" health[\"memory_error\"] = err.Error() } // Add performance metrics health[\"metrics\"] = map[string]interface{}{ \"requests_per_minute\": a.getRequestsPerMinute(), \"avg_response_time\": a.getAverageResponseTime(), \"error_rate\": a.getErrorRate(), } return health }\r3. Graceful Shutdown func (a *ScalableAgent) GracefulShutdown(ctx context.Context) error { log.Println(\"Starting graceful shutdown...\") // Stop accepting new requests a.stopAcceptingRequests() // Wait for ongoing requests to complete done := make(chan struct{}) go func() { a.waitForOngoingRequests() close(done) }() select { case \u003c-done: log.Println(\"All requests completed\") case \u003c-ctx.Done(): log.Println(\"Shutdown timeout reached, forcing shutdown\") } // Clean up resources a.cleanup() log.Println(\"Graceful shutdown completed\") return nil }\rLoad balancing and scaling are essential for building production-ready agent systems that can handle varying loads efficiently and maintain high availability.",
    "tags": [],
    "title": "load-balancing-scaling",
    "uri": "/AgenticGoKitDocs/tutorials/advanced/load-balancing-scaling/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials \u003e debugging",
    "content": "Logging and Tracing in AgenticGoKit Overview AgenticGoKit provides comprehensive logging and tracing capabilities through structured logging with zerolog and built-in trace collection. This guide covers how to configure logging, use the tracing system, and analyze execution traces to debug multi-agent systems effectively.\nPrerequisites Understanding of Agent Lifecycle Familiarity with Debugging Multi-Agent Systems AgenticGoKit project with agentflow.toml configuration Logging System Configuration AgenticGoKit uses structured logging with configurable levels and formats. Configure logging in your agentflow.toml:\n[logging] level = \"debug\" # debug, info, warn, error format = \"json\" # json or text [agent_flow] name = \"my-agent-system\" version = \"1.0.0\"\rLog Levels AgenticGoKit supports four log levels:\nDEBUG: Detailed information for debugging INFO: General information about system operation WARN: Warning messages for potential issues ERROR: Error messages for failures Using the Logger AgenticGoKit provides a global logger that can be used throughout your application:\npackage main import ( \"github.com/kunalkushwaha/agenticgokit/core\" ) func main() { // Get the global logger logger := core.Logger() // Log at different levels logger.Debug().Msg(\"Debug message\") logger.Info().Msg(\"Info message\") logger.Warn().Msg(\"Warning message\") logger.Error().Msg(\"Error message\") // Log with structured data logger.Info(). Str(\"agent_id\", \"research-agent\"). Int(\"event_count\", 5). Dur(\"duration\", time.Second*2). Msg(\"Agent processing completed\") }\rAgent-Specific Logging Each agent has its own logger with contextual information:\n// In your agent implementation type MyAgent struct { name string logger zerolog.Logger } func NewMyAgent(name string) *MyAgent { return \u0026MyAgent{ name: name, logger: core.Logger().With().Str(\"agent\", name).Logger(), } } func (a *MyAgent) Process(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { a.logger.Info(). Str(\"event_id\", event.GetID()). Str(\"event_type\", event.GetType()). Msg(\"Processing event\") // Your agent logic here a.logger.Info(). Str(\"event_id\", event.GetID()). Msg(\"Event processing completed\") return core.AgentResult{}, nil }\rTracing System How Tracing Works AgenticGoKit automatically collects trace entries during execution. Traces are stored as JSON files with the pattern \u003csession-id\u003e.trace.json in your project directory.\nTrace Entry Structure Each trace entry contains:\ntype TraceEntry struct { Timestamp time.Time `json:\"timestamp\"` Type string `json:\"type\"` EventID string `json:\"event_id\"` SessionID string `json:\"session_id\"` AgentID string `json:\"agent_id\"` State *State `json:\"state\"` AgentResult *AgentResult `json:\"agent_result,omitempty\"` Hook HookPoint `json:\"hook\"` Error string `json:\"error,omitempty\"` TargetAgentID string `json:\"target_agent_id,omitempty\"` SourceAgentID string `json:\"source_agent_id,omitempty\"` }\rHook Points AgenticGoKit traces execution at specific hook points:\nBeforeEventHandling: Before an event is processed AfterEventHandling: After an event is processed BeforeAgentRun: Before an agent processes an event AfterAgentRun: After an agent processes an event Configuring Tracing Tracing is enabled by default. You can configure it in your runner setup:\npackage main import ( \"github.com/kunalkushwaha/agenticgokit/core\" ) func main() { // Option 1: Use in-memory trace logger (default) traceLogger := core.NewInMemoryTraceLogger() // Option 2: Use file-based trace logger for persistent traces fileTraceLogger, err := core.NewFileTraceLogger(\"./traces\") if err != nil { log.Fatal(\"Failed to create file trace logger:\", err) } defer fileTraceLogger.Close() // Important: Close to finalize JSON files // Create runner configuration with custom trace logger config := core.RunnerConfig{ Memory: memory, SessionID: \"my-session\", TraceLogger: fileTraceLogger, // or traceLogger for in-memory ConfigPath: \"agentflow.toml\", } // Create runner with tracing runner, err := core.NewRunnerFromConfig(config) if err != nil { log.Fatal(err) } // Start the runner ctx := context.Background() if err := runner.Start(ctx); err != nil { log.Fatal(err) } defer runner.Stop() // Dump traces programmatically if needed traces, err := runner.DumpTrace(\"my-session\") if err != nil { log.Printf(\"Failed to dump traces: %v\", err) } else { log.Printf(\"Retrieved %d trace entries\", len(traces)) } }\rUsing agentcli for Trace Analysis Listing Available Traces # List all available trace sessions agentcli list\rViewing Traces # View complete trace for a session agentcli trace \u003csession-id\u003e # View only agent flow without state details agentcli trace --flow-only \u003csession-id\u003e # Filter trace to specific agent agentcli trace --filter agent=\u003cagent-name\u003e \u003csession-id\u003e # View verbose trace with full state details agentcli trace --verbose \u003csession-id\u003e # Debug trace structure agentcli trace --debug \u003csession-id\u003e\rExample Trace Output Trace for session req-17461596:\r┌───────────────────┬─────────────────────────┬────────────────┬────────────────────────────────────────┬──────────────────────────────┐\r│ TIMESTAMP │ HOOK │ AGENT │ STATE │ ERROR │\r├───────────────────┼─────────────────────────┼────────────────┼────────────────────────────────────────┼──────────────────────────────┤\r│ 14:32:15.123 │ BeforeEventHandling │ planner │ {message:\"Analyze this data\", ...} │ - │\r│ 14:32:15.145 │ AfterEventHandling │ planner │ {analysis:\"Data shows trends\", ...} │ - │\r│ 14:32:15.146 │ BeforeEventHandling │ summarizer │ {analysis:\"Data shows trends\", ...} │ - │\r│ 14:32:15.167 │ AfterEventHandling │ summarizer │ {summary:\"Key findings: ...\", ...} │ - │\r└───────────────────┴─────────────────────────┴────────────────┴────────────────────────────────────────┴──────────────────────────────┘\rAgent request flow for session req-17461596:\rTIME AGENT NEXT HOOK EVENT ID\r14:32:15.123 planner summarizer AfterEventHandling req-1746...\r14:32:15.167 summarizer (end) AfterEventHandling req-1746...\rSequence diagram:\r----------------\r1. planner → summarizer\r2. summarizer → (end)\rCondensed route:\rplanner → summarizer\rAdvanced Logging Patterns Correlation IDs Use correlation IDs to track requests across multiple agents:\nfunc processWithCorrelation(ctx context.Context, event core.Event, state core.State) { // Extract or generate correlation ID correlationID := event.GetID() // Add correlation ID to context logger := core.Logger().With(). Str(\"correlation_id\", correlationID). Logger() logger.Info().Msg(\"Starting request processing\") // Pass correlation ID through state state.SetMeta(\"correlation_id\", correlationID) // Your processing logic here logger.Info().Msg(\"Request processing completed\") }\rPerformance Logging Log performance metrics for monitoring:\nfunc logPerformanceMetrics(agentID string, duration time.Duration, success bool) { logger := core.Logger().With(). Str(\"component\", \"performance\"). Str(\"agent_id\", agentID). Logger() if success { logger.Info(). Dur(\"duration\", duration). Msg(\"Agent execution completed successfully\") } else { logger.Warn(). Dur(\"duration\", duration). Msg(\"Agent execution failed\") } }\rError Context Logging Provide rich context when logging errors:\nfunc handleAgentError(err error, agentID string, eventID string, state core.State) { logger := core.Logger().With(). Str(\"agent_id\", agentID). Str(\"event_id\", eventID). Logger() // Log error with context logger.Error(). Err(err). Strs(\"state_keys\", state.Keys()). Msg(\"Agent processing failed\") // Log additional context if available if errorCode, exists := state.GetMeta(\"error_code\"); exists { logger.Error(). Str(\"error_code\", errorCode). Msg(\"Error code context\") } }\rCustom Trace Logging Implementing Custom Trace Logger You can implement a custom trace logger for specific needs:\ntype FileTraceLogger struct { filePath string mu sync.Mutex } func NewFileTraceLogger(filePath string) *FileTraceLogger { return \u0026FileTraceLogger{ filePath: filePath, } } func (f *FileTraceLogger) Log(entry core.TraceEntry) error { f.mu.Lock() defer f.mu.Unlock() // Open file for appending file, err := os.OpenFile(f.filePath, os.O_APPEND|os.O_CREATE|os.O_WRONLY, 0644) if err != nil { return err } defer file.Close() // Marshal entry to JSON data, err := json.Marshal(entry) if err != nil { return err } // Write to file _, err = file.Write(append(data, '\\n')) return err } func (f *FileTraceLogger) GetTrace(sessionID string) ([]core.TraceEntry, error) { // Implementation to read and filter traces by session ID // This is a simplified example return nil, fmt.Errorf(\"not implemented\") }\rUsing Custom Trace Logger func main() { // Create custom trace logger traceLogger := NewFileTraceLogger(\"traces.jsonl\") // Create runner configuration config := core.RunnerConfig{ Memory: memory, SessionID: \"custom-session\", TraceLogger: traceLogger, ConfigPath: \"agentflow.toml\", } // Create runner with custom trace logger runner, err := core.NewRunnerFromConfig(config) if err != nil { log.Fatal(err) } // Use the runner // ... }\rMonitoring and Alerting Log-Based Monitoring Set up monitoring based on log patterns:\n// Monitor error rates func monitorErrorRate() { logger := core.Logger().With(). Str(\"component\", \"monitoring\"). Logger() ticker := time.NewTicker(1 * time.Minute) defer ticker.Stop() for range ticker.C { // Calculate error rate from logs errorRate := calculateErrorRate() logger.Info(). Float64(\"error_rate\", errorRate). Msg(\"Error rate metric\") if errorRate \u003e 0.1 { // 10% threshold logger.Warn(). Float64(\"error_rate\", errorRate). Msg(\"High error rate detected\") } } }\rHealth Check Logging Log health check results for monitoring:\nfunc logHealthCheck(component string, healthy bool, details map[string]interface{}) { logger := core.Logger().With(). Str(\"component\", \"health_check\"). Str(\"check_component\", component). Logger() if healthy { logger.Info(). Fields(details). Msg(\"Health check passed\") } else { logger.Error(). Fields(details). Msg(\"Health check failed\") } }\rBest Practices 1. Structured Logging Always use structured logging with key-value pairs Use consistent field names across your application Include relevant context in every log message 2. Log Levels Use DEBUG for detailed debugging information Use INFO for normal operational messages Use WARN for potential issues that don’t stop execution Use ERROR for actual failures 3. Performance Considerations Avoid logging large objects in production Use appropriate log levels to control verbosity Consider async logging for high-throughput scenarios 4. Security Never log sensitive information (passwords, tokens, etc.) Sanitize user input before logging Use log rotation to manage disk space 5. Trace Analysis Use correlation IDs to track requests across agents Analyze trace patterns to identify bottlenecks Monitor trace file sizes and implement rotation Troubleshooting Common Issues Missing Traces If traces are not being generated:\nCheck that tracing is enabled in your configuration Verify that the runner has write permissions to the directory Ensure that the callback registry is properly configured Large Trace Files If trace files become too large:\nImplement trace rotation Filter traces to specific sessions or agents Use sampling for high-volume scenarios Performance Impact If logging impacts performance:\nReduce log level in production Use async logging Monitor logging overhead Conclusion Effective logging and tracing are essential for debugging multi-agent systems. AgenticGoKit’s built-in capabilities provide comprehensive visibility into system behavior, making it easier to identify and resolve issues.\nIn the next tutorial, we’ll explore Performance Monitoring techniques for optimizing system performance.\nNext Steps Performance Monitoring Production Troubleshooting Debugging Multi-Agent Systems Further Reading Zerolog Documentation Structured Logging Best Practices Distributed Tracing Concepts",
    "description": "Logging and Tracing in AgenticGoKit Overview AgenticGoKit provides comprehensive logging and tracing capabilities through structured logging with zerolog and built-in trace collection. This guide covers how to configure logging, use the tracing system, and analyze execution traces to debug multi-agent systems effectively.\nPrerequisites Understanding of Agent Lifecycle Familiarity with Debugging Multi-Agent Systems AgenticGoKit project with agentflow.toml configuration Logging System Configuration AgenticGoKit uses structured logging with configurable levels and formats. Configure logging in your agentflow.toml:",
    "tags": [],
    "title": "logging-and-tracing",
    "uri": "/AgenticGoKitDocs/tutorials/debugging/logging-and-tracing/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials",
    "content": "Model Context Protocol (MCP) in AgenticGoKit Navigation: Documentation Home → Tutorials → MCP (Tools)\nOverview The Model Context Protocol (MCP) is a powerful framework within AgenticGoKit that enables agents to interact with external tools, APIs, and services. MCP bridges the gap between language models and the outside world, allowing agents to perform actions beyond text generation.\nWith MCP, agents can search the web, access databases, call APIs, manipulate files, perform calculations, and much more. This capability transforms agents from simple text processors into powerful assistants that can take meaningful actions.\nKey Concepts What is MCP? MCP (Model Context Protocol) is a standardized interface for connecting language models to external tools and capabilities. It defines:\nTool Registration: How tools are defined and registered with the system Tool Discovery: How agents discover available tools Tool Invocation: How agents call tools and receive results Tool Response Handling: How tool results are processed and incorporated into agent responses MCP Architecture ┌─────────────┐ ┌───────────────┐ ┌─────────────┐\r│ │ │ │ │ │\r│ Agent │────▶│ MCP Manager │────▶│ Tool │\r│ │ │ │ │ │\r└─────────────┘ └───────────────┘ └─────────────┘\r▲ │ │\r│ ▼ ▼\r│ ┌───────────────┐ ┌─────────────┐\r└──────────────│ Tool Result │◀───│ External │\r│ Processor │ │ Service │\r└───────────────┘ └─────────────┘\rTool Types AgenticGoKit supports various types of tools:\nBuilt-in Tools: Core functionality provided by the framework Custom Tools: User-defined tools for specific use cases API Tools: Wrappers around external APIs and services Stateful Tools: Tools that maintain state between invocations Composite Tools: Tools composed of multiple sub-tools Why Use MCP? MCP provides several key benefits:\nExtended Capabilities: Enables agents to perform actions beyond text generation Modularity: Tools can be developed and maintained independently Flexibility: Mix and match tools based on specific requirements Standardization: Consistent interface for all tool interactions Security: Controlled access to external systems MCP vs. Function Calling MCP is similar to function calling in LLMs but provides additional capabilities:\nFeature MCP Function Calling Tool Discovery Dynamic Static Tool Registration Runtime Design time Tool Composition Supported Limited State Management Built-in Manual Error Handling Comprehensive Basic Security Controls Fine-grained Limited Getting Started with MCP To start using MCP in AgenticGoKit, you’ll need to:\nCreate Tools: Define the tools your agents will use Register Tools: Make tools available to the MCP manager Configure Agents: Set up agents to use MCP Handle Tool Results: Process and incorporate tool outputs The following tutorials will guide you through these steps in detail:\nTool Development - Creating custom tools Tool Integration - Integrating tools with agents Advanced Tool Patterns - Complex tool usage patterns Example: Simple MCP Setup package main import ( \"context\" \"fmt\" \"log\" \"os\" \"time\" \"github.com/kunalkushwaha/agenticgokit/core\" ) func main() { // Initialize MCP for tool discovery core.QuickStartMCP() // Create LLM provider provider, err := core.NewOpenAIAdapter( os.Getenv(\"OPENAI_API_KEY\"), \"gpt-3.5-turbo\", 1000, 0.7, ) if err != nil { log.Fatalf(\"Failed to create provider: %v\", err) } // Create agent with MCP tools agent := core.NewLLMAgent(\"assistant\", provider). WithSystemPrompt(\"You are a helpful assistant with access to tools. Use them when needed.\"). WithTools([]string{\"calculator\", \"weather\"}) // Tools discovered via MCP // Create agents map agents := map[string]core.AgentHandler{ \"assistant\": agent, } // Create runner runner := core.CreateRouteRunner(agents) // Start runner ctx := context.Background() if err := runner.Start(ctx); err != nil { log.Fatalf(\"Failed to start runner: %v\", err) } defer runner.Stop() // Create event with user query event := core.NewEvent(\"assistant\", map[string]interface{}{ \"message\": \"What's 25 * 16 and what's the weather in New York?\", }) event.SetMetadata(core.RouteMetadataKey, \"assistant\") // Emit the event if err := runner.Emit(event); err != nil { log.Fatalf(\"Failed to emit event: %v\", err) } // Wait for processing time.Sleep(5 * time.Second) fmt.Println(\"MCP tool integration complete!\") }\rNext Steps Now that you understand the basics of MCP, proceed to the following tutorials to learn more:\nTool Development - Learn how to create custom tools Tool Integration - Integrate tools with your agents Advanced Tool Patterns - Explore complex tool usage patterns Further Reading API Reference: MCP Examples: Tool Usage Advanced Patterns - Advanced multi-agent patterns",
    "description": "Model Context Protocol (MCP) in AgenticGoKit Navigation: Documentation Home → Tutorials → MCP (Tools)\nOverview The Model Context Protocol (MCP) is a powerful framework within AgenticGoKit that enables agents to interact with external tools, APIs, and services. MCP bridges the gap between language models and the outside world, allowing agents to perform actions beyond text generation.\nWith MCP, agents can search the web, access databases, call APIs, manipulate files, perform calculations, and much more. This capability transforms agents from simple text processors into powerful assistants that can take meaningful actions.",
    "tags": [],
    "title": "mcp",
    "uri": "/AgenticGoKitDocs/tutorials/mcp/index.html"
  },
  {
    "breadcrumb": "content \u003e guides \u003e setup",
    "content": "MCP Tools Dynamic Tool Discovery and Execution with Model Context Protocol\nAgenticGoKit uses the Model Context Protocol (MCP) to provide agents with dynamic tool discovery and execution capabilities. This guide covers everything from basic tool usage to building custom MCP servers.\nOverview The MCP integration in AgenticGoKit provides:\nDynamic Discovery: Tools are discovered at runtime, not hard-coded Schema-Based: Tools provide their own descriptions and parameters LLM-Driven: The LLM decides which tools to use based on context Extensible: Add new tools by connecting MCP servers Quick Start (5 minutes) 1. Basic MCP Configuration agentflow.toml:\n[mcp] enabled = true # Web search capabilities [mcp.servers.search] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-web-search\"] transport = \"stdio\" # Docker container management [mcp.servers.docker] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-docker\"] transport = \"stdio\" # File system operations [mcp.servers.filesystem] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-filesystem\"] transport = \"stdio\"\r2. Tool-Enabled Agent package main import ( \"context\" \"fmt\" agentflow \"github.com/kunalkushwaha/agenticgokit/core\" ) type ToolEnabledAgent struct { llm agentflow.ModelProvider mcpManager agentflow.MCPManager } func NewToolEnabledAgent(llm agentflow.ModelProvider, mcp agentflow.MCPManager) *ToolEnabledAgent { return \u0026ToolEnabledAgent{ llm: llm, mcpManager: mcp, } } func (a *ToolEnabledAgent) Run(ctx context.Context, event agentflow.Event, state agentflow.State) (agentflow.AgentResult, error) { message := event.GetData()[\"message\"] // Build system prompt with tool awareness systemPrompt := `You are a helpful assistant with access to various tools. Use tools when they can provide current, specific, or actionable information. When using tools, format calls exactly like this: \u003ctool_call\u003e {\"name\": \"tool_name\", \"args\": {\"param\": \"value\"}} \u003c/tool_call\u003e` // Get available tools and add to prompt toolPrompt := agentflow.FormatToolsForPrompt(ctx, a.mcpManager) fullPrompt := fmt.Sprintf(\"%s\\n\\n%s\\n\\nUser: %s\", systemPrompt, toolPrompt, message) // Get initial LLM response response, err := a.llm.Generate(ctx, fullPrompt) if err != nil { return agentflow.AgentResult{}, fmt.Errorf(\"LLM generation failed: %w\", err) } // Execute any tool calls found in response toolResults := agentflow.ParseAndExecuteToolCalls(ctx, a.mcpManager, response) if len(toolResults) \u003e 0 { // Synthesize tool results with original response synthesisPrompt := fmt.Sprintf(`Original response: %s Tool execution results: %v Please provide a comprehensive final answer that incorporates the tool results.`, response, toolResults) finalResponse, err := a.llm.Generate(ctx, synthesisPrompt) if err != nil { finalResponse = response // Fallback to original response } state.Set(\"tools_used\", true) state.Set(\"tool_results\", toolResults) return agentflow.AgentResult{Result: finalResponse, State: state}, nil } return agentflow.AgentResult{Result: response, State: state}, nil }\rAvailable MCP Servers Development \u0026 System Tools # Docker management [mcp.servers.docker] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-docker\"] transport = \"stdio\" # File system operations [mcp.servers.filesystem] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-filesystem\"] transport = \"stdio\" # GitHub integration [mcp.servers.github] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-github\"] transport = \"stdio\" env = { \"GITHUB_TOKEN\" = \"${GITHUB_TOKEN}\" }\rWeb \u0026 Search Tools # Web search [mcp.servers.search] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-web-search\"] transport = \"stdio\" # URL content fetching [mcp.servers.fetch] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-fetch\"] transport = \"stdio\" # Brave search API [mcp.servers.brave] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-brave-search\"] transport = \"stdio\" env = { \"BRAVE_API_KEY\" = \"${BRAVE_API_KEY}\" }\rDatabase Tools # PostgreSQL [mcp.servers.postgres] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-postgres\"] transport = \"stdio\" env = { \"DATABASE_URL\" = \"${DATABASE_URL}\" } # SQLite [mcp.servers.sqlite] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-sqlite\"] transport = \"stdio\" # MongoDB [mcp.servers.mongodb] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-mongodb\"] transport = \"stdio\" env = { \"MONGODB_URI\" = \"${MONGODB_URI}\" }\rProduction Configuration With Caching [mcp] enabled = true cache_enabled = true cache_ttl = \"5m\" connection_timeout = \"30s\" max_retries = 3 [mcp.cache] type = \"memory\" max_size = 1000 # Production-ready servers [mcp.servers.search] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-web-search\"] transport = \"stdio\" env = { \"SEARCH_API_KEY\" = \"${SEARCH_API_KEY}\" } [mcp.servers.database] command = \"npx\" args = [\"-y\", \"@modelcontextprotocol/server-postgres\"] transport = \"stdio\" env = { \"DATABASE_URL\" = \"${DATABASE_URL}\" }\rTool Usage Patterns Information Gathering Pattern func (a *ResearchAgent) Run(ctx context.Context, event agentflow.Event, state agentflow.State) (agentflow.AgentResult, error) { query := event.GetData()[\"message\"] // System prompt optimized for research systemPrompt := `You are a research agent. For any query: 1. First, search for current information using the search tool 2. If specific URLs are mentioned or found, fetch their content 3. Gather multiple perspectives and sources 4. Organize findings in a structured way Always use tools for factual, current information rather than relying on training data.` toolPrompt := agentflow.FormatToolsForPrompt(ctx, a.mcpManager) prompt := fmt.Sprintf(\"%s\\n%s\\nResearch: %s\", systemPrompt, toolPrompt, query) response, err := a.llm.Generate(ctx, prompt) if err != nil { return agentflow.AgentResult{}, err } // Execute research tools toolResults := agentflow.ParseAndExecuteToolCalls(ctx, a.mcpManager, response) // Compile comprehensive research report if len(toolResults) \u003e 0 { reportPrompt := fmt.Sprintf(`Based on this research: %v Create a comprehensive research report with: 1. Executive summary 2. Key findings with sources 3. Detailed information 4. Implications and insights`, toolResults) finalReport, _ := a.llm.Generate(ctx, reportPrompt) state.Set(\"research_report\", finalReport) return agentflow.AgentResult{Result: finalReport, State: state}, nil } return agentflow.AgentResult{Result: response, State: state}, nil }\rCustom MCP Servers Building a Custom Tool You can create custom MCP servers for domain-specific tools:\n// custom-mcp-server.js const { MCPServer } = require('@modelcontextprotocol/server'); const server = new MCPServer({ name: \"custom-tools\", version: \"1.0.0\" }); // Define custom tool server.registerTool({ name: \"analyze_data\", description: \"Analyze CSV data and return insights\", parameters: { type: \"object\", properties: { data: { type: \"string\", description: \"CSV data to analyze\" }, analysis_type: { type: \"string\", description: \"Type of analysis: summary, trends, outliers\" } }, required: [\"data\", \"analysis_type\"] } }, async (params) =\u003e { // Custom analysis logic here const { data, analysis_type } = params; switch(analysis_type) { case \"summary\": return { result: \"Data summary: ...\" }; case \"trends\": return { result: \"Trend analysis: ...\" }; default: return { error: \"Unknown analysis type\" }; } }); server.start();\rConfigure in agentflow.toml:\n[mcp.servers.custom] command = \"node\" args = [\"custom-mcp-server.js\"] transport = \"stdio\"\rTesting Tool Integration Mock MCP Manager for Testing type MockMCPManager struct { tools []agentflow.ToolSchema toolResults map[string]interface{} } func NewMockMCPManager() *MockMCPManager { return \u0026MockMCPManager{ tools: []agentflow.ToolSchema{ { Name: \"search\", Description: \"Search for information\", Parameters: map[string]interface{}{ \"query\": map[string]interface{}{ \"type\": \"string\", \"description\": \"Search query\", }, }, }, }, toolResults: map[string]interface{}{ \"search\": map[string]interface{}{ \"results\": []string{\"Mock search result 1\", \"Mock search result 2\"}, }, }, } } func (m *MockMCPManager) ListTools(ctx context.Context) ([]agentflow.ToolSchema, error) { return m.tools, nil } func (m *MockMCPManager) CallTool(ctx context.Context, name string, args map[string]interface{}) (interface{}, error) { if result, exists := m.toolResults[name]; exists { return result, nil } return nil, fmt.Errorf(\"tool not found: %s\", name) }\rPerformance Considerations Tool Execution Optimization Cache Tool Schemas: Tool discovery is cached automatically Parallel Execution: Multiple tool calls execute concurrently Timeout Management: Tools have configurable timeouts Connection Pooling: MCP connections are reused // Production MCP configuration config := agentflow.MCPConfig{ CacheEnabled: true, CacheTTL: 5 * time.Minute, ConnectionTimeout: 30 * time.Second, MaxRetries: 3, MaxConcurrentTools: 5, } mcpManager, err := agentflow.InitializeProductionMCP(ctx, config)\rNext Steps LLM Providers - Configure different LLM providers Vector Databases - Set up persistent storage MCP API Reference - Complete MCP documentation",
    "description": "MCP Tools Dynamic Tool Discovery and Execution with Model Context Protocol\nAgenticGoKit uses the Model Context Protocol (MCP) to provide agents with dynamic tool discovery and execution capabilities. This guide covers everything from basic tool usage to building custom MCP servers.\nOverview The MCP integration in AgenticGoKit provides:\nDynamic Discovery: Tools are discovered at runtime, not hard-coded Schema-Based: Tools provide their own descriptions and parameters LLM-Driven: The LLM decides which tools to use based on context Extensible: Add new tools by connecting MCP servers Quick Start (5 minutes) 1. Basic MCP Configuration agentflow.toml:",
    "tags": [],
    "title": "mcp-tools",
    "uri": "/AgenticGoKitDocs/guides/setup/mcp-tools/index.html"
  },
  {
    "breadcrumb": "content \u003e reference",
    "content": "Memory API Persistent storage, RAG, and knowledge management\nThis document covers AgenticGoKit’s Memory API, which enables agents to store and retrieve information persistently. The Memory system is essential for building agents with long-term memory, knowledge bases, and RAG (Retrieval-Augmented Generation) capabilities.\n📋 Core Concepts Memory Interface The core interface for agent memory systems:\ntype Memory interface { // Store stores a memory with optional metadata Store(ctx context.Context, content string, metadata map[string]interface{}) (string, error) // Search finds memories similar to the query Search(ctx context.Context, query string, limit int, minScore float64) ([]MemorySearchResult, error) // Get retrieves a specific memory by ID Get(ctx context.Context, id string) (*MemorySearchResult, error) // Delete removes a memory by ID Delete(ctx context.Context, id string) error }\r🚀 Basic Usage Creating Memory System // Create memory configuration config := agentflow.AgentMemoryConfig{ Provider: \"pgvector\", Connection: \"postgres://user:password@localhost:5432/agentflow\", Dimensions: 1536, Embedding: agentflow.EmbeddingConfig{ Provider: \"openai\", APIKey: \"your-api-key\", Model: \"text-embedding-3-small\", }, } // Initialize memory memory, err := agentflow.NewMemory(config) if err != nil { log.Fatal(err) } defer memory.Close()\rFor complete documentation including RAG operations, document ingestion, and all memory providers, see the Agent API reference.",
    "description": "Memory API Persistent storage, RAG, and knowledge management\nThis document covers AgenticGoKit’s Memory API, which enables agents to store and retrieve information persistently. The Memory system is essential for building agents with long-term memory, knowledge bases, and RAG (Retrieval-Augmented Generation) capabilities.\n📋 Core Concepts Memory Interface The core interface for agent memory systems:\ntype Memory interface { // Store stores a memory with optional metadata Store(ctx context.Context, content string, metadata map[string]interface{}) (string, error) // Search finds memories similar to the query Search(ctx context.Context, query string, limit int, minScore float64) ([]MemorySearchResult, error) // Get retrieves a specific memory by ID Get(ctx context.Context, id string) (*MemorySearchResult, error) // Delete removes a memory by ID Delete(ctx context.Context, id string) error }\r🚀 Basic Usage Creating Memory System // Create memory configuration config := agentflow.AgentMemoryConfig{ Provider: \"pgvector\", Connection: \"postgres://user:password@localhost:5432/agentflow\", Dimensions: 1536, Embedding: agentflow.EmbeddingConfig{ Provider: \"openai\", APIKey: \"your-api-key\", Model: \"text-embedding-3-small\", }, } // Initialize memory memory, err := agentflow.NewMemory(config) if err != nil { log.Fatal(err) } defer memory.Close()\rFor complete documentation including RAG operations, document ingestion, and all memory providers, see the Agent API reference.",
    "tags": [],
    "title": "memory",
    "uri": "/AgenticGoKitDocs/reference/api/memory/index.html"
  },
  {
    "breadcrumb": "content \u003e guides",
    "content": "AgenticGoKit Memory System - Complete Implementation Guide 🎯 Overview AgenticGoKit provides a powerful, production-ready memory system that enables agents to maintain persistent context, conversation history, and knowledge bases. The system supports multiple storage backends, RAG (Retrieval-Augmented Generation) capabilities, and advanced features like vector embeddings, batch operations, and intelligent retry logic.\n📚 Table of Contents Quick Start Core Concepts Configuration Memory Providers API Reference RAG (Retrieval-Augmented Generation) Advanced Features Examples Performance \u0026 Optimization Troubleshooting 🚀 Quick Start 1. Basic Setup package main import ( \"context\" \"log\" \"github.com/kunalkushwaha/agenticgokit/core\" ) func main() { // Create memory configuration config := core.AgentMemoryConfig{ Provider: \"memory\", // In-memory for development Connection: \"memory\", Dimensions: 1536, Embedding: core.EmbeddingConfig{ Provider: \"dummy\", // For testing Model: \"text-embedding-3-small\", }, } // Create memory provider memory, err := core.NewMemory(config) if err != nil { log.Fatal(err) } defer memory.Close() // Create context with session ctx := memory.SetSession(context.Background(), \"user-123\") // Store a memory err = memory.Store(ctx, \"I love programming in Go\", \"programming\", \"preference\") if err != nil { log.Fatal(err) } // Query memories results, err := memory.Query(ctx, \"programming languages\", 5) if err != nil { log.Fatal(err) } for _, result := range results { fmt.Printf(\"Content: %s, Score: %.2f\\n\", result.Content, result.Score) } }\r2. Configuration File Setup Create an agentflow.toml configuration file:\n[agent_memory] # Basic settings provider = \"pgvector\" connection = \"postgres://user:password@localhost:5432/agentflow\" max_results = 10 dimensions = 1536 auto_embed = true # RAG settings enable_knowledge_base = true knowledge_max_results = 20 knowledge_score_threshold = 0.7 enable_rag = true rag_max_context_tokens = 4000 rag_personal_weight = 0.3 rag_knowledge_weight = 0.7 rag_include_sources = true # Embedding configuration [agent_memory.embedding] provider = \"openai\" api_key = \"${OPENAI_API_KEY}\" model = \"text-embedding-3-small\" max_batch_size = 50 timeout_seconds = 30\r🧠 Core Concepts Memory Interface The unified Memory interface provides all memory operations:\ntype Memory interface { // Personal memory operations Store(ctx context.Context, content string, tags ...string) error Query(ctx context.Context, query string, limit ...int) ([]Result, error) // Key-value storage Remember(ctx context.Context, key string, value any) error Recall(ctx context.Context, key string) (any, error) // Chat history AddMessage(ctx context.Context, role, content string) error GetHistory(ctx context.Context, limit ...int) ([]Message, error) // Knowledge base (RAG) IngestDocument(ctx context.Context, doc Document) error IngestDocuments(ctx context.Context, docs []Document) error SearchKnowledge(ctx context.Context, query string, options ...SearchOption) ([]KnowledgeResult, error) SearchAll(ctx context.Context, query string, options ...SearchOption) (*HybridResult, error) BuildContext(ctx context.Context, query string, options ...ContextOption) (*RAGContext, error) // Session management NewSession() string SetSession(ctx context.Context, sessionID string) context.Context ClearSession(ctx context.Context) error Close() error }\rSession Management All memory operations are session-scoped, providing complete isolation between different users or conversation threads:\n// Create new session sessionID := memory.NewSession() ctx := memory.SetSession(context.Background(), sessionID) // Different sessions see different data ctx1 := memory.SetSession(context.Background(), \"user-1\") ctx2 := memory.SetSession(context.Background(), \"user-2\") memory.Store(ctx1, \"I like coffee\", \"preference\") memory.Store(ctx2, \"I like tea\", \"preference\") // Queries are isolated by session results1, _ := memory.Query(ctx1, \"beverages\", 5) // Returns coffee preference results2, _ := memory.Query(ctx2, \"beverages\", 5) // Returns tea preference\rData Types Personal Memory Result type Result struct { Content string `json:\"content\"` Score float32 `json:\"score\"` Tags []string `json:\"tags\"` CreatedAt time.Time `json:\"created_at\"` }\rKnowledge Base Result type KnowledgeResult struct { Content string `json:\"content\"` Score float32 `json:\"score\"` Source string `json:\"source\"` Title string `json:\"title\"` DocumentID string `json:\"document_id\"` Metadata map[string]any `json:\"metadata\"` Tags []string `json:\"tags\"` CreatedAt time.Time `json:\"created_at\"` ChunkIndex int `json:\"chunk_index\"` }\rDocument Structure type Document struct { ID string `json:\"id\"` Title string `json:\"title\"` Content string `json:\"content\"` Source string `json:\"source\"` Type DocumentType `json:\"type\"` Metadata map[string]any `json:\"metadata\"` Tags []string `json:\"tags\"` CreatedAt time.Time `json:\"created_at\"` UpdatedAt time.Time `json:\"updated_at\"` ChunkIndex int `json:\"chunk_index\"` ChunkTotal int `json:\"chunk_total\"` }\r⚙️ Configuration Complete Configuration Reference [agent_memory] # Provider Selection provider = \"pgvector\" # Options: memory, pgvector, weaviate connection = \"postgres://user:password@localhost:5432/agentflow\" # Core Settings max_results = 10 # Maximum results for personal memory queries dimensions = 1536 # Vector embedding dimensions auto_embed = true # Automatically generate embeddings # Knowledge Base Settings enable_knowledge_base = true # Enable document ingestion and search knowledge_max_results = 20 # Maximum results from knowledge base knowledge_score_threshold = 0.7 # Minimum relevance score (0.0-1.0) chunk_size = 1000 # Document chunk size in characters chunk_overlap = 200 # Overlap between chunks in characters # RAG Context Assembly enable_rag = true # Enable RAG context building rag_max_context_tokens = 4000 # Maximum tokens in assembled context rag_personal_weight = 0.3 # Weight for personal memory (0.0-1.0) rag_knowledge_weight = 0.7 # Weight for knowledge base (0.0-1.0) rag_include_sources = true # Include source attribution in context # Embedding Service Configuration [agent_memory.embedding] provider = \"openai\" # Options: openai, azure, ollama, dummy api_key = \"${OPENAI_API_KEY}\" # API key (supports environment variables) model = \"text-embedding-3-small\" # Embedding model base_url = \"\" # Custom base URL (for local/custom endpoints) max_batch_size = 50 # Maximum items per batch request timeout_seconds = 30 # Request timeout cache_embeddings = true # Cache embeddings for repeated content # Advanced Settings (Optional) [agent_memory.advanced] retry_max_attempts = 3 # Maximum retry attempts for failed operations retry_base_delay = \"100ms\" # Base delay between retries retry_max_delay = \"5s\" # Maximum delay between retries connection_pool_size = 25 # Database connection pool size health_check_interval = \"1m\" # Health check interval for connections\rEnvironment Variables # Database connection export AGENTFLOW_DB_URL=\"postgres://user:password@localhost:5432/agentflow\" # Embedding service export OPENAI_API_KEY=\"your-openai-api-key\" export AZURE_OPENAI_ENDPOINT=\"https://your-resource.openai.azure.com/\" export AZURE_OPENAI_API_KEY=\"your-azure-api-key\" # Optional: Custom configuration file export AGENTFLOW_CONFIG_PATH=\"/path/to/agentflow.toml\"\r💾 Memory Providers 1. In-Memory Provider (memory) Best for: Development, testing, temporary sessions\nconfig := core.AgentMemoryConfig{ Provider: \"memory\", Connection: \"memory\", Dimensions: 1536, }\rFeatures:\n✅ All memory operations ✅ Session isolation ✅ RAG capabilities ❌ No persistence (data lost on restart) ❌ No scaling across instances 2. PostgreSQL + pgvector (pgvector) Best for: Production deployments, persistent storage\nconfig := core.AgentMemoryConfig{ Provider: \"pgvector\", Connection: \"postgres://user:password@localhost:5432/agentflow\", Dimensions: 1536, }\rFeatures:\n✅ Full persistence ✅ Excellent performance (~45ms queries) ✅ Enhanced retry logic with exponential backoff ✅ Advanced connection pooling ✅ Batch operations for efficiency ✅ ACID transactions ✅ Vector similarity search with indexing Setup Requirements:\n-- Install pgvector extension CREATE EXTENSION IF NOT EXISTS vector; -- Tables are created automatically by AgentFlow\r3. Weaviate (weaviate) Best for: Large-scale vector operations, advanced search\nconfig := core.AgentMemoryConfig{ Provider: \"weaviate\", Connection: \"http://localhost:8080\", Dimensions: 1536, }\rNote: Currently in development. Basic implementation available.\n📖 API Reference Personal Memory Operations Store Memory // Store with tags err := memory.Store(ctx, \"I prefer dark roast coffee\", \"preference\", \"coffee\") // Store without tags err := memory.Store(ctx, \"Meeting scheduled for tomorrow\")\rQuery Memory // Basic query results, err := memory.Query(ctx, \"coffee preferences\", 5) // Query with custom limit results, err := memory.Query(ctx, \"meetings\") // Uses default limit from config\rKey-Value Storage Remember/Recall // Store structured data err := memory.Remember(ctx, \"user_preferences\", map[string]interface{}{ \"theme\": \"dark\", \"language\": \"en\", \"timezone\": \"UTC\", }) // Retrieve data prefs, err := memory.Recall(ctx, \"user_preferences\") if prefs != nil { preferences := prefs.(map[string]interface{}) theme := preferences[\"theme\"].(string) }\rChat History Add Messages // Add user message err := memory.AddMessage(ctx, \"user\", \"How do I configure memory in AgentFlow?\") // Add assistant response err := memory.AddMessage(ctx, \"assistant\", \"You can configure memory using the agent_memory section in your TOML file...\")\rGet History // Get all history history, err := memory.GetHistory(ctx) // Get recent N messages recent, err := memory.GetHistory(ctx, 10)\rKnowledge Base Operations Document Ingestion // Single document doc := core.Document{ ID: \"guide-001\", Title: \"AgentFlow Memory Guide\", Content: \"AgentFlow provides a powerful memory system...\", Source: \"docs/memory-guide.md\", Type: core.DocumentTypeText, Tags: []string{\"documentation\", \"memory\"}, Metadata: map[string]any{ \"author\": \"AgentFlow Team\", \"version\": \"1.0\", }, } err := memory.IngestDocument(ctx, doc) // Multiple documents (more efficient) docs := []core.Document{doc1, doc2, doc3} err := memory.IngestDocuments(ctx, docs)\rKnowledge Search // Basic search results, err := memory.SearchKnowledge(ctx, \"memory configuration\") // Advanced search with options results, err := memory.SearchKnowledge(ctx, \"vector databases\", core.WithScoreThreshold(0.8), core.WithSources([]string{\"docs/\"}), core.WithTags([]string{\"database\", \"vector\"}), core.WithLimit(10), )\rAdvanced Search Options // Search options type SearchOption func(*SearchConfig) func WithLimit(limit int) SearchOption func WithScoreThreshold(threshold float32) SearchOption func WithSources(sources []string) SearchOption func WithTags(tags []string) SearchOption func WithDocumentTypes(types []DocumentType) SearchOption func WithDateRange(start, end time.Time) SearchOption func WithIncludePersonal(include bool) SearchOption func WithIncludeKnowledge(include bool) SearchOption\r🔄 RAG (Retrieval-Augmented Generation) Hybrid Search Combine personal memory and knowledge base searches:\n// Hybrid search across both personal memory and knowledge base result, err := memory.SearchAll(ctx, \"database best practices\") fmt.Printf(\"Personal memories: %d\\n\", len(result.PersonalMemory)) fmt.Printf(\"Knowledge results: %d\\n\", len(result.Knowledge)) fmt.Printf(\"Total results: %d\\n\", result.TotalResults) fmt.Printf(\"Search time: %v\\n\", result.SearchTime)\rRAG Context Building Automatically assemble context for LLM prompts:\n// Build RAG context ragContext, err := memory.BuildContext(ctx, \"How do I optimize database performance?\") // Use in LLM prompt prompt := fmt.Sprintf(` Context: %s Question: %s Please provide a comprehensive answer based on the context above. `, ragContext.ContextText, ragContext.Query) // ragContext contains: // - PersonalMemory: Relevant personal memories // - Knowledge: Relevant knowledge base entries // - ChatHistory: Recent conversation // - ContextText: Formatted text ready for LLM // - Sources: List of source documents // - TokenCount: Estimated token count\rContext Configuration // Configure context building ragContext, err := memory.BuildContext(ctx, \"database question\", core.WithMaxTokens(3000), core.WithPersonalWeight(0.4), // 40% personal memory core.WithKnowledgeWeight(0.6), // 60% knowledge base core.WithHistoryLimit(5), // Include last 5 messages core.WithIncludeSources(true), // Include source attribution )\r🚀 Advanced Features Batch Operations (PgVectorProvider) For improved performance with large datasets:\n// Cast to access enhanced features if provider, ok := memory.(*core.PgVectorProvider); ok { // Batch store multiple memories batchRequests := []core.BatchStoreRequest{ {Content: \"First item\", Tags: []string{\"batch\", \"test\"}}, {Content: \"Second item\", Tags: []string{\"batch\", \"test\"}}, {Content: \"Third item\", Tags: []string{\"batch\", \"test\"}}, } err := provider.BatchStore(ctx, batchRequests) // Batch ingest documents docs := []core.Document{doc1, doc2, doc3} err := provider.BatchIngestDocuments(ctx, docs) }\rRetry Logic and Error Handling The PgVectorProvider includes sophisticated retry logic:\n// Automatic retry with exponential backoff // - 3 retry attempts by default // - 100ms → 200ms → 400ms → 5s max delay // - Intelligent error classification // - Context cancellation support // Configure retry behavior retryConfig := core.DefaultRetryConfig() retryConfig.MaxRetries = 5 retryConfig.BackoffDuration = 200 * time.Millisecond retryConfig.MaxBackoff = 10 * time.Second\rConnection Pooling Optimized database connections:\n// PgVectorProvider automatically configures: // - MaxConns: 25 connections // - MinConns: 5 connections // - Connection lifetime: 1 hour // - Health checks: every minute // - Statement timeouts: 30 seconds\rSession Management // Create new session sessionID := memory.NewSession() // Set session context ctx := memory.SetSession(context.Background(), sessionID) // Get current session ID currentSession := core.GetSessionID(ctx) // Clear all data for session err := memory.ClearSession(ctx)\r💡 Examples Example 1: Personal Assistant with Memory package main import ( \"context\" \"fmt\" \"log\" \"github.com/kunalkushwaha/agentflow/core\" ) func personalAssistantExample() { // Setup memory config := core.AgentMemoryConfig{ Provider: \"pgvector\", Connection: \"postgres://user:password@localhost:5432/agentflow\", Dimensions: 1536, Embedding: core.EmbeddingConfig{ Provider: \"openai\", APIKey: \"your-api-key\", Model: \"text-embedding-3-small\", }, } memory, err := core.NewMemory(config) if err != nil { log.Fatal(err) } defer memory.Close() // User session ctx := memory.SetSession(context.Background(), \"alice-123\") // Store user preferences memory.Store(ctx, \"I prefer morning meetings\", \"scheduling\", \"preference\") memory.Store(ctx, \"I work in Pacific Time Zone\", \"timezone\", \"preference\") memory.Remember(ctx, \"notification_preferences\", map[string]interface{}{ \"email\": true, \"slack\": false, \"phone\": false, }) // User asks a question memory.AddMessage(ctx, \"user\", \"Schedule a meeting for me\") // Query relevant memories results, _ := memory.Query(ctx, \"scheduling preferences\", 3) // Build context for LLM ragContext, _ := memory.BuildContext(ctx, \"schedule meeting preferences\") // Generate response using LLM with context response := \"Based on your preferences for morning meetings and Pacific Time Zone...\" // Store assistant response memory.AddMessage(ctx, \"assistant\", response) fmt.Println(\"Personal assistant conversation stored in memory!\") }\rExample 2: Document-Based Q\u0026A System func documentQAExample() { memory, err := core.NewMemory(config) if err != nil { log.Fatal(err) } defer memory.Close() ctx := context.Background() // Ingest documentation docs := []core.Document{ { ID: \"memory-guide\", Title: \"Memory System Guide\", Content: \"AgentFlow memory system provides persistent storage...\", Source: \"docs/memory.md\", Type: core.DocumentTypeText, Tags: []string{\"documentation\", \"memory\"}, }, { ID: \"config-guide\", Title: \"Configuration Guide\", Content: \"Configure AgentFlow using TOML files...\", Source: \"docs/config.md\", Type: core.DocumentTypeText, Tags: []string{\"documentation\", \"configuration\"}, }, } err = memory.IngestDocuments(ctx, docs) if err != nil { log.Fatal(err) } // Answer user questions using knowledge base questions := []string{ \"How do I configure memory?\", \"What providers are available?\", \"How does RAG work?\", } for _, question := range questions { // Build RAG context ragContext, err := memory.BuildContext(ctx, question) if err != nil { log.Printf(\"Error building context: %v\", err) continue } fmt.Printf(\"Question: %s\\n\", question) fmt.Printf(\"Context (%d tokens):\\n%s\\n\", ragContext.TokenCount, ragContext.ContextText) fmt.Printf(\"Sources: %v\\n\\n\", ragContext.Sources) } }\rExample 3: Multi-User Chat Application func multiUserChatExample() { memory, err := core.NewMemory(config) if err != nil { log.Fatal(err) } defer memory.Close() // Different users with isolated sessions users := map[string]string{ \"alice\": memory.NewSession(), \"bob\": memory.NewSession(), \"charlie\": memory.NewSession(), } // Each user stores different preferences for username, sessionID := range users { userCtx := memory.SetSession(context.Background(), sessionID) memory.Store(userCtx, fmt.Sprintf(\"I am %s\", username), \"identity\") memory.Remember(userCtx, \"username\", username) // Chat history is isolated per user memory.AddMessage(userCtx, \"user\", \"Hello, I'm \"+username) memory.AddMessage(userCtx, \"assistant\", \"Hello \"+username+\"! How can I help you?\") } // Query each user's isolated data for username, sessionID := range users { userCtx := memory.SetSession(context.Background(), sessionID) results, _ := memory.Query(userCtx, \"identity\", 1) storedUsername, _ := memory.Recall(userCtx, \"username\") history, _ := memory.GetHistory(userCtx) fmt.Printf(\"User: %s\\n\", username) fmt.Printf(\" Memory: %s\\n\", results[0].Content) fmt.Printf(\" Stored username: %s\\n\", storedUsername) fmt.Printf(\" Messages: %d\\n\\n\", len(history)) } }\r⚡ Performance \u0026 Optimization Performance Benchmarks Based on comprehensive testing:\nOperation Provider Performance Notes Single Store PgVector ~50ms With retry logic Batch Store (50 items) PgVector ~2.3s ~22 ops/sec Query (10 results) PgVector ~45ms With vector similarity Knowledge Search PgVector ~45ms Indexed search RAG Context Build PgVector ~90ms Full hybrid search Document Ingest PgVector ~150ms Per document Batch Document Ingest PgVector ~150ms Per 3 documents Optimization Tips 1. Use Batch Operations // Instead of multiple Store() calls for _, item := range items { memory.Store(ctx, item.Content, item.Tags...) // Slower } // Use batch operations (PgVectorProvider) if provider, ok := memory.(*core.PgVectorProvider); ok { provider.BatchStore(ctx, batchRequests) // Much faster }\r2. Configure Appropriate Limits [agent_memory] max_results = 10 # Don't fetch more than needed knowledge_max_results = 20 # Limit knowledge base results rag_max_context_tokens = 4000 # Balance context size vs performance\r3. Use Score Thresholds // Filter low-quality results results, err := memory.SearchKnowledge(ctx, \"query\", core.WithScoreThreshold(0.7), // Only high-relevance results )\r4. Embedding Service Optimization [agent_memory.embedding] max_batch_size = 50 # Batch embedding requests cache_embeddings = true # Cache repeated content timeout_seconds = 30 # Reasonable timeout\r5. Connection Pooling (PgVector) [agent_memory.advanced] connection_pool_size = 25 # Optimize for your workload health_check_interval = \"1m\" # Monitor connection health\rMemory Usage Guidelines Session Management // Clean up unused sessions periodically err := memory.ClearSession(ctx) // Removes all session data\rDocument Chunking [agent_memory] chunk_size = 1000 # Smaller chunks = more granular search chunk_overlap = 200 # Overlap preserves context across chunks\r🔧 Troubleshooting Common Issues 1. Connection Errors // Error: failed to ping database // Solution: Check connection string and database accessibility config := core.AgentMemoryConfig{ Provider: \"pgvector\", Connection: \"postgres://user:password@localhost:5432/agentflow?sslmode=disable\", }\r2. Embedding Service Errors // Error: failed to generate embedding // Solution: Verify API key and provider configuration config.Embedding = core.EmbeddingConfig{ Provider: \"openai\", APIKey: os.Getenv(\"OPENAI_API_KEY\"), // Ensure API key is set Model: \"text-embedding-3-small\", }\r3. Low Search Scores // Issue: All similarity scores are very low // Solution: Use appropriate score thresholds for your embedding provider // For dummy embeddings (testing) core.WithScoreThreshold(-0.1) // For real embeddings core.WithScoreThreshold(0.7)\r4. Performance Issues // Issue: Slow queries // Solutions: // 1. Reduce result limits results, _ := memory.Query(ctx, \"query\", 5) // Instead of 50 // 2. Use batch operations provider.BatchStore(ctx, requests) // Instead of individual stores // 3. Optimize embedding batch size config.Embedding.MaxBatchSize = 25 // Tune for your provider\rDebugging Tips Enable Debug Logging import \"log\" // Set up debug logging log.SetFlags(log.LstdFlags | log.Lshortfile)\rCheck Session Isolation // Verify session ID sessionID := core.GetSessionID(ctx) fmt.Printf(\"Current session: %s\\n\", sessionID) // Test with known session ctx = memory.SetSession(context.Background(), \"test-session\")\rValidate Configuration // Test basic memory operations err := memory.Store(ctx, \"test content\", \"test\") if err != nil { log.Printf(\"Store failed: %v\", err) } results, err := memory.Query(ctx, \"test\", 1) if err != nil { log.Printf(\"Query failed: %v\", err) } else { log.Printf(\"Found %d results\", len(results)) }\rDatabase Setup (PgVector) PostgreSQL with pgvector -- Install pgvector extension CREATE EXTENSION IF NOT EXISTS vector; -- Verify installation SELECT * FROM pg_extension WHERE extname = 'vector'; -- Check tables (created automatically by AgentFlow) \\dt -- Tables created: -- - personal_memory (session-scoped memories) -- - key_value_store (session-scoped key-value pairs) -- - chat_history (conversation messages) -- - documents (document metadata) -- - knowledge_base (document embeddings)\rDocker Setup # Run PostgreSQL with pgvector docker run -d \\ --name agentflow-postgres \\ -e POSTGRES_DB=agentflow \\ -e POSTGRES_USER=agentflow \\ -e POSTGRES_PASSWORD=password \\ -p 5432:5432 \\ pgvector/pgvector:pg16 # Test connection psql \"postgres://agentflow:password@localhost:5432/agentflow\"\rEnvironment Variables Reference # Required export AGENTFLOW_DB_URL=\"postgres://user:password@localhost:5432/agentflow\" export OPENAI_API_KEY=\"your-openai-api-key\" # Optional export AGENTFLOW_CONFIG_PATH=\"/path/to/agentflow.toml\" export AGENTFLOW_LOG_LEVEL=\"debug\" export AGENTFLOW_EMBEDDING_CACHE=\"true\" # Azure OpenAI (if using Azure) export AZURE_OPENAI_ENDPOINT=\"https://your-resource.openai.azure.com/\" export AZURE_OPENAI_API_KEY=\"your-azure-api-key\" export AZURE_OPENAI_DEPLOYMENT_NAME=\"your-deployment-name\" # Testing export AGENTFLOW_TEST_DB_URL=\"postgres://agentflow:password@localhost:5432/agentflow_test\"\r📊 Monitoring \u0026 Metrics Built-in Metrics AgentFlow memory operations provide timing and performance metrics:\n// Hybrid search includes timing result, err := memory.SearchAll(ctx, \"query\") fmt.Printf(\"Search took: %v\\n\", result.SearchTime) fmt.Printf(\"Total results: %d\\n\", result.TotalResults) // RAG context includes token counting ragContext, err := memory.BuildContext(ctx, \"query\") fmt.Printf(\"Context tokens: %d\\n\", ragContext.TokenCount)\rHealth Checks // Test memory health func checkMemoryHealth(memory core.Memory) error { ctx := memory.SetSession(context.Background(), \"health-check\") // Test basic operations err := memory.Store(ctx, \"health check\", \"test\") if err != nil { return fmt.Errorf(\"store failed: %w\", err) } results, err := memory.Query(ctx, \"health\", 1) if err != nil { return fmt.Errorf(\"query failed: %w\", err) } if len(results) == 0 { return fmt.Errorf(\"no results returned\") } // Cleanup memory.ClearSession(ctx) return nil }\r🎯 Summary The AgentFlow Memory System provides:\n✅ Unified Interface: Single API for all memory operations\n✅ Multiple Providers: In-memory, PostgreSQL+pgvector, Weaviate\n✅ Session Isolation: Complete data separation between users/sessions\n✅ RAG Capabilities: Document ingestion, knowledge search, context building\n✅ Advanced Features: Batch operations, retry logic, connection pooling\n✅ Production Ready: Comprehensive error handling, performance optimization\n✅ Easy Configuration: TOML-based configuration with environment variable support\n✅ Comprehensive Testing: Full test coverage with integration tests\nThe system is designed for production use with enterprise-grade features while maintaining simplicity for development and testing scenarios.\nFor more examples and advanced usage, see the /examples directory in the AgentFlow repository.\nFor detailed RAG configuration options, see the RAG Configuration Guide.",
    "description": "AgenticGoKit Memory System - Complete Implementation Guide 🎯 Overview AgenticGoKit provides a powerful, production-ready memory system that enables agents to maintain persistent context, conversation history, and knowledge bases. The system supports multiple storage backends, RAG (Retrieval-Augmented Generation) capabilities, and advanced features like vector embeddings, batch operations, and intelligent retry logic.\n📚 Table of Contents Quick Start Core Concepts Configuration Memory Providers API Reference RAG (Retrieval-Augmented Generation) Advanced Features Examples Performance \u0026 Optimization Troubleshooting 🚀 Quick Start 1. Basic Setup package main import ( \"context\" \"log\" \"github.com/kunalkushwaha/agenticgokit/core\" ) func main() { // Create memory configuration config := core.AgentMemoryConfig{ Provider: \"memory\", // In-memory for development Connection: \"memory\", Dimensions: 1536, Embedding: core.EmbeddingConfig{ Provider: \"dummy\", // For testing Model: \"text-embedding-3-small\", }, } // Create memory provider memory, err := core.NewMemory(config) if err != nil { log.Fatal(err) } defer memory.Close() // Create context with session ctx := memory.SetSession(context.Background(), \"user-123\") // Store a memory err = memory.Store(ctx, \"I love programming in Go\", \"programming\", \"preference\") if err != nil { log.Fatal(err) } // Query memories results, err := memory.Query(ctx, \"programming languages\", 5) if err != nil { log.Fatal(err) } for _, result := range results { fmt.Printf(\"Content: %s, Score: %.2f\\n\", result.Content, result.Score) } }\r2. Configuration File Setup Create an agentflow.toml configuration file:",
    "tags": [],
    "title": "Memory",
    "uri": "/AgenticGoKitDocs/guides/memory/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials \u003e getting-started",
    "content": "Memory and RAG Tutorial (15 minutes) Overview Learn how to add persistent memory and knowledge systems to your agents using RAG (Retrieval-Augmented Generation). You’ll set up vector databases, implement document ingestion, and create knowledge-aware agents.\nPrerequisites Complete the Multi-Agent Collaboration tutorial Docker installed (for database setup) Basic understanding of vector databases Learning Objectives By the end of this tutorial, you’ll understand:\nHow to set up vector databases for agent memory Document ingestion and chunking strategies RAG implementation for knowledge-aware responses Hybrid search combining semantic and keyword matching What You’ll Build A knowledge-aware agent system that can:\nIngest documents into a vector database Search knowledge using semantic similarity Generate responses enhanced with retrieved context Remember conversations across sessions Part 1: Basic Memory Setup (5 minutes) Start with in-memory storage to understand the concepts.\nCreate a Memory-Enabled Project # Create project with basic memory agentcli create knowledge-agent --memory-enabled --agents 2 cd knowledge-agent\rUnderstanding Memory Configuration The generated agentflow.toml includes memory settings:\n[agent_memory] provider = \"memory\" # In-memory storage (temporary) auto_embed = true # Automatically create embeddings max_results = 10 # Maximum search results dimensions = 1536 # Embedding dimensions [agent_memory.embedding] provider = \"ollama\" # Local embeddings (recommended) model = \"nomic-embed-text:latest\"\rTest Basic Memory # Make sure Ollama is running with the embedding model ollama pull nomic-embed-text:latest # Set your API key and run export OPENAI_API_KEY=your-api-key-here go run main.go\rThe agents now have basic memory capabilities, but data is lost when the program stops.\nPart 2: Persistent Memory with PostgreSQL (5 minutes) Set up persistent memory using PostgreSQL with pgvector extension.\nCreate a Persistent Memory Project # Create project with PostgreSQL memory agentcli create persistent-agent --memory-enabled --memory-provider pgvector \\ --rag-enabled --agents 2 cd persistent-agent\rStart the Database The project includes a docker-compose.yml file:\n# Start PostgreSQL with pgvector extension docker-compose up -d # Wait for database to be ready (about 30 seconds) docker-compose logs -f postgres\rUnderstanding Persistent Configuration [agent_memory] provider = \"pgvector\" # PostgreSQL with vector extension connection = \"postgres://agentflow:password@localhost:5432/agentflow?sslmode=disable\" enable_knowledge_base = true # Enable document storage enable_rag = true # Enable RAG functionality [agent_memory.documents] supported_types = [\"pdf\", \"txt\", \"md\", \"web\"] auto_chunk = true # Automatically chunk documents chunk_size = 1000 # Tokens per chunk chunk_overlap = 200 # Overlap between chunks [agent_memory.search] hybrid_search = true # Combine semantic + keyword search keyword_weight = 0.3 # 30% keyword, 70% semantic semantic_weight = 0.7\rTest Persistent Memory export OPENAI_API_KEY=your-api-key-here go run main.go\rNow your agents have persistent memory that survives restarts!\nPart 3: RAG Implementation (5 minutes) Implement full RAG (Retrieval-Augmented Generation) with document ingestion.\nCreate a RAG-Enabled System # Create comprehensive RAG system agentcli create rag-system --memory-enabled --memory-provider pgvector \\ --rag-enabled --hybrid-search --session-memory --agents 3 cd rag-system\rStart the Enhanced Database docker-compose up -d\rUnderstanding RAG Configuration [agent_memory] provider = \"pgvector\" enable_rag = true rag_max_context_tokens = 4000 # Max context for RAG rag_personal_weight = 0.3 # Weight for personal memory rag_knowledge_weight = 0.7 # Weight for knowledge base [agent_memory.documents] enable_metadata_extraction = true # Extract document metadata enable_url_scraping = true # Support web URLs max_file_size = \"10MB\" # Maximum document size [agent_memory.search] hybrid_search = true # Semantic + keyword search top_k = 5 # Top results to retrieve score_threshold = 0.7 # Minimum similarity score\rAdd Documents to the Knowledge Base Create a sample document:\n# Create a sample knowledge document cat \u003e knowledge.md \u003c\u003c 'EOF' # AgenticGoKit Knowledge Base ## Multi-Agent Systems AgenticGoKit supports multiple orchestration patterns: - Collaborative: Agents work in parallel - Sequential: Agents work in pipeline - Mixed: Combination of both patterns ## Memory Systems AgenticGoKit provides several memory providers: - In-memory: Fast but temporary - PostgreSQL: Persistent with pgvector - Weaviate: Dedicated vector database ## Tool Integration Agents can use external tools through MCP: - Web search capabilities - File operations - Custom API integrations EOF\rTest RAG System export OPENAI_API_KEY=your-api-key-here go run main.go\rThe system will:\nIngest the knowledge document Chunk it into searchable pieces Embed chunks using vector embeddings Retrieve relevant context for queries Generate enhanced responses using RAG Query the Knowledge Base The agents can now answer questions using the ingested knowledge:\n“What orchestration patterns does AgenticGoKit support?” “How do I set up persistent memory?” “What tools can agents use?” Memory Providers Comparison Provider Persistence Performance Use Case memory ❌ Temporary ⚡ Fastest Development, testing pgvector ✅ Persistent 🚀 Fast Production, SQL integration weaviate ✅ Persistent 🚀 Fast Advanced vector operations RAG Configuration Options Document Processing # Customize document processing agentcli create doc-system --memory-enabled --memory-provider pgvector \\ --rag-enabled --rag-chunk-size 512 --rag-overlap 50 --rag-top-k 3\rSearch Configuration # Fine-tune search behavior agentcli create search-system --memory-enabled --memory-provider pgvector \\ --rag-enabled --hybrid-search --rag-score-threshold 0.8\rEmbedding Models # Use OpenAI embeddings (requires API key) agentcli create openai-system --memory-enabled --memory-provider pgvector \\ --embedding-provider openai --embedding-model text-embedding-ada-002 # Use local Ollama embeddings (recommended) agentcli create local-system --memory-enabled --memory-provider pgvector \\ --embedding-provider ollama --embedding-model nomic-embed-text:latest\rAdvanced Memory Features Session Memory # Enable session-based memory isolation agentcli create session-system --memory-enabled --session-memory\rSession memory keeps conversations separate for different users or contexts.\nHybrid Search # Configure hybrid search weights agentcli create hybrid-system --memory-enabled --hybrid-search\rHybrid search combines:\nSemantic search: Understanding meaning and context Keyword search: Exact term matching Troubleshooting Common Issues Database connection failed:\n# Check if PostgreSQL is running docker-compose ps # Check logs docker-compose logs postgres # Restart if needed docker-compose restart postgres\rEmbedding model not found:\n# For Ollama embeddings ollama pull nomic-embed-text:latest ollama list # Verify model is installed # Check Ollama is running curl http://localhost:11434/api/tags\rRAG not working:\n# Verify documents are ingested # Check agentflow.toml configuration # Ensure embedding provider is working\rPerformance Issues Slow search:\nReduce rag_top_k value Increase score_threshold Use smaller embedding models High memory usage:\nReduce chunk_size Limit max_results Use pgvector instead of in-memory Memory System Architecture ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐\r│ Documents │───▶│ Chunking │───▶│ Embeddings │\r│ (PDF, MD, │ │ (1000 tokens) │ │ (Vector DB) │\r│ TXT, Web) │ │ │ │ │\r└─────────────────┘ └─────────────────┘ └─────────────────┘\r│\r▼\r┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐\r│ Agent │◀───│ RAG Context │◀───│ Similarity │\r│ Response │ │ Injection │ │ Search │\r└─────────────────┘ └─────────────────┘ └─────────────────┘\rNext Steps Now that your agents have memory and knowledge capabilities:\nAdd Tools: Learn Tool Integration to connect external services Go Production: Check Production Deployment for scaling Advanced Memory: Explore Memory System Tutorials for deep dives Key Takeaways Memory Providers: Choose based on persistence and performance needs RAG: Combines retrieval and generation for knowledge-aware responses Document Processing: Automatic chunking and embedding for searchability Hybrid Search: Best results combining semantic and keyword matching Session Memory: Isolate conversations for multi-user scenarios Further Reading Memory Systems Deep Dive - Advanced memory concepts Vector Databases Guide - Database comparison RAG Implementation - Advanced RAG patterns",
    "description": "Memory and RAG Tutorial (15 minutes) Overview Learn how to add persistent memory and knowledge systems to your agents using RAG (Retrieval-Augmented Generation). You’ll set up vector databases, implement document ingestion, and create knowledge-aware agents.\nPrerequisites Complete the Multi-Agent Collaboration tutorial Docker installed (for database setup) Basic understanding of vector databases Learning Objectives By the end of this tutorial, you’ll understand:\nHow to set up vector databases for agent memory Document ingestion and chunking strategies RAG implementation for knowledge-aware responses Hybrid search combining semantic and keyword matching What You’ll Build A knowledge-aware agent system that can:",
    "tags": [],
    "title": "memory-and-rag",
    "uri": "/AgenticGoKitDocs/tutorials/getting-started/memory-and-rag/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials \u003e memory-systems",
    "content": "Memory Optimization in AgenticGoKit Overview Memory optimization is crucial for building scalable, high-performance agent systems. This tutorial covers advanced techniques for optimizing memory systems in AgenticGoKit, including performance tuning, caching strategies, resource management, and scaling patterns.\nPrerequisites Understanding of Vector Databases Familiarity with RAG Implementation Knowledge of Knowledge Bases Basic understanding of database performance tuning Performance Optimization Strategies 1. Vector Database Optimization // Optimized pgvector configuration func createOptimizedPgvectorMemory() (core.Memory, error) { config := core.AgentMemoryConfig{ Provider: \"pgvector\", Connection: \"postgres://user:pass@localhost:5432/agentdb\", EnableRAG: true, Dimensions: 1536, Embedding: core.EmbeddingConfig{ Provider: \"openai\", Model: \"text-embedding-3-small\", APIKey: os.Getenv(\"OPENAI_API_KEY\"), Dimensions: 1536, BatchSize: 100, }, Options: map[string]interface{}{ // Connection pool settings \"max_connections\": 20, \"max_idle_connections\": 5, \"connection_timeout\": \"30s\", // Index optimization \"index_type\": \"hnsw\", \"hnsw_m\": 16, \"hnsw_ef_construction\": 64, // Memory settings \"work_mem\": \"256MB\", \"effective_cache_size\": \"4GB\", }, } return core.NewMemory(config) }\r2. Caching Strategies type OptimizedMemoryCache struct { queryCache *LRUCache embeddingCache *LRUCache resultCache *LRUCache config CacheConfig } type CacheConfig struct { QueryCacheSize int EmbeddingCacheSize int ResultCacheSize int TTL time.Duration } func NewOptimizedMemoryCache(config CacheConfig) *OptimizedMemoryCache { return \u0026OptimizedMemoryCache{ queryCache: NewLRUCache(config.QueryCacheSize, config.TTL), embeddingCache: NewLRUCache(config.EmbeddingCacheSize, config.TTL), resultCache: NewLRUCache(config.ResultCacheSize, config.TTL), config: config, } } func (omc *OptimizedMemoryCache) GetCachedResults(query string) ([]core.MemoryResult, bool) { if results, found := omc.queryCache.Get(query); found { return results.([]core.MemoryResult), true } return nil, false } func (omc *OptimizedMemoryCache) CacheResults(query string, results []core.MemoryResult) { omc.queryCache.Set(query, results) } func (omc *OptimizedMemoryCache) GetCachedEmbedding(text string) ([]float32, bool) { if embedding, found := omc.embeddingCache.Get(text); found { return embedding.([]float32), true } return nil, false } func (omc *OptimizedMemoryCache) CacheEmbedding(text string, embedding []float32) { omc.embeddingCache.Set(text, embedding) }\r3. Batch Processing Optimization type BatchProcessor struct { memory core.Memory batchSize int timeout time.Duration semaphore chan struct{} } func NewBatchProcessor(memory core.Memory, batchSize int, concurrency int) *BatchProcessor { return \u0026BatchProcessor{ memory: memory, batchSize: batchSize, timeout: 30 * time.Second, semaphore: make(chan struct{}, concurrency), } } func (bp *BatchProcessor) ProcessBatch(ctx context.Context, items []string) error { // Process items in batches for i := 0; i \u003c len(items); i += bp.batchSize { end := i + bp.batchSize if end \u003e len(items) { end = len(items) } batch := items[i:end] // Acquire semaphore for concurrency control select { case bp.semaphore \u003c- struct{}{}: go func(b []string) { defer func() { \u003c-bp.semaphore }() bp.processBatch(ctx, b) }(batch) case \u003c-ctx.Done(): return ctx.Err() } } return nil } func (bp *BatchProcessor) processBatch(ctx context.Context, batch []string) error { ctx, cancel := context.WithTimeout(ctx, bp.timeout) defer cancel() for _, item := range batch { err := bp.memory.Store(ctx, item, \"batch-item\") if err != nil { log.Printf(\"Failed to store batch item: %v\", err) } } return nil }\rResource Management 1. Connection Pool Optimization type OptimizedConnectionManager struct { pool *sql.DB metrics *ConnectionMetrics config PoolConfig } type PoolConfig struct { MaxOpenConns int MaxIdleConns int ConnMaxLifetime time.Duration ConnMaxIdleTime time.Duration } func NewOptimizedConnectionManager(dsn string, config PoolConfig) (*OptimizedConnectionManager, error) { db, err := sql.Open(\"postgres\", dsn) if err != nil { return nil, err } // Configure connection pool db.SetMaxOpenConns(config.MaxOpenConns) db.SetMaxIdleConns(config.MaxIdleConns) db.SetConnMaxLifetime(config.ConnMaxLifetime) db.SetConnMaxIdleTime(config.ConnMaxIdleTime) return \u0026OptimizedConnectionManager{ pool: db, metrics: NewConnectionMetrics(), config: config, }, nil } func (ocm *OptimizedConnectionManager) GetConnection(ctx context.Context) (*sql.Conn, error) { start := time.Now() conn, err := ocm.pool.Conn(ctx) if err != nil { ocm.metrics.RecordError() return nil, err } ocm.metrics.RecordAcquisition(time.Since(start)) return conn, nil }\r2. Memory Usage Monitoring type MemoryMonitor struct { thresholds MemoryThresholds alerts chan Alert interval time.Duration } type MemoryThresholds struct { WarningMB int64 CriticalMB int64 } type Alert struct { Level string Message string Time time.Time } func NewMemoryMonitor(thresholds MemoryThresholds) *MemoryMonitor { return \u0026MemoryMonitor{ thresholds: thresholds, alerts: make(chan Alert, 100), interval: 30 * time.Second, } } func (mm *MemoryMonitor) Start(ctx context.Context) { ticker := time.NewTicker(mm.interval) defer ticker.Stop() for { select { case \u003c-ctx.Done(): return case \u003c-ticker.C: mm.checkMemoryUsage() } } } func (mm *MemoryMonitor) checkMemoryUsage() { var m runtime.MemStats runtime.ReadMemStats(\u0026m) heapMB := int64(m.HeapAlloc / 1024 / 1024) if heapMB \u003e mm.thresholds.CriticalMB { mm.alerts \u003c- Alert{ Level: \"CRITICAL\", Message: fmt.Sprintf(\"Memory usage: %d MB\", heapMB), Time: time.Now(), } runtime.GC() // Force garbage collection } else if heapMB \u003e mm.thresholds.WarningMB { mm.alerts \u003c- Alert{ Level: \"WARNING\", Message: fmt.Sprintf(\"Memory usage: %d MB\", heapMB), Time: time.Now(), } } } func (mm *MemoryMonitor) GetAlerts() \u003c-chan Alert { return mm.alerts }\rPerformance Monitoring 1. Metrics Collection type PerformanceMetrics struct { searchLatency []time.Duration cacheHitRate float64 throughput int64 errorCount int64 mu sync.RWMutex } func NewPerformanceMetrics() *PerformanceMetrics { return \u0026PerformanceMetrics{ searchLatency: make([]time.Duration, 0, 1000), } } func (pm *PerformanceMetrics) RecordSearch(duration time.Duration, cached bool) { pm.mu.Lock() defer pm.mu.Unlock() pm.searchLatency = append(pm.searchLatency, duration) pm.throughput++ // Keep only recent measurements if len(pm.searchLatency) \u003e 1000 { pm.searchLatency = pm.searchLatency[len(pm.searchLatency)-1000:] } // Update cache hit rate if cached { pm.cacheHitRate = (pm.cacheHitRate + 1.0) / 2.0 } else { pm.cacheHitRate = pm.cacheHitRate / 2.0 } } func (pm *PerformanceMetrics) GetAverageLatency() time.Duration { pm.mu.RLock() defer pm.mu.RUnlock() if len(pm.searchLatency) == 0 { return 0 } var total time.Duration for _, latency := range pm.searchLatency { total += latency } return total / time.Duration(len(pm.searchLatency)) } func (pm *PerformanceMetrics) GetReport() map[string]interface{} { pm.mu.RLock() defer pm.mu.RUnlock() return map[string]interface{}{ \"avg_latency_ms\": pm.GetAverageLatency().Milliseconds(), \"cache_hit_rate\": pm.cacheHitRate, \"throughput\": pm.throughput, \"error_count\": pm.errorCount, } }\rBest Practices 1. Configuration Optimization Connection Pooling: Configure appropriate pool sizes based on workload Index Selection: Choose HNSW for better query performance, IVFFlat for faster indexing Memory Settings: Tune PostgreSQL memory settings for vector operations Batch Sizes: Optimize batch sizes for embedding API calls Cache Sizes: Size caches based on available memory and hit rate requirements 2. Performance Monitoring Latency Tracking: Monitor search and indexing latencies Resource Usage: Track memory, CPU, and disk usage Cache Performance: Monitor cache hit rates and effectiveness Error Rates: Track and alert on error rates Throughput: Monitor requests per second and concurrent users 3. Scaling Strategies Vertical Scaling: Increase memory and CPU for single-node performance Read Replicas: Use read replicas for read-heavy workloads Sharding: Distribute data across multiple nodes for large datasets Caching Layers: Implement multiple caching layers for frequently accessed data Load Balancing: Distribute requests across multiple instances Conclusion Memory optimization is essential for production-ready agent systems. Key takeaways:\nImplement comprehensive caching strategies at multiple levels Monitor resource usage and performance metrics continuously Optimize database configurations for vector operations Use appropriate scaling patterns based on workload characteristics Plan for growth and implement monitoring from the beginning Proper optimization ensures your agents can handle production workloads efficiently and cost-effectively.\nNext Steps Production Deployment - Deploy optimized systems Monitoring and Observability - Advanced monitoring Advanced Patterns - Learn scaling techniques Further Reading PostgreSQL Performance Tuning Vector Database Benchmarks Go Performance Optimization",
    "description": "Memory Optimization in AgenticGoKit Overview Memory optimization is crucial for building scalable, high-performance agent systems. This tutorial covers advanced techniques for optimizing memory systems in AgenticGoKit, including performance tuning, caching strategies, resource management, and scaling patterns.\nPrerequisites Understanding of Vector Databases Familiarity with RAG Implementation Knowledge of Knowledge Bases Basic understanding of database performance tuning Performance Optimization Strategies 1. Vector Database Optimization // Optimized pgvector configuration func createOptimizedPgvectorMemory() (core.Memory, error) { config := core.AgentMemoryConfig{ Provider: \"pgvector\", Connection: \"postgres://user:pass@localhost:5432/agentdb\", EnableRAG: true, Dimensions: 1536, Embedding: core.EmbeddingConfig{ Provider: \"openai\", Model: \"text-embedding-3-small\", APIKey: os.Getenv(\"OPENAI_API_KEY\"), Dimensions: 1536, BatchSize: 100, }, Options: map[string]interface{}{ // Connection pool settings \"max_connections\": 20, \"max_idle_connections\": 5, \"connection_timeout\": \"30s\", // Index optimization \"index_type\": \"hnsw\", \"hnsw_m\": 16, \"hnsw_ef_construction\": 64, // Memory settings \"work_mem\": \"256MB\", \"effective_cache_size\": \"4GB\", }, } return core.NewMemory(config) }\r2. Caching Strategies type OptimizedMemoryCache struct { queryCache *LRUCache embeddingCache *LRUCache resultCache *LRUCache config CacheConfig } type CacheConfig struct { QueryCacheSize int EmbeddingCacheSize int ResultCacheSize int TTL time.Duration } func NewOptimizedMemoryCache(config CacheConfig) *OptimizedMemoryCache { return \u0026OptimizedMemoryCache{ queryCache: NewLRUCache(config.QueryCacheSize, config.TTL), embeddingCache: NewLRUCache(config.EmbeddingCacheSize, config.TTL), resultCache: NewLRUCache(config.ResultCacheSize, config.TTL), config: config, } } func (omc *OptimizedMemoryCache) GetCachedResults(query string) ([]core.MemoryResult, bool) { if results, found := omc.queryCache.Get(query); found { return results.([]core.MemoryResult), true } return nil, false } func (omc *OptimizedMemoryCache) CacheResults(query string, results []core.MemoryResult) { omc.queryCache.Set(query, results) } func (omc *OptimizedMemoryCache) GetCachedEmbedding(text string) ([]float32, bool) { if embedding, found := omc.embeddingCache.Get(text); found { return embedding.([]float32), true } return nil, false } func (omc *OptimizedMemoryCache) CacheEmbedding(text string, embedding []float32) { omc.embeddingCache.Set(text, embedding) }\r3. Batch Processing Optimization type BatchProcessor struct { memory core.Memory batchSize int timeout time.Duration semaphore chan struct{} } func NewBatchProcessor(memory core.Memory, batchSize int, concurrency int) *BatchProcessor { return \u0026BatchProcessor{ memory: memory, batchSize: batchSize, timeout: 30 * time.Second, semaphore: make(chan struct{}, concurrency), } } func (bp *BatchProcessor) ProcessBatch(ctx context.Context, items []string) error { // Process items in batches for i := 0; i \u003c len(items); i += bp.batchSize { end := i + bp.batchSize if end \u003e len(items) { end = len(items) } batch := items[i:end] // Acquire semaphore for concurrency control select { case bp.semaphore \u003c- struct{}{}: go func(b []string) { defer func() { \u003c-bp.semaphore }() bp.processBatch(ctx, b) }(batch) case \u003c-ctx.Done(): return ctx.Err() } } return nil } func (bp *BatchProcessor) processBatch(ctx context.Context, batch []string) error { ctx, cancel := context.WithTimeout(ctx, bp.timeout) defer cancel() for _, item := range batch { err := bp.memory.Store(ctx, item, \"batch-item\") if err != nil { log.Printf(\"Failed to store batch item: %v\", err) } } return nil }\rResource Management 1. Connection Pool Optimization type OptimizedConnectionManager struct { pool *sql.DB metrics *ConnectionMetrics config PoolConfig } type PoolConfig struct { MaxOpenConns int MaxIdleConns int ConnMaxLifetime time.Duration ConnMaxIdleTime time.Duration } func NewOptimizedConnectionManager(dsn string, config PoolConfig) (*OptimizedConnectionManager, error) { db, err := sql.Open(\"postgres\", dsn) if err != nil { return nil, err } // Configure connection pool db.SetMaxOpenConns(config.MaxOpenConns) db.SetMaxIdleConns(config.MaxIdleConns) db.SetConnMaxLifetime(config.ConnMaxLifetime) db.SetConnMaxIdleTime(config.ConnMaxIdleTime) return \u0026OptimizedConnectionManager{ pool: db, metrics: NewConnectionMetrics(), config: config, }, nil } func (ocm *OptimizedConnectionManager) GetConnection(ctx context.Context) (*sql.Conn, error) { start := time.Now() conn, err := ocm.pool.Conn(ctx) if err != nil { ocm.metrics.RecordError() return nil, err } ocm.metrics.RecordAcquisition(time.Since(start)) return conn, nil }\r2. Memory Usage Monitoring type MemoryMonitor struct { thresholds MemoryThresholds alerts chan Alert interval time.Duration } type MemoryThresholds struct { WarningMB int64 CriticalMB int64 } type Alert struct { Level string Message string Time time.Time } func NewMemoryMonitor(thresholds MemoryThresholds) *MemoryMonitor { return \u0026MemoryMonitor{ thresholds: thresholds, alerts: make(chan Alert, 100), interval: 30 * time.Second, } } func (mm *MemoryMonitor) Start(ctx context.Context) { ticker := time.NewTicker(mm.interval) defer ticker.Stop() for { select { case \u003c-ctx.Done(): return case \u003c-ticker.C: mm.checkMemoryUsage() } } } func (mm *MemoryMonitor) checkMemoryUsage() { var m runtime.MemStats runtime.ReadMemStats(\u0026m) heapMB := int64(m.HeapAlloc / 1024 / 1024) if heapMB \u003e mm.thresholds.CriticalMB { mm.alerts \u003c- Alert{ Level: \"CRITICAL\", Message: fmt.Sprintf(\"Memory usage: %d MB\", heapMB), Time: time.Now(), } runtime.GC() // Force garbage collection } else if heapMB \u003e mm.thresholds.WarningMB { mm.alerts \u003c- Alert{ Level: \"WARNING\", Message: fmt.Sprintf(\"Memory usage: %d MB\", heapMB), Time: time.Now(), } } } func (mm *MemoryMonitor) GetAlerts() \u003c-chan Alert { return mm.alerts }\rPerformance Monitoring 1. Metrics Collection type PerformanceMetrics struct { searchLatency []time.Duration cacheHitRate float64 throughput int64 errorCount int64 mu sync.RWMutex } func NewPerformanceMetrics() *PerformanceMetrics { return \u0026PerformanceMetrics{ searchLatency: make([]time.Duration, 0, 1000), } } func (pm *PerformanceMetrics) RecordSearch(duration time.Duration, cached bool) { pm.mu.Lock() defer pm.mu.Unlock() pm.searchLatency = append(pm.searchLatency, duration) pm.throughput++ // Keep only recent measurements if len(pm.searchLatency) \u003e 1000 { pm.searchLatency = pm.searchLatency[len(pm.searchLatency)-1000:] } // Update cache hit rate if cached { pm.cacheHitRate = (pm.cacheHitRate + 1.0) / 2.0 } else { pm.cacheHitRate = pm.cacheHitRate / 2.0 } } func (pm *PerformanceMetrics) GetAverageLatency() time.Duration { pm.mu.RLock() defer pm.mu.RUnlock() if len(pm.searchLatency) == 0 { return 0 } var total time.Duration for _, latency := range pm.searchLatency { total += latency } return total / time.Duration(len(pm.searchLatency)) } func (pm *PerformanceMetrics) GetReport() map[string]interface{} { pm.mu.RLock() defer pm.mu.RUnlock() return map[string]interface{}{ \"avg_latency_ms\": pm.GetAverageLatency().Milliseconds(), \"cache_hit_rate\": pm.cacheHitRate, \"throughput\": pm.throughput, \"error_count\": pm.errorCount, } }\rBest Practices 1. Configuration Optimization Connection Pooling: Configure appropriate pool sizes based on workload Index Selection: Choose HNSW for better query performance, IVFFlat for faster indexing Memory Settings: Tune PostgreSQL memory settings for vector operations Batch Sizes: Optimize batch sizes for embedding API calls Cache Sizes: Size caches based on available memory and hit rate requirements 2. Performance Monitoring Latency Tracking: Monitor search and indexing latencies Resource Usage: Track memory, CPU, and disk usage Cache Performance: Monitor cache hit rates and effectiveness Error Rates: Track and alert on error rates Throughput: Monitor requests per second and concurrent users 3. Scaling Strategies Vertical Scaling: Increase memory and CPU for single-node performance Read Replicas: Use read replicas for read-heavy workloads Sharding: Distribute data across multiple nodes for large datasets Caching Layers: Implement multiple caching layers for frequently accessed data Load Balancing: Distribute requests across multiple instances Conclusion Memory optimization is essential for production-ready agent systems. Key takeaways:",
    "tags": [],
    "title": "memory-optimization",
    "uri": "/AgenticGoKitDocs/tutorials/memory-systems/memory-optimization/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials",
    "content": "Memory Systems in AgenticGoKit Navigation: Documentation Home → Tutorials → Memory Systems\nOverview Memory systems are crucial for building intelligent agents that can learn, remember, and build upon previous interactions. This tutorial series explores AgenticGoKit’s memory capabilities, from basic in-memory storage to advanced RAG (Retrieval-Augmented Generation) systems with vector databases.\nMemory systems enable agents to maintain context across conversations, store knowledge, and retrieve relevant information to enhance their responses.\nPrerequisites Understanding of Core Concepts Basic knowledge of databases and data storage Familiarity with vector embeddings and similarity search Memory System Architecture AgenticGoKit’s memory system is built on a flexible architecture that supports multiple storage backends and retrieval strategies:\n┌─────────────┐ ┌──────────────┐ ┌─────────────────┐\r│ Agent │───▶│ Memory │───▶│ Storage Backend │\r│ │ │ Interface │ │ │\r└─────────────┘ └──────────────┘ └─────────────────┘\r│\r▼\r┌──────────────┐\r│ Embedding │\r│ Provider │\r└──────────────┘\rMemory Types 1. Conversational Memory Stores chat history and conversation context:\nMessage history User preferences Session context Conversation metadata 2. Knowledge Memory Stores factual information and documents:\nDocument chunks Factual knowledge Reference materials Structured data 3. Episodic Memory Stores experiences and events:\nPast interactions Learning experiences Feedback and corrections Temporal sequences 4. Working Memory Temporary storage during processing:\nIntermediate results Processing context Temporary variables Computation state Memory Interface The core memory interface provides a unified API for all memory operations including RAG:\ntype Memory interface { // Personal memory operations Store(ctx context.Context, content string, tags ...string) error Query(ctx context.Context, query string, limit ...int) ([]Result, error) Remember(ctx context.Context, key string, value any) error Recall(ctx context.Context, key string) (any, error) // Chat history management AddMessage(ctx context.Context, role, content string) error GetHistory(ctx context.Context, limit ...int) ([]Message, error) // Session management NewSession() string SetSession(ctx context.Context, sessionID string) context.Context ClearSession(ctx context.Context) error Close() error // RAG-Enhanced Knowledge Base Operations IngestDocument(ctx context.Context, doc Document) error IngestDocuments(ctx context.Context, docs []Document) error SearchKnowledge(ctx context.Context, query string, options ...SearchOption) ([]KnowledgeResult, error) // Hybrid Search (Personal Memory + Knowledge Base) SearchAll(ctx context.Context, query string, options ...SearchOption) (*HybridResult, error) // RAG Context Assembly for LLM Prompts BuildContext(ctx context.Context, query string, options ...ContextOption) (*RAGContext, error) }\rMemory Providers 1. In-Memory Provider Fast, non-persistent storage for development and testing:\n// Create in-memory storage memory, err := core.NewMemory(core.AgentMemoryConfig{ Provider: \"memory\", Connection: \"memory\", MaxResults: 10, Dimensions: 1536, AutoEmbed: true, })\rUse Cases:\nDevelopment and testing Temporary storage High-speed access Stateless applications 2. PostgreSQL with pgvector Production-ready vector storage with SQL capabilities:\n// Create pgvector storage memory, err := core.NewMemory(core.AgentMemoryConfig{ Provider: \"pgvector\", Connection: \"postgres://user:pass@localhost:5432/agentdb\", EnableRAG: true, EnableKnowledgeBase: true, Dimensions: 1536, // OpenAI embedding dimensions KnowledgeMaxResults: 20, KnowledgeScoreThreshold: 0.7, Embedding: core.EmbeddingConfig{ Provider: \"openai\", Model: \"text-embedding-3-small\", APIKey: os.Getenv(\"OPENAI_API_KEY\"), MaxBatchSize: 100, TimeoutSeconds: 30, CacheEmbeddings: true, }, })\rUse Cases:\nProduction applications Complex queries ACID compliance Structured and unstructured data 3. Weaviate Vector Database Specialized vector database with advanced features:\n// Create Weaviate storage memory, err := core.NewMemory(core.AgentMemoryConfig{ Provider: \"weaviate\", Connection: \"http://localhost:8080\", EnableRAG: true, EnableKnowledgeBase: true, Dimensions: 1536, KnowledgeMaxResults: 20, KnowledgeScoreThreshold: 0.7, Embedding: core.EmbeddingConfig{ Provider: \"openai\", Model: \"text-embedding-3-small\", APIKey: os.Getenv(\"OPENAI_API_KEY\"), MaxBatchSize: 100, TimeoutSeconds: 30, CacheEmbeddings: true, }, })\rUse Cases:\nLarge-scale vector search Multi-modal data Advanced filtering Real-time updates Basic Memory Operations 1. Storing Information // Store simple text err := memory.Store(ctx, \"Paris is the capital of France\", \"fact\") // Store with metadata err = memory.Store(ctx, \"The user prefers technical explanations\", \"preference\", core.WithSession(\"user-123\"), core.WithMetadata(map[string]string{ \"category\": \"user-preference\", \"priority\": \"high\", }), ) // Store conversation message err = memory.Store(ctx, \"How do I implement a binary search?\", \"user-message\", core.WithSession(\"user-123\"), core.WithTimestamp(time.Now()), )\r2. Searching Memory // Basic search results, err := memory.Search(ctx, \"capital of France\") // Search with options results, err = memory.Search(ctx, \"user preferences\", core.WithLimit(5), core.WithScoreThreshold(0.7), core.WithSession(\"user-123\"), core.WithContentType(\"preference\"), ) // Process results for _, result := range results { fmt.Printf(\"Content: %s (Score: %.3f)\\n\", result.Content, result.Score) }\r3. Conversation History // Get recent conversation history messages, err := memory.GetHistory(ctx, 10, core.WithSession(\"user-123\"), core.WithTimeRange(time.Now().Add(-24*time.Hour), time.Now()), ) // Process conversation history for _, msg := range messages { fmt.Printf(\"[%s] %s: %s\\n\", msg.Timestamp.Format(\"15:04\"), msg.Role, msg.Content) }\rRAG (Retrieval-Augmented Generation) RAG enhances agent responses by retrieving relevant information from memory:\n1. Basic RAG Implementation type RAGAgent struct { name string llm LLMProvider memory Memory } func (r *RAGAgent) Run(ctx context.Context, event Event, state State) (AgentResult, error) { query, _ := state.Get(\"message\") queryStr := query.(string) // Retrieve relevant context results, err := r.memory.Search(ctx, queryStr, core.WithLimit(3), core.WithScoreThreshold(0.7), ) if err != nil { return AgentResult{}, err } // Build context from retrieved results var context strings.Builder for _, result := range results { context.WriteString(fmt.Sprintf(\"- %s\\n\", result.Content)) } // Create enhanced prompt prompt := fmt.Sprintf(`Context: %s Question: %s Please answer the question using the provided context.`, context.String(), queryStr) // Generate response with context response, err := r.llm.Generate(ctx, prompt) if err != nil { return AgentResult{}, err } // Store the interaction r.memory.Store(ctx, queryStr, \"user-message\") r.memory.Store(ctx, response, \"assistant-response\") outputState := state.Clone() outputState.Set(\"response\", response) outputState.Set(\"context_used\", len(results)) return AgentResult{OutputState: outputState}, nil }\r2. Advanced RAG with Reranking type AdvancedRAGAgent struct { name string llm LLMProvider memory Memory reranker Reranker } func (r *AdvancedRAGAgent) Run(ctx context.Context, event Event, state State) (AgentResult, error) { query, _ := state.Get(\"message\") queryStr := query.(string) // Initial retrieval with higher limit results, err := r.memory.Search(ctx, queryStr, core.WithLimit(10), core.WithScoreThreshold(0.5), ) if err != nil { return AgentResult{}, err } // Rerank results for better relevance rerankedResults, err := r.reranker.Rerank(ctx, queryStr, results) if err != nil { return AgentResult{}, err } // Take top results after reranking topResults := rerankedResults[:min(3, len(rerankedResults))] // Build enhanced context context := r.buildContext(topResults) // Generate response response, err := r.generateWithContext(ctx, queryStr, context) if err != nil { return AgentResult{}, err } // Store interaction with metadata r.storeInteraction(ctx, queryStr, response, topResults) outputState := state.Clone() outputState.Set(\"response\", response) outputState.Set(\"sources\", r.extractSources(topResults)) return AgentResult{OutputState: outputState}, nil }\rMemory Configuration 1. Embedding Configuration type EmbeddingConfig struct { Provider string `yaml:\"provider\"` // \"openai\", \"huggingface\", \"local\" Model string `yaml:\"model\"` // Model name APIKey string `yaml:\"api_key\"` // API key if needed Dimensions int `yaml:\"dimensions\"` // Embedding dimensions BatchSize int `yaml:\"batch_size\"` // Batch size for processing Options map[string]string `yaml:\"options\"` // Provider-specific options } // OpenAI embeddings embeddingConfig := core.EmbeddingConfig{ Provider: \"openai\", Model: \"text-embedding-3-small\", APIKey: os.Getenv(\"OPENAI_API_KEY\"), Dimensions: 1536, BatchSize: 100, } // Hugging Face embeddings embeddingConfig := core.EmbeddingConfig{ Provider: \"huggingface\", Model: \"sentence-transformers/all-MiniLM-L6-v2\", Dimensions: 384, BatchSize: 32, }\r2. Memory Configuration type AgentMemoryConfig struct { // Core memory settings Provider string `toml:\"provider\"` // pgvector, weaviate, memory Connection string `toml:\"connection\"` // postgres://..., http://..., or \"memory\" MaxResults int `toml:\"max_results\"` // default: 10 Dimensions int `toml:\"dimensions\"` // default: 1536 AutoEmbed bool `toml:\"auto_embed\"` // default: true // RAG-enhanced settings EnableKnowledgeBase bool `toml:\"enable_knowledge_base\"` // default: true KnowledgeMaxResults int `toml:\"knowledge_max_results\"` // default: 20 KnowledgeScoreThreshold float32 `toml:\"knowledge_score_threshold\"` // default: 0.7 ChunkSize int `toml:\"chunk_size\"` // default: 1000 ChunkOverlap int `toml:\"chunk_overlap\"` // default: 200 // RAG context assembly settings EnableRAG bool `toml:\"enable_rag\"` // default: true RAGMaxContextTokens int `toml:\"rag_max_context_tokens\"` // default: 4000 RAGPersonalWeight float32 `toml:\"rag_personal_weight\"` // default: 0.3 RAGKnowledgeWeight float32 `toml:\"rag_knowledge_weight\"` // default: 0.7 RAGIncludeSources bool `toml:\"rag_include_sources\"` // default: true // Document processing settings Documents DocumentConfig `toml:\"documents\"` // Embedding service settings Embedding EmbeddingConfig `toml:\"embedding\"` // Search settings Search SearchConfigToml `toml:\"search\"` } // Complete memory configuration config := core.AgentMemoryConfig{ Provider: \"pgvector\", Connection: \"postgres://user:pass@localhost:5432/agentdb\", EnableRAG: true, EnableKnowledgeBase: true, ChunkSize: 1000, ChunkOverlap: 200, Dimensions: 1536, KnowledgeMaxResults: 20, KnowledgeScoreThreshold: 0.7, RAGMaxContextTokens: 4000, RAGPersonalWeight: 0.3, RAGKnowledgeWeight: 0.7, RAGIncludeSources: true, Embedding: core.EmbeddingConfig{ Provider: \"openai\", Model: \"text-embedding-3-small\", APIKey: os.Getenv(\"OPENAI_API_KEY\"), MaxBatchSize: 100, TimeoutSeconds: 30, CacheEmbeddings: true, }, Documents: core.DocumentConfig{ AutoChunk: true, SupportedTypes: []string{\"pdf\", \"txt\", \"md\", \"web\", \"code\"}, MaxFileSize: \"10MB\", EnableMetadataExtraction: true, EnableURLScraping: true, }, Search: core.SearchConfigToml{ HybridSearch: true, KeywordWeight: 0.3, SemanticWeight: 0.7, EnableReranking: false, EnableQueryExpansion: false, }, }\rMemory Integration with Agents 1. Agent Builder Integration // Create agent with memory agent, err := core.NewAgent(\"knowledge-agent\"). WithLLMAndConfig(llmProvider, llmConfig). WithMemory(memory). WithMCPAndConfig(mcpManager, mcpConfig). Build()\r2. Custom Memory Integration type MemoryEnabledAgent struct { name string llm LLMProvider memory Memory config MemoryConfig } func (m *MemoryEnabledAgent) Run(ctx context.Context, event Event, state State) (AgentResult, error) { // Extract user message message, _ := state.Get(\"message\") messageStr := message.(string) // Get conversation history for context history, err := m.memory.GetHistory(ctx, 5, core.WithSession(event.GetSessionID()), ) if err != nil { return AgentResult{}, err } // Search for relevant knowledge knowledge, err := m.memory.Search(ctx, messageStr, core.WithLimit(3), core.WithScoreThreshold(0.7), core.WithContentType(\"knowledge\"), ) if err != nil { return AgentResult{}, err } // Build enhanced prompt with history and knowledge prompt := m.buildEnhancedPrompt(messageStr, history, knowledge) // Generate response response, err := m.llm.Generate(ctx, prompt) if err != nil { return AgentResult{}, err } // Store the interaction m.storeInteraction(ctx, event.GetSessionID(), messageStr, response) outputState := state.Clone() outputState.Set(\"response\", response) outputState.Set(\"knowledge_used\", len(knowledge)) outputState.Set(\"history_length\", len(history)) return AgentResult{OutputState: outputState}, nil }\rTutorial Series Structure This memory systems tutorial series covers:\n1. Basic Memory In-memory storage Simple operations Session management Basic retrieval 2. Vector Databases pgvector setup and usage Weaviate integration Embedding strategies Performance optimization 3. Document Ingestion Document processing pipeline Text chunking strategies Metadata extraction Batch processing optimization 4. RAG Implementation Retrieval-Augmented Generation Context building Prompt engineering Response enhancement 5. Knowledge Bases Knowledge base architecture Advanced search patterns Multi-modal content Production deployment 6. Memory Optimization Performance tuning Scaling strategies Caching mechanisms Resource management Best Practices 1. Memory Design Principles Separate concerns: Use different memory types for different purposes Optimize for retrieval: Design storage for efficient search Manage lifecycle: Clean up old or irrelevant memories Monitor performance: Track memory usage and search performance 2. RAG Best Practices Chunk appropriately: Balance context size and relevance Use metadata: Enhance retrieval with structured metadata Implement reranking: Improve relevance with secondary ranking Handle failures: Gracefully degrade when memory is unavailable 3. Production Considerations Scale horizontally: Use distributed storage for large datasets Implement caching: Cache frequent queries and embeddings Monitor costs: Track embedding API usage and storage costs Backup data: Ensure memory data is backed up and recoverable Common Use Cases 1. Conversational AI Chat history maintenance User preference learning Context-aware responses Personalization 2. Knowledge Management Document Q\u0026A systems Technical support bots Research assistants Information retrieval 3. Learning Systems Adaptive agents Feedback incorporation Experience replay Continuous improvement 4. Multi-Agent Coordination Shared knowledge bases Inter-agent communication Collaborative learning Distributed memory Conclusion Memory systems are fundamental to building intelligent agents that can learn, remember, and improve over time. AgenticGoKit provides flexible memory abstractions that support various storage backends and retrieval strategies.\nThe key to effective memory systems is choosing the right combination of storage backend, embedding strategy, and retrieval approach for your specific use case.\nNext Steps Basic Memory - Start with simple memory operations Vector Databases - Learn about production storage RAG Implementation - Build retrieval-augmented systems Knowledge Bases - Create comprehensive knowledge systems Further Reading API Reference: Memory Interface Examples: Memory Systems Configuration Guide: Memory Settings",
    "description": "Memory Systems in AgenticGoKit Navigation: Documentation Home → Tutorials → Memory Systems\nOverview Memory systems are crucial for building intelligent agents that can learn, remember, and build upon previous interactions. This tutorial series explores AgenticGoKit’s memory capabilities, from basic in-memory storage to advanced RAG (Retrieval-Augmented Generation) systems with vector databases.\nMemory systems enable agents to maintain context across conversations, store knowledge, and retrieve relevant information to enhance their responses.\nPrerequisites Understanding of Core Concepts Basic knowledge of databases and data storage Familiarity with vector embeddings and similarity search Memory System Architecture AgenticGoKit’s memory system is built on a flexible architecture that supports multiple storage backends and retrieval strategies:",
    "tags": [],
    "title": "memory-systems",
    "uri": "/AgenticGoKitDocs/tutorials/memory-systems/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials \u003e core-concepts",
    "content": "Message Passing and Event Flow in AgenticGoKit Overview At the heart of AgenticGoKit is an event-driven architecture that enables flexible communication between agents. This tutorial explains how messages flow through the system, how the Runner orchestrates this flow, and how you can leverage these patterns in your own applications.\nUnderstanding message passing is crucial because it’s the foundation of how agents communicate, share data, and coordinate their work in AgenticGoKit.\nPrerequisites Basic understanding of Go programming Familiarity with interfaces and goroutines Completed the 5-minute quickstart Core Concepts Events: The Communication Currency In AgenticGoKit, all communication happens through Events. An Event is more than just a message - it’s a structured packet of information with routing metadata.\n// The Event interface - the core communication unit type Event interface { GetID() string // Unique identifier GetTimestamp() time.Time // When event was created GetTargetAgentID() string // Destination agent GetSourceAgentID() string // Source agent GetData() EventData // Actual payload data GetMetadata() map[string]string // Routing metadata GetSessionID() string // Session identifier // ... other methods for setting values }\rEvents carry:\nData: The actual payload (questions, responses, etc.) Metadata: Routing information, session IDs, etc. Source/Target: Which agent sent it and where it’s going Timestamps: When the event was created EventData: The Payload // EventData holds the payload of an event type EventData map[string]any // Example usage data := core.EventData{ \"message\": \"What's the weather today?\", \"user_id\": \"user-123\", \"priority\": \"high\", }\rRunners: The Traffic Controllers The Runner is responsible for routing events to the appropriate agents and managing the overall flow of communication.\n// The Runner interface - manages event flow type Runner interface { Emit(event Event) error // Send an event into the system RegisterAgent(name string, handler AgentHandler) error // Register an agent Start(ctx context.Context) error // Start processing Stop() // Stop processing // ... other methods }\rThe Runner:\nReceives events via Emit() Determines which agent(s) should handle the event Delivers the event to the appropriate agent(s) Collects results and routes them to the next destination How Message Passing Works 1. Creating Events Events are typically created using the NewEvent() function:\n// Create a new event event := core.NewEvent( \"weather-agent\", // Target agent ID core.EventData{ // Payload data \"message\": \"What's the weather in Paris?\", \"location\": \"Paris\", }, map[string]string{ // Metadata \"session_id\": \"user-123\", \"priority\": \"normal\", }, )\r2. Emitting Events Events are sent into the system using the Runner’s Emit() method:\n// Send the event into the system err := runner.Emit(event) if err != nil { log.Fatalf(\"Failed to emit event: %v\", err) }\rThis is an asynchronous operation - Emit() returns immediately, and processing happens in the background.\n3. Event Processing Flow Behind the scenes, the Runner follows this flow:\n┌─────────┐ ┌──────────┐ ┌─────────┐\r│ Client │────▶│ Runner │────▶│ Agent A │\r└─────────┘ └──────────┘ └─────────┘\r│ │\r│ ▼\r│ ┌─────────┐\r│◀──────────│ Result │\r│ └─────────┘\r▼\r┌─────────┐\r│ Agent B │\r└─────────┘\rQueues the event for processing Routes it to the target agent(s) via the Orchestrator Collects the results Forwards results to the next agent or back to the caller 4. Handling Results Results are typically handled through callbacks:\n// Register a callback for when an agent completes processing runner.RegisterCallback(core.HookAfterAgentRun, \"my-callback\", func(ctx context.Context, args core.CallbackArgs) (core.State, error) { fmt.Printf(\"Agent %s completed\\n\", args.AgentID) return args.State, nil }, )\rUnder the Hood: The Runner Implementation The Runner implementation uses a combination of channels, goroutines, and queues to manage event flow:\n// Simplified version of the internal runner loop func (r *RunnerImpl) loop(ctx context.Context) { defer r.wg.Done() for { select { case \u003c-ctx.Done(): return case \u003c-r.stopChan: return case event := \u003c-r.queue: // Process event in the main goroutine r.processEvent(ctx, event) } } } func (r *RunnerImpl) processEvent(ctx context.Context, event Event) { // 1. Invoke BeforeEventHandling callbacks // 2. Route to orchestrator for agent dispatch // 3. Handle agent result // 4. Invoke AfterEventHandling callbacks // 5. Emit new events if needed }\rPractical Example: Building a Conversational Agent Let’s see how this works in practice with a simple conversational agent:\npackage main import ( \"context\" \"fmt\" \"os\" \"time\" \"github.com/kunalkushwaha/agenticgokit/core\" ) func main() { // Create a provider provider, err := core.NewOpenAIAdapter( os.Getenv(\"OPENAI_API_KEY\"), \"gpt-3.5-turbo\", 500, 0.7, ) if err != nil { panic(err) } // Create an agent agent, err := core.NewAgent(\"assistant\"). WithLLMAndConfig(provider, core.LLMConfig{ SystemPrompt: \"You are a helpful assistant.\", }). Build() if err != nil { panic(err) } // Create a runner with orchestrator runner := core.NewRunner(100) orchestrator := core.NewRouteOrchestrator(runner.GetCallbackRegistry()) runner.SetOrchestrator(orchestrator) // Register the agent agentHandler := core.ConvertAgentToHandler(agent) runner.RegisterAgent(\"assistant\", agentHandler) // Start the runner ctx := context.Background() if err := runner.Start(ctx); err != nil { panic(err) } defer runner.Stop() // Set up result handling resultReceived := make(chan bool) runner.RegisterCallback(core.HookAfterAgentRun, \"result-handler\", func(ctx context.Context, args core.CallbackArgs) (core.State, error) { if args.AgentResult.OutputState != nil { if response, ok := args.AgentResult.OutputState.Get(\"response\"); ok { fmt.Printf(\"Agent Response: %s\\n\", response) } } resultReceived \u003c- true return args.State, nil }, ) // Create and emit an event event := core.NewEvent( \"assistant\", core.EventData{\"message\": \"Tell me about AgenticGoKit\"}, map[string]string{ \"session_id\": \"user-123\", \"route\": \"assistant\", // Required for routing }, ) if err := runner.Emit(event); err != nil { panic(err) } // Wait for result select { case \u003c-resultReceived: fmt.Println(\"Processing complete!\") case \u003c-time.After(30 * time.Second): fmt.Println(\"Timeout waiting for response\") } }\rAdvanced Message Passing Patterns 1. Session Management Events can carry session IDs to maintain conversation context:\nevent := core.NewEvent( \"assistant\", core.EventData{\"message\": \"What was my last question?\"}, map[string]string{\"session_id\": \"user-123\"}, )\r2. Event Chaining You can create chains of events where each agent’s output becomes the next agent’s input:\nrunner.RegisterCallback(core.HookAfterAgentRun, \"chain-handler\", func(ctx context.Context, args core.CallbackArgs) (core.State, error) { if args.AgentID == \"researcher\" \u0026\u0026 args.Error == nil { // Create a new event for the analyzer nextEvent := core.NewEvent( \"analyzer\", core.EventData{\"research_data\": args.AgentResult.OutputState}, map[string]string{ \"session_id\": args.Event.GetSessionID(), \"route\": \"analyzer\", }, ) runner.Emit(nextEvent) } return args.State, nil }, )\r3. Broadcast Events Send the same event to multiple agents simultaneously using collaborative orchestration:\n// This requires using a collaborative orchestrator // which we'll cover in the orchestration tutorial event := core.NewEvent( \"\", // Empty target for broadcast core.EventData{\"message\": \"New data available\"}, map[string]string{\"broadcast\": \"true\"}, )\r4. Priority Handling Use metadata to implement priority queues:\nevent := core.NewEvent( \"processor\", core.EventData{\"task\": \"urgent_analysis\"}, map[string]string{ \"priority\": \"high\", \"deadline\": time.Now().Add(5*time.Minute).Format(time.RFC3339), }, )\rEvent Lifecycle and Hooks AgenticGoKit provides several hooks where you can intercept and modify the event processing flow:\n// Available hooks const ( HookBeforeEventHandling // Before any processing HookBeforeAgentRun // Before agent execution HookAfterAgentRun // After successful agent execution HookAgentError // When agent execution fails HookAfterEventHandling // After all processing )\rExample: Adding Logging and Metrics // Add comprehensive logging runner.RegisterCallback(core.HookBeforeEventHandling, \"logger\", func(ctx context.Context, args core.CallbackArgs) (core.State, error) { fmt.Printf(\"[%s] Processing event %s\\n\", time.Now().Format(time.RFC3339), args.Event.GetID(), ) return args.State, nil }, ) runner.RegisterCallback(core.HookAfterAgentRun, \"metrics\", func(ctx context.Context, args core.CallbackArgs) (core.State, error) { duration := time.Since(args.Event.GetTimestamp()) fmt.Printf(\"Agent %s completed in %v\\n\", args.AgentID, duration) // Record metrics // metrics.RecordAgentDuration(args.AgentID, duration) return args.State, nil }, )\rError Handling in Message Passing When agents fail, AgenticGoKit provides sophisticated error routing:\n// Register error handler runner.RegisterCallback(core.HookAgentError, \"error-handler\", func(ctx context.Context, args core.CallbackArgs) (core.State, error) { fmt.Printf(\"Agent %s failed: %v\\n\", args.AgentID, args.Error) // Optionally emit a recovery event recoveryEvent := core.NewEvent( \"error-recovery-agent\", core.EventData{ \"original_event\": args.Event, \"error\": args.Error.Error(), \"failed_agent\": args.AgentID, }, map[string]string{ \"session_id\": args.Event.GetSessionID(), \"route\": \"error-recovery-agent\", }, ) runner.Emit(recoveryEvent) return args.State, nil }, )\rCommon Pitfalls and Solutions 1. Deadlocks Problem: Waiting for results in the same goroutine that processes events.\nSolution: Use callbacks or separate goroutines for waiting on results.\n// Bad - blocks the event loop result := \u003c-resultChannel // Good - use callbacks runner.RegisterCallback(core.HookAfterAgentRun, \"handler\", func(ctx context.Context, args core.CallbackArgs) (core.State, error) { // Handle result here return args.State, nil }, )\r2. Event Loops Problem: Creating circular event chains that never terminate.\nSolution: Implement loop detection or maximum hop counts in your metadata.\n// Add hop count to prevent infinite loops metadata := map[string]string{ \"session_id\": \"user-123\", \"hop_count\": \"1\", \"max_hops\": \"5\", }\r3. Lost Events Problem: Events that never get processed because the target agent doesn’t exist.\nSolution: Implement error routing and default handlers for unknown targets.\n// The orchestrator will automatically handle unknown targets // and emit error events that you can catch with error callbacks\r4. Memory Leaks Problem: Events accumulating in queues without being processed.\nSolution: Monitor queue sizes and implement proper shutdown procedures.\n// Always stop the runner when done defer runner.Stop() // Monitor queue health runner.RegisterCallback(core.HookBeforeEventHandling, \"monitor\", func(ctx context.Context, args core.CallbackArgs) (core.State, error) { // Check queue sizes, memory usage, etc. return args.State, nil }, )\rPerformance Considerations 1. Queue Sizing // Create runner with appropriate queue size runner := core.NewRunner(1000) // Larger queue for high throughput\r2. Event Batching // For high-volume scenarios, consider batching events batchEvent := core.NewEvent( \"batch-processor\", core.EventData{ \"events\": []core.Event{event1, event2, event3}, }, metadata, )\r3. Async Processing // Use goroutines for non-blocking operations runner.RegisterCallback(core.HookAfterAgentRun, \"async-handler\", func(ctx context.Context, args core.CallbackArgs) (core.State, error) { go func() { // Long-running operation processResult(args.AgentResult) }() return args.State, nil }, )\rDebugging Message Flow 1. Event Tracing // Enable detailed logging runner.RegisterCallback(core.HookBeforeEventHandling, \"tracer\", func(ctx context.Context, args core.CallbackArgs) (core.State, error) { fmt.Printf(\"Event %s: %+v\\n\", args.Event.GetID(), args.Event.GetData()) return args.State, nil }, )\r2. State Inspection // Inspect state at each step runner.RegisterCallback(core.HookAfterAgentRun, \"state-inspector\", func(ctx context.Context, args core.CallbackArgs) (core.State, error) { if args.AgentResult.OutputState != nil { fmt.Printf(\"State after %s: %+v\\n\", args.AgentID, args.AgentResult.OutputState.GetAll(), ) } return args.State, nil }, )\rBest Practices Always set session IDs for tracking related events Use meaningful event IDs for debugging Include sufficient metadata for routing and processing Handle errors gracefully with error callbacks Monitor queue health and processing times Use callbacks instead of blocking waits Implement proper shutdown procedures Add comprehensive logging for production systems Conclusion The event-driven architecture of AgenticGoKit provides a flexible foundation for building complex agent systems. By understanding how events flow through the Runner to agents and back, you can create sophisticated communication patterns between your agents.\nKey takeaways:\nEvents are the primary communication mechanism Runners manage event flow and routing Callbacks provide hooks for customization Proper error handling is crucial Async patterns prevent blocking Next Steps Orchestration Patterns - Learn how different orchestration modes build on message passing State Management - Understand how data flows between agents Error Handling - Master robust error management patterns Debugging Guide - Learn to trace and debug event flows Further Reading API Reference: Event Interface API Reference: Runner Interface Examples: Message Passing Patterns",
    "description": "Message Passing and Event Flow in AgenticGoKit Overview At the heart of AgenticGoKit is an event-driven architecture that enables flexible communication between agents. This tutorial explains how messages flow through the system, how the Runner orchestrates this flow, and how you can leverage these patterns in your own applications.\nUnderstanding message passing is crucial because it’s the foundation of how agents communicate, share data, and coordinate their work in AgenticGoKit.",
    "tags": [],
    "title": "message-passing",
    "uri": "/AgenticGoKitDocs/tutorials/core-concepts/message-passing/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials \u003e getting-started",
    "content": "Multi-Agent Collaboration Tutorial (15 minutes) Overview Learn how to orchestrate multiple agents working together using different patterns. You’ll explore collaborative, sequential, and mixed orchestration modes to build sophisticated multi-agent workflows.\nPrerequisites Complete the 5-Minute Quickstart Basic understanding of AgenticGoKit concepts Learning Objectives By the end of this tutorial, you’ll understand:\nDifferent orchestration patterns and when to use them How to configure collaborative vs sequential processing How to build mixed orchestration workflows Performance characteristics of each pattern What You’ll Build Three different multi-agent systems:\nCollaborative System: Agents work in parallel for faster processing Sequential Pipeline: Agents work in sequence for data transformation Mixed Workflow: Combination of parallel and sequential processing Part 1: Collaborative Orchestration (5 minutes) Collaborative orchestration runs all agents in parallel, combining their outputs.\nCreate a Collaborative Analysis System # Create a collaborative analysis project agentcli create analysis-system --agents 3 --orchestration-mode collaborative \\ --collaborative-agents \"researcher,analyst,validator\" cd analysis-system\rUnderstanding the Generated Code The generated main.go creates three agents that work together:\n// Generated agents work in parallel agents := map[string]core.AgentHandler{ \"researcher\": researcherHandler, \"analyst\": analystHandler, \"validator\": validatorHandler, } // Collaborative runner sends events to ALL agents simultaneously runner := core.CreateCollaborativeRunner(agents, 30*time.Second)\rTest Collaborative Processing # Set your API key export OPENAI_API_KEY=your-api-key-here # Run the collaborative system go run main.go\rYou’ll see all three agents process the same input simultaneously, then their outputs are combined.\nView the Collaboration # Check the trace to see parallel execution agentcli trace --flow-only \u003csession-id\u003e\rExpected Flow:\n14:32:15.123 researcher (processing)\r14:32:15.124 analyst (processing) ← All start simultaneously\r14:32:15.125 validator (processing)\r14:32:18.456 researcher (completed)\r14:32:18.789 analyst (completed) ← Results combined\r14:32:19.012 validator (completed)\rPart 2: Sequential Pipeline (5 minutes) Sequential orchestration processes agents one after another, passing data through a pipeline.\nCreate a Data Processing Pipeline # Create a sequential pipeline project agentcli create data-pipeline --agents 4 --orchestration-mode sequential \\ --sequential-agents \"extractor,transformer,enricher,formatter\" cd data-pipeline\rUnderstanding Sequential Processing // Generated sequential configuration runner := core.NewRunnerWithOrchestration(core.EnhancedRunnerConfig{ OrchestrationMode: core.OrchestrationSequential, SequentialAgents: []string{\"extractor\", \"transformer\", \"enricher\", \"formatter\"}, // ... other config })\rTest Sequential Processing export OPENAI_API_KEY=your-api-key-here go run main.go\rEach agent processes the output from the previous agent in sequence.\nView the Pipeline agentcli trace --flow-only \u003csession-id\u003e\rExpected Flow:\n14:32:15.123 extractor transformer\r14:32:16.456 transformer enricher ← Sequential processing\r14:32:17.789 enricher formatter\r14:32:19.012 formatter (end)\rPart 3: Mixed Orchestration (5 minutes) Mixed orchestration combines parallel and sequential processing for complex workflows.\nCreate a Mixed Workflow System # Create a mixed orchestration project agentcli create content-system --agents 5 --orchestration-mode mixed \\ --collaborative-agents \"researcher,fact-checker\" \\ --sequential-agents \"writer,editor,publisher\" cd content-system\rUnderstanding Mixed Processing // Mixed orchestration configuration runner := core.NewRunnerWithOrchestration(core.EnhancedRunnerConfig{ OrchestrationMode: core.OrchestrationMixed, CollaborativeAgents: []string{\"researcher\", \"fact-checker\"}, // Run in parallel SequentialAgents: []string{\"writer\", \"editor\", \"publisher\"}, // Run in sequence })\rTest Mixed Processing export OPENAI_API_KEY=your-api-key-here go run main.go\rThe system first runs collaborative agents in parallel, then processes sequential agents with the combined results.\nView the Mixed Flow agentcli trace --flow-only \u003csession-id\u003e\rExpected Flow:\nPhase 1 - Collaborative:\r14:32:15.123 researcher (processing)\r14:32:15.124 fact-checker (processing) ← Parallel phase\r14:32:17.456 researcher (completed)\r14:32:17.789 fact-checker (completed)\rPhase 2 - Sequential:\r14:32:18.012 writer editor ← Sequential phase\r14:32:19.345 editor publisher\r14:32:20.678 publisher (end)\rOrchestration Patterns Comparison Pattern Use Case Pros Cons Collaborative Analysis, validation, multiple perspectives Fast (parallel), diverse outputs Higher resource usage Sequential Data pipelines, step-by-step processing Efficient, clear flow Slower (serial) Mixed Complex workflows, content creation Best of both worlds More complex setup Performance Characteristics Collaborative Orchestration Speed: Fastest for independent tasks Resources: Higher CPU/memory usage Use When: Tasks can be done independently Sequential Orchestration Speed: Slower but predictable Resources: Lower resource usage Use When: Each step depends on the previous Mixed Orchestration Speed: Optimized for complex workflows Resources: Balanced usage Use When: Some tasks are independent, others dependent Advanced Configuration Timeout and Concurrency Settings # Create with custom settings agentcli create advanced-system --orchestration-mode collaborative \\ --orchestration-timeout 60 --max-concurrency 5 --failure-threshold 0.8\rError Handling Configuration # Create with fault tolerance agentcli create resilient-system --orchestration-mode mixed \\ --collaborative-agents \"agent1,agent2\" --sequential-agents \"agent3,agent4\" \\ --failure-threshold 0.7 --max-concurrency 3\rTroubleshooting Common Issues Agents not running in parallel:\n# Check orchestration mode in agentflow.toml [orchestration] mode = \"collaborative\" # Should be collaborative, not route\rSequential agents running out of order:\n# Verify agent sequence in configuration sequential_agents = [\"step1\", \"step2\", \"step3\"] # Order matters\rMixed orchestration not working:\n# Ensure both agent lists are specified collaborative_agents = [\"agent1\", \"agent2\"] sequential_agents = [\"agent3\", \"agent4\"]\rPerformance Issues Collaborative too slow:\nReduce number of agents Increase timeout settings Check agent complexity Sequential bottlenecks:\nIdentify slow agents with agentcli trace --verbose Optimize agent prompts Consider parallel alternatives Next Steps Now that you understand orchestration patterns, you can:\nAdd Memory: Learn Memory and RAG to give agents persistent knowledge Add Tools: Explore Tool Integration to connect external services Go Production: Check Production Deployment for scaling Key Takeaways Collaborative: Use for independent tasks that benefit from multiple perspectives Sequential: Use for data pipelines where each step builds on the previous Mixed: Use for complex workflows that need both parallel and sequential processing Configuration: AgenticGoKit makes it easy to switch between patterns Monitoring: Always use agentcli trace to understand agent interactions Further Reading Orchestration Patterns - Deep dive into patterns Performance Optimization - Advanced performance tuning State Management - How data flows between agents",
    "description": "Multi-Agent Collaboration Tutorial (15 minutes) Overview Learn how to orchestrate multiple agents working together using different patterns. You’ll explore collaborative, sequential, and mixed orchestration modes to build sophisticated multi-agent workflows.\nPrerequisites Complete the 5-Minute Quickstart Basic understanding of AgenticGoKit concepts Learning Objectives By the end of this tutorial, you’ll understand:\nDifferent orchestration patterns and when to use them How to configure collaborative vs sequential processing How to build mixed orchestration workflows Performance characteristics of each pattern What You’ll Build Three different multi-agent systems:",
    "tags": [],
    "title": "multi-agent-collaboration",
    "uri": "/AgenticGoKitDocs/tutorials/getting-started/multi-agent-collaboration/index.html"
  },
  {
    "breadcrumb": "content \u003e reference",
    "content": "Orchestration API Multi-agent coordination and workflow patterns\nThis document covers AgenticGoKit’s Orchestration API, which enables sophisticated coordination between multiple agents. The orchestration system provides various patterns for agent collaboration, from simple routing to complex hybrid workflows.\n📋 Core Concepts Orchestration Modes AgenticGoKit supports multiple orchestration patterns:\ntype OrchestrationMode string const ( // OrchestrationRoute sends each event to a single agent based on routing metadata (default) OrchestrationRoute OrchestrationMode = \"route\" // OrchestrationCollaborate sends each event to ALL registered agents in parallel OrchestrationCollaborate OrchestrationMode = \"collaborate\" // OrchestrationSequential processes agents one after another OrchestrationSequential OrchestrationMode = \"sequential\" // OrchestrationParallel processes agents in parallel (similar to collaborate) OrchestrationParallel OrchestrationMode = \"parallel\" // OrchestrationLoop repeats processing with a single agent OrchestrationLoop OrchestrationMode = \"loop\" // OrchestrationMixed combines collaborative and sequential patterns OrchestrationMixed OrchestrationMode = \"mixed\" )\r🚀 Basic Usage Route Orchestration (Default) // Create agents agents := map[string]core.AgentHandler{ \"greeter\": core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { name := event.Data[\"name\"].(string) return core.AgentResult{ Data: map[string]interface{}{ \"greeting\": fmt.Sprintf(\"Hello, %s!\", name), }, }, nil }), } // Create route runner (default behavior) runner := core.CreateRouteRunner(agents)\rCollaborative Orchestration // Create agents for parallel processing agents := map[string]core.AgentHandler{ \"researcher\": core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { query := event.Data[\"query\"].(string) // Research logic here return core.AgentResult{ Data: map[string]interface{}{ \"research_data\": \"research results for \" + query, }, }, nil }), \"analyzer\": core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { query := event.Data[\"query\"].(string) // Analysis logic here return core.AgentResult{ Data: map[string]interface{}{ \"analysis\": \"analysis of \" + query, }, }, nil }), \"validator\": core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { query := event.Data[\"query\"].(string) // Validation logic here return core.AgentResult{ Data: map[string]interface{}{ \"validation\": \"validation of \" + query, }, }, nil }), } // Create collaborative runner - all agents process in parallel runner := core.CreateCollaborativeRunner(agents, 2*time.Minute) // Process event - all agents will run simultaneously event := core.NewEvent(\"research\", map[string]interface{}{ \"query\": \"latest AI developments\", }) results, err := runner.ProcessEvent(context.Background(), event) if err != nil { log.Fatal(err) } // Results contain output from all agents fmt.Printf(\"Research: %v\\n\", results[\"researcher\"].Data[\"research_data\"]) fmt.Printf(\"Analysis: %v\\n\", results[\"analyzer\"].Data[\"analysis\"]) fmt.Printf(\"Validation: %v\\n\", results[\"validator\"].Data[\"validation\"])\rSequential Orchestration // Create agents for pipeline processing agents := map[string]core.AgentHandler{ \"collector\": core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { query := event.Data[\"query\"].(string) // Data collection logic collectedData := \"collected data for \" + query // Pass data to next agent via state state.Set(\"collected_data\", collectedData) return core.AgentResult{ Data: map[string]interface{}{ \"status\": \"data collected\", }, }, nil }), \"processor\": core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Get data from previous agent collectedData, _ := state.Get(\"collected_data\") // Process the data processedData := \"processed \" + collectedData.(string) state.Set(\"processed_data\", processedData) return core.AgentResult{ Data: map[string]interface{}{ \"status\": \"data processed\", }, }, nil }), \"formatter\": core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Get processed data processedData, _ := state.Get(\"processed_data\") // Format final output finalOutput := \"formatted \" + processedData.(string) return core.AgentResult{ Data: map[string]interface{}{ \"final_output\": finalOutput, }, }, nil }), } // Create sequential runner with ordered agent names runner := core.CreateSequentialRunner(agents, []string{\"collector\", \"processor\", \"formatter\"}) // Process event - agents run one after another event := core.NewEvent(\"process\", map[string]interface{}{ \"query\": \"user data\", }) result, err := runner.ProcessEvent(context.Background(), event) if err != nil { log.Fatal(err) } fmt.Printf(\"Final output: %v\\n\", result.Data[\"final_output\"])\rLoop Orchestration // Create agent for iterative processing qualityChecker := core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Get current iteration count iteration, _ := state.Get(\"iteration\") if iteration == nil { iteration = 0 } iterationCount := iteration.(int) + 1 state.Set(\"iteration\", iterationCount) // Get content to check content, ok := event.Data[\"content\"].(string) if !ok { content, _ = state.Get(\"content\").(string) } // Simulate quality checking qualityScore := calculateQuality(content) // Improve content if quality is low if qualityScore \u003c 0.8 \u0026\u0026 iterationCount \u003c 5 { improvedContent := improveContent(content) state.Set(\"content\", improvedContent) return core.AgentResult{ Data: map[string]interface{}{ \"quality_score\": qualityScore, \"iteration\": iterationCount, \"continue\": true, // Signal to continue loop }, }, nil } // Quality is good enough or max iterations reached return core.AgentResult{ Data: map[string]interface{}{ \"quality_score\": qualityScore, \"iteration\": iterationCount, \"final_content\": content, \"continue\": false, // Signal to stop loop }, }, nil }) // Create loop runner runner := core.CreateLoopRunner(qualityChecker, 5) // Max 5 iterations // Process event event := core.NewEvent(\"quality_check\", map[string]interface{}{ \"content\": \"initial content that needs improvement\", }) result, err := runner.ProcessEvent(context.Background(), event) if err != nil { log.Fatal(err) } fmt.Printf(\"Final quality score: %v\\n\", result.Data[\"quality_score\"]) fmt.Printf(\"Iterations: %v\\n\", result.Data[\"iteration\"]) fmt.Printf(\"Final content: %v\\n\", result.Data[\"final_content\"])\rMixed Orchestration // Create mixed orchestration with both collaborative and sequential phases builder := core.NewOrchestrationBuilder(core.OrchestrationMixed). WithCollaborativeAgents(map[string]core.AgentHandler{ \"analyzer\": analyzerAgent, \"validator\": validatorAgent, }). WithSequentialAgents(map[string]core.AgentHandler{ \"processor\": processorAgent, \"reporter\": reporterAgent, }). WithTimeout(5 * time.Minute). WithFailureThreshold(0.8) runner := builder.Build() // The mixed orchestration will: // 1. Run analyzer and validator in parallel (collaborative phase) // 2. Then run processor and reporter in sequence (sequential phase)\r🏗️ Orchestration Builder OrchestrationBuilder Interface type OrchestrationBuilder interface { WithAgents(agents map[string]AgentHandler) *OrchestrationBuilder WithCollaborativeAgents(agents map[string]AgentHandler) *OrchestrationBuilder WithSequentialAgents(agents map[string]AgentHandler) *OrchestrationBuilder WithTimeout(timeout time.Duration) *OrchestrationBuilder WithMaxIterations(max int) *OrchestrationBuilder WithFailureThreshold(threshold float64) *OrchestrationBuilder WithMaxConcurrency(max int) *OrchestrationBuilder WithRetryPolicy(policy *RetryPolicy) *OrchestrationBuilder Build() Runner GenerateMermaidDiagram() string GenerateMermaidDiagramWithConfig(config MermaidConfig) string }\rAdvanced Configuration // Create sophisticated orchestration with all options runner := core.NewOrchestrationBuilder(core.OrchestrationCollaborate). WithAgents(agents). WithTimeout(2 * time.Minute). WithFailureThreshold(0.8). // 80% of agents must succeed WithMaxConcurrency(5). // Max 5 concurrent agents WithRetryPolicy(\u0026core.RetryPolicy{ MaxRetries: 3, InitialDelay: time.Second, MaxDelay: 30 * time.Second, BackoffFactor: 2.0, }). Build()\r📊 Workflow Visualization Generating Workflow Diagrams // Generate Mermaid diagram for orchestration builder := core.NewOrchestrationBuilder(core.OrchestrationCollaborate). WithAgents(agents) // Generate basic diagram diagram := builder.GenerateMermaidDiagram() fmt.Println(diagram) // Generate diagram with custom configuration config := core.MermaidConfig{ DiagramType: core.FlowchartDiagram, Title: \"Research Workflow\", Direction: \"TD\", // Top-Down Theme: \"dark\", ShowMetadata: true, ShowAgentTypes: true, CompactMode: false, } customDiagram := builder.GenerateMermaidDiagramWithConfig(config) fmt.Println(customDiagram) // Save diagram to file err := core.SaveDiagramAsMarkdown(\"workflow.md\", \"Research Workflow\", customDiagram) if err != nil { log.Printf(\"Failed to save diagram: %v\", err) }\rExample Generated Diagram ---\rtitle: Collaborative Research Workflow\r---\rflowchart TD\rINPUT[\"🎯 Event Input\"]\rRESEARCHER[\"🤖 Researcher\u003cbr/\u003eType: Research Agent\u003cbr/\u003eTimeout: 2m\"]\rANALYZER[\"🤖 Analyzer\u003cbr/\u003eType: Analysis Agent\u003cbr/\u003eTimeout: 2m\"]\rVALIDATOR[\"🤖 Validator\u003cbr/\u003eType: Validation Agent\u003cbr/\u003eTimeout: 2m\"]\rOUTPUT[\"✅ Aggregated Result\"]\rINPUT --\u003e RESEARCHER\rINPUT --\u003e ANALYZER\rINPUT --\u003e VALIDATOR\rRESEARCHER --\u003e OUTPUT\rANALYZER --\u003e OUTPUT\rVALIDATOR --\u003e OUTPUT\rstyle RESEARCHER fill:#e1f5fe\rstyle ANALYZER fill:#e8f5e8\rstyle VALIDATOR fill:#fff3e0\r🔧 Runner Interface Core Runner Methods type Runner interface { // ProcessEvent processes a single event through the orchestration ProcessEvent(ctx context.Context, event Event) (map[string]AgentResult, error) // RegisterAgent adds an agent to the orchestration RegisterAgent(name string, handler AgentHandler) error // UnregisterAgent removes an agent from the orchestration UnregisterAgent(name string) error // ListAgents returns all registered agent names ListAgents() []string // GetAgent retrieves a specific agent by name GetAgent(name string) (AgentHandler, bool) // SetOrchestrationMode changes the orchestration pattern SetOrchestrationMode(mode OrchestrationMode) error // GetOrchestrationMode returns the current orchestration pattern GetOrchestrationMode() OrchestrationMode // Stop gracefully shuts down the runner Stop() error }\rRunner Factory Functions // Create runners for specific patterns func CreateRouteRunner(agents map[string]AgentHandler) Runner func CreateCollaborativeRunner(agents map[string]AgentHandler, timeout time.Duration) Runner func CreateSequentialRunner(agents map[string]AgentHandler, order []string) Runner func CreateLoopRunner(agent AgentHandler, maxIterations int) Runner func CreateFaultTolerantRunner(agents map[string]AgentHandler) Runner func CreateLoadBalancedRunner(agents map[string]AgentHandler, maxConcurrency int) Runner\r🔄 Event Routing Route-Based Processing // Create agents with different capabilities agents := map[string]core.AgentHandler{ \"chat\": chatAgent, \"search\": searchAgent, \"analyze\": analyzeAgent, } // Create route runner runner := core.CreateRouteRunner(agents) // Route to specific agent using metadata chatEvent := core.NewEvent(\"user_message\", map[string]interface{}{\"message\": \"Hello\"}, map[string]string{\"route\": \"chat\"}, // Route to chat agent ) searchEvent := core.NewEvent(\"search_query\", map[string]interface{}{\"query\": \"latest news\"}, map[string]string{\"route\": \"search\"}, // Route to search agent ) // Process events - each goes to specified agent chatResult, _ := runner.ProcessEvent(context.Background(), chatEvent) searchResult, _ := runner.ProcessEvent(context.Background(), searchEvent)\rDynamic Routing // Create router function func routeEvent(event core.Event) string { data := event.GetData() // Route based on event content if query, ok := data[\"query\"].(string); ok { if strings.Contains(strings.ToLower(query), \"search\") { return \"search\" } if strings.Contains(strings.ToLower(query), \"analyze\") { return \"analyze\" } } // Default to chat return \"chat\" } // Use custom routing logic runner := core.CreateRouteRunnerWithRouter(agents, routeEvent)\r📈 Performance and Monitoring Execution Metrics // Enable metrics collection runner := core.NewOrchestrationBuilder(core.OrchestrationCollaborate). WithAgents(agents). WithMetrics(true). Build() // Process events result, err := runner.ProcessEvent(ctx, event) // Get execution metrics metrics := runner.GetMetrics() fmt.Printf(\"Total execution time: %v\\n\", metrics.TotalDuration) fmt.Printf(\"Agent execution times: %v\\n\", metrics.AgentDurations) fmt.Printf(\"Success rate: %.2f%%\\n\", metrics.SuccessRate*100)\rHealth Checks // Check runner health health := runner.HealthCheck(ctx) if !health.Healthy { log.Printf(\"Runner unhealthy: %v\", health.Issues) } // Check individual agent health for agentName := range agents { agentHealth := runner.CheckAgentHealth(ctx, agentName) if !agentHealth.Healthy { log.Printf(\"Agent %s unhealthy: %v\", agentName, agentHealth.Issues) } }\r🛡️ Error Handling and Resilience Retry Policies retryPolicy := \u0026core.RetryPolicy{ MaxRetries: 3, InitialDelay: time.Second, MaxDelay: 30 * time.Second, BackoffFactor: 2.0, RetryableErrors: []error{ context.DeadlineExceeded, \u0026net.OpError{}, }, } runner := core.NewOrchestrationBuilder(core.OrchestrationCollaborate). WithAgents(agents). WithRetryPolicy(retryPolicy). Build()\rCircuit Breaker // Enable circuit breaker for fault tolerance runner := core.NewOrchestrationBuilder(core.OrchestrationCollaborate). WithAgents(agents). WithCircuitBreaker(\u0026core.CircuitBreakerConfig{ FailureThreshold: 5, // Open after 5 failures RecoveryTimeout: 30 * time.Second, HalfOpenRequests: 3, // Test with 3 requests when half-open }). Build()\rGraceful Degradation // Configure graceful degradation runner := core.NewOrchestrationBuilder(core.OrchestrationCollaborate). WithAgents(agents). WithFailureThreshold(0.6). // Continue if 60% of agents succeed WithGracefulDegradation(true). Build() // Even if some agents fail, you'll get partial results result, err := runner.ProcessEvent(ctx, event) if err != nil { // Check if it's a partial failure if partialErr, ok := err.(*core.PartialFailureError); ok { log.Printf(\"Partial failure: %d/%d agents succeeded\", partialErr.SuccessCount, partialErr.TotalCount) // Use partial results usePartialResults(result) } }\rThis comprehensive Orchestration API reference covers all aspects of multi-agent coordination in AgenticGoKit, from basic patterns to advanced configuration and monitoring.",
    "description": "Orchestration API Multi-agent coordination and workflow patterns\nThis document covers AgenticGoKit’s Orchestration API, which enables sophisticated coordination between multiple agents. The orchestration system provides various patterns for agent collaboration, from simple routing to complex hybrid workflows.\n📋 Core Concepts Orchestration Modes AgenticGoKit supports multiple orchestration patterns:\ntype OrchestrationMode string const ( // OrchestrationRoute sends each event to a single agent based on routing metadata (default) OrchestrationRoute OrchestrationMode = \"route\" // OrchestrationCollaborate sends each event to ALL registered agents in parallel OrchestrationCollaborate OrchestrationMode = \"collaborate\" // OrchestrationSequential processes agents one after another OrchestrationSequential OrchestrationMode = \"sequential\" // OrchestrationParallel processes agents in parallel (similar to collaborate) OrchestrationParallel OrchestrationMode = \"parallel\" // OrchestrationLoop repeats processing with a single agent OrchestrationLoop OrchestrationMode = \"loop\" // OrchestrationMixed combines collaborative and sequential patterns OrchestrationMixed OrchestrationMode = \"mixed\" )\r🚀 Basic Usage Route Orchestration (Default) // Create agents agents := map[string]core.AgentHandler{ \"greeter\": core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { name := event.Data[\"name\"].(string) return core.AgentResult{ Data: map[string]interface{}{ \"greeting\": fmt.Sprintf(\"Hello, %s!\", name), }, }, nil }), } // Create route runner (default behavior) runner := core.CreateRouteRunner(agents)\rCollaborative Orchestration // Create agents for parallel processing agents := map[string]core.AgentHandler{ \"researcher\": core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { query := event.Data[\"query\"].(string) // Research logic here return core.AgentResult{ Data: map[string]interface{}{ \"research_data\": \"research results for \" + query, }, }, nil }), \"analyzer\": core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { query := event.Data[\"query\"].(string) // Analysis logic here return core.AgentResult{ Data: map[string]interface{}{ \"analysis\": \"analysis of \" + query, }, }, nil }), \"validator\": core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { query := event.Data[\"query\"].(string) // Validation logic here return core.AgentResult{ Data: map[string]interface{}{ \"validation\": \"validation of \" + query, }, }, nil }), } // Create collaborative runner - all agents process in parallel runner := core.CreateCollaborativeRunner(agents, 2*time.Minute) // Process event - all agents will run simultaneously event := core.NewEvent(\"research\", map[string]interface{}{ \"query\": \"latest AI developments\", }) results, err := runner.ProcessEvent(context.Background(), event) if err != nil { log.Fatal(err) } // Results contain output from all agents fmt.Printf(\"Research: %v\\n\", results[\"researcher\"].Data[\"research_data\"]) fmt.Printf(\"Analysis: %v\\n\", results[\"analyzer\"].Data[\"analysis\"]) fmt.Printf(\"Validation: %v\\n\", results[\"validator\"].Data[\"validation\"])\rSequential Orchestration // Create agents for pipeline processing agents := map[string]core.AgentHandler{ \"collector\": core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { query := event.Data[\"query\"].(string) // Data collection logic collectedData := \"collected data for \" + query // Pass data to next agent via state state.Set(\"collected_data\", collectedData) return core.AgentResult{ Data: map[string]interface{}{ \"status\": \"data collected\", }, }, nil }), \"processor\": core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Get data from previous agent collectedData, _ := state.Get(\"collected_data\") // Process the data processedData := \"processed \" + collectedData.(string) state.Set(\"processed_data\", processedData) return core.AgentResult{ Data: map[string]interface{}{ \"status\": \"data processed\", }, }, nil }), \"formatter\": core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Get processed data processedData, _ := state.Get(\"processed_data\") // Format final output finalOutput := \"formatted \" + processedData.(string) return core.AgentResult{ Data: map[string]interface{}{ \"final_output\": finalOutput, }, }, nil }), } // Create sequential runner with ordered agent names runner := core.CreateSequentialRunner(agents, []string{\"collector\", \"processor\", \"formatter\"}) // Process event - agents run one after another event := core.NewEvent(\"process\", map[string]interface{}{ \"query\": \"user data\", }) result, err := runner.ProcessEvent(context.Background(), event) if err != nil { log.Fatal(err) } fmt.Printf(\"Final output: %v\\n\", result.Data[\"final_output\"])\rLoop Orchestration // Create agent for iterative processing qualityChecker := core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Get current iteration count iteration, _ := state.Get(\"iteration\") if iteration == nil { iteration = 0 } iterationCount := iteration.(int) + 1 state.Set(\"iteration\", iterationCount) // Get content to check content, ok := event.Data[\"content\"].(string) if !ok { content, _ = state.Get(\"content\").(string) } // Simulate quality checking qualityScore := calculateQuality(content) // Improve content if quality is low if qualityScore \u003c 0.8 \u0026\u0026 iterationCount \u003c 5 { improvedContent := improveContent(content) state.Set(\"content\", improvedContent) return core.AgentResult{ Data: map[string]interface{}{ \"quality_score\": qualityScore, \"iteration\": iterationCount, \"continue\": true, // Signal to continue loop }, }, nil } // Quality is good enough or max iterations reached return core.AgentResult{ Data: map[string]interface{}{ \"quality_score\": qualityScore, \"iteration\": iterationCount, \"final_content\": content, \"continue\": false, // Signal to stop loop }, }, nil }) // Create loop runner runner := core.CreateLoopRunner(qualityChecker, 5) // Max 5 iterations // Process event event := core.NewEvent(\"quality_check\", map[string]interface{}{ \"content\": \"initial content that needs improvement\", }) result, err := runner.ProcessEvent(context.Background(), event) if err != nil { log.Fatal(err) } fmt.Printf(\"Final quality score: %v\\n\", result.Data[\"quality_score\"]) fmt.Printf(\"Iterations: %v\\n\", result.Data[\"iteration\"]) fmt.Printf(\"Final content: %v\\n\", result.Data[\"final_content\"])\rMixed Orchestration // Create mixed orchestration with both collaborative and sequential phases builder := core.NewOrchestrationBuilder(core.OrchestrationMixed). WithCollaborativeAgents(map[string]core.AgentHandler{ \"analyzer\": analyzerAgent, \"validator\": validatorAgent, }). WithSequentialAgents(map[string]core.AgentHandler{ \"processor\": processorAgent, \"reporter\": reporterAgent, }). WithTimeout(5 * time.Minute). WithFailureThreshold(0.8) runner := builder.Build() // The mixed orchestration will: // 1. Run analyzer and validator in parallel (collaborative phase) // 2. Then run processor and reporter in sequence (sequential phase)\r🏗️ Orchestration Builder OrchestrationBuilder Interface type OrchestrationBuilder interface { WithAgents(agents map[string]AgentHandler) *OrchestrationBuilder WithCollaborativeAgents(agents map[string]AgentHandler) *OrchestrationBuilder WithSequentialAgents(agents map[string]AgentHandler) *OrchestrationBuilder WithTimeout(timeout time.Duration) *OrchestrationBuilder WithMaxIterations(max int) *OrchestrationBuilder WithFailureThreshold(threshold float64) *OrchestrationBuilder WithMaxConcurrency(max int) *OrchestrationBuilder WithRetryPolicy(policy *RetryPolicy) *OrchestrationBuilder Build() Runner GenerateMermaidDiagram() string GenerateMermaidDiagramWithConfig(config MermaidConfig) string }\rAdvanced Configuration // Create sophisticated orchestration with all options runner := core.NewOrchestrationBuilder(core.OrchestrationCollaborate). WithAgents(agents). WithTimeout(2 * time.Minute). WithFailureThreshold(0.8). // 80% of agents must succeed WithMaxConcurrency(5). // Max 5 concurrent agents WithRetryPolicy(\u0026core.RetryPolicy{ MaxRetries: 3, InitialDelay: time.Second, MaxDelay: 30 * time.Second, BackoffFactor: 2.0, }). Build()\r📊 Workflow Visualization Generating Workflow Diagrams // Generate Mermaid diagram for orchestration builder := core.NewOrchestrationBuilder(core.OrchestrationCollaborate). WithAgents(agents) // Generate basic diagram diagram := builder.GenerateMermaidDiagram() fmt.Println(diagram) // Generate diagram with custom configuration config := core.MermaidConfig{ DiagramType: core.FlowchartDiagram, Title: \"Research Workflow\", Direction: \"TD\", // Top-Down Theme: \"dark\", ShowMetadata: true, ShowAgentTypes: true, CompactMode: false, } customDiagram := builder.GenerateMermaidDiagramWithConfig(config) fmt.Println(customDiagram) // Save diagram to file err := core.SaveDiagramAsMarkdown(\"workflow.md\", \"Research Workflow\", customDiagram) if err != nil { log.Printf(\"Failed to save diagram: %v\", err) }\rExample Generated Diagram ---\rtitle: Collaborative Research Workflow\r---\rflowchart TD\rINPUT[\"🎯 Event Input\"]\rRESEARCHER[\"🤖 Researcher\u003cbr/\u003eType: Research Agent\u003cbr/\u003eTimeout: 2m\"]\rANALYZER[\"🤖 Analyzer\u003cbr/\u003eType: Analysis Agent\u003cbr/\u003eTimeout: 2m\"]\rVALIDATOR[\"🤖 Validator\u003cbr/\u003eType: Validation Agent\u003cbr/\u003eTimeout: 2m\"]\rOUTPUT[\"✅ Aggregated Result\"]\rINPUT --\u003e RESEARCHER\rINPUT --\u003e ANALYZER\rINPUT --\u003e VALIDATOR\rRESEARCHER --\u003e OUTPUT\rANALYZER --\u003e OUTPUT\rVALIDATOR --\u003e OUTPUT\rstyle RESEARCHER fill:#e1f5fe\rstyle ANALYZER fill:#e8f5e8\rstyle VALIDATOR fill:#fff3e0\r🔧 Runner Interface Core Runner Methods type Runner interface { // ProcessEvent processes a single event through the orchestration ProcessEvent(ctx context.Context, event Event) (map[string]AgentResult, error) // RegisterAgent adds an agent to the orchestration RegisterAgent(name string, handler AgentHandler) error // UnregisterAgent removes an agent from the orchestration UnregisterAgent(name string) error // ListAgents returns all registered agent names ListAgents() []string // GetAgent retrieves a specific agent by name GetAgent(name string) (AgentHandler, bool) // SetOrchestrationMode changes the orchestration pattern SetOrchestrationMode(mode OrchestrationMode) error // GetOrchestrationMode returns the current orchestration pattern GetOrchestrationMode() OrchestrationMode // Stop gracefully shuts down the runner Stop() error }\rRunner Factory Functions // Create runners for specific patterns func CreateRouteRunner(agents map[string]AgentHandler) Runner func CreateCollaborativeRunner(agents map[string]AgentHandler, timeout time.Duration) Runner func CreateSequentialRunner(agents map[string]AgentHandler, order []string) Runner func CreateLoopRunner(agent AgentHandler, maxIterations int) Runner func CreateFaultTolerantRunner(agents map[string]AgentHandler) Runner func CreateLoadBalancedRunner(agents map[string]AgentHandler, maxConcurrency int) Runner\r🔄 Event Routing Route-Based Processing // Create agents with different capabilities agents := map[string]core.AgentHandler{ \"chat\": chatAgent, \"search\": searchAgent, \"analyze\": analyzeAgent, } // Create route runner runner := core.CreateRouteRunner(agents) // Route to specific agent using metadata chatEvent := core.NewEvent(\"user_message\", map[string]interface{}{\"message\": \"Hello\"}, map[string]string{\"route\": \"chat\"}, // Route to chat agent ) searchEvent := core.NewEvent(\"search_query\", map[string]interface{}{\"query\": \"latest news\"}, map[string]string{\"route\": \"search\"}, // Route to search agent ) // Process events - each goes to specified agent chatResult, _ := runner.ProcessEvent(context.Background(), chatEvent) searchResult, _ := runner.ProcessEvent(context.Background(), searchEvent)\rDynamic Routing // Create router function func routeEvent(event core.Event) string { data := event.GetData() // Route based on event content if query, ok := data[\"query\"].(string); ok { if strings.Contains(strings.ToLower(query), \"search\") { return \"search\" } if strings.Contains(strings.ToLower(query), \"analyze\") { return \"analyze\" } } // Default to chat return \"chat\" } // Use custom routing logic runner := core.CreateRouteRunnerWithRouter(agents, routeEvent)\r📈 Performance and Monitoring Execution Metrics // Enable metrics collection runner := core.NewOrchestrationBuilder(core.OrchestrationCollaborate). WithAgents(agents). WithMetrics(true). Build() // Process events result, err := runner.ProcessEvent(ctx, event) // Get execution metrics metrics := runner.GetMetrics() fmt.Printf(\"Total execution time: %v\\n\", metrics.TotalDuration) fmt.Printf(\"Agent execution times: %v\\n\", metrics.AgentDurations) fmt.Printf(\"Success rate: %.2f%%\\n\", metrics.SuccessRate*100)\rHealth Checks // Check runner health health := runner.HealthCheck(ctx) if !health.Healthy { log.Printf(\"Runner unhealthy: %v\", health.Issues) } // Check individual agent health for agentName := range agents { agentHealth := runner.CheckAgentHealth(ctx, agentName) if !agentHealth.Healthy { log.Printf(\"Agent %s unhealthy: %v\", agentName, agentHealth.Issues) } }\r🛡️ Error Handling and Resilience Retry Policies retryPolicy := \u0026core.RetryPolicy{ MaxRetries: 3, InitialDelay: time.Second, MaxDelay: 30 * time.Second, BackoffFactor: 2.0, RetryableErrors: []error{ context.DeadlineExceeded, \u0026net.OpError{}, }, } runner := core.NewOrchestrationBuilder(core.OrchestrationCollaborate). WithAgents(agents). WithRetryPolicy(retryPolicy). Build()\rCircuit Breaker // Enable circuit breaker for fault tolerance runner := core.NewOrchestrationBuilder(core.OrchestrationCollaborate). WithAgents(agents). WithCircuitBreaker(\u0026core.CircuitBreakerConfig{ FailureThreshold: 5, // Open after 5 failures RecoveryTimeout: 30 * time.Second, HalfOpenRequests: 3, // Test with 3 requests when half-open }). Build()\rGraceful Degradation // Configure graceful degradation runner := core.NewOrchestrationBuilder(core.OrchestrationCollaborate). WithAgents(agents). WithFailureThreshold(0.6). // Continue if 60% of agents succeed WithGracefulDegradation(true). Build() // Even if some agents fail, you'll get partial results result, err := runner.ProcessEvent(ctx, event) if err != nil { // Check if it's a partial failure if partialErr, ok := err.(*core.PartialFailureError); ok { log.Printf(\"Partial failure: %d/%d agents succeeded\", partialErr.SuccessCount, partialErr.TotalCount) // Use partial results usePartialResults(result) } }\rThis comprehensive Orchestration API reference covers all aspects of multi-agent coordination in AgenticGoKit, from basic patterns to advanced configuration and monitoring.",
    "tags": [],
    "title": "orchestration",
    "uri": "/AgenticGoKitDocs/reference/api/orchestration/index.html"
  },
  {
    "breadcrumb": "content \u003e guides \u003e setup",
    "content": "Orchestration Configuration Guide Complete guide to configuring multi-agent orchestration in AgentFlow\nAgentFlow supports configuration-driven orchestration through agentflow.toml files, allowing you to change orchestration patterns without modifying code. This guide covers all orchestration modes, configuration options, and best practices.\nQuick Start 1. Generate Project with Orchestration # Sequential pipeline agentcli create data-pipeline \\ --orchestration-mode sequential \\ --sequential-agents \"collector,processor,formatter\" \\ --orchestration-timeout 45 # Collaborative workflow agentcli create research-system \\ --orchestration-mode collaborative \\ --agents 3 \\ --orchestration-timeout 60 # Loop-based processing agentcli create quality-checker \\ --orchestration-mode loop \\ --loop-agent \"validator\" \\ --max-iterations 5\r2. Configuration-Based Runner Generated projects automatically use configuration-based orchestration:\n// Load configuration and create runner automatically runner, err := core.NewRunnerFromConfig(\"agentflow.toml\") if err != nil { log.Fatal(err) } // Create provider from configuration provider, err := core.NewProviderFromConfig(\"agentflow.toml\") if err != nil { log.Fatal(err) } // Create agents agents := map[string]core.AgentHandler{ \"agent1\": core.NewLLMAgent(\"agent1\", provider), \"agent2\": core.NewLLMAgent(\"agent2\", provider), \"agent3\": core.NewLLMAgent(\"agent3\", provider), } // Register agents with the runner for name, handler := range agents { if err := runner.RegisterAgent(name, handler); err != nil { log.Fatalf(\"Failed to register agent %s: %v\", name, err) } } // Start the runner - orchestration mode is handled automatically from config ctx := context.Background() if err := runner.Start(ctx); err != nil { log.Fatal(err) } defer runner.Stop()\rOrchestration Configuration Reference Basic Configuration Structure [orchestration] mode = \"sequential\" # Required: orchestration mode timeout_seconds = 30 # Required: timeout for operations max_iterations = 5 # Optional: for loop mode # Mode-specific agent configuration sequential_agents = [\"agent1\", \"agent2\", \"agent3\"] collaborative_agents = [\"analyzer\", \"validator\"] loop_agent = \"processor\"\rConfiguration Options Option Type Description Default Required mode string Orchestration mode \"route\" Yes timeout_seconds integer Operation timeout 30 Yes max_iterations integer Max loop iterations 5 For loop mode sequential_agents array Sequential agent list [] For sequential/mixed collaborative_agents array Collaborative agent list [] For collaborative/mixed loop_agent string Loop agent name \"\" For loop mode Orchestration Modes 1. Sequential Mode Agents process events in a defined order, creating a processing pipeline.\nConfiguration [orchestration] mode = \"sequential\" timeout_seconds = 30 sequential_agents = [\"collector\", \"processor\", \"formatter\"]\rCLI Generation agentcli create data-pipeline \\ --orchestration-mode sequential \\ --sequential-agents \"collector,processor,formatter\" \\ --orchestration-timeout 30\rUse Cases Data processing pipelines Document transformation workflows Multi-step analysis processes Quality assurance chains Flow Diagram flowchart TD\rINPUT[\"📨 Input Event\"]\rCOLLECTOR[\"🤖 Collector\"]\rPROCESSOR[\"🤖 Processor\"]\rFORMATTER[\"🤖 Formatter\"]\rOUTPUT[\"📤 Final Result\"]\rINPUT --\u003e COLLECTOR\rCOLLECTOR --\u003e PROCESSOR\rPROCESSOR --\u003e FORMATTER\rFORMATTER --\u003e OUTPUT\r2. Collaborative Mode All agents process events in parallel, with results aggregated.\nConfiguration [orchestration] mode = \"collaborative\" timeout_seconds = 60 # Agents are automatically determined from registered agents\rCLI Generation agentcli create research-system \\ --orchestration-mode collaborative \\ --agents 3 \\ --orchestration-timeout 60\rUse Cases Research and analysis systems Multi-perspective evaluation Parallel data processing Consensus-building workflows Flow Diagram flowchart TD\rINPUT[\"📨 Input Event\"]\rAGENT1[\"🤖 Agent1\"]\rAGENT2[\"🤖 Agent2\"]\rAGENT3[\"🤖 Agent3\"]\rAGGREGATOR[\"📊 Result Aggregator\"]\rOUTPUT[\"📤 Final Result\"]\rINPUT --\u003e AGENT1\rINPUT --\u003e AGENT2\rINPUT --\u003e AGENT3\rAGENT1 --\u003e AGGREGATOR\rAGENT2 --\u003e AGGREGATOR\rAGENT3 --\u003e AGGREGATOR\rAGGREGATOR --\u003e OUTPUT\r3. Loop Mode Single agent processes events iteratively until conditions are met.\nConfiguration [orchestration] mode = \"loop\" timeout_seconds = 120 loop_agent = \"quality-checker\" max_iterations = 5\rCLI Generation agentcli create quality-loop \\ --orchestration-mode loop \\ --loop-agent \"quality-checker\" \\ --max-iterations 5 \\ --orchestration-timeout 120\rUse Cases Quality assurance loops Iterative refinement processes Retry mechanisms with improvement Convergence-based processing Flow Diagram flowchart TD\rINPUT[\"📨 Input Event\"]\rAGENT[\"🤖 Quality Checker\"]\rCONDITION{\"✅ Quality Met?\"}\rITERATE{\"🔄 Max Iterations?\"}\rOUTPUT[\"📤 Final Result\"]\rINPUT --\u003e AGENT\rAGENT --\u003e CONDITION\rCONDITION --\u003e|No| ITERATE\rITERATE --\u003e|No| AGENT\rITERATE --\u003e|Yes| OUTPUT\rCONDITION --\u003e|Yes| OUTPUT\r4. Mixed Mode Combines collaborative and sequential processing patterns.\nConfiguration [orchestration] mode = \"mixed\" timeout_seconds = 90 collaborative_agents = [\"analyzer\", \"validator\"] sequential_agents = [\"processor\", \"reporter\"]\rCLI Generation agentcli create complex-workflow \\ --orchestration-mode mixed \\ --collaborative-agents \"analyzer,validator\" \\ --sequential-agents \"processor,reporter\" \\ --orchestration-timeout 90\rUse Cases Complex business processes Multi-stage workflows with parallel components Hybrid processing patterns Enterprise workflow automation Flow Diagram flowchart TD\rINPUT[\"📨 Input Event\"]\rsubgraph PARALLEL[\"Collaborative Phase\"]\rANALYZER[\"🤖 Analyzer\"]\rVALIDATOR[\"🤖 Validator\"]\rend\rsubgraph SEQUENTIAL[\"Sequential Phase\"]\rPROCESSOR[\"🤖 Processor\"]\rREPORTER[\"🤖 Reporter\"]\rend\rOUTPUT[\"📤 Final Result\"]\rINPUT --\u003e PARALLEL\rANALYZER --\u003e PROCESSOR\rVALIDATOR --\u003e PROCESSOR\rPROCESSOR --\u003e REPORTER\rREPORTER --\u003e OUTPUT\r5. Route Mode Default event routing based on event metadata (legacy mode).\nConfiguration [orchestration] mode = \"route\" timeout_seconds = 30\rCLI Generation agentcli create simple-router \\ --orchestration-mode route \\ --orchestration-timeout 30\rUse Cases Simple event routing Legacy system compatibility Dynamic agent selection Conditional processing Advanced Configuration Environment-Specific Configurations Development Configuration # agentflow.dev.toml [orchestration] mode = \"sequential\" timeout_seconds = 60 # Longer timeout for debugging sequential_agents = [\"agent1\", \"agent2\"]\rProduction Configuration # agentflow.prod.toml [orchestration] mode = \"mixed\" timeout_seconds = 30 # Shorter timeout for performance collaborative_agents = [\"analyzer\", \"validator\"] sequential_agents = [\"processor\", \"reporter\"]\rLoading Environment-Specific Configs configFile := \"agentflow.toml\" if env := os.Getenv(\"AGENTFLOW_ENV\"); env != \"\" { configFile = fmt.Sprintf(\"agentflow.%s.toml\", env) } runner, err := core.NewRunnerFromConfig(configFile) if err != nil { log.Fatal(err) } // Create provider from configuration provider, err := core.NewProviderFromConfig(configFile) if err != nil { log.Fatal(err) } // Create agents agents := map[string]core.AgentHandler{ \"agent1\": core.NewLLMAgent(\"agent1\", provider), \"agent2\": core.NewLLMAgent(\"agent2\", provider), } // Register agents with the runner for name, handler := range agents { if err := runner.RegisterAgent(name, handler); err != nil { log.Fatalf(\"Failed to register agent %s: %v\", name, err) } }\rDynamic Configuration Updates // Load initial configuration runner, err := core.NewRunnerFromConfig(\"agentflow.toml\") if err != nil { log.Fatal(err) } // Create provider from configuration provider, err := core.NewProviderFromConfig(\"agentflow.toml\") if err != nil { log.Fatal(err) } // Create agents agents := map[string]core.AgentHandler{ \"agent1\": core.NewLLMAgent(\"agent1\", provider), \"agent2\": core.NewLLMAgent(\"agent2\", provider), } // Register agents with the runner for name, handler := range agents { if err := runner.RegisterAgent(name, handler); err != nil { log.Fatalf(\"Failed to register agent %s: %v\", name, err) } } // Watch for configuration changes watcher, err := fsnotify.NewWatcher() if err != nil { log.Fatal(err) } go func() { for { select { case event := \u003c-watcher.Events: if event.Op\u0026fsnotify.Write == fsnotify.Write { // Reload configuration and create new runner newRunner, err := core.NewRunnerFromConfig(\"agentflow.toml\") if err != nil { log.Printf(\"Failed to reload config: %v\", err) continue } // Stop current runner runner.Stop() // Re-register agents with new runner for name, handler := range agents { if err := newRunner.RegisterAgent(name, handler); err != nil { log.Printf(\"Failed to register agent %s: %v\", name, err) } } // Start new runner runner = newRunner runner.Start(context.Background()) } } } }() watcher.Add(\"agentflow.toml\")\rConfiguration Validation Built-in Validation AgenticGoKit automatically validates orchestration configurations:\nconfig, err := core.LoadConfigFromWorkingDir() if err != nil { // Configuration validation errors are returned here log.Printf(\"Configuration error: %v\", err) return } // Validate orchestration configuration if err := config.ValidateOrchestrationConfig(); err != nil { log.Printf(\"Orchestration validation error: %v\", err) return }\rCommon Validation Errors Error Cause Solution missing orchestration mode No mode specified Add mode = \"sequential\" invalid orchestration mode Unknown mode value Use valid mode: sequential, collaborative, loop, mixed, route sequential mode missing agents No sequential_agents for sequential mode Add sequential_agents = [\"agent1\", \"agent2\"] loop mode missing agent No loop_agent for loop mode Add loop_agent = \"agent1\" mixed mode missing agents No agents specified for mixed mode Add both collaborative and sequential agents invalid timeout Timeout ≤ 0 Set timeout_seconds to positive integer invalid max iterations Max iterations ≤ 0 for loop mode Set max_iterations to positive integer Manual Validation # Validate configuration file agentcli config validate agentflow.toml # Validate with detailed output agentcli config validate --verbose agentflow.toml\rBest Practices 1. Configuration Organization # Group related configurations together [orchestration] # Core orchestration settings mode = \"mixed\" timeout_seconds = 60 max_iterations = 5 # Agent assignments collaborative_agents = [\"analyzer\", \"validator\"] sequential_agents = [\"processor\", \"reporter\"] # Comments for complex configurations # Analyzer and validator run in parallel for speed # Processor and reporter run sequentially for data consistency\r2. Timeout Configuration [orchestration] # Set timeouts based on expected processing time timeout_seconds = 30 # Fast operations # timeout_seconds = 120 # Complex analysis # timeout_seconds = 300 # Heavy processing\r3. Agent Naming Conventions [orchestration] # Use descriptive, consistent names sequential_agents = [ \"data-collector\", \"data-processor\", \"data-formatter\", \"data-validator\" ] # Avoid generic names like \"agent1\", \"agent2\"\r4. Mode Selection Guidelines Use Case Recommended Mode Reason Data pipelines Sequential Maintains data flow order Research/analysis Collaborative Leverages parallel processing Quality assurance Loop Iterative improvement Complex workflows Mixed Combines benefits of both Simple routing Route Minimal overhead 5. Testing Configurations # Test different orchestration modes agentcli create test-sequential --orchestration-mode sequential agentcli create test-collaborative --orchestration-mode collaborative agentcli create test-loop --orchestration-mode loop # To test different configurations, you can create multiple config files # and run your application with each one to compare performance\rMigration Guide From Hardcoded to Configuration-Based Before (Hardcoded) // Old approach - hardcoded orchestration agents := map[string]core.AgentHandler{ \"agent1\": myAgent1, \"agent2\": myAgent2, } runner := core.NewRunnerWithOrchestration(core.EnhancedRunnerConfig{ RunnerConfig: core.RunnerConfig{ Agents: agents, Memory: memory, SessionID: sessionID, }, OrchestrationMode: core.OrchestrationSequential, SequentialAgents: []string{\"agent1\", \"agent2\"}, })\rAfter (Configuration-Based) // New approach - configuration-driven runner, err := core.NewRunnerFromConfig(\"agentflow.toml\") if err != nil { log.Fatal(err) } // Create provider from configuration provider, err := core.NewProviderFromConfig(\"agentflow.toml\") if err != nil { log.Fatal(err) } // Create agents agents := map[string]core.AgentHandler{ \"agent1\": core.NewLLMAgent(\"agent1\", provider), \"agent2\": core.NewLLMAgent(\"agent2\", provider), } // Register agents with the runner for name, handler := range agents { if err := runner.RegisterAgent(name, handler); err != nil { log.Fatalf(\"Failed to register agent %s: %v\", name, err) } } // Start the runner ctx := context.Background() if err := runner.Start(ctx); err != nil { log.Fatal(err) } defer runner.Stop()\rConfiguration File [orchestration] mode = \"sequential\" timeout_seconds = 30 sequential_agents = [\"agent1\", \"agent2\"]\rMigration Steps Create Configuration File\nagentcli config show \u003e agentflow.toml\rAdd Orchestration Section\n[orchestration] mode = \"sequential\" timeout_seconds = 30 sequential_agents = [\"agent1\", \"agent2\"]\rUpdate Code\n// Replace hardcoded orchestration runner, err := core.NewRunnerFromConfig(\"agentflow.toml\")\rTest Configuration\nagentcli config validate agentflow.toml\rTroubleshooting Common Issues Configuration Not Loading # Check file exists and is readable ls -la agentflow.toml # Validate configuration syntax agentcli config validate agentflow.toml # Check for TOML syntax errors toml-lint agentflow.toml\rAgents Not Found // Ensure agents are registered before starting for name, handler := range agents { if err := runner.RegisterAgent(name, handler); err != nil { log.Printf(\"Failed to register agent %s: %v\", name, err) } }\rTimeout Issues [orchestration] # Increase timeout for complex operations timeout_seconds = 120 # Instead of 30 # For loop mode, also check max_iterations max_iterations = 10 # Instead of 5\rDebug Configuration // Enable debug logging logger := zerolog.New(os.Stdout).Level(zerolog.DebugLevel) ctx := logger.WithContext(context.Background()) // Load configuration with debug context runner, err := core.NewRunnerFromConfig(\"agentflow.toml\")\rPerformance Monitoring # Monitor orchestration performance agentcli status --detailed # Check agent execution times agentcli trace list --since 1h # To test different configurations, you can create multiple config files # and run your application with each one\rExamples Complete Configuration Examples E-commerce Order Processing [orchestration] mode = \"sequential\" timeout_seconds = 45 sequential_agents = [ \"order-validator\", \"inventory-checker\", \"payment-processor\", \"fulfillment-handler\", \"notification-sender\" ]\rContent Moderation System [orchestration] mode = \"collaborative\" timeout_seconds = 30 # All registered agents participate in parallel moderation\rDocument Analysis Pipeline [orchestration] mode = \"mixed\" timeout_seconds = 120 collaborative_agents = [\"text-analyzer\", \"image-analyzer\"] sequential_agents = [\"content-merger\", \"report-generator\"]\rQuality Assurance Loop [orchestration] mode = \"loop\" timeout_seconds = 180 loop_agent = \"quality-checker\" max_iterations = 3\rThis guide provides comprehensive coverage of AgentFlow’s orchestration configuration system, enabling you to build flexible, maintainable multi-agent workflows.",
    "description": "Orchestration Configuration Guide Complete guide to configuring multi-agent orchestration in AgentFlow\nAgentFlow supports configuration-driven orchestration through agentflow.toml files, allowing you to change orchestration patterns without modifying code. This guide covers all orchestration modes, configuration options, and best practices.\nQuick Start 1. Generate Project with Orchestration # Sequential pipeline agentcli create data-pipeline \\ --orchestration-mode sequential \\ --sequential-agents \"collector,processor,formatter\" \\ --orchestration-timeout 45 # Collaborative workflow agentcli create research-system \\ --orchestration-mode collaborative \\ --agents 3 \\ --orchestration-timeout 60 # Loop-based processing agentcli create quality-checker \\ --orchestration-mode loop \\ --loop-agent \"validator\" \\ --max-iterations 5\r2. Configuration-Based Runner Generated projects automatically use configuration-based orchestration:",
    "tags": [],
    "title": "orchestration-configuration",
    "uri": "/AgenticGoKitDocs/guides/setup/orchestration-configuration/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials \u003e core-concepts",
    "content": "Multi-Agent Orchestration Patterns Overview AgenticGoKit provides powerful multi-agent orchestration patterns that enable you to build complex workflows with multiple agents working together. The system supports various orchestration modes and includes built-in workflow visualization using Mermaid diagrams.\nPrerequisites Understanding of Agent Lifecycle Familiarity with State Management Basic knowledge of Go concurrency Learning Objectives By the end of this tutorial, you’ll understand:\nDifferent orchestration patterns and their use cases How to configure each orchestration mode using CLI and configuration files Performance characteristics of each pattern When to use which pattern How to visualize your orchestration workflows CLI Quick Start The fastest way to create multi-agent workflows is using the AgentFlow CLI:\n# Collaborative workflow - all agents process events in parallel agentcli create research-system \\ --orchestration-mode collaborative \\ --agents 3 \\ --orchestration-timeout 60 \\ --visualize \\ --mcp-enabled # Sequential pipeline - agents process one after another agentcli create data-pipeline \\ --orchestration-mode sequential \\ --sequential-agents \"collector,processor,formatter\" \\ --orchestration-timeout 45 \\ --visualize-output \"docs/diagrams\" # Loop-based workflow - single agent repeats with conditions agentcli create quality-loop \\ --orchestration-mode loop \\ --loop-agent \"quality-checker\" \\ --max-iterations 5 \\ --orchestration-timeout 120 \\ --visualize # Mixed orchestration - combine collaborative and sequential agentcli create complex-workflow \\ --orchestration-mode mixed \\ --collaborative-agents \"analyzer,validator\" \\ --sequential-agents \"processor,reporter\" \\ --orchestration-timeout 90 \\ --visualize\rAll generated projects use configuration-based orchestration via agentflow.toml, making it easy to modify orchestration patterns without changing code.\nConfiguration-Based Orchestration AgentFlow supports configuration-driven orchestration through agentflow.toml files. This approach allows you to change orchestration patterns without modifying code.\nagentflow.toml Configuration [orchestration] mode = \"sequential\" # sequential, collaborative, loop, mixed, route timeout_seconds = 30 # Timeout for orchestration operations max_iterations = 5 # Maximum iterations for loop mode # Sequential mode: agents process in order sequential_agents = [\"agent1\", \"agent2\", \"agent3\"] # Collaborative mode: agents process in parallel collaborative_agents = [\"analyzer\", \"validator\", \"processor\"] # Loop mode: single agent repeats loop_agent = \"processor\" # Mixed mode: combine collaborative and sequential # collaborative_agents = [\"analyzer\", \"validator\"] # sequential_agents = [\"processor\", \"reporter\"]\rUsing Configuration-Based Runners Generated projects automatically use configuration-based orchestration:\n// Load configuration from agentflow.toml config, err := core.LoadConfigFromWorkingDir() if err != nil { log.Fatal(err) } // Create provider from configuration provider, err := config.InitializeProvider() if err != nil { log.Fatal(err) } // Create agents agents := map[string]core.AgentHandler{ \"agent1\": core.NewLLMAgent(\"agent1\", provider), \"agent2\": core.NewLLMAgent(\"agent2\", provider), \"agent3\": core.NewLLMAgent(\"agent3\", provider), } // Create runner with orchestration from config var runner core.Runner switch config.Orchestration.Mode { case \"collaborative\": runner = core.CreateCollaborativeRunner(agents, time.Duration(config.Orchestration.TimeoutSeconds)*time.Second) case \"sequential\": runner = core.NewOrchestrationBuilder(core.OrchestrationSequential). WithAgents(agents). WithTimeout(time.Duration(config.Orchestration.TimeoutSeconds)*time.Second). Build() default: runner = core.CreateRouteRunner(agents) } // Start the runner ctx := context.Background() if err := runner.Start(ctx); err != nil { log.Fatal(err) } defer runner.Stop()\rBenefits of Configuration-Based Approach No Code Changes: Switch orchestration modes by editing TOML files Environment-Specific: Different configs for dev/staging/production Runtime Flexibility: Change orchestration without rebuilding Validation: Built-in validation of orchestration parameters Consistency: Same configuration format across all projects Orchestration Patterns 1. Route Pattern (Default) Use Case: Simple request-response scenarios where each event goes to a specific agent.\n// Route orchestration (default) runner := core.NewRunner() runner.RegisterAgent(\"research\", researchAgent) runner.RegisterAgent(\"analysis\", analysisAgent) // Events are routed to specific agents based on routing metadata event := core.NewEvent(\"research\", data) event.SetMeta(core.RouteMetadataKey, \"research\") // Routes to research agent\rCharacteristics:\nSingle agent processes each event Fast and efficient for simple scenarios No coordination overhead Easy to debug and understand 2. Collaborative Pattern Use Case: When you need multiple perspectives or parallel processing of the same input.\nThink of collaborative orchestration like a research team where multiple experts examine the same problem from different angles and contribute their unique insights to create a comprehensive solution.\n// Collaborative orchestration - all agents process the same event agents := map[string]core.AgentHandler{ \"researcher\": NewResearchAgent(), \"analyzer\": NewAnalysisAgent(), \"validator\": NewValidationAgent(), } runner := core.CreateCollaborativeRunner(agents, 30*time.Second) // All agents process the event in parallel event := core.NewEvent(\"analyze\", data) result, err := runner.ProcessEvent(ctx, event)\rFlow Diagram:\n┌─────────────┐\r│ Researcher │\r└─────────────┘\r▲\r│\r┌─────────┐ ┌──────────────┐\r│ Client │────▶│ Collaborative│\r└─────────┘ │ Orchestrator │\r└──────────────┘\r│\r▼\r┌─────────────┐\r│ Analyzer │\r└─────────────┘\r│\r▼\r┌─────────────┐\r│ Validator │\r└─────────────┘\rCharacteristics:\nAll agents process the same event simultaneously Results are automatically aggregated Higher throughput for complex analysis Built-in fault tolerance Perfect for ensemble approaches and multiple perspectives 3. Sequential Pattern Use Case: Data processing pipelines where each step builds on the previous.\nThink of sequential orchestration like an assembly line where each worker (agent) performs a specific task and passes the work to the next worker in line, with each step adding value to the final product.\n// Sequential orchestration - agents process in order runner := core.NewOrchestrationBuilder(core.OrchestrationSequential). WithAgents(map[string]core.AgentHandler{ \"collector\": NewCollectorAgent(), \"processor\": NewProcessorAgent(), \"formatter\": NewFormatterAgent(), }). WithSequentialOrder([]string{\"collector\", \"processor\", \"formatter\"}). Build()\rFlow Diagram:\n┌─────────┐ ┌──────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐\r│ Client │────▶│ Sequence │────▶│Collector│────▶│Processor│────▶│Formatter│\r└─────────┘ │Orchestr. │ └─────────┘ └─────────┘ └─────────┘\r└──────────┘ │\r▲ │\r└────────────────────────────────────────────────┘\rState Flow: The state object carries data through the entire pipeline, with each agent adding or transforming data for the next agent.\nCharacteristics:\nAgents process events in a specific order Output of one agent becomes input to the next Perfect for data transformation pipelines Easy to reason about data flow State propagates through the entire sequence 4. Loop Pattern Use Case: Iterative refinement or quality improvement workflows.\n// Loop orchestration - single agent repeats until conditions are met runner := core.NewOrchestrationBuilder(core.OrchestrationLoop). WithAgent(\"quality-checker\", NewQualityCheckerAgent()). WithMaxIterations(10). Build()\rCharacteristics:\nSingle agent processes the same event multiple times Continues until completion condition is met or max iterations reached Useful for iterative improvement Built-in loop detection and termination 5. Mixed Pattern Use Case: Complex workflows that need both parallel and sequential processing.\n// Mixed orchestration - combines collaborative and sequential patterns runner := core.NewOrchestrationBuilder(core.OrchestrationMixed). WithCollaborativeAgents(map[string]core.AgentHandler{ \"analyzer\": NewAnalyzerAgent(), \"validator\": NewValidatorAgent(), }). WithSequentialAgents([]string{\"processor\", \"reporter\"}). Build()\rCharacteristics:\nCollaborative agents run first in parallel Sequential agents run after collaborative phase completes Combines benefits of both patterns More complex but very powerful Configuration-Based Orchestration You can configure orchestration patterns using agentflow.toml:\n[orchestration] mode = \"collaborative\" # route, collaborative, sequential, loop, mixed timeout_seconds = 30 max_iterations = 5 # For loop mode # For sequential mode sequential_agents = [\"collector\", \"processor\", \"formatter\"] # For mixed mode collaborative_agents = [\"analyzer\", \"validator\"] sequential_agents = [\"processor\", \"reporter\"] # For loop mode loop_agent = \"quality-checker\"\rThen load the configuration:\nrunner, err := core.NewRunnerFromConfig(\"agentflow.toml\") if err != nil { log.Fatal(err) } // Register your agents runner.RegisterAgent(\"collector\", collectorAgent) runner.RegisterAgent(\"processor\", processorAgent) // ... register other agents // The orchestration mode is handled automatically result, err := runner.ProcessEvent(ctx, event)\rPattern Selection Guide Pattern Best For Latency Throughput Complexity Route Simple requests Low Medium Low Collaborative Analysis, multiple perspectives Medium High Medium Sequential Data pipelines High Low Low Loop Iterative refinement High Low Medium Mixed Complex workflows High Medium High Performance Considerations Collaborative Pattern Pros: High throughput, fault tolerance, multiple perspectives Cons: Higher resource usage, result aggregation complexity Best for: Analysis tasks, validation workflows Sequential Pattern Pros: Clear data flow, easy debugging, resource efficient Cons: Higher latency, single point of failure Best for: Data transformation, step-by-step processing Loop Pattern Pros: Iterative improvement, quality assurance Cons: Potentially long execution time, complexity in termination Best for: Quality checking, iterative refinement Error Handling in Orchestration Different patterns handle errors differently:\n// Collaborative: Continues if some agents succeed runner := core.CreateCollaborativeRunner(agents, timeout). WithFailureThreshold(0.5) // Continue if 50% succeed // Sequential: Stops on first failure runner := core.NewOrchestrationBuilder(core.OrchestrationSequential). WithRetryPolicy(\u0026core.RetryPolicy{ MaxRetries: 3, BackoffFactor: 2.0, }). Build() // Loop: Can retry iterations runner := core.NewOrchestrationBuilder(core.OrchestrationLoop). WithMaxIterations(10). WithRetryPolicy(retryPolicy). Build()\rMonitoring and Observability All orchestration patterns support built-in monitoring:\n// Enable tracing for orchestration runner.RegisterCallback(core.HookBeforeOrchestration, func(ctx context.Context, event core.Event) { core.Logger().Info().Str(\"event_id\", event.GetID()).Msg(\"Starting orchestration\") }) runner.RegisterCallback(core.HookAfterOrchestration, func(ctx context.Context, result core.AgentResult) { core.Logger().Info(). Dur(\"duration\", result.Duration). Bool(\"success\", result.Error == \"\"). Msg(\"Orchestration completed\") })\rBest Practices Start Simple: Begin with Route pattern and add complexity as needed Consider Latency: Sequential patterns have higher latency than parallel ones Resource Management: Collaborative patterns use more resources Error Handling: Design appropriate error handling for your pattern Monitoring: Always add monitoring for production orchestration Testing: Test each pattern thoroughly with your specific use case Common Patterns Research and Analysis Workflow // Collaborative for research, sequential for reporting runner := core.NewOrchestrationBuilder(core.OrchestrationMixed). WithCollaborativeAgents(map[string]core.AgentHandler{ \"web-researcher\": webResearchAgent, \"doc-analyzer\": docAnalysisAgent, \"fact-checker\": factCheckAgent, }). WithSequentialAgents([]string{\"synthesizer\", \"formatter\"}). Build()\rData Processing Pipeline // Sequential processing with error handling runner := core.NewOrchestrationBuilder(core.OrchestrationSequential). WithAgents(map[string]core.AgentHandler{ \"validator\": dataValidatorAgent, \"transformer\": dataTransformerAgent, \"enricher\": dataEnricherAgent, \"publisher\": dataPublisherAgent, }). WithSequentialOrder([]string{\"validator\", \"transformer\", \"enricher\", \"publisher\"}). WithRetryPolicy(\u0026core.RetryPolicy{MaxRetries: 3}). Build()\rQuality Assurance Loop // Loop until quality threshold is met runner := core.NewOrchestrationBuilder(core.OrchestrationLoop). WithAgent(\"quality-checker\", qualityAgent). WithMaxIterations(5). WithTerminationCondition(func(result core.AgentResult) bool { quality, _ := result.State.Get(\"quality_score\") return quality.(float64) \u003e= 0.95 }). Build()\rWorkflow Visualization AgentFlow automatically generates Mermaid diagrams for all orchestration patterns:\n# Generate diagrams in default location (./workflow.mmd) agentcli create my-workflow --visualize # Specify custom output directory agentcli create my-workflow --visualize-output \"docs/diagrams\"\rGenerated Mermaid Diagram Example:\n---\rtitle: Collaborative Multi-Agent Workflow\r---\rflowchart TD\rINPUT[\"🎯 Event Input\"]\rAGENT1[\"🤖 Researcher\"]\rAGENT2[\"🤖 Analyzer\"] AGENT3[\"🤖 Validator\"]\rOUTPUT[\"✅ Aggregated Result\"]\rINPUT --\u003e AGENT1\rINPUT --\u003e AGENT2\rINPUT --\u003e AGENT3\rAGENT1 --\u003e OUTPUT\rAGENT2 --\u003e OUTPUT\rAGENT3 --\u003e OUTPUT\rAPI Reference CLI Configuration Options All CLI flags for multi-agent orchestration:\n# Orchestration mode flags --orchestration-mode string # collaborative, sequential, loop, mixed --collaborative-agents string # Comma-separated list of agents --sequential-agents string # Comma-separated list of agents --loop-agent string # Single agent name for loop mode --max-iterations int # Maximum loop iterations (default: 10) # Configuration flags --orchestration-timeout int # Timeout in seconds (default: 60) --failure-threshold float # Failure threshold 0.0-1.0 (default: 0.5) --max-concurrency int # Maximum concurrent agents (default: 5) # Visualization flags --visualize # Generate Mermaid diagrams --visualize-output string # Custom output directory for diagrams\rOrchestration Modes type OrchestrationMode string const ( OrchestrationRoute OrchestrationMode = \"route\" // Route to single agent OrchestrationCollaborate OrchestrationMode = \"collaborate\" // Send to all agents OrchestrationSequential OrchestrationMode = \"sequential\" // Process in sequence OrchestrationLoop OrchestrationMode = \"loop\" // Loop single agent OrchestrationMixed OrchestrationMode = \"mixed\" // Combine patterns )\rOrchestrator Interface type Orchestrator interface { Dispatch(ctx context.Context, event Event) (AgentResult, error) RegisterAgent(name string, handler AgentHandler) error GetCallbackRegistry() *CallbackRegistry Stop() }\rAdvanced Configuration Options OrchestrationConfig type OrchestrationConfig struct { Timeout time.Duration // Overall orchestration timeout MaxConcurrency int // Maximum concurrent agents FailureThreshold float64 // Failure threshold (0.0-1.0) RetryPolicy *RetryPolicy // Retry configuration }\rRetryPolicy type RetryPolicy struct { MaxRetries int // Maximum retry attempts InitialDelay time.Duration // Initial delay before first retry MaxDelay time.Duration // Maximum delay between retries BackoffFactor float64 // Exponential backoff multiplier Jitter bool // Add random jitter to delays RetryableErrors []string // List of retryable error codes }\rConvenience Functions CreateCollaborativeRunner Creates a runner where all agents process events in parallel.\nfunc CreateCollaborativeRunner(agents map[string]AgentHandler, timeout time.Duration) Runner\rCreateFaultTolerantRunner Creates a collaborative runner with aggressive retry policies for environments with transient failures.\nfunc CreateFaultTolerantRunner(agents map[string]AgentHandler) Runner\rCreateLoadBalancedRunner Creates a runner that distributes load across multiple agent instances.\nfunc CreateLoadBalancedRunner(agents map[string]AgentHandler, maxConcurrency int) Runner\rMigration from Internal APIs If you were previously using internal orchestrator packages:\nReplace internal/orchestrator imports with core Use core.NewCollaborativeOrchestrator() instead of internal constructors Update agent handlers to use public core.AgentHandler interface Use public core.Event and core.State types Performance Considerations Collaborative orchestration runs agents concurrently for better performance Configure MaxConcurrency to limit resource usage Use timeouts to prevent resource leaks Monitor agent execution times and optimize slow agents Consider using FailureThreshold to fail fast when many agents are failing Next Steps Error Handling - Learn advanced error handling patterns State Management - Understand state flow in orchestration Visualization Guide - Learn workflow visualization Performance Optimization - Scale your orchestration Troubleshooting Common Issues:\nDeadlocks in Mixed Mode: Ensure proper state management between phases Memory Leaks in Loop Mode: Implement proper termination conditions Timeout Issues: Adjust timeout values based on your agent complexity State Corruption: Use proper state isolation in collaborative mode Agents Not Registered: Ensure all agents are registered before dispatching events Configuration Errors: Validate your agentflow.toml configuration For more help, see the Troubleshooting Guide.",
    "description": "Multi-Agent Orchestration Patterns Overview AgenticGoKit provides powerful multi-agent orchestration patterns that enable you to build complex workflows with multiple agents working together. The system supports various orchestration modes and includes built-in workflow visualization using Mermaid diagrams.\nPrerequisites Understanding of Agent Lifecycle Familiarity with State Management Basic knowledge of Go concurrency Learning Objectives By the end of this tutorial, you’ll understand:\nDifferent orchestration patterns and their use cases How to configure each orchestration mode using CLI and configuration files Performance characteristics of each pattern When to use which pattern How to visualize your orchestration workflows CLI Quick Start The fastest way to create multi-agent workflows is using the AgentFlow CLI:",
    "tags": [],
    "title": "orchestration-patterns",
    "uri": "/AgenticGoKitDocs/tutorials/core-concepts/orchestration-patterns/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials \u003e getting-started",
    "content": "Production Deployment Tutorial (15 minutes) Overview Learn how to deploy and scale your AgenticGoKit agents for production use. You’ll containerize your application, set up monitoring, implement fault tolerance, and configure for high availability.\nPrerequisites Complete the Tool Integration tutorial Docker installed Basic understanding of containerization and deployment Learning Objectives By the end of this tutorial, you’ll understand:\nHow to containerize AgenticGoKit applications Production configuration management Monitoring and observability setup Fault tolerance and error handling Scaling strategies for multi-agent systems What You’ll Build A production-ready agent system with:\nDocker containerization for consistent deployment Configuration management for different environments Monitoring and logging for observability Fault tolerance with circuit breakers and retries Health checks and graceful shutdown Part 1: Containerization (5 minutes) Package your agents for consistent deployment across environments.\nCreate a Production-Ready Project # Create production-ready project with all features agentcli create production-system --memory-enabled --memory-provider pgvector \\ --mcp-production --with-cache --with-metrics --rag-enabled --agents 3 cd production-system\rUnderstanding the Generated Dockerfile The project includes a multi-stage Dockerfile:\n# Build stage FROM golang:1.21-alpine AS builder WORKDIR /app COPY go.mod go.sum ./ RUN go mod download COPY . . RUN CGO_ENABLED=0 GOOS=linux go build -o main . # Production stage FROM alpine:latest RUN apk --no-cache add ca-certificates tzdata WORKDIR /root/ COPY --from=builder /app/main . COPY --from=builder /app/agentflow.toml . EXPOSE 8080 CMD [\"./main\"]\rBuild and Test Locally # Build the Docker image docker build -t production-system:latest . # Test the container docker run --rm -e OPENAI_API_KEY=your-key production-system:latest\rUnderstanding Docker Compose Setup The generated docker-compose.yml includes all services:\nversion: '3.8' services: app: build: . ports: - \"8080:8080\" environment: - OPENAI_API_KEY=${OPENAI_API_KEY} - DATABASE_URL=postgres://agentflow:password@postgres:5432/agentflow depends_on: - postgres - redis restart: unless-stopped postgres: image: pgvector/pgvector:pg15 environment: POSTGRES_DB: agenticgokit POSTGRES_USER: agenticgokit POSTGRES_PASSWORD: password volumes: - postgres_data:/var/lib/postgresql/data ports: - \"5432:5432\" redis: image: redis:7-alpine ports: - \"6379:6379\" volumes: - redis_data:/data volumes: postgres_data: redis_data:\rPart 2: Configuration Management (5 minutes) Set up proper configuration for different environments.\nEnvironment-Specific Configuration The project includes multiple configuration files:\nagentflow.toml (Development):\n[agent_flow] name = \"production-system\" version = \"1.0.0\" provider = \"openai\" [logging] level = \"debug\" format = \"json\" [runtime] max_concurrent_agents = 5 timeout_seconds = 30 [agent_memory] provider = \"pgvector\" connection = \"postgres://agenticgokit:password@localhost:5432/agenticgokit\"\ragentflow.prod.toml (Production):\n[agent_flow] name = \"production-system\" version = \"1.0.0\" provider = \"openai\" [logging] level = \"info\" format = \"json\" file = \"/var/log/agenticgokit.log\" [runtime] max_concurrent_agents = 20 timeout_seconds = 60 [agent_memory] provider = \"pgvector\" connection = \"${DATABASE_URL}\" [error_routing] enabled = true max_retries = 3 enable_circuit_breaker = true [mcp.production] enabled = true metrics_enabled = true circuit_breaker_enabled = true\rEnvironment Variable Management Create .env.example:\n# LLM Provider OPENAI_API_KEY=your-openai-api-key-here # Database DATABASE_URL=postgres://agenticgokit:password@localhost:5432/agenticgokit # External APIs WEATHER_API_KEY=your-weather-api-key WEB_SEARCH_API_KEY=your-search-api-key # Application LOG_LEVEL=info MAX_CONCURRENT_AGENTS=20\rConfiguration Loading The generated code includes environment-aware configuration:\nfunc loadConfig() (*core.Config, error) { // Determine environment env := os.Getenv(\"ENVIRONMENT\") if env == \"\" { env = \"development\" } // Load appropriate config file var configPath string switch env { case \"production\": configPath = \"agentflow.prod.toml\" case \"staging\": configPath = \"agentflow.staging.toml\" default: configPath = \"agentflow.toml\" } return core.LoadConfig(configPath) }\rPart 3: Monitoring and Observability (5 minutes) Set up comprehensive monitoring for production systems.\nMetrics and Health Checks The production configuration includes metrics:\n[mcp.metrics] enabled = true port = 8080 path = \"/metrics\" [monitoring] enable_health_checks = true health_check_port = 8081\rHealth Check Endpoint The generated code includes health checks:\nfunc setupHealthChecks(runner core.Runner) { http.HandleFunc(\"/health\", func(w http.ResponseWriter, r *http.Request) { // Check runner status if runner == nil { http.Error(w, \"Runner not initialized\", http.StatusServiceUnavailable) return } // Check database connectivity if err := checkDatabase(); err != nil { http.Error(w, \"Database unavailable\", http.StatusServiceUnavailable) return } // Check MCP tools if err := checkMCPTools(); err != nil { http.Error(w, \"Tools unavailable\", http.StatusServiceUnavailable) return } w.WriteHeader(http.StatusOK) json.NewEncoder(w).Encode(map[string]string{ \"status\": \"healthy\", \"timestamp\": time.Now().Format(time.RFC3339), }) }) log.Println(\"Health checks available at :8081/health\") go http.ListenAndServe(\":8081\", nil) }\rStructured Logging Production logging configuration:\nfunc setupLogging(config *core.Config) { // Configure structured logging logger := zerolog.New(os.Stdout).With(). Timestamp(). Str(\"service\", \"production-system\"). Str(\"version\", \"1.0.0\"). Logger() // Set global logger core.SetLogger(logger) // Log startup information logger.Info(). Str(\"environment\", os.Getenv(\"ENVIRONMENT\")). Str(\"log_level\", config.Logging.Level). Msg(\"Application starting\") }\rMetrics Collection func setupMetrics() { // Agent execution metrics agentExecutionTime := prometheus.NewHistogramVec( prometheus.HistogramOpts{ Name: \"agent_execution_duration_seconds\", Help: \"Time spent executing agents\", }, []string{\"agent_name\", \"status\"}, ) // Tool execution metrics toolExecutionTime := prometheus.NewHistogramVec( prometheus.HistogramOpts{ Name: \"tool_execution_duration_seconds\", Help: \"Time spent executing tools\", }, []string{\"tool_name\", \"status\"}, ) // Register metrics prometheus.MustRegister(agentExecutionTime, toolExecutionTime) // Expose metrics endpoint http.Handle(\"/metrics\", promhttp.Handler()) }\rProduction Deployment Strategies Docker Deployment # Build for production docker build -t production-system:v1.0.0 . # Run with production config docker run -d \\ --name production-system \\ -p 8080:8080 \\ -p 8081:8081 \\ -e ENVIRONMENT=production \\ -e OPENAI_API_KEY=${OPENAI_API_KEY} \\ -e DATABASE_URL=${DATABASE_URL} \\ --restart unless-stopped \\ production-system:v1.0.0\rDocker Compose Production # Start all services docker-compose -f docker-compose.prod.yml up -d # Check service health docker-compose ps curl http://localhost:8081/health # View logs docker-compose logs -f app\rKubernetes Deployment Create k8s/deployment.yaml:\napiVersion: apps/v1 kind: Deployment metadata: name: production-system spec: replicas: 3 selector: matchLabels: app: production-system template: metadata: labels: app: production-system spec: containers: - name: app image: production-system:v1.0.0 ports: - containerPort: 8080 - containerPort: 8081 env: - name: ENVIRONMENT value: \"production\" - name: OPENAI_API_KEY valueFrom: secretKeyRef: name: api-keys key: openai-key livenessProbe: httpGet: path: /health port: 8081 initialDelaySeconds: 30 periodSeconds: 10 readinessProbe: httpGet: path: /health port: 8081 initialDelaySeconds: 5 periodSeconds: 5 --- apiVersion: v1 kind: Service metadata: name: production-system-service spec: selector: app: production-system ports: - name: app port: 8080 targetPort: 8080 - name: health port: 8081 targetPort: 8081\rFault Tolerance Configuration Circuit Breaker Setup [error_routing.circuit_breaker] failure_threshold = 10 success_threshold = 5 timeout_ms = 60000 max_concurrent_calls = 3 [mcp.production.circuit_breaker] failure_threshold = 5 success_threshold = 3 timeout_ms = 30000\rRetry Policies [error_routing.retry] max_retries = 3 base_delay_ms = 1000 max_delay_ms = 30000 backoff_factor = 2.0 enable_jitter = true\rGraceful Shutdown func main() { // ... setup code ... // Setup graceful shutdown c := make(chan os.Signal, 1) signal.Notify(c, os.Interrupt, syscall.SIGTERM) go func() { \u003c-c log.Println(\"Shutting down gracefully...\") // Stop runner runner.Stop() // Close database connections if db != nil { db.Close() } // Close trace logger if traceLogger != nil { traceLogger.Close() } os.Exit(0) }() // ... start services ... }\rMonitoring and Alerting Prometheus Metrics Key metrics to monitor:\nAgent execution time and success rate Tool execution time and error rate Memory usage and garbage collection Database connection pool status Circuit breaker state changes Grafana Dashboard Create dashboards for:\nSystem overview (CPU, memory, requests) Agent performance (execution time, error rate) Tool usage (calls per minute, cache hit rate) Database metrics (connections, query time) Alerting Rules # prometheus-alerts.yml groups: - name: agenticgokit rules: - alert: HighErrorRate expr: rate(agent_execution_errors_total[5m]) \u003e 0.1 for: 2m annotations: summary: \"High error rate detected\" - alert: DatabaseDown expr: up{job=\"postgres\"} == 0 for: 1m annotations: summary: \"Database is down\"\rPerformance Optimization Resource Limits # In Kubernetes deployment resources: requests: memory: \"256Mi\" cpu: \"250m\" limits: memory: \"512Mi\" cpu: \"500m\"\rConnection Pooling [agent_memory] provider = \"pgvector\" connection = \"${DATABASE_URL}\" max_connections = 20 idle_connections = 5 connection_lifetime = \"1h\"\rCaching Strategy [mcp.cache] backend = \"redis\" redis_url = \"${REDIS_URL}\" default_timeout_ms = 300000 max_size = 10000\rTroubleshooting Production Issues Common Issues High memory usage:\n# Check memory metrics curl http://localhost:8080/metrics | grep memory # Adjust configuration [runtime] max_concurrent_agents = 10 # Reduce if needed\rDatabase connection issues:\n# Check database connectivity docker-compose exec postgres psql -U agentflow -d agentflow -c \"SELECT 1;\" # Check connection pool curl http://localhost:8081/health\rTool execution failures:\n# Check MCP server status agentcli mcp servers # Check circuit breaker status curl http://localhost:8080/metrics | grep circuit_breaker\rDebugging Commands # View application logs docker-compose logs -f app # Check system resources docker stats # Monitor database docker-compose exec postgres pg_stat_activity # Test health endpoints curl http://localhost:8081/health curl http://localhost:8080/metrics\rSecurity Best Practices Environment Variables # Use secrets management export OPENAI_API_KEY=$(vault kv get -field=key secret/openai) # Rotate keys regularly # Use least-privilege access # Monitor API usage\rNetwork Security # In docker-compose.yml networks: internal: driver: bridge internal: true external: driver: bridge services: app: networks: - internal - external postgres: networks: - internal # Database not exposed externally\rNext Steps Your agents are now production-ready! Consider:\nAdvanced Monitoring: Set up distributed tracing with Jaeger Auto-scaling: Implement horizontal pod autoscaling in Kubernetes Multi-region: Deploy across multiple regions for high availability CI/CD: Set up automated deployment pipelines Key Takeaways Containerization: Docker ensures consistent deployment across environments Configuration: Environment-specific configs for different deployment stages Monitoring: Comprehensive observability with metrics, logs, and health checks Fault Tolerance: Circuit breakers and retries for resilient systems Security: Proper secret management and network isolation Further Reading Advanced Patterns - Production patterns and best practices Monitoring Guide - Deep dive into monitoring Security Best Practices - Comprehensive security guide",
    "description": "Production Deployment Tutorial (15 minutes) Overview Learn how to deploy and scale your AgenticGoKit agents for production use. You’ll containerize your application, set up monitoring, implement fault tolerance, and configure for high availability.\nPrerequisites Complete the Tool Integration tutorial Docker installed Basic understanding of containerization and deployment Learning Objectives By the end of this tutorial, you’ll understand:\nHow to containerize AgenticGoKit applications Production configuration management Monitoring and observability setup Fault tolerance and error handling Scaling strategies for multi-agent systems What You’ll Build A production-ready agent system with:",
    "tags": [],
    "title": "production-deployment",
    "uri": "/AgenticGoKitDocs/tutorials/getting-started/production-deployment/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials \u003e getting-started",
    "content": "🚀 5-Minute Quickstart Get your first AgenticGoKit multi-agent system running in 5 minutes. No complex setup, no configuration files—just working code.\nWhat You’ll Build A simple but powerful multi-agent system where:\n🤖 Agent 1 processes your request 🤖 Agent 2 enhances the response 🤖 Agent 3 formats the final output All working together automatically!\nPrerequisites Go 1.21+ (install here) OpenAI API Key (get one here) That’s it! No Docker, no databases, no complex setup.\nChoose Your Approach 🚀 Option A: CLI Approach (Fastest - 2 minutes) Perfect for getting started quickly with scaffolded projects.\nStep 1: Install CLI and Create Project # Install the AgenticGoKit CLI go install github.com/kunalkushwaha/agenticgokit/cmd/agentcli@latest # Create a collaborative multi-agent project agentcli create my-agents --agents 3 --orchestration-mode collaborative cd my-agents\rStep 2: Configure and Run # Set your OpenAI API key export OPENAI_API_KEY=your-api-key-here # Run your multi-agent system go run main.go\r💻 Option B: Code-First Approach (Learn by doing - 3 minutes) Perfect for understanding how AgenticGoKit works under the hood.\nStep 1: Create Your Project mkdir my-agents \u0026\u0026 cd my-agents go mod init my-agents go get github.com/kunalkushwaha/agenticgokit\rStep 2: Create Configuration Create agentflow.toml:\n[agent_flow] name = \"my-agents\" version = \"1.0.0\" provider = \"openai\" [logging] level = \"info\" format = \"json\" [providers.openai] # API key will be read from OPENAI_API_KEY environment variable\rStep 3: Write Your Multi-Agent System Create main.go:\npackage main import ( \"context\" \"fmt\" \"log\" \"strings\" \"time\" \"github.com/kunalkushwaha/agenticgokit/core\" ) func main() { // 🔑 Set up your LLM provider from environment provider, err := core.NewProviderFromWorkingDir() if err != nil { log.Fatalf(\"Failed to create LLM provider: %v\", err) } // 🤖 Create three specialized agents agents := map[string]core.AgentHandler{ \"processor\": \u0026ProcessorAgent{llm: provider}, \"enhancer\": \u0026EnhancerAgent{llm: provider}, \"formatter\": \u0026FormatterAgent{llm: provider}, } // 🚀 Create a collaborative runner (agents work together) runner := core.CreateCollaborativeRunner(agents, 30*time.Second) // 💬 Process a message - watch the magic happen! fmt.Println(\"🤖 Starting multi-agent collaboration...\") // Start the runner ctx := context.Background() runner.Start(ctx) defer runner.Stop() // Create an event for processing event := core.NewEvent(\"processor\", core.EventData{ \"input\": \"Explain quantum computing in simple terms\", }, map[string]string{ \"route\": \"processor\", }) // Emit the event to the runner if err := runner.Emit(event); err != nil { log.Fatalf(\"Failed to emit event: %v\", err) } // Wait for processing to complete time.Sleep(5 * time.Second) fmt.Println(\"\\n✅ Multi-Agent Processing Complete!\") fmt.Println(\"=\" + strings.Repeat(\"=\", 50)) fmt.Printf(\"📊 Execution Stats:\\n\") fmt.Printf(\" • Agents involved: %d\\n\", len(agents)) fmt.Printf(\" • Event ID: %s\\n\", event.GetID()) } // ProcessorAgent handles initial processing type ProcessorAgent struct { llm core.ModelProvider } func (a *ProcessorAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Get user input from event data input, ok := event.GetData()[\"input\"].(string) if !ok { return core.AgentResult{}, fmt.Errorf(\"no input provided\") } // Process with LLM prompt := core.Prompt{ System: \"You are a processor agent. Extract and organize key information from user requests.\", User: fmt.Sprintf(\"Process this request and extract key information: %s\", input), } response, err := a.llm.Call(ctx, prompt) if err != nil { return core.AgentResult{}, err } // Update state with processed result outputState := core.NewState() outputState.Set(\"processed\", response.Content) outputState.Set(\"message\", response.Content) // Route to enhancer outputState.SetMeta(core.RouteMetadataKey, \"enhancer\") return core.AgentResult{OutputState: outputState}, nil } // EnhancerAgent enhances the processed information type EnhancerAgent struct { llm core.ModelProvider } func (a *EnhancerAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Get processed result from state var processed interface{} if processedData, exists := state.Get(\"processed\"); exists { processed = processedData } else if msg, exists := state.Get(\"message\"); exists { processed = msg } else { return core.AgentResult{}, fmt.Errorf(\"no processed data found\") } // Enhance with LLM prompt := core.Prompt{ System: \"You are an enhancer agent. Add insights, context, and additional valuable information.\", User: fmt.Sprintf(\"Enhance this response with additional insights: %v\", processed), } response, err := a.llm.Call(ctx, prompt) if err != nil { return core.AgentResult{}, err } // Update state with enhanced result outputState := core.NewState() outputState.Set(\"enhanced\", response.Content) outputState.Set(\"message\", response.Content) // Route to formatter outputState.SetMeta(core.RouteMetadataKey, \"formatter\") return core.AgentResult{OutputState: outputState}, nil } // FormatterAgent formats the final response type FormatterAgent struct { llm core.ModelProvider } func (a *FormatterAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Get enhanced result from state var enhanced interface{} if enhancedData, exists := state.Get(\"enhanced\"); exists { enhanced = enhancedData } else if msg, exists := state.Get(\"message\"); exists { enhanced = msg } else { return core.AgentResult{}, fmt.Errorf(\"no enhanced data found\") } // Format with LLM prompt := core.Prompt{ System: \"You are a formatter agent. Present information in a clear, professional, and well-structured manner.\", User: fmt.Sprintf(\"Format this response in a clear, professional manner: %v\", enhanced), } response, err := a.llm.Call(ctx, prompt) if err != nil { return core.AgentResult{}, err } // Update state with final result outputState := core.NewState() outputState.Set(\"final_response\", response.Content) outputState.Set(\"message\", response.Content) // Print the final result fmt.Printf(\"\\n📝 Final Response:\\n%s\\n\", response.Content) return core.AgentResult{OutputState: outputState}, nil }\rStep 4: Run It! export OPENAI_API_KEY=your-api-key-here go mod tidy go run main.go\rYou should see:\n🤖 Starting multi-agent collaboration...\r📝 Final Response:\rQuantum computing is a revolutionary technology that uses quantum mechanics principles to process information in fundamentally different ways than classical computers. Instead of using traditional bits that can only be 0 or 1, quantum computers use quantum bits (qubits) that can exist in multiple states simultaneously through a property called superposition...\r✅ Multi-Agent Processing Complete!\r==================================================\r📊 Execution Stats:\r• Agents involved: 3\r• Event ID: evt_abc123\r🎉 Congratulations! You just created a multi-agent system that:\n✅ Runs three agents in parallel ✅ Combines their outputs intelligently ✅ Handles errors gracefully ✅ Provides execution metrics And it took less than 5 minutes!\n🤔 What Just Happened? The Magic Behind the Scenes 🏗️ Agent Creation: Each agent has a specialized role and system prompt 🤝 Collaborative Orchestration: CreateCollaborativeRunner makes agents work together ⚡ Parallel Processing: All agents process your message simultaneously 🧠 Intelligent Combination: Results are automatically merged and enhanced 📊 Built-in Monitoring: You get metrics and error handling for free Key Concepts You Just Used core.AgentHandler: The interface for all agents with Run() method core.ModelProvider: Interface for LLM providers with Call() method core.CreateCollaborativeRunner: Orchestrates multiple agents in parallel runner.Start() and runner.Emit(): Start runner and emit events for processing core.NewEvent(): Creates events with data and metadata core.State: Thread-safe state management between agents core.AgentResult: Result structure with output state and error handling core.Prompt: Structured prompt with system and user messages 🚀 Next Steps Now that you have a working multi-agent system, here’s what to explore next:\n🎓 15-Minute Tutorials (Choose Your Path) 🤝 Multi-Agent Patterns Learn different orchestration modes:\nCollaborative (parallel) Sequential (pipeline) Mixed (hybrid workflows) → Multi-Agent Tutorial\n🧠 Memory \u0026 RAG Add persistent memory and knowledge:\nVector databases Document ingestion Semantic search → Memory Tutorial\n🔧 Tool Integration Connect to external tools:\nWeb search File operations API integrations Custom tools → Tools Tutorial\n🏭 Production Ready Deploy and scale your agents:\nDocker deployment Monitoring setup Performance optimization → Production Tutorial\n🎯 Quick Wins (5-10 minutes each) 🔄 Try Sequential Processing - Build a data processing pipeline 🌐 Add Web Search - Give your agents internet access 💾 Add Memory - Make agents remember conversations 📊 Add Monitoring - See what your agents are doing 🏗️ Build Something Cool Ready to build a real application? Try these examples:\n# Research assistant with web search and analysis agentcli create research-assistant --mcp-enabled --mcp-tools \"web_search,summarize\" # Data processing pipeline with error handling agentcli create data-pipeline --orchestration-mode sequential --agents 4 # Chat system with persistent memory agentcli create chat-system --memory-enabled --memory-provider pgvector # Knowledge base with document ingestion and RAG agentcli create knowledge-base --memory-enabled --memory-provider pgvector --rag-enabled\r🆘 Need Help? Common Issues ❌ “OpenAI API key not found”\n# Make sure your API key is set export OPENAI_API_KEY=sk-your-key-here echo $OPENAI_API_KEY # Should show your key\r❌ “Module not found”\n# Make sure you're in the right directory and ran go mod init go mod tidy\r❌ “Context deadline exceeded”\n# Increase the timeout if agents are taking too long runner := core.CreateCollaborativeRunner(agents, 60*time.Second) // Increased to 60s\rGet Support 💬 Discord Community - Real-time help 💡 GitHub Discussions - Q\u0026A 📖 Troubleshooting Guide - Common solutions 🎯 What’s Next? You’ve successfully created your first multi-agent system! Here are some paths to continue your AgenticGoKit journey:\n🎓 Take the 15-Minute Tutorial Learn advanced orchestration patterns\n🏗️ Build a Real Application Explore production-ready examples\n📖 Read the Full Documentation Dive deep into all features\n⏱️ Actual time: Most developers complete this in 3-4 minutes. The extra minute is for reading and understanding!",
    "description": "🚀 5-Minute Quickstart Get your first AgenticGoKit multi-agent system running in 5 minutes. No complex setup, no configuration files—just working code.\nWhat You’ll Build A simple but powerful multi-agent system where:\n🤖 Agent 1 processes your request 🤖 Agent 2 enhances the response 🤖 Agent 3 formats the final output All working together automatically!\nPrerequisites Go 1.21+ (install here) OpenAI API Key (get one here) That’s it! No Docker, no databases, no complex setup.",
    "tags": [],
    "title": "quickstart",
    "uri": "/AgenticGoKitDocs/tutorials/getting-started/quickstart/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials \u003e memory-systems",
    "content": "RAG Implementation in AgenticGoKit Overview Retrieval-Augmented Generation (RAG) combines the power of large language models with external knowledge retrieval to provide more accurate, up-to-date, and contextually relevant responses. This tutorial covers implementing RAG systems in AgenticGoKit, from basic retrieval to advanced techniques.\nRAG enables agents to access vast amounts of information while maintaining the conversational abilities of language models, making them more knowledgeable and reliable.\nPrerequisites Understanding of Vector Databases Familiarity with Basic Memory Operations Knowledge of language model APIs Basic understanding of information retrieval concepts RAG Architecture Basic RAG Flow ┌─────────────┐ ┌──────────────┐ ┌─────────────────┐\r│ User Query │───▶│ Retrieval │───▶│ Context + Query │\r└─────────────┘ │ System │ └─────────────────┘\r└──────────────┘ │\r│ ▼\r▼ ┌─────────────────┐\r┌──────────────┐ │ Language Model │\r│ Vector Store │ │ Generation │\r└──────────────┘ └─────────────────┘\r│\r▼\r┌─────────────────┐\r│ Enhanced │\r│ Response │\r└─────────────────┘\rAdvanced RAG Components Query Processing: Understanding and reformulating user queries Retrieval: Finding relevant information from knowledge base Reranking: Improving relevance of retrieved results Context Building: Constructing effective prompts Generation: Producing responses with retrieved context Response Enhancement: Post-processing and validation Basic RAG Implementation 1. Simple RAG Agent package main import ( \"context\" \"fmt\" \"log\" \"os\" \"strings\" \"github.com/kunalkushwaha/agenticgokit/core\" ) type BasicRAGAgent struct { name string memory core.Memory llm core.ModelProvider config RAGConfig } type RAGConfig struct { MaxRetrievalResults int ScoreThreshold float32 MaxContextLength int ContextTemplate string } func NewBasicRAGAgent(name string, memory core.Memory, llm core.ModelProvider) *BasicRAGAgent { return \u0026BasicRAGAgent{ name: name, memory: memory, llm: llm, config: RAGConfig{ MaxRetrievalResults: 5, ScoreThreshold: 0.7, MaxContextLength: 2000, ContextTemplate: `Based on the following information: %s Please answer the question: %s`, }, } } func (r *BasicRAGAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Extract user query query, ok := state.Get(\"message\") if !ok { return core.AgentResult{}, fmt.Errorf(\"no message in state\") } queryStr := query.(string) // Retrieve relevant context contextStr, sources, err := r.retrieveContext(ctx, queryStr) if err != nil { return core.AgentResult{}, fmt.Errorf(\"retrieval failed: %w\", err) } // Generate response with context response, err := r.generateResponse(ctx, queryStr, contextStr) if err != nil { return core.AgentResult{}, fmt.Errorf(\"generation failed: %w\", err) } // Store the interaction sessionID, _ := event.GetMetadataValue(core.SessionIDKey) err = r.storeInteraction(ctx, sessionID, queryStr, response, sources) if err != nil { log.Printf(\"Failed to store interaction: %v\", err) } // Return result outputState := state.Clone() outputState.Set(\"response\", response) outputState.Set(\"sources\", sources) outputState.Set(\"context_used\", len(sources) \u003e 0) return core.AgentResult{OutputState: outputState}, nil } func (r *BasicRAGAgent) retrieveContext(ctx context.Context, query string) (string, []string, error) { // Search for relevant information results, err := r.memory.Search(ctx, query, core.WithLimit(r.config.MaxRetrievalResults), core.WithScoreThreshold(r.config.ScoreThreshold), ) if err != nil { return \"\", nil, fmt.Errorf(\"search failed: %w\", err) } if len(results) == 0 { return \"\", nil, nil // No relevant context found } // Build context string var contextBuilder strings.Builder sources := make([]string, 0, len(results)) for i, result := range results { // Add numbered context item contextBuilder.WriteString(fmt.Sprintf(\"%d. %s\\n\", i+1, result.Content)) // Track sources if source, ok := result.Metadata[\"source\"]; ok { sources = append(sources, source) } else { sources = append(sources, fmt.Sprintf(\"Document %s\", result.ID)) } } context := contextBuilder.String() // Truncate if too long if len(context) \u003e r.config.MaxContextLength { context = context[:r.config.MaxContextLength] + \"...\" } return context, sources, nil } func (r *BasicRAGAgent) generateResponse(ctx context.Context, query, context string) (string, error) { var prompt string if context != \"\" { // Use context template prompt = fmt.Sprintf(r.config.ContextTemplate, context, query) } else { // Fallback to direct query prompt = fmt.Sprintf(\"Please answer the following question: %s\", query) } // Generate response response, err := r.llm.Generate(ctx, prompt) if err != nil { return \"\", fmt.Errorf(\"LLM generation failed: %w\", err) } return response, nil } func (r *BasicRAGAgent) storeInteraction(ctx context.Context, sessionID, query, response string, sources []string) error { // Store user query err := r.memory.Store(ctx, query, \"user-query\", core.WithSession(sessionID), core.WithTimestamp(time.Now()), core.WithMetadata(map[string]string{ \"interaction_type\": \"rag-query\", }), ) if err != nil { return err } // Store agent response with sources sourcesStr := strings.Join(sources, \", \") err = r.memory.Store(ctx, response, \"agent-response\", core.WithSession(sessionID), core.WithTimestamp(time.Now()), core.WithMetadata(map[string]string{ \"interaction_type\": \"rag-response\", \"sources_used\": sourcesStr, \"sources_count\": fmt.Sprintf(\"%d\", len(sources)), }), ) return err } func main() { // Setup memory with vector database memory, err := core.NewMemory(core.AgentMemoryConfig{ Provider: \"pgvector\", Connection: \"postgres://user:pass@localhost:5432/agentdb\", EnableRAG: true, Dimensions: 1536, Embedding: core.EmbeddingConfig{ Provider: \"openai\", Model: \"text-embedding-3-small\", APIKey: os.Getenv(\"OPENAI_API_KEY\"), Dimensions: 1536, }, }) if err != nil { log.Fatalf(\"Failed to create memory: %v\", err) } // Setup LLM provider llm, err := core.NewOpenAIAdapter( os.Getenv(\"OPENAI_API_KEY\"), \"gpt-3.5-turbo\", 2000, 0.7, ) if err != nil { log.Fatalf(\"Failed to create LLM: %v\", err) } // Create RAG agent ragAgent := NewBasicRAGAgent(\"rag-assistant\", memory, llm) // Test the agent ctx := context.Background() // First, populate some knowledge knowledge := []string{ \"AgenticGoKit is a Go framework for building multi-agent systems with support for LLM integration, memory systems, and orchestration patterns.\", \"Vector databases like pgvector and Weaviate are used in AgenticGoKit for semantic search and RAG implementations.\", \"The framework supports multiple orchestration patterns including route, collaborative, sequential, and loop modes.\", } for _, info := range knowledge { memory.Store(ctx, info, \"knowledge\", core.WithMetadata(map[string]string{ \"source\": \"documentation\", \"topic\": \"agenticgokit\", }), ) } // Test query event := core.NewEvent( \"rag-assistant\", core.EventData{\"message\": \"What is AgenticGoKit and what databases does it support?\"}, map[string]string{\"session_id\": \"test-session\"}, ) state := core.NewState() state.Set(\"message\", \"What is AgenticGoKit and what databases does it support?\") result, err := ragAgent.Run(ctx, event, state) if err != nil { log.Fatalf(\"RAG agent failed: %v\", err) } response, _ := result.OutputState.Get(\"response\") sources, _ := result.OutputState.Get(\"sources\") fmt.Printf(\"Response: %s\\n\", response) fmt.Printf(\"Sources: %v\\n\", sources) }\rAdvanced RAG Techniques 1. Query Enhancement type QueryEnhancer struct { llm core.LLMProvider } func NewQueryEnhancer(llm core.LLMProvider) *QueryEnhancer { return \u0026QueryEnhancer{llm: llm} } func (qe *QueryEnhancer) EnhanceQuery(ctx context.Context, originalQuery string, conversationHistory []core.Message) (string, error) { // Build context from conversation history var historyBuilder strings.Builder for _, msg := range conversationHistory { historyBuilder.WriteString(fmt.Sprintf(\"%s: %s\\n\", msg.Role, msg.Content)) } // Create enhancement prompt prompt := fmt.Sprintf(`Given the conversation history: %s The user's current query is: \"%s\" Please rewrite this query to be more specific and searchable, incorporating relevant context from the conversation history. The enhanced query should be optimized for semantic search. Enhanced query:`, historyBuilder.String(), originalQuery) enhancedQuery, err := qe.llm.Generate(ctx, prompt) if err != nil { // Fallback to original query return originalQuery, nil } return strings.TrimSpace(enhancedQuery), nil } // Multi-query generation for better retrieval func (qe *QueryEnhancer) GenerateMultipleQueries(ctx context.Context, originalQuery string) ([]string, error) { prompt := fmt.Sprintf(`Given the query: \"%s\" Generate 3 different ways to ask the same question that would help find relevant information: 1. 2. 3.`, originalQuery) response, err := qe.llm.Generate(ctx, prompt) if err != nil { return []string{originalQuery}, nil } // Parse the numbered list lines := strings.Split(response, \"\\n\") queries := make([]string, 0, 3) for _, line := range lines { line = strings.TrimSpace(line) if strings.HasPrefix(line, \"1.\") || strings.HasPrefix(line, \"2.\") || strings.HasPrefix(line, \"3.\") { query := strings.TrimSpace(line[2:]) if query != \"\" { queries = append(queries, query) } } } // Always include original query if len(queries) == 0 { queries = append(queries, originalQuery) } return queries, nil }\r2. Advanced Retrieval with Reranking type AdvancedRetriever struct { memory core.Memory reranker *Reranker config RetrievalConfig } type RetrievalConfig struct { InitialRetrievalLimit int FinalResultLimit int ScoreThreshold float64 RerankingEnabled bool DiversityThreshold float64 } type Reranker struct { llm core.LLMProvider } func NewReranker(llm core.LLMProvider) *Reranker { return \u0026Reranker{llm: llm} } func (r *Reranker) Rerank(ctx context.Context, query string, results []core.MemoryResult) ([]core.MemoryResult, error) { if len(results) \u003c= 1 { return results, nil } // Create reranking prompt var resultsBuilder strings.Builder resultsBuilder.WriteString(\"Rank the following passages by relevance to the query:\\n\\n\") resultsBuilder.WriteString(fmt.Sprintf(\"Query: %s\\n\\n\", query)) for i, result := range results { resultsBuilder.WriteString(fmt.Sprintf(\"Passage %d: %s\\n\\n\", i+1, result.Content)) } resultsBuilder.WriteString(\"Please rank these passages from most relevant (1) to least relevant, providing only the numbers separated by commas (e.g., 3,1,4,2):\") response, err := r.llm.Generate(ctx, resultsBuilder.String()) if err != nil { // Fallback to original order return results, nil } // Parse ranking ranking := r.parseRanking(response, len(results)) // Reorder results based on ranking rerankedResults := make([]core.MemoryResult, 0, len(results)) for _, idx := range ranking { if idx \u003e= 0 \u0026\u0026 idx \u003c len(results) { rerankedResults = append(rerankedResults, results[idx]) } } // Add any missing results used := make(map[int]bool) for _, idx := range ranking { used[idx] = true } for i, result := range results { if !used[i] { rerankedResults = append(rerankedResults, result) } } return rerankedResults, nil } func (r *Reranker) parseRanking(response string, maxItems int) []int { // Clean and split the response response = strings.TrimSpace(response) parts := strings.Split(response, \",\") ranking := make([]int, 0, len(parts)) for _, part := range parts { part = strings.TrimSpace(part) if num, err := strconv.Atoi(part); err == nil \u0026\u0026 num \u003e= 1 \u0026\u0026 num \u003c= maxItems { ranking = append(ranking, num-1) // Convert to 0-based index } } return ranking } func (ar *AdvancedRetriever) Retrieve(ctx context.Context, query string) ([]core.MemoryResult, error) { // Initial retrieval with higher limit results, err := ar.memory.Search(ctx, query, core.WithLimit(ar.config.InitialRetrievalLimit), core.WithScoreThreshold(ar.config.ScoreThreshold*0.8), // Lower threshold initially ) if err != nil { return nil, fmt.Errorf(\"initial retrieval failed: %w\", err) } if len(results) == 0 { return results, nil } // Apply reranking if enabled if ar.config.RerankingEnabled \u0026\u0026 ar.reranker != nil { results, err = ar.reranker.Rerank(ctx, query, results) if err != nil { log.Printf(\"Reranking failed, using original order: %v\", err) } } // Apply diversity filtering if ar.config.DiversityThreshold \u003e 0 { results = ar.applyDiversityFilter(results) } // Limit final results if len(results) \u003e ar.config.FinalResultLimit { results = results[:ar.config.FinalResultLimit] } return results, nil } func (ar *AdvancedRetriever) applyDiversityFilter(results []core.MemoryResult) []core.MemoryResult { if len(results) \u003c= 1 { return results } filtered := []core.MemoryResult{results[0]} // Always include the top result for _, candidate := range results[1:] { isDiverse := true for _, selected := range filtered { similarity := ar.calculateSimilarity(candidate.Content, selected.Content) if similarity \u003e ar.config.DiversityThreshold { isDiverse = false break } } if isDiverse { filtered = append(filtered, candidate) } } return filtered } func (ar *AdvancedRetriever) calculateSimilarity(text1, text2 string) float64 { // Simple similarity calculation (in production, use proper similarity metrics) words1 := strings.Fields(strings.ToLower(text1)) words2 := strings.Fields(strings.ToLower(text2)) wordSet1 := make(map[string]bool) for _, word := range words1 { wordSet1[word] = true } common := 0 for _, word := range words2 { if wordSet1[word] { common++ } } if len(words1) == 0 || len(words2) == 0 { return 0 } return float64(common) / float64(len(words1)+len(words2)-common) // Jaccard similarity }\r3. Context-Aware Response Generation type ContextAwareGenerator struct { llm core.LLMProvider config GenerationConfig } type GenerationConfig struct { MaxContextLength int ResponseMaxLength int IncludeSources bool FactCheckingEnabled bool TemperatureAdjustment float32 } func NewContextAwareGenerator(llm core.LLMProvider, config GenerationConfig) *ContextAwareGenerator { return \u0026ContextAwareGenerator{ llm: llm, config: config, } } func (cag *ContextAwareGenerator) Generate(ctx context.Context, query string, retrievedContext []core.MemoryResult, conversationHistory []core.Message) (string, error) { // Build comprehensive context context := cag.buildContext(query, retrievedContext, conversationHistory) // Create generation prompt prompt := cag.createPrompt(query, context, retrievedContext) // Generate response response, err := cag.llm.Generate(ctx, prompt) if err != nil { return \"\", fmt.Errorf(\"generation failed: %w\", err) } // Post-process response response = cag.postProcessResponse(response, retrievedContext) // Fact-check if enabled if cag.config.FactCheckingEnabled { response, err = cag.factCheck(ctx, response, retrievedContext) if err != nil { log.Printf(\"Fact-checking failed: %v\", err) } } return response, nil } func (cag *ContextAwareGenerator) buildContext(query string, retrievedContext []core.MemoryResult, history []core.Message) string { var contextBuilder strings.Builder // Add conversation history if relevant if len(history) \u003e 0 { contextBuilder.WriteString(\"Recent conversation:\\n\") for _, msg := range history { contextBuilder.WriteString(fmt.Sprintf(\"%s: %s\\n\", msg.Role, msg.Content)) } contextBuilder.WriteString(\"\\n\") } // Add retrieved context if len(retrievedContext) \u003e 0 { contextBuilder.WriteString(\"Relevant information:\\n\") for i, result := range retrievedContext { contextBuilder.WriteString(fmt.Sprintf(\"%d. %s\", i+1, result.Content)) // Add source information if available if source, ok := result.Metadata[\"source\"]; ok { contextBuilder.WriteString(fmt.Sprintf(\" (Source: %s)\", source)) } contextBuilder.WriteString(\"\\n\") } } context := contextBuilder.String() // Truncate if too long if len(context) \u003e cag.config.MaxContextLength { context = context[:cag.config.MaxContextLength] + \"...\\n[Context truncated]\" } return context } func (cag *ContextAwareGenerator) createPrompt(query, context string, retrievedContext []core.MemoryResult) string { var promptBuilder strings.Builder promptBuilder.WriteString(\"You are a knowledgeable assistant. Use the provided context to answer the user's question accurately and helpfully.\\n\\n\") if context != \"\" { promptBuilder.WriteString(\"Context:\\n\") promptBuilder.WriteString(context) promptBuilder.WriteString(\"\\n\") } promptBuilder.WriteString(fmt.Sprintf(\"Question: %s\\n\\n\", query)) promptBuilder.WriteString(\"Instructions:\\n\") promptBuilder.WriteString(\"- Answer based on the provided context\\n\") promptBuilder.WriteString(\"- If the context doesn't contain enough information, say so\\n\") promptBuilder.WriteString(\"- Be accurate and cite sources when possible\\n\") if cag.config.IncludeSources \u0026\u0026 len(retrievedContext) \u003e 0 { promptBuilder.WriteString(\"- Include source references in your response\\n\") } promptBuilder.WriteString(\"\\nAnswer:\") return promptBuilder.String() } func (cag *ContextAwareGenerator) postProcessResponse(response string, context []core.MemoryResult) string { // Clean up response response = strings.TrimSpace(response) // Add source citations if configured if cag.config.IncludeSources \u0026\u0026 len(context) \u003e 0 { response = cag.addSourceCitations(response, context) } // Truncate if too long if len(response) \u003e cag.config.ResponseMaxLength { response = response[:cag.config.ResponseMaxLength] + \"...\" } return response } func (cag *ContextAwareGenerator) addSourceCitations(response string, context []core.MemoryResult) string { if len(context) == 0 { return response } var sourcesBuilder strings.Builder sourcesBuilder.WriteString(\"\\n\\nSources:\\n\") for i, result := range context { if source, ok := result.Metadata[\"source\"]; ok { sourcesBuilder.WriteString(fmt.Sprintf(\"[%d] %s\\n\", i+1, source)) } else { sourcesBuilder.WriteString(fmt.Sprintf(\"[%d] Internal knowledge base\\n\", i+1)) } } return response + sourcesBuilder.String() } func (cag *ContextAwareGenerator) factCheck(ctx context.Context, response string, context []core.MemoryResult) (string, error) { // Create fact-checking prompt var contextBuilder strings.Builder for _, result := range context { contextBuilder.WriteString(fmt.Sprintf(\"- %s\\n\", result.Content)) } prompt := fmt.Sprintf(`Please fact-check the following response against the provided context: Context: %s Response to check: %s Is the response factually accurate based on the context? If there are any inaccuracies, please provide a corrected version. Fact-check result:`, contextBuilder.String(), response) factCheckResult, err := cag.llm.Generate(ctx, prompt) if err != nil { return response, err // Return original response if fact-checking fails } // Simple heuristic: if the fact-check suggests corrections, use them if strings.Contains(strings.ToLower(factCheckResult), \"corrected version\") || strings.Contains(strings.ToLower(factCheckResult), \"inaccurate\") { // Extract corrected version (this is a simplified approach) lines := strings.Split(factCheckResult, \"\\n\") for i, line := range lines { if strings.Contains(strings.ToLower(line), \"corrected\") \u0026\u0026 i+1 \u003c len(lines) { return strings.TrimSpace(lines[i+1]), nil } } } return response, nil }\rRAG Agent Integration 1. Complete RAG Agent type ComprehensiveRAGAgent struct { name string memory core.Memory llm core.LLMProvider queryEnhancer *QueryEnhancer retriever *AdvancedRetriever generator *ContextAwareGenerator conversationMgr *ConversationManager } type ConversationManager struct { memory core.Memory } func NewConversationManager(memory core.Memory) *ConversationManager { return \u0026ConversationManager{memory: memory} } func (cm *ConversationManager) GetRecentHistory(ctx context.Context, sessionID string, limit int) ([]core.Message, error) { return cm.memory.GetHistory(ctx, limit, core.WithSession(sessionID), core.WithTimeRange(time.Now().Add(-24*time.Hour), time.Now()), ) } func NewComprehensiveRAGAgent(name string, memory core.Memory, llm core.LLMProvider) *ComprehensiveRAGAgent { return \u0026ComprehensiveRAGAgent{ name: name, memory: memory, llm: llm, queryEnhancer: NewQueryEnhancer(llm), retriever: \u0026AdvancedRetriever{ memory: memory, reranker: NewReranker(llm), config: RetrievalConfig{ InitialRetrievalLimit: 10, FinalResultLimit: 5, ScoreThreshold: 0.7, RerankingEnabled: true, DiversityThreshold: 0.8, }, }, generator: NewContextAwareGenerator(llm, GenerationConfig{ MaxContextLength: 3000, ResponseMaxLength: 1500, IncludeSources: true, FactCheckingEnabled: true, TemperatureAdjustment: 0.1, }), conversationMgr: NewConversationManager(memory), } } func (cra *ComprehensiveRAGAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Extract query query, ok := state.Get(\"message\") if !ok { return core.AgentResult{}, fmt.Errorf(\"no message in state\") } queryStr := query.(string) sessionID := event.GetSessionID() // Get conversation history history, err := cra.conversationMgr.GetRecentHistory(ctx, sessionID, 5) if err != nil { log.Printf(\"Failed to get conversation history: %v\", err) history = []core.Message{} // Continue without history } // Enhance query with conversation context enhancedQuery, err := cra.queryEnhancer.EnhanceQuery(ctx, queryStr, history) if err != nil { log.Printf(\"Query enhancement failed: %v\", err) enhancedQuery = queryStr // Fallback to original } // Retrieve relevant context retrievedContext, err := cra.retriever.Retrieve(ctx, enhancedQuery) if err != nil { return core.AgentResult{}, fmt.Errorf(\"retrieval failed: %w\", err) } // Generate response with context response, err := cra.generator.Generate(ctx, queryStr, retrievedContext, history) if err != nil { return core.AgentResult{}, fmt.Errorf(\"generation failed: %w\", err) } // Store interaction err = cra.storeInteraction(ctx, sessionID, queryStr, response, retrievedContext) if err != nil { log.Printf(\"Failed to store interaction: %v\", err) } // Prepare result outputState := state.Clone() outputState.Set(\"response\", response) outputState.Set(\"enhanced_query\", enhancedQuery) outputState.Set(\"sources_count\", len(retrievedContext)) outputState.Set(\"context_used\", len(retrievedContext) \u003e 0) // Add source information sources := make([]string, 0, len(retrievedContext)) for _, result := range retrievedContext { if source, ok := result.Metadata[\"source\"]; ok { sources = append(sources, source) } } outputState.Set(\"sources\", sources) return core.AgentResult{OutputState: outputState}, nil } func (cra *ComprehensiveRAGAgent) storeInteraction(ctx context.Context, sessionID, query, response string, context []core.MemoryResult) error { // Store user query err := cra.memory.Store(ctx, query, \"user-message\", core.WithSession(sessionID), core.WithTimestamp(time.Now()), core.WithMetadata(map[string]string{ \"agent_type\": \"comprehensive-rag\", }), ) if err != nil { return err } // Store agent response with context metadata contextSources := make([]string, 0, len(context)) for _, result := range context { if source, ok := result.Metadata[\"source\"]; ok { contextSources = append(contextSources, source) } } err = cra.memory.Store(ctx, response, \"assistant-message\", core.WithSession(sessionID), core.WithTimestamp(time.Now()), core.WithMetadata(map[string]string{ \"agent_type\": \"comprehensive-rag\", \"sources_used\": strings.Join(contextSources, \", \"), \"sources_count\": fmt.Sprintf(\"%d\", len(context)), \"context_length\": fmt.Sprintf(\"%d\", len(strings.Join(contextSources, \" \"))), }), ) return err }\rRAG Performance Optimization 1. Caching Strategies type RAGCache struct { retrievalCache map[string][]core.MemoryResult responseCache map[string]string mu sync.RWMutex ttl time.Duration timestamps map[string]time.Time } func NewRAGCache(ttl time.Duration) *RAGCache { cache := \u0026RAGCache{ retrievalCache: make(map[string][]core.MemoryResult), responseCache: make(map[string]string), timestamps: make(map[string]time.Time), ttl: ttl, } // Start cleanup goroutine go cache.cleanup() return cache } func (rc *RAGCache) GetRetrievalResults(query string) ([]core.MemoryResult, bool) { rc.mu.RLock() defer rc.mu.RUnlock() if timestamp, exists := rc.timestamps[query]; exists { if time.Since(timestamp) \u003c rc.ttl { if results, exists := rc.retrievalCache[query]; exists { return results, true } } } return nil, false } func (rc *RAGCache) SetRetrievalResults(query string, results []core.MemoryResult) { rc.mu.Lock() defer rc.mu.Unlock() rc.retrievalCache[query] = results rc.timestamps[query] = time.Now() } func (rc *RAGCache) cleanup() { ticker := time.NewTicker(rc.ttl / 2) defer ticker.Stop() for range ticker.C { rc.mu.Lock() now := time.Now() for query, timestamp := range rc.timestamps { if now.Sub(timestamp) \u003e rc.ttl { delete(rc.retrievalCache, query) delete(rc.responseCache, query) delete(rc.timestamps, query) } } rc.mu.Unlock() } }\r2. Batch Processing type BatchRAGProcessor struct { agent *ComprehensiveRAGAgent batchSize int timeout time.Duration } func NewBatchRAGProcessor(agent *ComprehensiveRAGAgent, batchSize int, timeout time.Duration) *BatchRAGProcessor { return \u0026BatchRAGProcessor{ agent: agent, batchSize: batchSize, timeout: timeout, } } func (brp *BatchRAGProcessor) ProcessBatch(ctx context.Context, queries []string, sessionID string) ([]string, error) { responses := make([]string, len(queries)) // Process in batches for i := 0; i \u003c len(queries); i += brp.batchSize { end := i + brp.batchSize if end \u003e len(queries) { end = len(queries) } batch := queries[i:end] batchResponses, err := brp.processBatch(ctx, batch, sessionID) if err != nil { return nil, fmt.Errorf(\"batch processing failed: %w\", err) } copy(responses[i:], batchResponses) } return responses, nil } func (brp *BatchRAGProcessor) processBatch(ctx context.Context, queries []string, sessionID string) ([]string, error) { ctx, cancel := context.WithTimeout(ctx, brp.timeout) defer cancel() responses := make([]string, len(queries)) var wg sync.WaitGroup var mu sync.Mutex var firstError error for i, query := range queries { wg.Add(1) go func(index int, q string) { defer wg.Done() event := core.NewEvent( brp.agent.name, core.EventData{\"message\": q}, map[string]string{\"session_id\": sessionID}, ) state := core.NewState() state.Set(\"message\", q) result, err := brp.agent.Run(ctx, event, state) mu.Lock() defer mu.Unlock() if err != nil \u0026\u0026 firstError == nil { firstError = err } else if err == nil { if response, ok := result.OutputState.Get(\"response\"); ok { responses[index] = response.(string) } } }(i, query) } wg.Wait() if firstError != nil { return nil, firstError } return responses, nil }\rRAG Evaluation and Monitoring 1. RAG Metrics type RAGMetrics struct { RetrievalLatency []time.Duration GenerationLatency []time.Duration RetrievalAccuracy float64 ResponseQuality float64 SourceUtilization map[string]int mu sync.RWMutex } func NewRAGMetrics() *RAGMetrics { return \u0026RAGMetrics{ SourceUtilization: make(map[string]int), } } func (rm *RAGMetrics) RecordRetrieval(latency time.Duration, resultsCount int, accuracy float64) { rm.mu.Lock() defer rm.mu.Unlock() rm.RetrievalLatency = append(rm.RetrievalLatency, latency) rm.RetrievalAccuracy = (rm.RetrievalAccuracy + accuracy) / 2 // Simple moving average } func (rm *RAGMetrics) RecordGeneration(latency time.Duration, quality float64) { rm.mu.Lock() defer rm.mu.Unlock() rm.GenerationLatency = append(rm.GenerationLatency, latency) rm.ResponseQuality = (rm.ResponseQuality + quality) / 2 } func (rm *RAGMetrics) RecordSourceUsage(sources []string) { rm.mu.Lock() defer rm.mu.Unlock() for _, source := range sources { rm.SourceUtilization[source]++ } } func (rm *RAGMetrics) GetAverageRetrievalLatency() time.Duration { rm.mu.RLock() defer rm.mu.RUnlock() if len(rm.RetrievalLatency) == 0 { return 0 } var total time.Duration for _, latency := range rm.RetrievalLatency { total += latency } return total / time.Duration(len(rm.RetrievalLatency)) }\r2. Quality Assessment type RAGQualityAssessor struct { llm core.LLMProvider } func NewRAGQualityAssessor(llm core.LLMProvider) *RAGQualityAssessor { return \u0026RAGQualityAssessor{llm: llm} } func (rqa *RAGQualityAssessor) AssessResponse(ctx context.Context, query, response string, sources []core.MemoryResult) (float64, error) { // Build assessment prompt var sourcesBuilder strings.Builder for i, source := range sources { sourcesBuilder.WriteString(fmt.Sprintf(\"%d. %s\\n\", i+1, source.Content)) } prompt := fmt.Sprintf(`Please assess the quality of this RAG response on a scale of 0.0 to 1.0: Query: %s Sources used: %s Response: %s Assessment criteria: - Accuracy: Is the response factually correct based on the sources? - Relevance: Does the response directly address the query? - Completeness: Does the response provide sufficient information? - Coherence: Is the response well-structured and clear? Please provide only a numerical score between 0.0 and 1.0:`, query, sourcesBuilder.String(), response) scoreStr, err := rqa.llm.Generate(ctx, prompt) if err != nil { return 0.0, fmt.Errorf(\"quality assessment failed: %w\", err) } // Parse score scoreStr = strings.TrimSpace(scoreStr) score, err := strconv.ParseFloat(scoreStr, 64) if err != nil { return 0.0, fmt.Errorf(\"failed to parse quality score: %w\", err) } // Clamp score to valid range if score \u003c 0.0 { score = 0.0 } else if score \u003e 1.0 { score = 1.0 } return score, nil }\rBest Practices 1. RAG System Design Chunk Size Optimization: Balance context and specificity Embedding Quality: Use appropriate embedding models for your domain Retrieval Tuning: Optimize similarity thresholds and result limits Context Management: Manage context length to avoid token limits Source Attribution: Always track and cite information sources 2. Performance Optimization Caching: Cache frequent queries and embeddings Batch Processing: Process multiple queries efficiently Index Optimization: Use appropriate vector database indexes Async Processing: Use asynchronous operations where possible 3. Quality Assurance Evaluation Metrics: Implement comprehensive evaluation Human Feedback: Collect and incorporate user feedback Continuous Monitoring: Monitor system performance and quality A/B Testing: Test different RAG configurations Conclusion RAG implementation in AgenticGoKit enables agents to provide accurate, contextual, and up-to-date responses by combining retrieval and generation. Key takeaways:\nStart with basic RAG and gradually add advanced features Optimize retrieval quality through query enhancement and reranking Implement proper caching and performance monitoring Continuously evaluate and improve system quality RAG transforms agents from static responders to dynamic, knowledgeable assistants that can access and utilize vast amounts of information effectively.\nNext Steps Knowledge Bases - Build comprehensive knowledge systems Memory Optimization - Advanced performance tuning Production Deployment - Deploy RAG systems at scale Further Reading RAG Research Papers Vector Database Comparison Embedding Model Evaluation",
    "description": "RAG Implementation in AgenticGoKit Overview Retrieval-Augmented Generation (RAG) combines the power of large language models with external knowledge retrieval to provide more accurate, up-to-date, and contextually relevant responses. This tutorial covers implementing RAG systems in AgenticGoKit, from basic retrieval to advanced techniques.\nRAG enables agents to access vast amounts of information while maintaining the conversational abilities of language models, making them more knowledgeable and reliable.\nPrerequisites Understanding of Vector Databases Familiarity with Basic Memory Operations Knowledge of language model APIs Basic understanding of information retrieval concepts RAG Architecture Basic RAG Flow ┌─────────────┐ ┌──────────────┐ ┌─────────────────┐\r│ User Query │───▶│ Retrieval │───▶│ Context + Query │\r└─────────────┘ │ System │ └─────────────────┘\r└──────────────┘ │\r│ ▼\r▼ ┌─────────────────┐\r┌──────────────┐ │ Language Model │\r│ Vector Store │ │ Generation │\r└──────────────┘ └─────────────────┘\r│\r▼\r┌─────────────────┐\r│ Enhanced │\r│ Response │\r└─────────────────┘\rAdvanced RAG Components Query Processing: Understanding and reformulating user queries Retrieval: Finding relevant information from knowledge base Reranking: Improving relevance of retrieved results Context Building: Constructing effective prompts Generation: Producing responses with retrieved context Response Enhancement: Post-processing and validation Basic RAG Implementation 1. Simple RAG Agent package main import ( \"context\" \"fmt\" \"log\" \"os\" \"strings\" \"github.com/kunalkushwaha/agenticgokit/core\" ) type BasicRAGAgent struct { name string memory core.Memory llm core.ModelProvider config RAGConfig } type RAGConfig struct { MaxRetrievalResults int ScoreThreshold float32 MaxContextLength int ContextTemplate string } func NewBasicRAGAgent(name string, memory core.Memory, llm core.ModelProvider) *BasicRAGAgent { return \u0026BasicRAGAgent{ name: name, memory: memory, llm: llm, config: RAGConfig{ MaxRetrievalResults: 5, ScoreThreshold: 0.7, MaxContextLength: 2000, ContextTemplate: `Based on the following information: %s Please answer the question: %s`, }, } } func (r *BasicRAGAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Extract user query query, ok := state.Get(\"message\") if !ok { return core.AgentResult{}, fmt.Errorf(\"no message in state\") } queryStr := query.(string) // Retrieve relevant context contextStr, sources, err := r.retrieveContext(ctx, queryStr) if err != nil { return core.AgentResult{}, fmt.Errorf(\"retrieval failed: %w\", err) } // Generate response with context response, err := r.generateResponse(ctx, queryStr, contextStr) if err != nil { return core.AgentResult{}, fmt.Errorf(\"generation failed: %w\", err) } // Store the interaction sessionID, _ := event.GetMetadataValue(core.SessionIDKey) err = r.storeInteraction(ctx, sessionID, queryStr, response, sources) if err != nil { log.Printf(\"Failed to store interaction: %v\", err) } // Return result outputState := state.Clone() outputState.Set(\"response\", response) outputState.Set(\"sources\", sources) outputState.Set(\"context_used\", len(sources) \u003e 0) return core.AgentResult{OutputState: outputState}, nil } func (r *BasicRAGAgent) retrieveContext(ctx context.Context, query string) (string, []string, error) { // Search for relevant information results, err := r.memory.Search(ctx, query, core.WithLimit(r.config.MaxRetrievalResults), core.WithScoreThreshold(r.config.ScoreThreshold), ) if err != nil { return \"\", nil, fmt.Errorf(\"search failed: %w\", err) } if len(results) == 0 { return \"\", nil, nil // No relevant context found } // Build context string var contextBuilder strings.Builder sources := make([]string, 0, len(results)) for i, result := range results { // Add numbered context item contextBuilder.WriteString(fmt.Sprintf(\"%d. %s\\n\", i+1, result.Content)) // Track sources if source, ok := result.Metadata[\"source\"]; ok { sources = append(sources, source) } else { sources = append(sources, fmt.Sprintf(\"Document %s\", result.ID)) } } context := contextBuilder.String() // Truncate if too long if len(context) \u003e r.config.MaxContextLength { context = context[:r.config.MaxContextLength] + \"...\" } return context, sources, nil } func (r *BasicRAGAgent) generateResponse(ctx context.Context, query, context string) (string, error) { var prompt string if context != \"\" { // Use context template prompt = fmt.Sprintf(r.config.ContextTemplate, context, query) } else { // Fallback to direct query prompt = fmt.Sprintf(\"Please answer the following question: %s\", query) } // Generate response response, err := r.llm.Generate(ctx, prompt) if err != nil { return \"\", fmt.Errorf(\"LLM generation failed: %w\", err) } return response, nil } func (r *BasicRAGAgent) storeInteraction(ctx context.Context, sessionID, query, response string, sources []string) error { // Store user query err := r.memory.Store(ctx, query, \"user-query\", core.WithSession(sessionID), core.WithTimestamp(time.Now()), core.WithMetadata(map[string]string{ \"interaction_type\": \"rag-query\", }), ) if err != nil { return err } // Store agent response with sources sourcesStr := strings.Join(sources, \", \") err = r.memory.Store(ctx, response, \"agent-response\", core.WithSession(sessionID), core.WithTimestamp(time.Now()), core.WithMetadata(map[string]string{ \"interaction_type\": \"rag-response\", \"sources_used\": sourcesStr, \"sources_count\": fmt.Sprintf(\"%d\", len(sources)), }), ) return err } func main() { // Setup memory with vector database memory, err := core.NewMemory(core.AgentMemoryConfig{ Provider: \"pgvector\", Connection: \"postgres://user:pass@localhost:5432/agentdb\", EnableRAG: true, Dimensions: 1536, Embedding: core.EmbeddingConfig{ Provider: \"openai\", Model: \"text-embedding-3-small\", APIKey: os.Getenv(\"OPENAI_API_KEY\"), Dimensions: 1536, }, }) if err != nil { log.Fatalf(\"Failed to create memory: %v\", err) } // Setup LLM provider llm, err := core.NewOpenAIAdapter( os.Getenv(\"OPENAI_API_KEY\"), \"gpt-3.5-turbo\", 2000, 0.7, ) if err != nil { log.Fatalf(\"Failed to create LLM: %v\", err) } // Create RAG agent ragAgent := NewBasicRAGAgent(\"rag-assistant\", memory, llm) // Test the agent ctx := context.Background() // First, populate some knowledge knowledge := []string{ \"AgenticGoKit is a Go framework for building multi-agent systems with support for LLM integration, memory systems, and orchestration patterns.\", \"Vector databases like pgvector and Weaviate are used in AgenticGoKit for semantic search and RAG implementations.\", \"The framework supports multiple orchestration patterns including route, collaborative, sequential, and loop modes.\", } for _, info := range knowledge { memory.Store(ctx, info, \"knowledge\", core.WithMetadata(map[string]string{ \"source\": \"documentation\", \"topic\": \"agenticgokit\", }), ) } // Test query event := core.NewEvent( \"rag-assistant\", core.EventData{\"message\": \"What is AgenticGoKit and what databases does it support?\"}, map[string]string{\"session_id\": \"test-session\"}, ) state := core.NewState() state.Set(\"message\", \"What is AgenticGoKit and what databases does it support?\") result, err := ragAgent.Run(ctx, event, state) if err != nil { log.Fatalf(\"RAG agent failed: %v\", err) } response, _ := result.OutputState.Get(\"response\") sources, _ := result.OutputState.Get(\"sources\") fmt.Printf(\"Response: %s\\n\", response) fmt.Printf(\"Sources: %v\\n\", sources) }\rAdvanced RAG Techniques 1. Query Enhancement type QueryEnhancer struct { llm core.LLMProvider } func NewQueryEnhancer(llm core.LLMProvider) *QueryEnhancer { return \u0026QueryEnhancer{llm: llm} } func (qe *QueryEnhancer) EnhanceQuery(ctx context.Context, originalQuery string, conversationHistory []core.Message) (string, error) { // Build context from conversation history var historyBuilder strings.Builder for _, msg := range conversationHistory { historyBuilder.WriteString(fmt.Sprintf(\"%s: %s\\n\", msg.Role, msg.Content)) } // Create enhancement prompt prompt := fmt.Sprintf(`Given the conversation history: %s The user's current query is: \"%s\" Please rewrite this query to be more specific and searchable, incorporating relevant context from the conversation history. The enhanced query should be optimized for semantic search. Enhanced query:`, historyBuilder.String(), originalQuery) enhancedQuery, err := qe.llm.Generate(ctx, prompt) if err != nil { // Fallback to original query return originalQuery, nil } return strings.TrimSpace(enhancedQuery), nil } // Multi-query generation for better retrieval func (qe *QueryEnhancer) GenerateMultipleQueries(ctx context.Context, originalQuery string) ([]string, error) { prompt := fmt.Sprintf(`Given the query: \"%s\" Generate 3 different ways to ask the same question that would help find relevant information: 1. 2. 3.`, originalQuery) response, err := qe.llm.Generate(ctx, prompt) if err != nil { return []string{originalQuery}, nil } // Parse the numbered list lines := strings.Split(response, \"\\n\") queries := make([]string, 0, 3) for _, line := range lines { line = strings.TrimSpace(line) if strings.HasPrefix(line, \"1.\") || strings.HasPrefix(line, \"2.\") || strings.HasPrefix(line, \"3.\") { query := strings.TrimSpace(line[2:]) if query != \"\" { queries = append(queries, query) } } } // Always include original query if len(queries) == 0 { queries = append(queries, originalQuery) } return queries, nil }\r2. Advanced Retrieval with Reranking type AdvancedRetriever struct { memory core.Memory reranker *Reranker config RetrievalConfig } type RetrievalConfig struct { InitialRetrievalLimit int FinalResultLimit int ScoreThreshold float64 RerankingEnabled bool DiversityThreshold float64 } type Reranker struct { llm core.LLMProvider } func NewReranker(llm core.LLMProvider) *Reranker { return \u0026Reranker{llm: llm} } func (r *Reranker) Rerank(ctx context.Context, query string, results []core.MemoryResult) ([]core.MemoryResult, error) { if len(results) \u003c= 1 { return results, nil } // Create reranking prompt var resultsBuilder strings.Builder resultsBuilder.WriteString(\"Rank the following passages by relevance to the query:\\n\\n\") resultsBuilder.WriteString(fmt.Sprintf(\"Query: %s\\n\\n\", query)) for i, result := range results { resultsBuilder.WriteString(fmt.Sprintf(\"Passage %d: %s\\n\\n\", i+1, result.Content)) } resultsBuilder.WriteString(\"Please rank these passages from most relevant (1) to least relevant, providing only the numbers separated by commas (e.g., 3,1,4,2):\") response, err := r.llm.Generate(ctx, resultsBuilder.String()) if err != nil { // Fallback to original order return results, nil } // Parse ranking ranking := r.parseRanking(response, len(results)) // Reorder results based on ranking rerankedResults := make([]core.MemoryResult, 0, len(results)) for _, idx := range ranking { if idx \u003e= 0 \u0026\u0026 idx \u003c len(results) { rerankedResults = append(rerankedResults, results[idx]) } } // Add any missing results used := make(map[int]bool) for _, idx := range ranking { used[idx] = true } for i, result := range results { if !used[i] { rerankedResults = append(rerankedResults, result) } } return rerankedResults, nil } func (r *Reranker) parseRanking(response string, maxItems int) []int { // Clean and split the response response = strings.TrimSpace(response) parts := strings.Split(response, \",\") ranking := make([]int, 0, len(parts)) for _, part := range parts { part = strings.TrimSpace(part) if num, err := strconv.Atoi(part); err == nil \u0026\u0026 num \u003e= 1 \u0026\u0026 num \u003c= maxItems { ranking = append(ranking, num-1) // Convert to 0-based index } } return ranking } func (ar *AdvancedRetriever) Retrieve(ctx context.Context, query string) ([]core.MemoryResult, error) { // Initial retrieval with higher limit results, err := ar.memory.Search(ctx, query, core.WithLimit(ar.config.InitialRetrievalLimit), core.WithScoreThreshold(ar.config.ScoreThreshold*0.8), // Lower threshold initially ) if err != nil { return nil, fmt.Errorf(\"initial retrieval failed: %w\", err) } if len(results) == 0 { return results, nil } // Apply reranking if enabled if ar.config.RerankingEnabled \u0026\u0026 ar.reranker != nil { results, err = ar.reranker.Rerank(ctx, query, results) if err != nil { log.Printf(\"Reranking failed, using original order: %v\", err) } } // Apply diversity filtering if ar.config.DiversityThreshold \u003e 0 { results = ar.applyDiversityFilter(results) } // Limit final results if len(results) \u003e ar.config.FinalResultLimit { results = results[:ar.config.FinalResultLimit] } return results, nil } func (ar *AdvancedRetriever) applyDiversityFilter(results []core.MemoryResult) []core.MemoryResult { if len(results) \u003c= 1 { return results } filtered := []core.MemoryResult{results[0]} // Always include the top result for _, candidate := range results[1:] { isDiverse := true for _, selected := range filtered { similarity := ar.calculateSimilarity(candidate.Content, selected.Content) if similarity \u003e ar.config.DiversityThreshold { isDiverse = false break } } if isDiverse { filtered = append(filtered, candidate) } } return filtered } func (ar *AdvancedRetriever) calculateSimilarity(text1, text2 string) float64 { // Simple similarity calculation (in production, use proper similarity metrics) words1 := strings.Fields(strings.ToLower(text1)) words2 := strings.Fields(strings.ToLower(text2)) wordSet1 := make(map[string]bool) for _, word := range words1 { wordSet1[word] = true } common := 0 for _, word := range words2 { if wordSet1[word] { common++ } } if len(words1) == 0 || len(words2) == 0 { return 0 } return float64(common) / float64(len(words1)+len(words2)-common) // Jaccard similarity }\r3. Context-Aware Response Generation type ContextAwareGenerator struct { llm core.LLMProvider config GenerationConfig } type GenerationConfig struct { MaxContextLength int ResponseMaxLength int IncludeSources bool FactCheckingEnabled bool TemperatureAdjustment float32 } func NewContextAwareGenerator(llm core.LLMProvider, config GenerationConfig) *ContextAwareGenerator { return \u0026ContextAwareGenerator{ llm: llm, config: config, } } func (cag *ContextAwareGenerator) Generate(ctx context.Context, query string, retrievedContext []core.MemoryResult, conversationHistory []core.Message) (string, error) { // Build comprehensive context context := cag.buildContext(query, retrievedContext, conversationHistory) // Create generation prompt prompt := cag.createPrompt(query, context, retrievedContext) // Generate response response, err := cag.llm.Generate(ctx, prompt) if err != nil { return \"\", fmt.Errorf(\"generation failed: %w\", err) } // Post-process response response = cag.postProcessResponse(response, retrievedContext) // Fact-check if enabled if cag.config.FactCheckingEnabled { response, err = cag.factCheck(ctx, response, retrievedContext) if err != nil { log.Printf(\"Fact-checking failed: %v\", err) } } return response, nil } func (cag *ContextAwareGenerator) buildContext(query string, retrievedContext []core.MemoryResult, history []core.Message) string { var contextBuilder strings.Builder // Add conversation history if relevant if len(history) \u003e 0 { contextBuilder.WriteString(\"Recent conversation:\\n\") for _, msg := range history { contextBuilder.WriteString(fmt.Sprintf(\"%s: %s\\n\", msg.Role, msg.Content)) } contextBuilder.WriteString(\"\\n\") } // Add retrieved context if len(retrievedContext) \u003e 0 { contextBuilder.WriteString(\"Relevant information:\\n\") for i, result := range retrievedContext { contextBuilder.WriteString(fmt.Sprintf(\"%d. %s\", i+1, result.Content)) // Add source information if available if source, ok := result.Metadata[\"source\"]; ok { contextBuilder.WriteString(fmt.Sprintf(\" (Source: %s)\", source)) } contextBuilder.WriteString(\"\\n\") } } context := contextBuilder.String() // Truncate if too long if len(context) \u003e cag.config.MaxContextLength { context = context[:cag.config.MaxContextLength] + \"...\\n[Context truncated]\" } return context } func (cag *ContextAwareGenerator) createPrompt(query, context string, retrievedContext []core.MemoryResult) string { var promptBuilder strings.Builder promptBuilder.WriteString(\"You are a knowledgeable assistant. Use the provided context to answer the user's question accurately and helpfully.\\n\\n\") if context != \"\" { promptBuilder.WriteString(\"Context:\\n\") promptBuilder.WriteString(context) promptBuilder.WriteString(\"\\n\") } promptBuilder.WriteString(fmt.Sprintf(\"Question: %s\\n\\n\", query)) promptBuilder.WriteString(\"Instructions:\\n\") promptBuilder.WriteString(\"- Answer based on the provided context\\n\") promptBuilder.WriteString(\"- If the context doesn't contain enough information, say so\\n\") promptBuilder.WriteString(\"- Be accurate and cite sources when possible\\n\") if cag.config.IncludeSources \u0026\u0026 len(retrievedContext) \u003e 0 { promptBuilder.WriteString(\"- Include source references in your response\\n\") } promptBuilder.WriteString(\"\\nAnswer:\") return promptBuilder.String() } func (cag *ContextAwareGenerator) postProcessResponse(response string, context []core.MemoryResult) string { // Clean up response response = strings.TrimSpace(response) // Add source citations if configured if cag.config.IncludeSources \u0026\u0026 len(context) \u003e 0 { response = cag.addSourceCitations(response, context) } // Truncate if too long if len(response) \u003e cag.config.ResponseMaxLength { response = response[:cag.config.ResponseMaxLength] + \"...\" } return response } func (cag *ContextAwareGenerator) addSourceCitations(response string, context []core.MemoryResult) string { if len(context) == 0 { return response } var sourcesBuilder strings.Builder sourcesBuilder.WriteString(\"\\n\\nSources:\\n\") for i, result := range context { if source, ok := result.Metadata[\"source\"]; ok { sourcesBuilder.WriteString(fmt.Sprintf(\"[%d] %s\\n\", i+1, source)) } else { sourcesBuilder.WriteString(fmt.Sprintf(\"[%d] Internal knowledge base\\n\", i+1)) } } return response + sourcesBuilder.String() } func (cag *ContextAwareGenerator) factCheck(ctx context.Context, response string, context []core.MemoryResult) (string, error) { // Create fact-checking prompt var contextBuilder strings.Builder for _, result := range context { contextBuilder.WriteString(fmt.Sprintf(\"- %s\\n\", result.Content)) } prompt := fmt.Sprintf(`Please fact-check the following response against the provided context: Context: %s Response to check: %s Is the response factually accurate based on the context? If there are any inaccuracies, please provide a corrected version. Fact-check result:`, contextBuilder.String(), response) factCheckResult, err := cag.llm.Generate(ctx, prompt) if err != nil { return response, err // Return original response if fact-checking fails } // Simple heuristic: if the fact-check suggests corrections, use them if strings.Contains(strings.ToLower(factCheckResult), \"corrected version\") || strings.Contains(strings.ToLower(factCheckResult), \"inaccurate\") { // Extract corrected version (this is a simplified approach) lines := strings.Split(factCheckResult, \"\\n\") for i, line := range lines { if strings.Contains(strings.ToLower(line), \"corrected\") \u0026\u0026 i+1 \u003c len(lines) { return strings.TrimSpace(lines[i+1]), nil } } } return response, nil }\rRAG Agent Integration 1. Complete RAG Agent type ComprehensiveRAGAgent struct { name string memory core.Memory llm core.LLMProvider queryEnhancer *QueryEnhancer retriever *AdvancedRetriever generator *ContextAwareGenerator conversationMgr *ConversationManager } type ConversationManager struct { memory core.Memory } func NewConversationManager(memory core.Memory) *ConversationManager { return \u0026ConversationManager{memory: memory} } func (cm *ConversationManager) GetRecentHistory(ctx context.Context, sessionID string, limit int) ([]core.Message, error) { return cm.memory.GetHistory(ctx, limit, core.WithSession(sessionID), core.WithTimeRange(time.Now().Add(-24*time.Hour), time.Now()), ) } func NewComprehensiveRAGAgent(name string, memory core.Memory, llm core.LLMProvider) *ComprehensiveRAGAgent { return \u0026ComprehensiveRAGAgent{ name: name, memory: memory, llm: llm, queryEnhancer: NewQueryEnhancer(llm), retriever: \u0026AdvancedRetriever{ memory: memory, reranker: NewReranker(llm), config: RetrievalConfig{ InitialRetrievalLimit: 10, FinalResultLimit: 5, ScoreThreshold: 0.7, RerankingEnabled: true, DiversityThreshold: 0.8, }, }, generator: NewContextAwareGenerator(llm, GenerationConfig{ MaxContextLength: 3000, ResponseMaxLength: 1500, IncludeSources: true, FactCheckingEnabled: true, TemperatureAdjustment: 0.1, }), conversationMgr: NewConversationManager(memory), } } func (cra *ComprehensiveRAGAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Extract query query, ok := state.Get(\"message\") if !ok { return core.AgentResult{}, fmt.Errorf(\"no message in state\") } queryStr := query.(string) sessionID := event.GetSessionID() // Get conversation history history, err := cra.conversationMgr.GetRecentHistory(ctx, sessionID, 5) if err != nil { log.Printf(\"Failed to get conversation history: %v\", err) history = []core.Message{} // Continue without history } // Enhance query with conversation context enhancedQuery, err := cra.queryEnhancer.EnhanceQuery(ctx, queryStr, history) if err != nil { log.Printf(\"Query enhancement failed: %v\", err) enhancedQuery = queryStr // Fallback to original } // Retrieve relevant context retrievedContext, err := cra.retriever.Retrieve(ctx, enhancedQuery) if err != nil { return core.AgentResult{}, fmt.Errorf(\"retrieval failed: %w\", err) } // Generate response with context response, err := cra.generator.Generate(ctx, queryStr, retrievedContext, history) if err != nil { return core.AgentResult{}, fmt.Errorf(\"generation failed: %w\", err) } // Store interaction err = cra.storeInteraction(ctx, sessionID, queryStr, response, retrievedContext) if err != nil { log.Printf(\"Failed to store interaction: %v\", err) } // Prepare result outputState := state.Clone() outputState.Set(\"response\", response) outputState.Set(\"enhanced_query\", enhancedQuery) outputState.Set(\"sources_count\", len(retrievedContext)) outputState.Set(\"context_used\", len(retrievedContext) \u003e 0) // Add source information sources := make([]string, 0, len(retrievedContext)) for _, result := range retrievedContext { if source, ok := result.Metadata[\"source\"]; ok { sources = append(sources, source) } } outputState.Set(\"sources\", sources) return core.AgentResult{OutputState: outputState}, nil } func (cra *ComprehensiveRAGAgent) storeInteraction(ctx context.Context, sessionID, query, response string, context []core.MemoryResult) error { // Store user query err := cra.memory.Store(ctx, query, \"user-message\", core.WithSession(sessionID), core.WithTimestamp(time.Now()), core.WithMetadata(map[string]string{ \"agent_type\": \"comprehensive-rag\", }), ) if err != nil { return err } // Store agent response with context metadata contextSources := make([]string, 0, len(context)) for _, result := range context { if source, ok := result.Metadata[\"source\"]; ok { contextSources = append(contextSources, source) } } err = cra.memory.Store(ctx, response, \"assistant-message\", core.WithSession(sessionID), core.WithTimestamp(time.Now()), core.WithMetadata(map[string]string{ \"agent_type\": \"comprehensive-rag\", \"sources_used\": strings.Join(contextSources, \", \"), \"sources_count\": fmt.Sprintf(\"%d\", len(context)), \"context_length\": fmt.Sprintf(\"%d\", len(strings.Join(contextSources, \" \"))), }), ) return err }\rRAG Performance Optimization 1. Caching Strategies type RAGCache struct { retrievalCache map[string][]core.MemoryResult responseCache map[string]string mu sync.RWMutex ttl time.Duration timestamps map[string]time.Time } func NewRAGCache(ttl time.Duration) *RAGCache { cache := \u0026RAGCache{ retrievalCache: make(map[string][]core.MemoryResult), responseCache: make(map[string]string), timestamps: make(map[string]time.Time), ttl: ttl, } // Start cleanup goroutine go cache.cleanup() return cache } func (rc *RAGCache) GetRetrievalResults(query string) ([]core.MemoryResult, bool) { rc.mu.RLock() defer rc.mu.RUnlock() if timestamp, exists := rc.timestamps[query]; exists { if time.Since(timestamp) \u003c rc.ttl { if results, exists := rc.retrievalCache[query]; exists { return results, true } } } return nil, false } func (rc *RAGCache) SetRetrievalResults(query string, results []core.MemoryResult) { rc.mu.Lock() defer rc.mu.Unlock() rc.retrievalCache[query] = results rc.timestamps[query] = time.Now() } func (rc *RAGCache) cleanup() { ticker := time.NewTicker(rc.ttl / 2) defer ticker.Stop() for range ticker.C { rc.mu.Lock() now := time.Now() for query, timestamp := range rc.timestamps { if now.Sub(timestamp) \u003e rc.ttl { delete(rc.retrievalCache, query) delete(rc.responseCache, query) delete(rc.timestamps, query) } } rc.mu.Unlock() } }\r2. Batch Processing type BatchRAGProcessor struct { agent *ComprehensiveRAGAgent batchSize int timeout time.Duration } func NewBatchRAGProcessor(agent *ComprehensiveRAGAgent, batchSize int, timeout time.Duration) *BatchRAGProcessor { return \u0026BatchRAGProcessor{ agent: agent, batchSize: batchSize, timeout: timeout, } } func (brp *BatchRAGProcessor) ProcessBatch(ctx context.Context, queries []string, sessionID string) ([]string, error) { responses := make([]string, len(queries)) // Process in batches for i := 0; i \u003c len(queries); i += brp.batchSize { end := i + brp.batchSize if end \u003e len(queries) { end = len(queries) } batch := queries[i:end] batchResponses, err := brp.processBatch(ctx, batch, sessionID) if err != nil { return nil, fmt.Errorf(\"batch processing failed: %w\", err) } copy(responses[i:], batchResponses) } return responses, nil } func (brp *BatchRAGProcessor) processBatch(ctx context.Context, queries []string, sessionID string) ([]string, error) { ctx, cancel := context.WithTimeout(ctx, brp.timeout) defer cancel() responses := make([]string, len(queries)) var wg sync.WaitGroup var mu sync.Mutex var firstError error for i, query := range queries { wg.Add(1) go func(index int, q string) { defer wg.Done() event := core.NewEvent( brp.agent.name, core.EventData{\"message\": q}, map[string]string{\"session_id\": sessionID}, ) state := core.NewState() state.Set(\"message\", q) result, err := brp.agent.Run(ctx, event, state) mu.Lock() defer mu.Unlock() if err != nil \u0026\u0026 firstError == nil { firstError = err } else if err == nil { if response, ok := result.OutputState.Get(\"response\"); ok { responses[index] = response.(string) } } }(i, query) } wg.Wait() if firstError != nil { return nil, firstError } return responses, nil }\rRAG Evaluation and Monitoring 1. RAG Metrics type RAGMetrics struct { RetrievalLatency []time.Duration GenerationLatency []time.Duration RetrievalAccuracy float64 ResponseQuality float64 SourceUtilization map[string]int mu sync.RWMutex } func NewRAGMetrics() *RAGMetrics { return \u0026RAGMetrics{ SourceUtilization: make(map[string]int), } } func (rm *RAGMetrics) RecordRetrieval(latency time.Duration, resultsCount int, accuracy float64) { rm.mu.Lock() defer rm.mu.Unlock() rm.RetrievalLatency = append(rm.RetrievalLatency, latency) rm.RetrievalAccuracy = (rm.RetrievalAccuracy + accuracy) / 2 // Simple moving average } func (rm *RAGMetrics) RecordGeneration(latency time.Duration, quality float64) { rm.mu.Lock() defer rm.mu.Unlock() rm.GenerationLatency = append(rm.GenerationLatency, latency) rm.ResponseQuality = (rm.ResponseQuality + quality) / 2 } func (rm *RAGMetrics) RecordSourceUsage(sources []string) { rm.mu.Lock() defer rm.mu.Unlock() for _, source := range sources { rm.SourceUtilization[source]++ } } func (rm *RAGMetrics) GetAverageRetrievalLatency() time.Duration { rm.mu.RLock() defer rm.mu.RUnlock() if len(rm.RetrievalLatency) == 0 { return 0 } var total time.Duration for _, latency := range rm.RetrievalLatency { total += latency } return total / time.Duration(len(rm.RetrievalLatency)) }\r2. Quality Assessment type RAGQualityAssessor struct { llm core.LLMProvider } func NewRAGQualityAssessor(llm core.LLMProvider) *RAGQualityAssessor { return \u0026RAGQualityAssessor{llm: llm} } func (rqa *RAGQualityAssessor) AssessResponse(ctx context.Context, query, response string, sources []core.MemoryResult) (float64, error) { // Build assessment prompt var sourcesBuilder strings.Builder for i, source := range sources { sourcesBuilder.WriteString(fmt.Sprintf(\"%d. %s\\n\", i+1, source.Content)) } prompt := fmt.Sprintf(`Please assess the quality of this RAG response on a scale of 0.0 to 1.0: Query: %s Sources used: %s Response: %s Assessment criteria: - Accuracy: Is the response factually correct based on the sources? - Relevance: Does the response directly address the query? - Completeness: Does the response provide sufficient information? - Coherence: Is the response well-structured and clear? Please provide only a numerical score between 0.0 and 1.0:`, query, sourcesBuilder.String(), response) scoreStr, err := rqa.llm.Generate(ctx, prompt) if err != nil { return 0.0, fmt.Errorf(\"quality assessment failed: %w\", err) } // Parse score scoreStr = strings.TrimSpace(scoreStr) score, err := strconv.ParseFloat(scoreStr, 64) if err != nil { return 0.0, fmt.Errorf(\"failed to parse quality score: %w\", err) } // Clamp score to valid range if score \u003c 0.0 { score = 0.0 } else if score \u003e 1.0 { score = 1.0 } return score, nil }\rBest Practices 1. RAG System Design Chunk Size Optimization: Balance context and specificity Embedding Quality: Use appropriate embedding models for your domain Retrieval Tuning: Optimize similarity thresholds and result limits Context Management: Manage context length to avoid token limits Source Attribution: Always track and cite information sources 2. Performance Optimization Caching: Cache frequent queries and embeddings Batch Processing: Process multiple queries efficiently Index Optimization: Use appropriate vector database indexes Async Processing: Use asynchronous operations where possible 3. Quality Assurance Evaluation Metrics: Implement comprehensive evaluation Human Feedback: Collect and incorporate user feedback Continuous Monitoring: Monitor system performance and quality A/B Testing: Test different RAG configurations Conclusion RAG implementation in AgenticGoKit enables agents to provide accurate, contextual, and up-to-date responses by combining retrieval and generation. Key takeaways:",
    "tags": [],
    "title": "rag-implementation",
    "uri": "/AgenticGoKitDocs/tutorials/memory-systems/rag-implementation/index.html"
  },
  {
    "breadcrumb": "content",
    "content": "AgenticGoKit API Reference Navigation: Documentation Home → API Reference\nComplete API documentation for building multi-agent systems\nThis section provides comprehensive documentation for all public APIs in AgenticGoKit. The framework is designed with a clean, intuitive interface that makes building complex agent systems straightforward.\n📚 API Overview AgenticGoKit’s API is organized around several core concepts:\nAgent API - Individual agents that process events and states Orchestration API - Multi-agent coordination and workflow patterns State \u0026 Events - Data flow and communication between agents Memory API - Persistent storage, RAG, and knowledge management MCP Integration - Tool integration via Model Context Protocol Configuration API - System configuration and setup 🏗️ Architecture Overview graph TB\rsubgraph \"Core Interfaces\"\rAgent[Agent Interface]\rAgentHandler[AgentHandler Interface]\rRunner[Runner Interface]\rOrchestrator[Orchestrator Interface]\rend\rsubgraph \"Data Flow\"\rEvent[Event]\rState[State]\rAgentResult[AgentResult]\rend\rsubgraph \"Advanced Features\"\rMemory[Memory System]\rMCP[MCP Tools]\rConfig[Configuration]\rend\rAgent --\u003e AgentHandler\rAgentHandler --\u003e Runner\rRunner --\u003e Orchestrator\rEvent --\u003e AgentHandler\rState --\u003e AgentHandler\rAgentHandler --\u003e AgentResult\rMemory --\u003e Agent\rMCP --\u003e Agent\rConfig --\u003e Runner\r🚀 Quick Start Basic Agent Creation package main import ( \"context\" \"fmt\" \"github.com/kunalkushwaha/agenticgokit/core\" ) func main() { // Create a simple agent agent := core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { message := event.Data[\"message\"].(string) return core.AgentResult{ Data: map[string]interface{}{ \"response\": fmt.Sprintf(\"Processed: %s\", message), }, }, nil }) // Create a runner and register the agent runner := core.NewRunner() runner.RegisterAgent(\"processor\", agent) // Process an event event := core.NewEvent(\"process\", map[string]interface{}{ \"message\": \"Hello, AgenticGoKit!\", }) results, err := runner.ProcessEvent(context.Background(), event) if err != nil { panic(err) } fmt.Printf(\"Response: %s\\n\", results[\"processor\"].Data[\"response\"]) }\rMulti-Agent Collaboration func collaborativeExample() { // Create multiple agents agents := map[string]core.AgentHandler{ \"analyzer\": core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { text := event.Data[\"text\"].(string) analysis := analyzeText(text) // Your analysis logic return core.AgentResult{ Data: map[string]interface{}{ \"analysis\": analysis, \"word_count\": len(strings.Fields(text)), }, }, nil }), \"summarizer\": core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { text := event.Data[\"text\"].(string) summary := summarizeText(text) // Your summarization logic return core.AgentResult{ Data: map[string]interface{}{ \"summary\": summary, }, }, nil }), } // Create collaborative runner runner := core.CreateCollaborativeRunner(agents, 30*time.Second) // Process with multiple agents event := core.NewEvent(\"analyze\", map[string]interface{}{ \"text\": \"Long document text here...\", }) results, _ := runner.ProcessEvent(context.Background(), event) // Both agents processed the event in parallel fmt.Printf(\"Analysis: %v\\n\", results[\"analyzer\"].Data[\"analysis\"]) fmt.Printf(\"Summary: %s\\n\", results[\"summarizer\"].Data[\"summary\"]) }\r🔧 Core Interfaces Agent vs AgentHandler AgenticGoKit provides two main interfaces for creating agents:\nAgent Interface - Simple, state-based processing:\ntype Agent interface { Run(ctx context.Context, inputState State) (State, error) Name() string }\rAgentHandler Interface - Event-driven processing with rich results:\ntype AgentHandler interface { Run(ctx context.Context, event Event, state State) (AgentResult, error) }\rWhen to use which:\nUse Agent for simple, stateful transformations Use AgentHandler for event-driven systems with complex orchestration Use AgentHandlerFunc for quick function-based agents Conversion Between Interfaces // Convert Agent to AgentHandler func ConvertAgentToHandler(agent Agent) AgentHandler { return core.ConvertAgentToHandler(agent) } // Convert function to AgentHandler func ConvertFuncToHandler(fn func(context.Context, Event, State) (AgentResult, error)) AgentHandler { return core.AgentHandlerFunc(fn) }\r📊 Orchestration Patterns AgenticGoKit supports multiple orchestration patterns:\nPattern Description Use Case Route Single agent per event (default) Simple request-response Collaborate All agents process in parallel Analysis, multiple perspectives Sequential Agents process in order Data pipelines, workflows Mixed Hybrid collaborative + sequential Complex business processes Loop Single agent iterative processing Refinement, optimization Pattern Selection Guide // Choose orchestration pattern based on your needs func chooseOrchestration(useCase string) core.OrchestrationMode { switch useCase { case \"simple_processing\": return core.OrchestrationRoute case \"parallel_analysis\": return core.OrchestrationCollaborate case \"data_pipeline\": return core.OrchestrationSequential case \"complex_workflow\": return core.OrchestrationMixed case \"iterative_refinement\": return core.OrchestrationLoop default: return core.OrchestrationRoute } }\r🧠 Memory and RAG AgenticGoKit provides built-in support for persistent memory and RAG (Retrieval-Augmented Generation):\n// Create memory-enabled agent memoryConfig := core.AgentMemoryConfig{ Provider: \"pgvector\", Connection: \"postgres://user:pass@localhost:5432/agents\", AutoEmbed: true, } memory, _ := core.NewMemory(memoryConfig) llmProvider, _ := core.NewOpenAIProvider() agent := core.NewMemoryEnabledAgent(\"assistant\", llmProvider, memory)\r🔧 Tool Integration Integrate external tools via MCP (Model Context Protocol):\n// Initialize MCP core.QuickStartMCP() // Create MCP-aware agent llmProvider, _ := core.NewOpenAIProvider() agent, _ := core.NewMCPAgent(\"assistant\", llmProvider) // Agent can now discover and use MCP tools automatically\rCommand Line Interface CLI Reference Complete reference for the agentcli command-line tool.\nAvailable commands:\ncreate - Create new projects with multi-agent orchestration trace - View execution traces and debugging information mcp - Manage MCP servers and tools cache - Cache management and optimization run - Run agents interactively for testing test - Run automated agent tests benchmark - Performance benchmarking config - Configuration management API Design Principles AgenticGoKit APIs follow these principles:\nInterface-based design - Clear separation of concerns Context-aware - All operations accept Go context Error handling - Explicit error returns and handling Configuration-driven - Behavior controlled through configuration Extensible - Plugin architecture for custom implementations Type Definitions All APIs use consistent type definitions:\n// Core types type State map[string]interface{} type Event struct { /* ... */ } type AgentResult struct { /* ... */ } // Configuration types type Config struct { /* ... */ } type OrchestrationMode string\rUsage Patterns Common patterns across all APIs:\nContext Usage ctx := context.Background() result, err := agent.Run(ctx, inputState)\rError Handling if err != nil { log.Printf(\"Operation failed: %v\", err) return err }\rConfiguration Loading config, err := core.LoadConfigFromWorkingDir() if err != nil { return fmt.Errorf(\"failed to load config: %w\", err) }\rVersioning AgenticGoKit follows semantic versioning:\nMajor versions - Breaking API changes Minor versions - New features, backward compatible Patch versions - Bug fixes, backward compatible Migration Guides When APIs change between versions, migration guides are provided:\nBreaking changes are documented Migration examples are provided Deprecation notices give advance warning Getting Help For API questions:\nCheck this reference documentation Review code examples in tutorials See how-to guides for specific use cases Check the troubleshooting guide for common issues",
    "description": "AgenticGoKit API Reference Navigation: Documentation Home → API Reference\nComplete API documentation for building multi-agent systems\nThis section provides comprehensive documentation for all public APIs in AgenticGoKit. The framework is designed with a clean, intuitive interface that makes building complex agent systems straightforward.\n📚 API Overview AgenticGoKit’s API is organized around several core concepts:\nAgent API - Individual agents that process events and states Orchestration API - Multi-agent coordination and workflow patterns State \u0026 Events - Data flow and communication between agents Memory API - Persistent storage, RAG, and knowledge management MCP Integration - Tool integration via Model Context Protocol Configuration API - System configuration and setup 🏗️ Architecture Overview graph TB\rsubgraph \"Core Interfaces\"\rAgent[Agent Interface]\rAgentHandler[AgentHandler Interface]\rRunner[Runner Interface]\rOrchestrator[Orchestrator Interface]\rend\rsubgraph \"Data Flow\"\rEvent[Event]\rState[State]\rAgentResult[AgentResult]\rend\rsubgraph \"Advanced Features\"\rMemory[Memory System]\rMCP[MCP Tools]\rConfig[Configuration]\rend\rAgent --\u003e AgentHandler\rAgentHandler --\u003e Runner\rRunner --\u003e Orchestrator\rEvent --\u003e AgentHandler\rState --\u003e AgentHandler\rAgentHandler --\u003e AgentResult\rMemory --\u003e Agent\rMCP --\u003e Agent\rConfig --\u003e Runner\r🚀 Quick Start Basic Agent Creation package main import ( \"context\" \"fmt\" \"github.com/kunalkushwaha/agenticgokit/core\" ) func main() { // Create a simple agent agent := core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { message := event.Data[\"message\"].(string) return core.AgentResult{ Data: map[string]interface{}{ \"response\": fmt.Sprintf(\"Processed: %s\", message), }, }, nil }) // Create a runner and register the agent runner := core.NewRunner() runner.RegisterAgent(\"processor\", agent) // Process an event event := core.NewEvent(\"process\", map[string]interface{}{ \"message\": \"Hello, AgenticGoKit!\", }) results, err := runner.ProcessEvent(context.Background(), event) if err != nil { panic(err) } fmt.Printf(\"Response: %s\\n\", results[\"processor\"].Data[\"response\"]) }\rMulti-Agent Collaboration func collaborativeExample() { // Create multiple agents agents := map[string]core.AgentHandler{ \"analyzer\": core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { text := event.Data[\"text\"].(string) analysis := analyzeText(text) // Your analysis logic return core.AgentResult{ Data: map[string]interface{}{ \"analysis\": analysis, \"word_count\": len(strings.Fields(text)), }, }, nil }), \"summarizer\": core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { text := event.Data[\"text\"].(string) summary := summarizeText(text) // Your summarization logic return core.AgentResult{ Data: map[string]interface{}{ \"summary\": summary, }, }, nil }), } // Create collaborative runner runner := core.CreateCollaborativeRunner(agents, 30*time.Second) // Process with multiple agents event := core.NewEvent(\"analyze\", map[string]interface{}{ \"text\": \"Long document text here...\", }) results, _ := runner.ProcessEvent(context.Background(), event) // Both agents processed the event in parallel fmt.Printf(\"Analysis: %v\\n\", results[\"analyzer\"].Data[\"analysis\"]) fmt.Printf(\"Summary: %s\\n\", results[\"summarizer\"].Data[\"summary\"]) }\r🔧 Core Interfaces Agent vs AgentHandler AgenticGoKit provides two main interfaces for creating agents:",
    "tags": [],
    "title": "reference",
    "uri": "/AgenticGoKitDocs/reference/index.html"
  },
  {
    "breadcrumb": "content \u003e guides \u003e development",
    "content": "Research Assistant Create a multi-agent research system with web search, analysis, and synthesis capabilities\nThis guide shows you how to build a comprehensive research assistant that can search the web, analyze information, and provide well-structured research reports using AgenticGoKit’s multi-agent orchestration.\nWhat You’ll Build A research assistant system that can:\nSearch the web for relevant information Analyze and validate sources Synthesize findings into coherent reports Handle complex multi-step research queries Maintain research context across interactions Prerequisites Basic AgenticGoKit project setup Understanding of multi-agent orchestration MCP tools configured for web search LLM provider configured (OpenAI, Azure, etc.) Quick Start 1. Create Research Assistant Project # Create project with collaborative orchestration agentcli create research-assistant \\\\ --orchestration-mode collaborative \\\\ --collaborative-agents \\\"researcher,analyzer,synthesizer\\\" \\\\ --mcp-enabled \\\\ --mcp-tools \\\"web_search,summarize\\\" \\\\ --visualize cd research-assistant\r2. Configure Environment # Set up API keys export OPENAI_API_KEY=your-openai-key export BRAVE_API_KEY=your-brave-search-key # Optional # Install dependencies go mod tidy\r3. Test the System # Run a research query go run . -m \\\"Research the latest developments in quantum computing and their potential impact on cryptography\\\"\rArchitecture Overview The research assistant uses a collaborative multi-agent architecture:\ngraph TD\rINPUT[\\\"🎯 Research Query\\\"]\rRESEARCHER[\\\"🔍 Researcher Agent\\\"]\rANALYZER[\\\"📊 Analyzer Agent\\\"]\rSYNTHESIZER[\\\"📝 Synthesizer Agent\\\"]\rOUTPUT[\\\"📋 Research Report\\\"]\rINPUT --\u003e RESEARCHER\rINPUT --\u003e ANALYZER\rINPUT --\u003e SYNTHESIZER\rRESEARCHER --\u003e OUTPUT\rANALYZER --\u003e OUTPUT\rSYNTHESIZER --\u003e OUTPUT\rAgent Implementation 1. Researcher Agent The researcher agent handles web search and information gathering:\npackage agents import ( \\\"context\\\" \\\"fmt\\\" \\\"strings\\\" \\\"time\\\" \\\"github.com/kunalkushwaha/agenticgokit/core\\\" ) type ResearcherAgent struct { name string llmProvider core.ModelProvider mcpManager core.MCPManager } func NewResearcherAgent(name string, llm core.ModelProvider, mcp core.MCPManager) *ResearcherAgent { return \u0026ResearcherAgent{ name: name, llmProvider: llm, mcpManager: mcp, } } func (a *ResearcherAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { query, ok := event.GetData()[\\\"query\\\"].(string) if !ok { return core.AgentResult{}, fmt.Errorf(\\\"missing query in event data\\\") } // Generate search queries searchQueries, err := a.generateSearchQueries(ctx, query) if err != nil { return core.AgentResult{}, fmt.Errorf(\\\"failed to generate search queries: %w\\\", err) } // Perform searches var allResults []SearchResult for _, searchQuery := range searchQueries { results, err := a.performSearch(ctx, searchQuery) if err != nil { // Log error but continue with other searches fmt.Printf(\\\"Search failed for query '%s': %v\\\\n\\\", searchQuery, err) continue } allResults = append(allResults, results...) } // Filter and rank results filteredResults := a.filterResults(allResults, query) // Store research findings in state state.Set(\\\"research_findings\\\", filteredResults) state.Set(\\\"search_queries\\\", searchQueries) state.SetMeta(\\\"researcher_completed\\\", \\\"true\\\") return core.AgentResult{ OutputState: state, }, nil } func (a *ResearcherAgent) generateSearchQueries(ctx context.Context, query string) ([]string, error) { prompt := fmt.Sprintf(` Given this research query: \\\"%s\\\" Generate 3-5 specific search queries that would help gather comprehensive information. Each query should focus on a different aspect of the topic. Return only the search queries, one per line. `, query) response, err := a.llmProvider.Generate(ctx, prompt) if err != nil { return nil, err } queries := strings.Split(strings.TrimSpace(response), \\\"\\\\n\\\") var cleanQueries []string for _, q := range queries { if cleaned := strings.TrimSpace(q); cleaned != \\\"\\\" { cleanQueries = append(cleanQueries, cleaned) } } return cleanQueries, nil } func (a *ResearcherAgent) performSearch(ctx context.Context, query string) ([]SearchResult, error) { // Use MCP web search tool result, err := a.mcpManager.CallTool(ctx, \\\"web_search\\\", map[string]interface{}{ \\\"query\\\": query, \\\"num_results\\\": 10, }) if err != nil { return nil, err } // Parse search results (implementation depends on search provider) return parseSearchResults(result), nil } func (a *ResearcherAgent) filterResults(results []SearchResult, originalQuery string) []SearchResult { // Implement relevance filtering and deduplication seen := make(map[string]bool) var filtered []SearchResult for _, result := range results { if seen[result.URL] { continue } seen[result.URL] = true // Add relevance scoring logic here if a.isRelevant(result, originalQuery) { filtered = append(filtered, result) } } return filtered } type SearchResult struct { Title string `json:\\\"title\\\"` URL string `json:\\\"url\\\"` Snippet string `json:\\\"snippet\\\"` Source string `json:\\\"source\\\"` }\r2. Analyzer Agent The analyzer agent evaluates source credibility and extracts key insights:\ntype AnalyzerAgent struct { name string llmProvider core.ModelProvider } func NewAnalyzerAgent(name string, llm core.ModelProvider) *AnalyzerAgent { return \u0026AnalyzerAgent{ name: name, llmProvider: llm, } } func (a *AnalyzerAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Get research findings from state findings, exists := state.Get(\\\"research_findings\\\") if !exists { return core.AgentResult{}, fmt.Errorf(\\\"no research findings available\\\") } results := findings.([]SearchResult) query := event.Data[\\\"query\\\"].(string) // Analyze each source var analyses []SourceAnalysis for _, result := range results { analysis, err := a.analyzeSource(ctx, result, query) if err != nil { // Log error but continue with other sources fmt.Printf(\\\"Analysis failed for source %s: %v\\\\n\\\", result.URL, err) continue } analyses = append(analyses, analysis) } // Generate overall analysis overallAnalysis, err := a.generateOverallAnalysis(ctx, analyses, query) if err != nil { return core.AgentResult{}, fmt.Errorf(\\\"failed to generate overall analysis: %w\\\", err) } // Store analysis in state state.Set(\\\"source_analyses\\\", analyses) state.Set(\\\"overall_analysis\\\", overallAnalysis) state.SetMeta(\\\"analyzer_completed\\\", \\\"true\\\") return core.AgentResult{ OutputState: state, }, nil } func (a *AnalyzerAgent) analyzeSource(ctx context.Context, result SearchResult, query string) (SourceAnalysis, error) { prompt := fmt.Sprintf(` Analyze this source for a research query about \\\"%s\\\": Title: %s URL: %s Content: %s Provide analysis in JSON format: { \\\"credibility_score\\\": 0-10, \\\"relevance_score\\\": 0-10, \\\"key_insights\\\": [\\\"insight1\\\", \\\"insight2\\\"], \\\"potential_bias\\\": \\\"description\\\", \\\"source_type\\\": \\\"academic|news|blog|government|commercial\\\" } `, query, result.Title, result.URL, result.Snippet) response, err := a.llmProvider.Generate(ctx, prompt) if err != nil { return SourceAnalysis{}, err } // Parse JSON response var analysis SourceAnalysis if err := json.Unmarshal([]byte(response), \u0026analysis); err != nil { return SourceAnalysis{}, fmt.Errorf(\\\"failed to parse analysis: %w\\\", err) } analysis.Source = result return analysis, nil } type SourceAnalysis struct { Source SearchResult `json:\\\"source\\\"` CredibilityScore int `json:\\\"credibility_score\\\"` RelevanceScore int `json:\\\"relevance_score\\\"` KeyInsights []string `json:\\\"key_insights\\\"` PotentialBias string `json:\\\"potential_bias\\\"` SourceType string `json:\\\"source_type\\\"` }\r3. Synthesizer Agent The synthesizer agent creates the final research report:\ntype SynthesizerAgent struct { name string llmProvider core.ModelProvider } func NewSynthesizerAgent(name string, llm core.ModelProvider) *SynthesizerAgent { return \u0026SynthesizerAgent{ name: name, llmProvider: llm, } } func (a *SynthesizerAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Get analysis from state analyses, exists := state.Get(\\\"source_analyses\\\") if !exists { return core.AgentResult{}, fmt.Errorf(\\\"no source analyses available\\\") } overallAnalysis, exists := state.Get(\\\"overall_analysis\\\") if !exists { return core.AgentResult{}, fmt.Errorf(\\\"no overall analysis available\\\") } query := event.GetData()[\\\"query\\\"].(string) sourceAnalyses := analyses.([]SourceAnalysis) // Generate comprehensive report report, err := a.generateReport(ctx, query, sourceAnalyses, overallAnalysis.(string)) if err != nil { return core.AgentResult{}, fmt.Errorf(\\\"failed to generate report: %w\\\", err) } // Store final report state.Set(\\\"final_report\\\", report) state.SetMeta(\\\"synthesizer_completed\\\", \\\"true\\\") return core.AgentResult{ OutputState: state, }, nil } func (a *SynthesizerAgent) generateReport(ctx context.Context, query string, analyses []SourceAnalysis, overallAnalysis string) (string, error) { // Prepare source summaries var sourceSummaries []string for _, analysis := range analyses { if analysis.CredibilityScore \u003e= 7 \u0026\u0026 analysis.RelevanceScore \u003e= 7 { summary := fmt.Sprintf(\\\"- %s (%s): %s\\\", analysis.Source.Title, analysis.Source.URL, strings.Join(analysis.KeyInsights, \\\"; \\\")) sourceSummaries = append(sourceSummaries, summary) } } prompt := fmt.Sprintf(` Create a comprehensive research report for the query: \\\"%s\\\" Overall Analysis: %s Key Sources and Insights: %s Structure the report with: 1. Executive Summary 2. Key Findings 3. Detailed Analysis 4. Sources and References 5. Conclusions and Implications Make it professional, well-structured, and cite sources appropriately. `, query, overallAnalysis, strings.Join(sourceSummaries, \\\"\\\\n\\\")) return a.llmProvider.Generate(ctx, prompt) }\rConfiguration Agent Flow Configuration The generated agentflow.toml includes:\n[runner] session_id = \\\"research-session\\\" orchestration_mode = \\\"collaborate\\\" timeout = \\\"120s\\\" max_concurrency = 3 failure_threshold = 0.7 # Collaborative agents process in parallel collaborative_agents = [\\\"researcher\\\", \\\"analyzer\\\", \\\"synthesizer\\\"] [mcp] enabled = true enable_discovery = true connection_timeout = \\\"30s\\\" max_retries = 3 enable_caching = true cache_timeout = \\\"1800s\\\" # 30 minutes [[mcp.servers]] name = \\\"brave-search\\\" type = \\\"stdio\\\" command = \\\"npx @modelcontextprotocol/server-brave-search\\\" enabled = true\rMain Application package main import ( \\\"context\\\" \\\"fmt\\\" \\\"log\\\" \\\"os\\\" \\\"github.com/kunalkushwaha/agenticgokit/core\\\" \\\"./agents\\\" ) func main() { ctx := context.Background() // Initialize LLM provider llmProvider, err := core.NewOpenAIProvider(core.OpenAIConfig{ APIKey: os.Getenv(\\\"OPENAI_API_KEY\\\"), Model: \\\"gpt-4\\\", }) if err != nil { log.Fatal(\\\"Failed to create LLM provider:\\\", err) } // Initialize MCP manager mcpManager, err := core.QuickStartMCP() if err != nil { log.Fatal(\\\"Failed to initialize MCP:\\\", err) } defer mcpManager.Close() // Create agents agentHandlers := map[string]core.AgentHandler{ \\\"researcher\\\": agents.NewResearcherAgent(\\\"researcher\\\", llmProvider, mcpManager), \\\"analyzer\\\": agents.NewAnalyzerAgent(\\\"analyzer\\\", llmProvider), \\\"synthesizer\\\": agents.NewSynthesizerAgent(\\\"synthesizer\\\", llmProvider), } // Create collaborative runner runner := core.CreateCollaborativeRunner(agentHandlers, 120*time.Second) // Get query from command line if len(os.Args) \u003c 3 || os.Args[1] != \\\"-m\\\" { log.Fatal(\\\"Usage: go run . -m \\\\\\\"your research query\\\\\\\"\\\") } query := os.Args[2] // Create research event event := core.NewEvent(\\\"\\\", core.EventData{ \\\"query\\\": query, }, nil) // Process research request results, err := runner.ProcessEvent(ctx, event) if err != nil { log.Fatal(\\\"Research failed:\\\", err) } // Display results fmt.Println(\\\"\\\\n=== RESEARCH REPORT ===\\\") if synthResult, ok := results[\\\"synthesizer\\\"]; ok \u0026\u0026 synthResult.Error == \\\"\\\" { if report, exists := synthResult.OutputState.Get(\\\"final_report\\\"); exists { fmt.Println(report) } } // Display statistics fmt.Println(\\\"\\\\n=== RESEARCH STATISTICS ===\\\") for agentName, result := range results { if result.Error != \\\"\\\" { fmt.Printf(\\\"%s: Error - %s\\\\n\\\", agentName, result.Error) } else { fmt.Printf(\\\"%s: Success\\\\n\\\", agentName) } } }\rAdvanced Features Research Context Management Maintain research context across multiple queries:\ntype ResearchContext struct { Topic string PreviousQueries []string KnowledgeBase map[string]interface{} Sources []SearchResult } func (a *ResearcherAgent) RunWithContext(ctx context.Context, event core.Event, state core.State, researchContext *ResearchContext) (core.AgentResult, error) { query := event.GetData()[\\\"query\\\"].(string) // Add to context researchContext.PreviousQueries = append(researchContext.PreviousQueries, query) // Use context to refine search contextualQuery := a.refineQueryWithContext(query, researchContext) // Continue with normal processing return a.performContextualSearch(ctx, contextualQuery, researchContext) }\rSource Validation Implement advanced source validation:\nfunc (a *AnalyzerAgent) validateSource(ctx context.Context, result SearchResult) (bool, error) { // Check domain reputation if a.isBlacklistedDomain(result.URL) { return false, nil } // Check for academic or government sources if a.isAuthoritative(result.URL) { return true, nil } // Use LLM to assess credibility prompt := fmt.Sprintf(` Assess the credibility of this source: URL: %s Title: %s Content: %s Is this a credible source? Respond with YES or NO and brief explanation. `, result.URL, result.Title, result.Snippet) response, err := a.llmProvider.Generate(ctx, prompt) if err != nil { return false, err } return strings.HasPrefix(strings.ToUpper(response), \\\"YES\\\"), nil }\rReport Templates Create different report formats:\ntype ReportTemplate struct { Name string Sections []string Format string } var ReportTemplates = map[string]ReportTemplate{ \\\"academic\\\": { Name: \\\"Academic Research Report\\\", Sections: []string{\\\"Abstract\\\", \\\"Introduction\\\", \\\"Literature Review\\\", \\\"Analysis\\\", \\\"Conclusion\\\", \\\"References\\\"}, Format: \\\"formal\\\", }, \\\"business\\\": { Name: \\\"Business Intelligence Report\\\", Sections: []string{\\\"Executive Summary\\\", \\\"Key Findings\\\", \\\"Market Analysis\\\", \\\"Recommendations\\\", \\\"Appendix\\\"}, Format: \\\"executive\\\", }, \\\"news\\\": { Name: \\\"News Summary\\\", Sections: []string{\\\"Headline\\\", \\\"Summary\\\", \\\"Key Points\\\", \\\"Sources\\\"}, Format: \\\"journalistic\\\", }, }\rTesting Unit Tests func TestResearcherAgent(t *testing.T) { // Create mock LLM and MCP mockLLM := \u0026MockLLMProvider{} mockMCP := \u0026MockMCPManager{} agent := NewResearcherAgent(\\\"test-researcher\\\", mockLLM, mockMCP) event := core.NewEvent(\\\"research\\\", map[string]interface{}{ \\\"query\\\": \\\"test query\\\", }) state := core.NewState() result, err := agent.Run(context.Background(), event, state) assert.NoError(t, err) assert.True(t, result.Success) assert.Contains(t, result.Data, \\\"findings\\\") }\rIntegration Tests # Test the complete research flow go test -v ./tests/integration/research_test.go\rDeployment Docker Configuration FROM golang:1.21-alpine AS builder WORKDIR /app COPY go.mod go.sum ./ RUN go mod download COPY . . RUN CGO_ENABLED=0 go build -o research-assistant . FROM alpine:latest RUN apk --no-cache add ca-certificates nodejs npm WORKDIR /app # Install MCP servers RUN npm install -g @modelcontextprotocol/server-brave-search COPY --from=builder /app/research-assistant . COPY agentflow.toml . CMD [\\\"./research-assistant\\\"]\rPerformance Optimization Parallel Processing The collaborative orchestration automatically processes agents in parallel, but you can optimize further:\n// Optimize search parallelization func (a *ResearcherAgent) performParallelSearches(ctx context.Context, queries []string) ([]SearchResult, error) { resultsChan := make(chan []SearchResult, len(queries)) errorsChan := make(chan error, len(queries)) for _, query := range queries { go func(q string) { results, err := a.performSearch(ctx, q) if err != nil { errorsChan \u003c- err return } resultsChan \u003c- results }(query) } var allResults []SearchResult for i := 0; i \u003c len(queries); i++ { select { case results := \u003c-resultsChan: allResults = append(allResults, results...) case err := \u003c-errorsChan: log.Printf(\\\"Search error: %v\\\", err) case \u003c-ctx.Done(): return nil, ctx.Err() } } return allResults, nil }\rTroubleshooting Common Issues Agents not collaborating properly:\nCheck orchestration mode is set to \"collaborate\" Verify all agents are registered correctly Check timeout settings Search results poor quality:\nRefine search query generation Implement better result filtering Use multiple search providers Report generation slow:\nOptimize LLM prompts Implement result caching Use streaming responses Next Steps Enhance your research assistant with:\nMemory Integration: Store research findings for future queries Document Analysis: Add PDF and document processing capabilities Citation Management: Implement proper academic citation formats Real-time Updates: Monitor topics for new information Collaborative Features: Allow multiple users to contribute to research Related Guides Web Search Integration - Detailed search setup Multi-Agent Orchestration - Orchestration patterns MCP Tools - Tool integration Best Practices - Development guidelines This research assistant demonstrates the power of AgenticGoKit’s collaborative multi-agent orchestration for complex, multi-step tasks.",
    "description": "Research Assistant Create a multi-agent research system with web search, analysis, and synthesis capabilities\nThis guide shows you how to build a comprehensive research assistant that can search the web, analyze information, and provide well-structured research reports using AgenticGoKit’s multi-agent orchestration.\nWhat You’ll Build A research assistant system that can:\nSearch the web for relevant information Analyze and validate sources Synthesize findings into coherent reports Handle complex multi-step research queries Maintain research context across interactions Prerequisites Basic AgenticGoKit project setup Understanding of multi-agent orchestration MCP tools configured for web search LLM provider configured (OpenAI, Azure, etc.) Quick Start 1. Create Research Assistant Project # Create project with collaborative orchestration agentcli create research-assistant \\\\ --orchestration-mode collaborative \\\\ --collaborative-agents \\\"researcher,analyzer,synthesizer\\\" \\\\ --mcp-enabled \\\\ --mcp-tools \\\"web_search,summarize\\\" \\\\ --visualize cd research-assistant\r2. Configure Environment # Set up API keys export OPENAI_API_KEY=your-openai-key export BRAVE_API_KEY=your-brave-search-key # Optional # Install dependencies go mod tidy\r3. Test the System # Run a research query go run . -m \\\"Research the latest developments in quantum computing and their potential impact on cryptography\\\"\rArchitecture Overview The research assistant uses a collaborative multi-agent architecture:",
    "tags": [],
    "title": "research-assistant",
    "uri": "/AgenticGoKitDocs/guides/development/research-assistant/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials \u003e advanced",
    "content": "Retry Policies and Error Handling Building resilient agent systems with intelligent retry strategies and comprehensive error handling\nRetry policies are crucial for handling transient failures in distributed agent systems. This guide shows you how to implement intelligent retry strategies, handle different types of errors appropriately, and build robust error recovery mechanisms in AgenticGoKit.\n🔄 Understanding Retry Patterns Different types of failures require different retry strategies:\nflowchart TD\rREQUEST[Request] --\u003e EXECUTE[Execute]\rEXECUTE --\u003e SUCCESS{Success?}\rSUCCESS --\u003e|Yes| RETURN[Return Result]\rSUCCESS --\u003e|No| CLASSIFY[Classify Error]\rCLASSIFY --\u003e TRANSIENT{Transient Error?}\rCLASSIFY --\u003e PERMANENT{Permanent Error?}\rCLASSIFY --\u003e RATE_LIMIT{Rate Limited?}\rTRANSIENT --\u003e|Yes| BACKOFF[Apply Backoff]\rPERMANENT --\u003e|Yes| FAIL[Fail Immediately]\rRATE_LIMIT --\u003e|Yes| WAIT[Wait for Reset]\rBACKOFF --\u003e RETRY_CHECK{Retries Left?}\rWAIT --\u003e RETRY_CHECK\rRETRY_CHECK --\u003e|Yes| EXECUTE\rRETRY_CHECK --\u003e|No| FAIL\rclassDef success fill:#d4edda,stroke:#155724\rclassDef error fill:#f8d7da,stroke:#721c24\rclassDef retry fill:#fff3cd,stroke:#856404\rclass RETURN success\rclass FAIL error\rclass BACKOFF,WAIT,RETRY_CHECK retry\r🛠️ Built-in Retry Configuration AgenticGoKit provides built-in retry policies for LLM providers and MCP tools:\nLLM Provider Retry Policies # agentflow.toml [providers.openai.retry] enabled = true max_retries = 3 base_delay_ms = 1000 # Start with 1 second max_delay_ms = 30000 # Cap at 30 seconds backoff_factor = 2.0 # Exponential backoff enable_jitter = true # Add randomization # Retry conditions retry_on_timeout = true retry_on_rate_limit = true retry_on_server_error = true retry_on_network_error = true [providers.azure.retry] enabled = true max_retries = 5 # Higher for enterprise base_delay_ms = 500 max_delay_ms = 60000 backoff_factor = 1.5 # More conservative enable_jitter = true\rMCP Tool Retry Policies # agentflow.toml [mcp.retry] enabled = true max_retries = 3 base_delay_ms = 1000 max_delay_ms = 15000 backoff_factor = 2.0 enable_jitter = true # Per-server retry settings [[mcp.servers]] name = \"web-search\" type = \"stdio\" command = \"npx @modelcontextprotocol/server-web-search\" enabled = true [mcp.servers.web-search.retry] max_retries = 5 # Web searches can be flaky base_delay_ms = 2000 retry_on_timeout = true\r🔧 Custom Retry Implementation Basic Retry Policy package patterns import ( \"context\" \"errors\" \"math\" \"math/rand\" \"time\" ) type RetryPolicy struct { MaxRetries int BaseDelay time.Duration MaxDelay time.Duration BackoffFactor float64 EnableJitter bool RetryCondition func(error) bool } func NewExponentialRetryPolicy(maxRetries int, baseDelay time.Duration) *RetryPolicy { return \u0026RetryPolicy{ MaxRetries: maxRetries, BaseDelay: baseDelay, MaxDelay: 30 * time.Second, BackoffFactor: 2.0, EnableJitter: true, RetryCondition: func(err error) bool { return IsRetryableError(err) }, } } func (rp *RetryPolicy) Execute(ctx context.Context, fn func() error) error { var lastErr error for attempt := 0; attempt \u003c= rp.MaxRetries; attempt++ { // Execute the function err := fn() if err == nil { return nil // Success } lastErr = err // Check if we should retry if !rp.RetryCondition(err) { return err // Don't retry permanent errors } // Don't wait after the last attempt if attempt == rp.MaxRetries { break } // Calculate delay delay := rp.calculateDelay(attempt) // Wait with context cancellation support select { case \u003c-time.After(delay): continue case \u003c-ctx.Done(): return ctx.Err() } } return lastErr } func (rp *RetryPolicy) calculateDelay(attempt int) time.Duration { // Exponential backoff delay := float64(rp.BaseDelay) * math.Pow(rp.BackoffFactor, float64(attempt)) // Apply maximum delay cap if delay \u003e float64(rp.MaxDelay) { delay = float64(rp.MaxDelay) } // Add jitter to prevent thundering herd if rp.EnableJitter { jitter := delay * 0.1 * rand.Float64() // ±10% jitter delay += jitter - (delay * 0.05) } return time.Duration(delay) } // Error classification func IsRetryableError(err error) bool { if err == nil { return false } // Check for specific error types switch { case IsTimeoutError(err): return true case IsRateLimitError(err): return true case IsServerError(err): return true case IsNetworkError(err): return true case IsTemporaryError(err): return true default: return false } } func IsTimeoutError(err error) bool { return errors.Is(err, context.DeadlineExceeded) || strings.Contains(err.Error(), \"timeout\") } func IsRateLimitError(err error) bool { return strings.Contains(err.Error(), \"rate limit\") || strings.Contains(err.Error(), \"429\") } func IsServerError(err error) bool { return strings.Contains(err.Error(), \"500\") || strings.Contains(err.Error(), \"502\") || strings.Contains(err.Error(), \"503\") || strings.Contains(err.Error(), \"504\") } func IsNetworkError(err error) bool { return strings.Contains(err.Error(), \"connection refused\") || strings.Contains(err.Error(), \"no such host\") || strings.Contains(err.Error(), \"network unreachable\") } func IsTemporaryError(err error) bool { type temporary interface { Temporary() bool } if te, ok := err.(temporary); ok { return te.Temporary() } return false }\rAgent with Retry Policy package agents import ( \"context\" \"fmt\" \"time\" \"github.com/kunalkushwaha/agenticgokit/core\" \"your-project/patterns\" ) type RetryableAgent struct { name string llmProvider core.ModelProvider retryPolicy *patterns.RetryPolicy metrics *RetryMetrics } func NewRetryableAgent(name string, provider core.ModelProvider) *RetryableAgent { return \u0026RetryableAgent{ name: name, llmProvider: provider, retryPolicy: patterns.NewExponentialRetryPolicy(3, 1*time.Second), metrics: NewRetryMetrics(), } } func (a *RetryableAgent) Execute(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { var result *core.AgentResult var err error // Execute with retry policy retryErr := a.retryPolicy.Execute(ctx, func() error { result, err = a.executeOnce(ctx, event, state) return err }) // Record metrics a.metrics.RecordExecution(retryErr == nil, a.retryPolicy.MaxRetries) if retryErr != nil { return nil, retryErr } return result, err } func (a *RetryableAgent) executeOnce(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { query := fmt.Sprintf(\"Process this request: %v\", event.Data) response, err := a.llmProvider.GenerateResponse(ctx, query, nil) if err != nil { return nil, fmt.Errorf(\"LLM request failed: %w\", err) } return \u0026core.AgentResult{ Data: map[string]interface{}{ \"response\": response, \"agent\": a.name, }, }, nil } func (a *RetryableAgent) SetRetryPolicy(policy *patterns.RetryPolicy) { a.retryPolicy = policy }\r📊 Advanced Retry Strategies Adaptive Retry Policy type AdaptiveRetryPolicy struct { *RetryPolicy // Adaptive parameters successRate float64 avgResponseTime time.Duration recentResults []RetryResult mu sync.RWMutex } type RetryResult struct { Success bool Attempts int Duration time.Duration ErrorType string Timestamp time.Time } func NewAdaptiveRetryPolicy(basePolicy *RetryPolicy) *AdaptiveRetryPolicy { return \u0026AdaptiveRetryPolicy{ RetryPolicy: basePolicy, recentResults: make([]RetryResult, 0, 100), } } func (arp *AdaptiveRetryPolicy) Execute(ctx context.Context, fn func() error) error { start := time.Now() attempts := 0 // Adapt policy based on recent performance arp.adaptPolicy() var lastErr error for attempt := 0; attempt \u003c= arp.MaxRetries; attempt++ { attempts++ err := fn() if err == nil { // Record successful execution arp.recordResult(RetryResult{ Success: true, Attempts: attempts, Duration: time.Since(start), Timestamp: time.Now(), }) return nil } lastErr = err if !arp.RetryCondition(err) || attempt == arp.MaxRetries { break } delay := arp.calculateDelay(attempt) select { case \u003c-time.After(delay): continue case \u003c-ctx.Done(): return ctx.Err() } } // Record failed execution arp.recordResult(RetryResult{ Success: false, Attempts: attempts, Duration: time.Since(start), ErrorType: classifyError(lastErr), Timestamp: time.Now(), }) return lastErr } func (arp *AdaptiveRetryPolicy) adaptPolicy() { arp.mu.Lock() defer arp.mu.Unlock() if len(arp.recentResults) \u003c 10 { return // Not enough data } // Calculate recent success rate recent := arp.recentResults[len(arp.recentResults)-20:] successes := 0 totalDuration := time.Duration(0) for _, result := range recent { if result.Success { successes++ } totalDuration += result.Duration } arp.successRate = float64(successes) / float64(len(recent)) arp.avgResponseTime = totalDuration / time.Duration(len(recent)) // Adapt retry parameters based on performance if arp.successRate \u003e 0.9 { // High success rate - reduce retries and delays arp.MaxRetries = max(1, arp.MaxRetries-1) arp.BaseDelay = time.Duration(float64(arp.BaseDelay) * 0.8) } else if arp.successRate \u003c 0.5 { // Low success rate - increase retries and delays arp.MaxRetries = min(10, arp.MaxRetries+1) arp.BaseDelay = time.Duration(float64(arp.BaseDelay) * 1.2) } // Adapt based on response time if arp.avgResponseTime \u003e 10*time.Second { // Slow responses - increase delays arp.BaseDelay = time.Duration(float64(arp.BaseDelay) * 1.1) } } func (arp *AdaptiveRetryPolicy) recordResult(result RetryResult) { arp.mu.Lock() defer arp.mu.Unlock() arp.recentResults = append(arp.recentResults, result) // Keep only recent results if len(arp.recentResults) \u003e 100 { arp.recentResults = arp.recentResults[1:] } }\rContext-Aware Retry Policy type ContextAwareRetryPolicy struct { policies map[string]*RetryPolicy defaultPolicy *RetryPolicy mu sync.RWMutex } func NewContextAwareRetryPolicy() *ContextAwareRetryPolicy { return \u0026ContextAwareRetryPolicy{ policies: map[string]*RetryPolicy{ \"timeout\": NewTimeoutRetryPolicy(), \"rate_limit\": NewRateLimitRetryPolicy(), \"server_error\": NewServerErrorRetryPolicy(), \"network\": NewNetworkRetryPolicy(), }, defaultPolicy: patterns.NewExponentialRetryPolicy(3, 1*time.Second), } } func (carp *ContextAwareRetryPolicy) Execute(ctx context.Context, fn func() error) error { var lastErr error for attempt := 0; attempt \u003c= 5; attempt++ { // Max attempts across all policies err := fn() if err == nil { return nil } lastErr = err // Select appropriate retry policy based on error type policy := carp.selectPolicy(err) if policy == nil { return err // No retry policy for this error } // Check if we should retry with this policy if !policy.RetryCondition(err) { return err } // Calculate delay based on error-specific policy delay := policy.calculateDelay(attempt) select { case \u003c-time.After(delay): continue case \u003c-ctx.Done(): return ctx.Err() } } return lastErr } func (carp *ContextAwareRetryPolicy) selectPolicy(err error) *RetryPolicy { carp.mu.RLock() defer carp.mu.RUnlock() switch { case IsTimeoutError(err): return carp.policies[\"timeout\"] case IsRateLimitError(err): return carp.policies[\"rate_limit\"] case IsServerError(err): return carp.policies[\"server_error\"] case IsNetworkError(err): return carp.policies[\"network\"] default: return carp.defaultPolicy } } func NewTimeoutRetryPolicy() *RetryPolicy { return \u0026RetryPolicy{ MaxRetries: 2, // Fewer retries for timeouts BaseDelay: 500 * time.Millisecond, MaxDelay: 5 * time.Second, BackoffFactor: 1.5, // Conservative backoff EnableJitter: true, RetryCondition: IsTimeoutError, } } func NewRateLimitRetryPolicy() *RetryPolicy { return \u0026RetryPolicy{ MaxRetries: 5, // More retries for rate limits BaseDelay: 2 * time.Second, // Longer initial delay MaxDelay: 60 * time.Second, BackoffFactor: 2.0, EnableJitter: true, RetryCondition: IsRateLimitError, } } func NewServerErrorRetryPolicy() *RetryPolicy { return \u0026RetryPolicy{ MaxRetries: 3, BaseDelay: 1 * time.Second, MaxDelay: 30 * time.Second, BackoffFactor: 2.0, EnableJitter: true, RetryCondition: IsServerError, } } func NewNetworkRetryPolicy() *RetryPolicy { return \u0026RetryPolicy{ MaxRetries: 4, // Network issues might resolve quickly BaseDelay: 200 * time.Millisecond, MaxDelay: 10 * time.Second, BackoffFactor: 2.5, EnableJitter: true, RetryCondition: IsNetworkError, } }\r🔄 Retry with Circuit Breaker Integration type ResilientExecutor struct { retryPolicy *patterns.RetryPolicy circuitBreaker *patterns.CircuitBreaker metrics *ExecutionMetrics } func NewResilientExecutor() *ResilientExecutor { return \u0026ResilientExecutor{ retryPolicy: patterns.NewExponentialRetryPolicy(3, 1*time.Second), circuitBreaker: patterns.NewCircuitBreaker(5, 3, 30*time.Second), metrics: NewExecutionMetrics(), } } func (re *ResilientExecutor) Execute(ctx context.Context, fn func() error) error { start := time.Now() // Check circuit breaker first if !re.circuitBreaker.canExecute() { re.metrics.RecordCircuitBreakerOpen() return errors.New(\"circuit breaker is open\") } var lastErr error var totalAttempts int // Execute with retry policy retryErr := re.retryPolicy.Execute(ctx, func() error { totalAttempts++ // Execute within circuit breaker err := re.circuitBreaker.Execute(ctx, fn) lastErr = err return err }) // Record metrics re.metrics.RecordExecution(ExecutionResult{ Success: retryErr == nil, Attempts: totalAttempts, Duration: time.Since(start), ErrorType: classifyError(lastErr), CircuitState: re.circuitBreaker.GetState(), }) return retryErr }\r📊 Error Classification and Handling Comprehensive Error Classification type ErrorClassifier struct { patterns map[ErrorType][]ErrorPattern } type ErrorType int const ( ErrorTypeTransient ErrorType = iota ErrorTypePermanent ErrorTypeRateLimit ErrorTypeTimeout ErrorTypeNetwork ErrorTypeAuthentication ErrorTypeAuthorization ErrorTypeQuota ErrorTypeValidation ) type ErrorPattern struct { Pattern string Regex *regexp.Regexp StatusCodes []int Checker func(error) bool } func NewErrorClassifier() *ErrorClassifier { ec := \u0026ErrorClassifier{ patterns: make(map[ErrorType][]ErrorPattern), } // Define error patterns ec.patterns[ErrorTypeTransient] = []ErrorPattern{ {Pattern: \"connection reset\", Checker: func(err error) bool { return strings.Contains(err.Error(), \"connection reset\") }}, {StatusCodes: []int{500, 502, 503, 504}}, {Pattern: \"temporary failure\", Checker: IsTemporaryError}, } ec.patterns[ErrorTypeRateLimit] = []ErrorPattern{ {StatusCodes: []int{429}}, {Pattern: \"rate limit\", Checker: func(err error) bool { return strings.Contains(strings.ToLower(err.Error()), \"rate limit\") }}, {Pattern: \"quota exceeded\", Checker: func(err error) bool { return strings.Contains(strings.ToLower(err.Error()), \"quota\") }}, } ec.patterns[ErrorTypeTimeout] = []ErrorPattern{ {Checker: func(err error) bool { return errors.Is(err, context.DeadlineExceeded) }}, {Pattern: \"timeout\", Checker: func(err error) bool { return strings.Contains(strings.ToLower(err.Error()), \"timeout\") }}, } ec.patterns[ErrorTypeNetwork] = []ErrorPattern{ {Pattern: \"connection refused\", Checker: func(err error) bool { return strings.Contains(err.Error(), \"connection refused\") }}, {Pattern: \"no such host\", Checker: func(err error) bool { return strings.Contains(err.Error(), \"no such host\") }}, } ec.patterns[ErrorTypeAuthentication] = []ErrorPattern{ {StatusCodes: []int{401}}, {Pattern: \"unauthorized\", Checker: func(err error) bool { return strings.Contains(strings.ToLower(err.Error()), \"unauthorized\") }}, } ec.patterns[ErrorTypeAuthorization] = []ErrorPattern{ {StatusCodes: []int{403}}, {Pattern: \"forbidden\", Checker: func(err error) bool { return strings.Contains(strings.ToLower(err.Error()), \"forbidden\") }}, } ec.patterns[ErrorTypeValidation] = []ErrorPattern{ {StatusCodes: []int{400}}, {Pattern: \"invalid request\", Checker: func(err error) bool { return strings.Contains(strings.ToLower(err.Error()), \"invalid\") }}, } return ec } func (ec *ErrorClassifier) Classify(err error) ErrorType { if err == nil { return ErrorTypePermanent // Shouldn't happen } // Check each error type for errorType, patterns := range ec.patterns { for _, pattern := range patterns { if ec.matchesPattern(err, pattern) { return errorType } } } return ErrorTypePermanent // Default to permanent if unknown } func (ec *ErrorClassifier) matchesPattern(err error, pattern ErrorPattern) bool { // Check custom checker first if pattern.Checker != nil { return pattern.Checker(err) } // Check status codes (if error contains HTTP status) if len(pattern.StatusCodes) \u003e 0 { for _, code := range pattern.StatusCodes { if strings.Contains(err.Error(), fmt.Sprintf(\"%d\", code)) { return true } } } // Check regex pattern if pattern.Regex != nil { return pattern.Regex.MatchString(err.Error()) } // Check simple string pattern if pattern.Pattern != \"\" { return strings.Contains(strings.ToLower(err.Error()), strings.ToLower(pattern.Pattern)) } return false } func (ec *ErrorClassifier) ShouldRetry(err error) bool { errorType := ec.Classify(err) switch errorType { case ErrorTypeTransient, ErrorTypeTimeout, ErrorTypeNetwork, ErrorTypeRateLimit: return true case ErrorTypePermanent, ErrorTypeAuthentication, ErrorTypeAuthorization, ErrorTypeValidation: return false default: return false } }\r📈 Retry Metrics and Monitoring Comprehensive Retry Metrics type RetryMetrics struct { totalExecutions int64 successfulExecutions int64 failedExecutions int64 totalRetries int64 retriesByErrorType map[string]int64 avgRetryCount float64 maxRetryCount int mu sync.RWMutex } func NewRetryMetrics() *RetryMetrics { return \u0026RetryMetrics{ retriesByErrorType: make(map[string]int64), } } func (rm *RetryMetrics) RecordExecution(success bool, attempts int) { rm.mu.Lock() defer rm.mu.Unlock() rm.totalExecutions++ if success { rm.successfulExecutions++ } else { rm.failedExecutions++ } if attempts \u003e 1 { retries := attempts - 1 rm.totalRetries += int64(retries) // Update average rm.avgRetryCount = float64(rm.totalRetries) / float64(rm.totalExecutions) // Update max if retries \u003e rm.maxRetryCount { rm.maxRetryCount = retries } } } func (rm *RetryMetrics) RecordRetryByErrorType(errorType string) { rm.mu.Lock() defer rm.mu.Unlock() rm.retriesByErrorType[errorType]++ } func (rm *RetryMetrics) GetStats() map[string]interface{} { rm.mu.RLock() defer rm.mu.RUnlock() successRate := 0.0 if rm.totalExecutions \u003e 0 { successRate = float64(rm.successfulExecutions) / float64(rm.totalExecutions) } return map[string]interface{}{ \"total_executions\": rm.totalExecutions, \"successful_executions\": rm.successfulExecutions, \"failed_executions\": rm.failedExecutions, \"success_rate\": successRate, \"total_retries\": rm.totalRetries, \"avg_retry_count\": rm.avgRetryCount, \"max_retry_count\": rm.maxRetryCount, \"retries_by_error_type\": rm.retriesByErrorType, } }\r🎯 Best Practices 1. Error-Specific Retry Strategies // Configure different strategies for different error types func ConfigureRetryPolicies() map[string]*RetryPolicy { return map[string]*RetryPolicy{ \"llm_timeout\": { MaxRetries: 2, BaseDelay: 1 * time.Second, BackoffFactor: 1.5, }, \"llm_rate_limit\": { MaxRetries: 5, BaseDelay: 5 * time.Second, BackoffFactor: 2.0, }, \"tool_network\": { MaxRetries: 3, BaseDelay: 500 * time.Millisecond, BackoffFactor: 2.0, }, \"tool_server_error\": { MaxRetries: 4, BaseDelay: 2 * time.Second, BackoffFactor: 1.8, }, } }\r2. Context-Aware Timeouts func (a *RetryableAgent) ExecuteWithContext(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { // Set different timeouts based on operation type var timeout time.Duration switch event.Type { case \"quick_query\": timeout = 5 * time.Second case \"complex_analysis\": timeout = 30 * time.Second case \"document_processing\": timeout = 60 * time.Second default: timeout = 15 * time.Second } // Create context with timeout ctx, cancel := context.WithTimeout(ctx, timeout) defer cancel() return a.Execute(ctx, event, state) }\r3. Graceful Degradation func (a *RetryableAgent) ExecuteWithFallback(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { // Try primary execution with retries result, err := a.Execute(ctx, event, state) if err == nil { return result, nil } // If all retries failed, try fallback strategies if fallbackResult := a.tryCache(event); fallbackResult != nil { return fallbackResult, nil } if fallbackResult := a.trySimplifiedResponse(event); fallbackResult != nil { return fallbackResult, nil } // Return error response as last resort return \u0026core.AgentResult{ Data: map[string]interface{}{ \"error\": \"Service temporarily unavailable\", \"message\": \"Please try again later\", \"code\": \"RETRY_EXHAUSTED\", }, }, nil }\r4. Testing Retry Logic func TestRetryPolicy(t *testing.T) { policy := patterns.NewExponentialRetryPolicy(3, 100*time.Millisecond) // Test successful execution attempts := 0 err := policy.Execute(context.Background(), func() error { attempts++ if attempts \u003c 3 { return errors.New(\"temporary failure\") } return nil }) assert.NoError(t, err) assert.Equal(t, 3, attempts) // Test permanent failure attempts = 0 err = policy.Execute(context.Background(), func() error { attempts++ return errors.New(\"permanent failure\") }) assert.Error(t, err) assert.Equal(t, 1, attempts) // Should not retry permanent errors }\rRetry policies are essential for building resilient agent systems that can handle transient failures gracefully while avoiding unnecessary retries for permanent errors.\n🚀 Next Steps Circuit Breaker Patterns - Combine with circuit breakers for comprehensive fault tolerance Load Balancing - Distribute load to reduce individual service pressure Testing Strategies - Test your retry and error handling logic Production Monitoring - Monitor retry patterns and success rates",
    "description": "Retry Policies and Error Handling Building resilient agent systems with intelligent retry strategies and comprehensive error handling\nRetry policies are crucial for handling transient failures in distributed agent systems. This guide shows you how to implement intelligent retry strategies, handle different types of errors appropriately, and build robust error recovery mechanisms in AgenticGoKit.\n🔄 Understanding Retry Patterns Different types of failures require different retry strategies:\nflowchart TD\rREQUEST[Request] --\u003e EXECUTE[Execute]\rEXECUTE --\u003e SUCCESS{Success?}\rSUCCESS --\u003e|Yes| RETURN[Return Result]\rSUCCESS --\u003e|No| CLASSIFY[Classify Error]\rCLASSIFY --\u003e TRANSIENT{Transient Error?}\rCLASSIFY --\u003e PERMANENT{Permanent Error?}\rCLASSIFY --\u003e RATE_LIMIT{Rate Limited?}\rTRANSIENT --\u003e|Yes| BACKOFF[Apply Backoff]\rPERMANENT --\u003e|Yes| FAIL[Fail Immediately]\rRATE_LIMIT --\u003e|Yes| WAIT[Wait for Reset]\rBACKOFF --\u003e RETRY_CHECK{Retries Left?}\rWAIT --\u003e RETRY_CHECK\rRETRY_CHECK --\u003e|Yes| EXECUTE\rRETRY_CHECK --\u003e|No| FAIL\rclassDef success fill:#d4edda,stroke:#155724\rclassDef error fill:#f8d7da,stroke:#721c24\rclassDef retry fill:#fff3cd,stroke:#856404\rclass RETURN success\rclass FAIL error\rclass BACKOFF,WAIT,RETRY_CHECK retry\r🛠️ Built-in Retry Configuration AgenticGoKit provides built-in retry policies for LLM providers and MCP tools:",
    "tags": [],
    "title": "retry-policies",
    "uri": "/AgenticGoKitDocs/tutorials/advanced/retry-policies/index.html"
  },
  {
    "breadcrumb": "content \u003e guides",
    "content": "Scaffold Memory System Guide This guide explains how to use AgentFlow’s enhanced scaffold system to create projects with intelligent memory configuration. The scaffold now includes automatic embedding model intelligence, configuration validation, and comprehensive troubleshooting support.\nQuick Start Basic Memory-Enabled Project # Create a project with intelligent defaults agentcli create myproject --memory-enabled # The system automatically: # - Selects nomic-embed-text:latest (768 dimensions) # - Configures pgvector as memory provider # - Sets up RAG with optimal parameters # - Generates validation and troubleshooting code\rProduction-Ready Setup # Create a production-ready project agentcli create myapp --memory-enabled \\ --memory-provider pgvector \\ --embedding-provider ollama \\ --embedding-model nomic-embed-text:latest \\ --rag-enabled \\ --hybrid-search \\ --session-memory\rEmbedding Model Intelligence The scaffold system now includes intelligent embedding model configuration:\nAutomatic Dimension Detection # System automatically detects dimensions agentcli create myproject --memory-enabled --embedding-model nomic-embed-text:latest # Output: ✓ Using embedding model: nomic-embed-text:latest (768 dimensions) # Excellent general-purpose embedding model with good performance\rModel Recommendations # Get suggestions for unknown models agentcli create myproject --memory-enabled --embedding-model unknown-model # Output: ⚠️ Unknown embedding model: ollama/unknown-model # 💡 Recommended models for this provider: # • nomic-embed-text:latest (768 dimensions) - Excellent general-purpose... # • mxbai-embed-large (1024 dimensions) - Larger model with better quality...\rCompatibility Validation # System validates model compatibility agentcli create myproject --memory-enabled \\ --memory-provider weaviate \\ --embedding-provider dummy # Output: ⚠️ Compatibility warning: dummy embeddings are not recommended with Weaviate\rEnhanced Project Generation What Gets Generated When you create a memory-enabled project, the scaffold generates:\nIntelligent Configuration (agentflow.toml)\nCorrect [agent_memory] structure (not old [memory]) Auto-calculated dimensions based on embedding model Provider-specific optimizations Validated Main Code (main.go)\nConfiguration validation at startup Helpful error messages with troubleshooting steps Graceful fallback handling Enhanced Agent Code (agent*.go)\nRobust memory integration with error handling RAG context building with fallback behavior Session management with validation Database Setup (for pgvector/weaviate)\nDocker Compose with correct vector dimensions Database initialization scripts Setup scripts for easy deployment Comprehensive Documentation (README.md)\nSetup instructions specific to your configuration Troubleshooting guide for your providers Performance optimization tips Generated Configuration Structure The scaffold generates the modern configuration format:\n# Generated agentflow.toml [agent_memory] provider = \"pgvector\" connection = \"postgres://user:password@localhost:5432/agentflow?sslmode=disable\" dimensions = 768 # Auto-calculated from embedding model auto_embed = true enable_rag = true chunk_size = 1000 chunk_overlap = 100 [agent_memory.embedding] provider = \"ollama\" model = \"nomic-embed-text:latest\" base_url = \"http://localhost:11434\" cache_embeddings = true timeout_seconds = 30 [agent_memory.search] hybrid_search = true keyword_weight = 0.3 semantic_weight = 0.7\rConfiguration Validation Startup Validation Generated projects include comprehensive validation:\n// Generated in main.go func validateMemoryConfig(memoryConfig core.AgentMemoryConfig, expectedModel string) error { // Validate embedding dimensions expectedDimensions := 768 if memoryConfig.Dimensions != expectedDimensions { return fmt.Errorf(\"%s requires %d dimensions, but %d configured\\n💡 Solution: Update [agent_memory] dimensions = %d\", expectedModel, expectedDimensions, memoryConfig.Dimensions, expectedDimensions) } // Validate provider compatibility // ... comprehensive validation logic }\rRuntime Error Handling // Generated error handling with specific troubleshooting if err := memory.Store(ctx, testContent, \"system-init\"); err != nil { switch memoryConfig.Provider { case \"pgvector\": fmt.Printf(\"💡 PostgreSQL/PgVector Troubleshooting:\\n\") fmt.Printf(\" 1. Start database: docker compose up -d\\n\") fmt.Printf(\" 2. Run setup script: ./setup.sh\\n\") // ... more specific guidance } }\rMemory Debug Integration Built-in Debug Support Every generated project works with the memory debug command:\n# Navigate to your generated project cd myproject # Debug your memory system agentcli memory --validate agentcli memory --stats agentcli memory --search \"test query\"\rProject-Specific Troubleshooting The generated README includes troubleshooting specific to your configuration:\n## Troubleshooting ### Memory System Issues **Ollama Connection Failed:** 1. Start Ollama: `ollama serve` 2. Pull model: `ollama pull nomic-embed-text:latest` 3. Test connection: `curl http://localhost:11434/api/tags` **Database Connection Failed:** 1. Start database: `docker compose up -d` 2. Run setup: `./setup.sh` 3. Check connection: `psql -h localhost -U user -d agentflow`\rAdvanced Configuration Options Interactive Setup # Use interactive mode for guided setup agentcli create --interactive # The system will guide you through: # 1. Project name and basic settings # 2. Memory provider selection with explanations # 3. Embedding model selection with recommendations # 4. RAG configuration with optimal defaults # 5. Advanced features (hybrid search, sessions, etc.)\rCommand Line Options # Full configuration example agentcli create myproject \\ --memory-enabled \\ --memory-provider pgvector \\ --embedding-provider ollama \\ --embedding-model nomic-embed-text:latest \\ --rag-enabled \\ --rag-chunk-size 1000 \\ --rag-overlap 100 \\ --rag-top-k 5 \\ --rag-score-threshold 0.7 \\ --hybrid-search \\ --session-memory\rProvider-Specific Optimizations PostgreSQL/PgVector agentcli create myproject --memory-enabled --memory-provider pgvector # Generates: # - Docker Compose with pgvector extension # - Optimized connection pooling settings # - Vector index creation scripts # - Performance monitoring queries\rWeaviate agentcli create myproject --memory-enabled --memory-provider weaviate # Generates: # - Weaviate Docker configuration # - Schema definitions for your embedding dimensions # - Batch import optimization settings # - Backup and restore scripts\rIn-Memory (Development) agentcli create myproject --memory-enabled --memory-provider memory # Generates: # - Fast startup configuration # - Development-optimized settings # - Migration path to persistent storage\rMigration from Old Projects Automatic Detection The scaffold can detect and help migrate old projects:\n# If you have an old project with [memory] configuration agentcli create myproject-new --memory-enabled # Copy your agent logic from the old project # The new project will have the correct configuration structure\rConfiguration Migration Old format:\n[memory] # ❌ Old format provider = \"pgvector\" dimensions = 1536\rNew format:\n[agent_memory] # ✅ New format provider = \"pgvector\" dimensions = 768 # Auto-calculated connection = \"postgres://...\" [agent_memory.embedding] provider = \"ollama\" model = \"nomic-embed-text:latest\"\rBest Practices Development Workflow Start Local:\nagentcli create myproject --memory-enabled # Uses Ollama + nomic-embed-text by default\rTest Configuration:\ncd myproject agentcli memory --validate\rDevelop and Test:\ngo mod tidy go run . -m \"test message\"\rDebug Issues:\nagentcli memory --stats agentcli memory --search \"your query\"\rProduction Deployment Create Production Project:\nagentcli create myapp-prod --memory-enabled \\ --memory-provider pgvector \\ --embedding-provider openai \\ --embedding-model text-embedding-3-small\rConfigure Environment:\nexport OPENAI_API_KEY=\"your-key\" # Update connection strings in agentflow.toml\rDeploy Database:\ndocker compose up -d ./setup.sh\rValidate Production Setup:\nagentcli memory --validate agentcli memory --stats\rPerformance Optimization Choose Appropriate Dimensions:\n384 dims: Fastest (all-minilm) 768 dims: Balanced (nomic-embed-text) 1536 dims: Standard (OpenAI small) 3072 dims: Highest quality (OpenAI large) Optimize RAG Settings:\n# For faster responses --rag-chunk-size 500 --rag-top-k 3 # For better quality --rag-chunk-size 1500 --rag-top-k 7\rEnable Caching:\n[agent_memory.embedding] cache_embeddings = true max_batch_size = 100\rTroubleshooting Common Issues Dimension Mismatch ❌ Configuration Error: nomic-embed-text requires 768 dimensions, but 1536 configured\rSolution: The scaffold prevents this, but if you encounter it, regenerate the project or update dimensions manually.\nWrong Configuration Format ❌ Memory system not configured in agentflow.toml\rSolution: Ensure you’re using [agent_memory] not [memory]. Regenerate with the new scaffold.\nProvider Not Available ❌ Cannot connect to Ollama service\rSolution: Follow the generated troubleshooting guide in your project’s README.\nGetting Help Use the debug command:\nagentcli memory --validate\rCheck the generated README for project-specific guidance\nRefer to the troubleshooting guide: docs/guides/MemoryTroubleshooting.md\nCreate an issue with your configuration and error messages\nExamples Complete Examples Local Development Project agentcli create dev-project --memory-enabled \\ --memory-provider pgvector \\ --embedding-provider ollama \\ --embedding-model nomic-embed-text:latest \\ --rag-enabled cd dev-project docker compose up -d ./setup.sh go mod tidy go run . -m \"Hello, world!\"\rProduction RAG System agentcli create rag-system --memory-enabled \\ --memory-provider pgvector \\ --embedding-provider openai \\ --embedding-model text-embedding-3-small \\ --rag-enabled \\ --hybrid-search \\ --session-memory \\ --rag-chunk-size 1000 \\ --rag-top-k 5 cd rag-system export OPENAI_API_KEY=\"your-key\" # Update connection string in agentflow.toml for production database docker compose up -d ./setup.sh agentcli memory --validate go run . -m \"Analyze this document...\"\rHigh-Performance Setup agentcli create high-perf --memory-enabled \\ --memory-provider weaviate \\ --embedding-provider openai \\ --embedding-model text-embedding-3-large \\ --rag-enabled \\ --hybrid-search cd high-perf export OPENAI_API_KEY=\"your-key\" docker compose up -d agentcli memory --validate agentcli memory --stats\rThe enhanced scaffold system makes it easy to create production-ready AgentFlow projects with intelligent memory configuration, comprehensive validation, and built-in troubleshooting support.",
    "description": "Scaffold Memory System Guide This guide explains how to use AgentFlow’s enhanced scaffold system to create projects with intelligent memory configuration. The scaffold now includes automatic embedding model intelligence, configuration validation, and comprehensive troubleshooting support.\nQuick Start Basic Memory-Enabled Project # Create a project with intelligent defaults agentcli create myproject --memory-enabled # The system automatically: # - Selects nomic-embed-text:latest (768 dimensions) # - Configures pgvector as memory provider # - Sets up RAG with optimal parameters # - Generates validation and troubleshooting code\rProduction-Ready Setup # Create a production-ready project agentcli create myapp --memory-enabled \\ --memory-provider pgvector \\ --embedding-provider ollama \\ --embedding-model nomic-embed-text:latest \\ --rag-enabled \\ --hybrid-search \\ --session-memory\rEmbedding Model Intelligence The scaffold system now includes intelligent embedding model configuration:",
    "tags": [],
    "title": "ScaffoldMemoryGuide",
    "uri": "/AgenticGoKitDocs/guides/scaffoldmemoryguide/index.html"
  },
  {
    "breadcrumb": "content \u003e guides",
    "content": "Setup \u0026 Configuration Guides Guides for setting up and configuring AgenticGoKit components.\nAvailable Guides LLM Providers Configure different Large Language Model providers including OpenAI, Anthropic, and local models.\nWhen to use: Setting up your first agent or switching LLM providers.\nVector Databases Set up vector storage systems for Retrieval-Augmented Generation (RAG) including pgvector and Weaviate.\nWhen to use: Building agents that need to search through documents or knowledge bases.\nMCP Tools Integrate Model Context Protocol tools to extend agent capabilities with external services.\nWhen to use: Adding specific tools like web search, file operations, or API integrations.\nCommon Setup Patterns Most AgenticGoKit applications follow this setup pattern:\nChoose your LLM provider - Start with LLM Providers Configure memory (optional) - Add Vector Databases if needed Add tools (optional) - Integrate MCP Tools for extended capabilities Test your configuration - Use Testing Agents Configuration Files AgenticGoKit uses TOML configuration files (agentflow.toml) for most settings. Each guide shows you how to configure the relevant sections.\nNext Steps After setup, explore:\nDevelopment Guides for building and testing Deployment Guides for production deployment Getting Started Tutorials for hands-on learning",
    "description": "Setup \u0026 Configuration Guides Guides for setting up and configuring AgenticGoKit components.\nAvailable Guides LLM Providers Configure different Large Language Model providers including OpenAI, Anthropic, and local models.\nWhen to use: Setting up your first agent or switching LLM providers.\nVector Databases Set up vector storage systems for Retrieval-Augmented Generation (RAG) including pgvector and Weaviate.\nWhen to use: Building agents that need to search through documents or knowledge bases.\nMCP Tools Integrate Model Context Protocol tools to extend agent capabilities with external services.",
    "tags": [],
    "title": "setup",
    "uri": "/AgenticGoKitDocs/guides/setup/index.html"
  },
  {
    "breadcrumb": "content \u003e reference",
    "content": "State \u0026 Event API Reference Complete reference for data flow and communication between agents\nThis document provides comprehensive API reference for AgenticGoKit’s state management and event system, which enables data flow and communication between agents in multi-agent systems.\n🏗️ Core Interfaces Event Interface The Event interface represents data that flows between agents in the system.\ntype Event interface { GetID() string GetData() EventData GetMetadata() map[string]string GetMetadataValue(key string) (string, bool) GetSourceAgentID() string GetTargetAgentID() string }\rMethods GetID GetID() string\rReturns the unique identifier for this event.\nGetData GetData() EventData\rReturns the event payload as key-value pairs.\nExample:\ndata := event.GetData() message, ok := data[\\\"message\\\"] if ok { fmt.Printf(\\\"User message: %s\\\\n\\\", message) }\rGetMetadata GetMetadata() map[string]string\rReturns all metadata associated with the event.\nGetMetadataValue GetMetadataValue(key string) (string, bool)\rReturns a specific metadata value by key.\nExample:\nsessionID, exists := event.GetMetadataValue(\\\"session_id\\\") if exists { fmt.Printf(\\\"Session: %s\\\\n\\\", sessionID) }\rGetSourceAgentID / GetTargetAgentID GetSourceAgentID() string GetTargetAgentID() string\rReturns the source and target agent identifiers for event routing.\nState Interface Thread-safe state management interface for maintaining data across agent interactions.\ntype State interface { Get(key string) (any, bool) Set(key string, value any) Keys() []string GetMeta(key string) (string, bool) SetMeta(key, value string) MetaKeys() []string Clone() State }\rData Methods Get/Set Set(key string, value any) Get(key string) (any, bool)\rStore and retrieve typed values in the state.\nExample:\n// Store different types of data state.Set(\\\"user_preferences\\\", []string{\\\"technical\\\", \\\"detailed\\\"}) state.Set(\\\"conversation_turn\\\", 3) state.Set(\\\"user_context\\\", map[string]interface{}{ \\\"name\\\": \\\"John\\\", \\\"role\\\": \\\"developer\\\", }) // Retrieve data with type assertion prefs, exists := state.Get(\\\"user_preferences\\\") if exists { preferences := prefs.([]string) fmt.Printf(\\\"User preferences: %v\\\\n\\\", preferences) } turn, exists := state.Get(\\\"conversation_turn\\\") if exists { turnNumber := turn.(int) fmt.Printf(\\\"Turn: %d\\\\n\\\", turnNumber) }\rKeys Keys() []string\rReturns all data keys currently stored in the state.\nExample:\nkeys := state.Keys() fmt.Printf(\\\"State contains: %v\\\\n\\\", keys)\rMetadata Methods GetMeta/SetMeta SetMeta(key, value string) GetMeta(key string) (string, bool)\rStore and retrieve string metadata about the state.\nExample:\n// Store metadata state.SetMeta(\\\"processed_by\\\", \\\"agent1\\\") state.SetMeta(\\\"timestamp\\\", time.Now().Format(time.RFC3339)) state.SetMeta(\\\"session_id\\\", \\\"abc123\\\") // Retrieve metadata agent, exists := state.GetMeta(\\\"processed_by\\\") if exists { fmt.Printf(\\\"Processed by: %s\\\\n\\\", agent) }\rMetaKeys MetaKeys() []string\rReturns all metadata keys currently stored in the state.\nUtility Methods Clone Clone() State\rCreates a deep copy of the state for safe concurrent access.\nExample:\n// Create a copy for modification stateCopy := state.Clone() stateCopy.Set(\\\"modified\\\", true) // Original state is unchanged original, _ := state.Get(\\\"modified\\\") fmt.Printf(\\\"Original modified: %v\\\\n\\\", original) // nil\r📊 Type Definitions EventData Type alias for event payload data.\ntype EventData map[string]interface{}\rUsage:\neventData := core.EventData{ \\\"message\\\": \\\"Hello, world!\\\", \\\"user_id\\\": \\\"12345\\\", \\\"timestamp\\\": time.Now(), \\\"context\\\": map[string]string{\\\"session\\\": \\\"abc123\\\"}, } event := core.NewEvent(\\\"user-message\\\", eventData, nil)\rAgentResult Result returned by agent execution, containing response data and updated state.\ntype AgentResult struct { Data map[string]interface{} `json:\\\"data\\\"` State State `json:\\\"state,omitempty\\\"` Metadata map[string]interface{} `json:\\\"metadata,omitempty\\\"` Errors []error `json:\\\"errors,omitempty\\\"` Success bool `json:\\\"success\\\"` }\rFields Data The primary response data that will be returned to the caller:\nresult := core.AgentResult{ Data: map[string]interface{}{ \\\"answer\\\": \\\"Paris is the capital of France\\\", \\\"confidence\\\": 0.95, \\\"sources\\\": []string{\\\"wikipedia\\\", \\\"britannica\\\"}, }, }\rState Updated state to persist for the session:\n// Update conversation state state.Set(\\\"last_query\\\", query) state.Set(\\\"query_count\\\", state.GetInt(\\\"query_count\\\")+1) result := core.AgentResult{ Data: responseData, State: state, }\rMetadata Additional execution information:\nresult := core.AgentResult{ Data: responseData, Metadata: map[string]interface{}{ \\\"execution_time\\\": time.Since(start), \\\"tokens_used\\\": tokenCount, \\\"model\\\": \\\"gpt-4o\\\", \\\"tools_called\\\": []string{\\\"search\\\", \\\"calculator\\\"}, }, }\rErrors Non-fatal errors that occurred during processing:\nresult := core.AgentResult{ Data: partialData, Errors: []error{ fmt.Errorf(\\\"tool 'advanced_search' failed: %w\\\", searchErr), fmt.Errorf(\\\"cache miss for query: %s\\\", query), }, Success: true, // Still successful despite errors }\r🏭 Factory Functions Event Creation NewEvent func NewEvent(eventType string, data EventData, metadata map[string]string) Event\rCreates a new event with the specified data and metadata.\nParameters:\neventType - Type identifier for the event data - Event payload data metadata - Optional metadata map Example:\neventData := core.EventData{\\\"message\\\": \\\"Hello, world!\\\"} metadata := map[string]string{\\\"session_id\\\": \\\"123\\\", \\\"user_id\\\": \\\"456\\\"} event := core.NewEvent(\\\"user-message\\\", eventData, metadata)\rState Creation NewState func NewState() State\rCreates a new empty state instance.\nExample:\nstate := core.NewState() state.Set(\\\"conversation_history\\\", []string{}) state.SetMeta(\\\"session_id\\\", \\\"abc123\\\")\r🔄 State Management Patterns Conversation State Pattern Managing conversation history and context:\ntype ConversationAgent struct { llm core.ModelProvider } func (a *ConversationAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Get current conversation history history, exists := state.Get(\\\"conversation_history\\\") var messages []core.Message if exists { messages = history.([]core.Message) } // Extract user message from event data := event.GetData() userMessage := data[\\\"message\\\"].(string) // Add user message to history messages = append(messages, core.Message{ Role: \\\"user\\\", Content: userMessage, }) // Generate response response, err := a.llm.GenerateWithHistory(ctx, messages) if err != nil { return core.AgentResult{}, err } // Add assistant response to history messages = append(messages, core.Message{ Role: \\\"assistant\\\", Content: response, }) // Update state with new history state.Set(\\\"conversation_history\\\", messages) state.Set(\\\"last_response\\\", response) state.SetMeta(\\\"last_updated\\\", time.Now().Format(time.RFC3339)) return core.AgentResult{ Data: map[string]interface{}{ \\\"response\\\": response, }, State: state, Success: true, }, nil }\rSession State Pattern Managing user session data across multiple interactions:\ntype SessionAgent struct { sessionStore SessionStore } func (a *SessionAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Get session ID from event metadata sessionID, exists := event.GetMetadataValue(\\\"session_id\\\") if !exists { return core.AgentResult{}, fmt.Errorf(\\\"missing session_id in event metadata\\\") } // Load session data if not in state if _, exists := state.Get(\\\"session_data\\\"); !exists { sessionData, err := a.sessionStore.Load(sessionID) if err != nil { // Create new session sessionData = map[string]interface{}{ \\\"created_at\\\": time.Now(), \\\"user_id\\\": event.GetMetadataValue(\\\"user_id\\\"), } } state.Set(\\\"session_data\\\", sessionData) } // Process the event data := event.GetData() query := data[\\\"query\\\"].(string) // Update session with query count sessionData := state.Get(\\\"session_data\\\").(map[string]interface{}) queryCount, _ := sessionData[\\\"query_count\\\"].(int) sessionData[\\\"query_count\\\"] = queryCount + 1 sessionData[\\\"last_query\\\"] = query sessionData[\\\"last_activity\\\"] = time.Now() state.Set(\\\"session_data\\\", sessionData) // Save session data if err := a.sessionStore.Save(sessionID, sessionData); err != nil { // Log error but don't fail the request log.Printf(\\\"Failed to save session data: %v\\\", err) } return core.AgentResult{ Data: map[string]interface{}{ \\\"processed\\\": true, \\\"query_count\\\": queryCount + 1, }, State: state, Success: true, }, nil }\rMulti-Agent State Sharing Pattern Sharing state between multiple agents in a workflow:\ntype AnalysisAgent struct { name string } func (a *AnalysisAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Get shared analysis context analysisContext, exists := state.Get(\\\"analysis_context\\\") if !exists { analysisContext = map[string]interface{}{ \\\"started_at\\\": time.Now(), \\\"agents_completed\\\": []string{}, \\\"findings\\\": map[string]interface{}{}, } } context := analysisContext.(map[string]interface{}) // Perform analysis data := event.GetData() text := data[\\\"text\\\"].(string) analysis := a.performAnalysis(text) // Add findings to shared context findings := context[\\\"findings\\\"].(map[string]interface{}) findings[a.name] = analysis // Mark this agent as completed completed := context[\\\"agents_completed\\\"].([]string) completed = append(completed, a.name) context[\\\"agents_completed\\\"] = completed // Update shared state state.Set(\\\"analysis_context\\\", context) return core.AgentResult{ Data: map[string]interface{}{ \\\"analysis\\\": analysis, \\\"agent\\\": a.name, }, State: state, Success: true, }, nil } func (a *AnalysisAgent) performAnalysis(text string) map[string]interface{} { // Implement specific analysis logic return map[string]interface{}{ \\\"word_count\\\": len(strings.Fields(text)), \\\"sentiment\\\": \\\"positive\\\", \\\"topics\\\": []string{\\\"technology\\\", \\\"programming\\\"}, } }\r🔍 Event Routing Patterns Route-Based Event Handling Using event metadata for routing decisions:\ntype RouterAgent struct { handlers map[string]core.AgentHandler } func (r *RouterAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Get route from event metadata route, exists := event.GetMetadataValue(\\\"route\\\") if !exists { // Default routing based on event data data := event.GetData() if _, hasQuery := data[\\\"query\\\"]; hasQuery { route = \\\"query_handler\\\" } else if _, hasCommand := data[\\\"command\\\"]; hasCommand { route = \\\"command_handler\\\" } else { route = \\\"default_handler\\\" } } // Find appropriate handler handler, exists := r.handlers[route] if !exists { return core.AgentResult{}, fmt.Errorf(\\\"no handler found for route: %s\\\", route) } // Delegate to specific handler return handler.Run(ctx, event, state) }\rEvent Transformation Pattern Transforming events between different formats:\ntype TransformAgent struct { transformer EventTransformer } func (t *TransformAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Transform incoming event transformedData, err := t.transformer.Transform(event.GetData()) if err != nil { return core.AgentResult{}, fmt.Errorf(\\\"event transformation failed: %w\\\", err) } // Create new event with transformed data newEvent := core.NewEvent(\\\"transformed\\\", transformedData, event.GetMetadata()) // Store transformation history in state history, exists := state.Get(\\\"transformation_history\\\") if !exists { history = []string{} } historyList := history.([]string) historyList = append(historyList, fmt.Sprintf(\\\"Transformed at %s\\\", time.Now().Format(time.RFC3339))) state.Set(\\\"transformation_history\\\", historyList) return core.AgentResult{ Data: map[string]interface{}{ \\\"transformed_event\\\": newEvent, \\\"original_event\\\": event, }, State: state, Success: true, }, nil }\r🧪 Testing State and Events State Testing Utilities func TestStateOperations(t *testing.T) { state := core.NewState() // Test basic operations state.Set(\\\"key1\\\", \\\"value1\\\") state.Set(\\\"key2\\\", 42) value1, exists := state.Get(\\\"key1\\\") assert.True(t, exists) assert.Equal(t, \\\"value1\\\", value1.(string)) value2, exists := state.Get(\\\"key2\\\") assert.True(t, exists) assert.Equal(t, 42, value2.(int)) // Test metadata state.SetMeta(\\\"meta1\\\", \\\"metavalue1\\\") metaValue, exists := state.GetMeta(\\\"meta1\\\") assert.True(t, exists) assert.Equal(t, \\\"metavalue1\\\", metaValue) // Test cloning clonedState := state.Clone() clonedState.Set(\\\"key3\\\", \\\"value3\\\") _, exists = state.Get(\\\"key3\\\") assert.False(t, exists) // Original state unchanged _, exists = clonedState.Get(\\\"key3\\\") assert.True(t, exists) // Cloned state has new value }\rEvent Testing Utilities func TestEventCreation(t *testing.T) { eventData := core.EventData{ \\\"message\\\": \\\"test message\\\", \\\"user_id\\\": \\\"123\\\", } metadata := map[string]string{ \\\"session_id\\\": \\\"session123\\\", \\\"route\\\": \\\"test_route\\\", } event := core.NewEvent(\\\"test_event\\\", eventData, metadata) // Test event data data := event.GetData() assert.Equal(t, \\\"test message\\\", data[\\\"message\\\"]) assert.Equal(t, \\\"123\\\", data[\\\"user_id\\\"]) // Test metadata sessionID, exists := event.GetMetadataValue(\\\"session_id\\\") assert.True(t, exists) assert.Equal(t, \\\"session123\\\", sessionID) route, exists := event.GetMetadataValue(\\\"route\\\") assert.True(t, exists) assert.Equal(t, \\\"test_route\\\", route) }\rAgent Result Testing func TestAgentResult(t *testing.T) { state := core.NewState() state.Set(\\\"processed\\\", true) result := core.AgentResult{ Data: map[string]interface{}{ \\\"response\\\": \\\"test response\\\", \\\"confidence\\\": 0.95, }, State: state, Metadata: map[string]interface{}{ \\\"execution_time\\\": \\\"100ms\\\", }, Success: true, } // Test result data assert.Equal(t, \\\"test response\\\", result.Data[\\\"response\\\"]) assert.Equal(t, 0.95, result.Data[\\\"confidence\\\"]) // Test state processed, exists := result.State.Get(\\\"processed\\\") assert.True(t, exists) assert.True(t, processed.(bool)) // Test metadata assert.Equal(t, \\\"100ms\\\", result.Metadata[\\\"execution_time\\\"]) // Test success assert.True(t, result.Success) }\r🔧 Best Practices State Management Use appropriate data types: Store data in the most appropriate Go type Namespace keys: Use prefixes to avoid key collisions (e.g., \"user.preferences\", \"session.data\") Clean up state: Remove unnecessary data to prevent memory leaks Use metadata for system info: Store system-level information in metadata Event Design Consistent event types: Use consistent naming for event types Include context: Always include necessary context in event data Use metadata for routing: Use metadata for system-level routing information Validate event data: Always validate event data before processing Error Handling Graceful degradation: Handle missing state gracefully Preserve partial results: Return partial results when possible Log state changes: Log important state changes for debugging Use non-fatal errors: Use the Errors field for non-fatal issues This comprehensive reference covers all aspects of state and event management in AgenticGoKit, providing the foundation for building robust multi-agent systems.",
    "description": "State \u0026 Event API Reference Complete reference for data flow and communication between agents\nThis document provides comprehensive API reference for AgenticGoKit’s state management and event system, which enables data flow and communication between agents in multi-agent systems.\n🏗️ Core Interfaces Event Interface The Event interface represents data that flows between agents in the system.\ntype Event interface { GetID() string GetData() EventData GetMetadata() map[string]string GetMetadataValue(key string) (string, bool) GetSourceAgentID() string GetTargetAgentID() string }\rMethods GetID GetID() string\rReturns the unique identifier for this event.",
    "tags": [],
    "title": "state-event",
    "uri": "/AgenticGoKitDocs/reference/api/state-event/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials \u003e core-concepts",
    "content": "State Management and Data Flow in AgenticGoKit Overview State management is the backbone of data flow in AgenticGoKit. It determines how information is stored, passed between agents, and persisted across interactions. This tutorial explores how State objects work, how data flows through agent systems, and best practices for managing complex data transformations.\nUnderstanding state management is crucial because it’s how agents share information, maintain context, and build upon each other’s work.\nPrerequisites Understanding of Message Passing and Event Flow Basic knowledge of Go interfaces and concurrency Familiarity with AgenticGoKit’s core concepts Core Concepts State: The Agent’s Working Memory State represents the current context and data an agent is working with. It’s a thread-safe container that holds both data and metadata:\ntype State interface { // Data operations Get(key string) (any, bool) // Retrieve a value by key Set(key string, value any) // Store a value by key Keys() []string // Get all data keys // Metadata operations GetMeta(key string) (string, bool) // Retrieve metadata by key SetMeta(key string, value string) // Store metadata by key MetaKeys() []string // Get all metadata keys // State operations Clone() State // Create a deep copy Merge(source State) // Merge another state into this one }\rData vs Metadata Data contains the actual information agents work with:\nUser messages Processing results Intermediate calculations Business logic data Metadata contains information about the data:\nProcessing instructions Routing information Quality scores Timestamps and tracking info // Example state with data and metadata state := core.NewState() // Data - what the agent works with state.Set(\"user_message\", \"What's the weather in Paris?\") state.Set(\"location\", \"Paris\") state.Set(\"temperature\", 22.5) // Metadata - information about the data state.SetMeta(\"confidence\", \"0.95\") state.SetMeta(\"source\", \"weather-api\") state.SetMeta(\"timestamp\", time.Now().Format(time.RFC3339))\rState Lifecycle 1. State Creation States are typically created at the beginning of agent processing:\n// Create empty state state := core.NewState() // Create state with initial data initialData := map[string]any{ \"user_id\": \"user-123\", \"session_id\": \"session-456\", \"query\": \"Tell me about AI\", } state := core.NewStateWithData(initialData) // Alternative creation method state := core.NewSimpleState(initialData)\r2. State Transformation Agents receive state as input and produce modified state as output:\nfunc (a *MyAgent) Run(ctx context.Context, event Event, inputState State) (AgentResult, error) { // Read from input state query, ok := inputState.Get(\"query\") if !ok { return AgentResult{}, errors.New(\"no query in state\") } // Process the query response := a.processQuery(query.(string)) // Create output state with new data outputState := inputState.Clone() // Start with input state outputState.Set(\"response\", response) outputState.Set(\"processed_at\", time.Now()) outputState.SetMeta(\"agent\", a.name) return AgentResult{ OutputState: outputState, }, nil }\r3. State Propagation State flows between agents through the orchestration system:\n┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐\r│ Initial │───▶│ Agent A │───▶│ Agent B │───▶│ Final │\r│ State │ │ State │ │ State │ │ State │\r└─────────┘ └─────────┘ └─────────┘ └─────────┘\rData Flow Patterns 1. Linear Data Flow Data flows sequentially through agents, with each agent adding or modifying information:\n// Agent 1: Data Collection func (a *CollectorAgent) Run(ctx context.Context, event Event, state State) (AgentResult, error) { // Collect raw data rawData := a.collectData() outputState := state.Clone() outputState.Set(\"raw_data\", rawData) outputState.SetMeta(\"stage\", \"collection\") return AgentResult{OutputState: outputState}, nil } // Agent 2: Data Processing func (a *ProcessorAgent) Run(ctx context.Context, event Event, state State) (AgentResult, error) { // Get raw data from previous agent rawData, ok := state.Get(\"raw_data\") if !ok { return AgentResult{}, errors.New(\"no raw data to process\") } // Process the data processedData := a.processData(rawData) outputState := state.Clone() outputState.Set(\"processed_data\", processedData) outputState.SetMeta(\"stage\", \"processing\") return AgentResult{OutputState: outputState}, nil } // Agent 3: Data Formatting func (a *FormatterAgent) Run(ctx context.Context, event Event, state State) (AgentResult, error) { // Get processed data processedData, ok := state.Get(\"processed_data\") if !ok { return AgentResult{}, errors.New(\"no processed data to format\") } // Format the data formattedData := a.formatData(processedData) outputState := state.Clone() outputState.Set(\"final_result\", formattedData) outputState.SetMeta(\"stage\", \"formatting\") return AgentResult{OutputState: outputState}, nil }\r2. Branching Data Flow Data flows to multiple agents, each processing different aspects:\n// Main agent creates branches func (a *MainAgent) Run(ctx context.Context, event Event, state State) (AgentResult, error) { query, _ := state.Get(\"query\") outputState := state.Clone() // Create different processing branches outputState.Set(\"text_analysis_query\", query) outputState.Set(\"sentiment_analysis_query\", query) outputState.Set(\"entity_extraction_query\", query) // Set routing for different agents outputState.SetMeta(\"next_agents\", \"text_analyzer,sentiment_analyzer,entity_extractor\") return AgentResult{OutputState: outputState}, nil }\r3. Merging Data Flow Multiple agents contribute to a shared state that gets combined:\n// Collaborative agents contribute to shared state func (a *ResearchAgent) Run(ctx context.Context, event Event, state State) (AgentResult, error) { query, _ := state.Get(\"research_query\") // Perform research findings := a.research(query.(string)) outputState := state.Clone() // Add findings to shared research data existingFindings, _ := outputState.Get(\"research_findings\") if existingFindings == nil { existingFindings = make([]ResearchFinding, 0) } allFindings := append(existingFindings.([]ResearchFinding), findings...) outputState.Set(\"research_findings\", allFindings) outputState.SetMeta(\"contributor\", a.name) return AgentResult{OutputState: outputState}, nil }\rAdvanced State Management Patterns 1. Namespaced State Keys Use namespaces to organize state data and avoid conflicts:\n// Namespaced keys prevent conflicts state.Set(\"user.profile.name\", \"Alice\") state.Set(\"user.profile.preferences\", []string{\"history\", \"science\"}) state.Set(\"user.session.start_time\", time.Now()) state.Set(\"system.version\", \"1.0.0\") state.Set(\"system.environment\", \"production\") state.Set(\"processing.stage\", \"analysis\") state.Set(\"processing.confidence\", 0.95)\r2. Structured State Data Store complex objects as structured data:\n// Define structured data types type UserProfile struct { Name string `json:\"name\"` Email string `json:\"email\"` Preferences []string `json:\"preferences\"` LastSeen time.Time `json:\"last_seen\"` } type ProcessingContext struct { Stage string `json:\"stage\"` Confidence float64 `json:\"confidence\"` Metadata map[string]string `json:\"metadata\"` } // Store structured data in state profile := UserProfile{ Name: \"Alice\", Email: \"alice@example.com\", Preferences: []string{\"AI\", \"Technology\"}, LastSeen: time.Now(), } context := ProcessingContext{ Stage: \"analysis\", Confidence: 0.95, Metadata: map[string]string{\"source\": \"llm\"}, } state.Set(\"user_profile\", profile) state.Set(\"processing_context\", context)\r3. State Validation Implement validation to ensure state integrity:\n// State validator interface type StateValidator interface { Validate(state State) error } // Example validator type UserQueryValidator struct{} func (v *UserQueryValidator) Validate(state State) error { // Check required fields if _, ok := state.Get(\"user_query\"); !ok { return errors.New(\"user_query is required\") } if _, ok := state.Get(\"user_id\"); !ok { return errors.New(\"user_id is required\") } // Validate data types if query, ok := state.Get(\"user_query\"); ok { if _, isString := query.(string); !isString { return errors.New(\"user_query must be a string\") } } return nil } // Use validator in agent func (a *MyAgent) Run(ctx context.Context, event Event, state State) (AgentResult, error) { validator := \u0026UserQueryValidator{} if err := validator.Validate(state); err != nil { return AgentResult{}, fmt.Errorf(\"state validation failed: %w\", err) } // Continue with processing... }\r4. State Transformation Helpers Create helper functions for common state transformations:\n// State transformation helpers func AddUserContext(state State, userID, sessionID string) State { newState := state.Clone() newState.Set(\"user_id\", userID) newState.Set(\"session_id\", sessionID) newState.SetMeta(\"context_added\", time.Now().Format(time.RFC3339)) return newState } func AddProcessingMetadata(state State, agentName string, confidence float64) State { newState := state.Clone() newState.SetMeta(\"processed_by\", agentName) newState.SetMeta(\"confidence\", fmt.Sprintf(\"%.2f\", confidence)) newState.SetMeta(\"processed_at\", time.Now().Format(time.RFC3339)) return newState } func ExtractUserQuery(state State) (string, error) { query, ok := state.Get(\"user_query\") if !ok { return \"\", errors.New(\"no user query in state\") } queryStr, ok := query.(string) if !ok { return \"\", errors.New(\"user query is not a string\") } return queryStr, nil } // Usage in agents func (a *MyAgent) Run(ctx context.Context, event Event, state State) (AgentResult, error) { // Add context state = AddUserContext(state, \"user-123\", \"session-456\") // Extract query query, err := ExtractUserQuery(state) if err != nil { return AgentResult{}, err } // Process query response := a.processQuery(query) // Add processing metadata outputState := AddProcessingMetadata(state, a.name, 0.95) outputState.Set(\"response\", response) return AgentResult{OutputState: outputState}, nil }\rState Persistence and Serialization 1. JSON Serialization State objects can be serialized to JSON for storage or transmission:\n// Serialize state to JSON func SerializeState(state State) ([]byte, error) { return json.Marshal(state) } // Deserialize state from JSON func DeserializeState(data []byte) (State, error) { var state core.SimpleState err := json.Unmarshal(data, \u0026state) if err != nil { return nil, err } return \u0026state, nil } // Example usage func saveStateToFile(state State, filename string) error { data, err := SerializeState(state) if err != nil { return err } return os.WriteFile(filename, data, 0644) } func loadStateFromFile(filename string) (State, error) { data, err := os.ReadFile(filename) if err != nil { return nil, err } return DeserializeState(data) }\r2. State Snapshots Capture state at specific points for debugging or rollback:\n// State snapshot manager type StateSnapshot struct { Timestamp time.Time `json:\"timestamp\"` AgentID string `json:\"agent_id\"` State State `json:\"state\"` } type StateSnapshotManager struct { snapshots []StateSnapshot mu sync.RWMutex } func (sm *StateSnapshotManager) TakeSnapshot(agentID string, state State) { sm.mu.Lock() defer sm.mu.Unlock() snapshot := StateSnapshot{ Timestamp: time.Now(), AgentID: agentID, State: state.Clone(), } sm.snapshots = append(sm.snapshots, snapshot) } func (sm *StateSnapshotManager) GetSnapshots() []StateSnapshot { sm.mu.RLock() defer sm.mu.RUnlock() // Return a copy to avoid race conditions snapshots := make([]StateSnapshot, len(sm.snapshots)) copy(snapshots, sm.snapshots) return snapshots } // Usage in agent processing var snapshotManager = \u0026StateSnapshotManager{} func (a *MyAgent) Run(ctx context.Context, event Event, state State) (AgentResult, error) { // Take snapshot before processing snapshotManager.TakeSnapshot(a.name+\"-input\", state) // Process... outputState := state.Clone() outputState.Set(\"response\", \"processed\") // Take snapshot after processing snapshotManager.TakeSnapshot(a.name+\"-output\", outputState) return AgentResult{OutputState: outputState}, nil }\rState in Different Orchestration Patterns 1. Route Orchestration State Flow // Simple state passing between specific agents event := core.NewEvent( \"agent-a\", core.EventData{\"input\": \"data\"}, map[string]string{\"route\": \"agent-a\"}, ) // Agent A processes and routes to Agent B func (a *AgentA) Run(ctx context.Context, event Event, state State) (AgentResult, error) { // Process input input, _ := state.Get(\"input\") result := a.process(input) outputState := state.Clone() outputState.Set(\"intermediate_result\", result) outputState.SetMeta(\"route\", \"agent-b\") // Route to next agent return AgentResult{OutputState: outputState}, nil }\r2. Collaborative Orchestration State Merging // Multiple agents contribute to shared state type CollaborativeStateManager struct { contributions map[string]State mu sync.RWMutex } func (csm *CollaborativeStateManager) AddContribution(agentID string, state State) { csm.mu.Lock() defer csm.mu.Unlock() if csm.contributions == nil { csm.contributions = make(map[string]State) } csm.contributions[agentID] = state.Clone() } func (csm *CollaborativeStateManager) MergeContributions() State { csm.mu.RLock() defer csm.mu.RUnlock() mergedState := core.NewState() for agentID, contribution := range csm.contributions { // Merge each contribution mergedState.Merge(contribution) // Add contributor metadata mergedState.SetMeta(\"contributor_\"+agentID, \"true\") } return mergedState }\r3. Sequential Orchestration State Pipeline // State flows through pipeline stages type PipelineStage struct { Name string Transform func(State) (State, error) } func ProcessPipeline(initialState State, stages []PipelineStage) (State, error) { currentState := initialState.Clone() for i, stage := range stages { // Add stage metadata currentState.SetMeta(\"current_stage\", stage.Name) currentState.SetMeta(\"stage_number\", fmt.Sprintf(\"%d\", i+1)) // Transform state newState, err := stage.Transform(currentState) if err != nil { return currentState, fmt.Errorf(\"stage %s failed: %w\", stage.Name, err) } currentState = newState // Add completion metadata currentState.SetMeta(\"completed_stage_\"+stage.Name, time.Now().Format(time.RFC3339)) } return currentState, nil }\rPerformance Considerations 1. State Cloning Optimization // Efficient state cloning for large states type OptimizedState struct { *core.SimpleState copyOnWrite bool } func (os *OptimizedState) Clone() State { if !os.copyOnWrite { // Shallow copy for read-only scenarios return \u0026OptimizedState{ SimpleState: os.SimpleState, copyOnWrite: true, } } // Deep copy when modifications are needed return \u0026OptimizedState{ SimpleState: os.SimpleState.Clone().(*core.SimpleState), copyOnWrite: false, } }\r2. State Size Management // Monitor and limit state size func CheckStateSize(state State) error { data, err := json.Marshal(state) if err != nil { return err } const maxStateSize = 1024 * 1024 // 1MB if len(data) \u003e maxStateSize { return fmt.Errorf(\"state size %d exceeds maximum %d bytes\", len(data), maxStateSize) } return nil } // Compress large state data func CompressStateData(state State, key string) error { value, ok := state.Get(key) if !ok { return nil } // Serialize and compress large data data, err := json.Marshal(value) if err != nil { return err } if len(data) \u003e 10240 { // 10KB threshold compressed := compress(data) // Your compression function state.Set(key+\"_compressed\", compressed) state.SetMeta(key+\"_compressed\", \"true\") // Remove original large data state.Set(key, nil) } return nil }\r3. State Caching // State cache for expensive computations type StateCache struct { cache map[string]State mu sync.RWMutex ttl time.Duration } func NewStateCache(ttl time.Duration) *StateCache { return \u0026StateCache{ cache: make(map[string]State), ttl: ttl, } } func (sc *StateCache) Get(key string) (State, bool) { sc.mu.RLock() defer sc.mu.RUnlock() state, exists := sc.cache[key] if !exists { return nil, false } // Check TTL if timestamp, ok := state.GetMeta(\"cached_at\"); ok { if cachedAt, err := time.Parse(time.RFC3339, timestamp); err == nil { if time.Since(cachedAt) \u003e sc.ttl { delete(sc.cache, key) return nil, false } } } return state.Clone(), true } func (sc *StateCache) Set(key string, state State) { sc.mu.Lock() defer sc.mu.Unlock() cachedState := state.Clone() cachedState.SetMeta(\"cached_at\", time.Now().Format(time.RFC3339)) sc.cache[key] = cachedState }\rDebugging State Flow 1. State Tracing // State tracer for debugging type StateTracer struct { traces []StateTrace mu sync.RWMutex } type StateTrace struct { Timestamp time.Time `json:\"timestamp\"` AgentID string `json:\"agent_id\"` Operation string `json:\"operation\"` Key string `json:\"key,omitempty\"` Value any `json:\"value,omitempty\"` StateSize int `json:\"state_size\"` } func (st *StateTracer) TraceGet(agentID, key string, value any, stateSize int) { st.mu.Lock() defer st.mu.Unlock() st.traces = append(st.traces, StateTrace{ Timestamp: time.Now(), AgentID: agentID, Operation: \"GET\", Key: key, Value: value, StateSize: stateSize, }) } func (st *StateTracer) TraceSet(agentID, key string, value any, stateSize int) { st.mu.Lock() defer st.mu.Unlock() st.traces = append(st.traces, StateTrace{ Timestamp: time.Now(), AgentID: agentID, Operation: \"SET\", Key: key, Value: value, StateSize: stateSize, }) } // Traced state wrapper type TracedState struct { State tracer *StateTracer agentID string } func (ts *TracedState) Get(key string) (any, bool) { value, ok := ts.State.Get(key) if ts.tracer != nil { ts.tracer.TraceGet(ts.agentID, key, value, len(ts.State.Keys())) } return value, ok } func (ts *TracedState) Set(key string, value any) { ts.State.Set(key, value) if ts.tracer != nil { ts.tracer.TraceSet(ts.agentID, key, value, len(ts.State.Keys())) } }\r2. State Visualization // Generate state visualization func VisualizeState(state State) string { var builder strings.Builder builder.WriteString(\"State Visualization\\n\") builder.WriteString(\"==================\\n\\n\") // Data section builder.WriteString(\"Data:\\n\") for _, key := range state.Keys() { if value, ok := state.Get(key); ok { builder.WriteString(fmt.Sprintf(\" %s: %v\\n\", key, value)) } } // Metadata section builder.WriteString(\"\\nMetadata:\\n\") for _, key := range state.MetaKeys() { if value, ok := state.GetMeta(key); ok { builder.WriteString(fmt.Sprintf(\" %s: %s\\n\", key, value)) } } return builder.String() } // Generate state diff func DiffStates(before, after State) string { var builder strings.Builder builder.WriteString(\"State Diff\\n\") builder.WriteString(\"==========\\n\\n\") // Check for added/modified data for _, key := range after.Keys() { afterValue, _ := after.Get(key) beforeValue, existed := before.Get(key) if !existed { builder.WriteString(fmt.Sprintf(\"+ %s: %v\\n\", key, afterValue)) } else if !reflect.DeepEqual(beforeValue, afterValue) { builder.WriteString(fmt.Sprintf(\"~ %s: %v -\u003e %v\\n\", key, beforeValue, afterValue)) } } // Check for removed data for _, key := range before.Keys() { if _, exists := after.Get(key); !exists { beforeValue, _ := before.Get(key) builder.WriteString(fmt.Sprintf(\"- %s: %v\\n\", key, beforeValue)) } } return builder.String() }\rBest Practices 1. State Design Principles // Good: Clear, descriptive keys state.Set(\"user_query\", \"What's the weather?\") state.Set(\"weather_data\", weatherInfo) state.Set(\"response_confidence\", 0.95) // Bad: Unclear, abbreviated keys state.Set(\"q\", \"What's the weather?\") state.Set(\"wd\", weatherInfo) state.Set(\"conf\", 0.95) // Good: Consistent naming conventions state.Set(\"user_profile\", profile) state.Set(\"user_preferences\", preferences) state.Set(\"user_history\", history) // Bad: Inconsistent naming state.Set(\"userProfile\", profile) state.Set(\"user_prefs\", preferences) state.Set(\"UserHistory\", history)\r2. Error Handling // Always check if values exist func SafeGetString(state State, key string) (string, error) { value, ok := state.Get(key) if !ok { return \"\", fmt.Errorf(\"key %s not found in state\", key) } str, ok := value.(string) if !ok { return \"\", fmt.Errorf(\"key %s is not a string, got %T\", key, value) } return str, nil } // Use type-safe getters func GetUserID(state State) (string, error) { return SafeGetString(state, \"user_id\") } func GetConfidence(state State) (float64, error) { value, ok := state.Get(\"confidence\") if !ok { return 0, errors.New(\"confidence not found in state\") } confidence, ok := value.(float64) if !ok { return 0, fmt.Errorf(\"confidence is not a float64, got %T\", value) } return confidence, nil }\r3. State Documentation // Document expected state structure type ExpectedState struct { // Required fields UserQuery string `json:\"user_query\" required:\"true\" description:\"The user's input query\"` UserID string `json:\"user_id\" required:\"true\" description:\"Unique user identifier\"` // Optional fields Context string `json:\"context,omitempty\" description:\"Additional context for the query\"` Confidence float64 `json:\"confidence,omitempty\" description:\"Confidence score (0.0-1.0)\"` // Metadata ProcessedBy string `json:\"processed_by,omitempty\" metadata:\"true\" description:\"Agent that processed this state\"` Timestamp string `json:\"timestamp,omitempty\" metadata:\"true\" description:\"Processing timestamp\"` } // Validate state against expected structure func ValidateExpectedState(state State) error { // Check required fields if _, ok := state.Get(\"user_query\"); !ok { return errors.New(\"user_query is required\") } if _, ok := state.Get(\"user_id\"); !ok { return errors.New(\"user_id is required\") } return nil }\rCommon Pitfalls and Solutions 1. State Mutation Issues Problem: Modifying shared state without proper cloning.\n// Bad: Modifying shared state func (a *BadAgent) Run(ctx context.Context, event Event, state State) (AgentResult, error) { state.Set(\"modified\", true) // Modifies input state! return AgentResult{OutputState: state}, nil } // Good: Clone before modifying func (a *GoodAgent) Run(ctx context.Context, event Event, state State) (AgentResult, error) { outputState := state.Clone() outputState.Set(\"modified\", true) return AgentResult{OutputState: outputState}, nil }\r2. Memory Leaks Problem: Accumulating large amounts of data in state without cleanup.\n// Bad: Accumulating data indefinitely func (a *BadAgent) Run(ctx context.Context, event Event, state State) (AgentResult, error) { history, _ := state.Get(\"processing_history\") if history == nil { history = make([]string, 0) } // This grows indefinitely! newHistory := append(history.([]string), \"processed by \"+a.name) outputState := state.Clone() outputState.Set(\"processing_history\", newHistory) return AgentResult{OutputState: outputState}, nil } // Good: Limit data accumulation func (a *GoodAgent) Run(ctx context.Context, event Event, state State) (AgentResult, error) { history, _ := state.Get(\"processing_history\") if history == nil { history = make([]string, 0) } newHistory := append(history.([]string), \"processed by \"+a.name) // Keep only last 10 entries const maxHistory = 10 if len(newHistory) \u003e maxHistory { newHistory = newHistory[len(newHistory)-maxHistory:] } outputState := state.Clone() outputState.Set(\"processing_history\", newHistory) return AgentResult{OutputState: outputState}, nil }\r3. Type Safety Issues Problem: Assuming data types without checking.\n// Bad: Assuming types func (a *BadAgent) Run(ctx context.Context, event Event, state State) (AgentResult, error) { query := state.Get(\"user_query\").(string) // Panic if not string! // ... } // Good: Type checking func (a *GoodAgent) Run(ctx context.Context, event Event, state State) (AgentResult, error) { queryValue, ok := state.Get(\"user_query\") if !ok { return AgentResult{}, errors.New(\"user_query not found\") } query, ok := queryValue.(string) if !ok { return AgentResult{}, fmt.Errorf(\"user_query is not a string, got %T\", queryValue) } // Safe to use query // ... }\rConclusion State management is the foundation of data flow in AgenticGoKit. By understanding how State objects work, how data flows between agents, and following best practices, you can build robust multi-agent systems that handle complex data transformations reliably.\nKey takeaways:\nAlways clone state before modification Use clear, consistent naming for state keys Implement proper type checking and error handling Monitor state size and prevent memory leaks Document expected state structure for maintainability Use namespaced keys to avoid conflicts Implement validation for critical state data Next Steps Memory Systems - Learn about persistent state storage Error Handling - Master robust error management with state Debugging Guide - Learn to trace state flow Advanced Patterns - Explore advanced state management patterns Further Reading API Reference: State Interface Examples: State Management Patterns Configuration Guide: State Settings",
    "description": "State Management and Data Flow in AgenticGoKit Overview State management is the backbone of data flow in AgenticGoKit. It determines how information is stored, passed between agents, and persisted across interactions. This tutorial explores how State objects work, how data flows through agent systems, and best practices for managing complex data transformations.\nUnderstanding state management is crucial because it’s how agents share information, maintain context, and build upon each other’s work.",
    "tags": [],
    "title": "state-management",
    "uri": "/AgenticGoKitDocs/tutorials/core-concepts/state-management/index.html"
  },
  {
    "breadcrumb": "content \u003e guides \u003e development",
    "content": "Testing Agents Comprehensive testing strategies for multi-agent systems\nThis guide covers testing approaches for AgenticGoKit applications, from unit testing individual agents to integration testing complex multi-agent workflows.\nTesting Philosophy AgenticGoKit testing follows these principles:\nTest at multiple levels: Unit, integration, and end-to-end Mock external dependencies: LLM providers, databases, external APIs Test orchestration patterns: Verify agent interactions work correctly Performance testing: Ensure agents meet performance requirements Deterministic testing: Use mocks to ensure reproducible results Quick Start (10 minutes) 1. Basic Agent Unit Test package main import ( \"context\" \"testing\" \"github.com/stretchr/testify/assert\" \"github.com/stretchr/testify/require\" \"github.com/kunalkushwaha/agenticgokit/core\" ) func TestGreetingAgent(t *testing.T) { // Create mock LLM provider mockLLM := \u0026MockLLMProvider{ response: \"Hello, World! Nice to meet you.\", } // Create agent agent := NewGreetingAgent(\"greeter\", mockLLM) // Create test event event := core.NewEvent(\"greeting\", map[string]interface{}{ \"name\": \"World\", }) // Execute agent result, err := agent.Run(context.Background(), event, core.NewState()) // Verify results require.NoError(t, err) assert.Contains(t, result.Data[\"response\"], \"Hello\") assert.Contains(t, result.Data[\"response\"], \"World\") } // Mock LLM Provider for testing type MockLLMProvider struct { response string err error } func (m *MockLLMProvider) Generate(ctx context.Context, prompt string) (string, error) { if m.err != nil { return \"\", m.err } return m.response, nil } func (m *MockLLMProvider) Name() string { return \"mock\" }\r2. Multi-Agent Integration Test func TestMultiAgentWorkflow(t *testing.T) { // Create mock providers mockLLM := \u0026MockLLMProvider{ response: \"Processed successfully\", } // Create agents agents := map[string]core.AgentHandler{ \"analyzer\": NewAnalyzerAgent(mockLLM), \"processor\": NewProcessorAgent(mockLLM), } // Create sequential runner runner := core.CreateSequentialRunner(agents, []string{\"analyzer\", \"processor\"}, 30*time.Second) // Test workflow event := core.NewEvent(\"analyze\", map[string]interface{}{ \"data\": \"test data\", }) results, err := runner.ProcessEvent(context.Background(), event) require.NoError(t, err) assert.Len(t, results, 2) assert.Contains(t, results, \"analyzer\") assert.Contains(t, results, \"processor\") }\rUnit Testing Patterns Testing Agent Logic func TestDataProcessingAgent(t *testing.T) { tests := []struct { name string input map[string]interface{} expected map[string]interface{} wantErr bool }{ { name: \"valid data processing\", input: map[string]interface{}{ \"numbers\": []int{1, 2, 3, 4, 5}, }, expected: map[string]interface{}{ \"sum\": 15, \"average\": 3.0, \"count\": 5, }, wantErr: false, }, { name: \"empty data\", input: map[string]interface{}{ \"numbers\": []int{}, }, expected: nil, wantErr: true, }, { name: \"invalid input type\", input: map[string]interface{}{ \"numbers\": \"not a slice\", }, expected: nil, wantErr: true, }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { agent := NewDataProcessingAgent() event := core.NewEvent(\"process\", tt.input) result, err := agent.Run(context.Background(), event, core.NewState()) if tt.wantErr { assert.Error(t, err) return } require.NoError(t, err) for key, expectedValue := range tt.expected { assert.Equal(t, expectedValue, result.Data[key]) } }) } }\rTesting State Management func TestAgentStateHandling(t *testing.T) { agent := NewStatefulAgent() // Test state initialization initialState := core.NewState() initialState.Set(\"counter\", 0) event := core.NewEvent(\"increment\", nil) // First execution result1, err := agent.Run(context.Background(), event, initialState) require.NoError(t, err) assert.Equal(t, 1, result1.Data[\"counter\"]) // Second execution with updated state result2, err := agent.Run(context.Background(), event, result1.State) require.NoError(t, err) assert.Equal(t, 2, result2.Data[\"counter\"]) // Verify state persistence assert.Equal(t, 2, result2.State.Get(\"counter\")) }\rTesting Error Handling func TestAgentErrorHandling(t *testing.T) { // Test with failing LLM provider failingLLM := \u0026MockLLMProvider{ err: errors.New(\"LLM service unavailable\"), } agent := NewResilientAgent(failingLLM) event := core.NewEvent(\"process\", map[string]interface{}{ \"input\": \"test\", }) result, err := agent.Run(context.Background(), event, core.NewState()) // Agent should handle LLM failure gracefully require.NoError(t, err) assert.Contains(t, result.Data[\"response\"], \"service temporarily unavailable\") assert.True(t, result.Data[\"fallback_used\"].(bool)) }\rIntegration Testing Testing Orchestration Patterns func TestSequentialOrchestration(t *testing.T) { // Create agents that depend on each other agents := map[string]core.AgentHandler{ \"step1\": core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { input := event.Data[\"input\"].(string) return core.AgentResult{ Data: map[string]interface{}{ \"step1_output\": input + \"_processed\", }, }, nil }), \"step2\": core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { step1Output := state.Data[\"step1_output\"].(string) return core.AgentResult{ Data: map[string]interface{}{ \"final_output\": step1Output + \"_finalized\", }, }, nil }), } runner := core.CreateSequentialRunner(agents, []string{\"step1\", \"step2\"}, 30*time.Second) event := core.NewEvent(\"process\", map[string]interface{}{ \"input\": \"test\", }) results, err := runner.ProcessEvent(context.Background(), event) require.NoError(t, err) assert.Equal(t, \"test_processed_finalized\", results[\"step2\"].Data[\"final_output\"]) } func TestCollaborativeOrchestration(t *testing.T) { // Create agents that work in parallel agents := map[string]core.AgentHandler{ \"analyzer\": core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { return core.AgentResult{ Data: map[string]interface{}{ \"analysis\": \"positive sentiment\", }, }, nil }), \"summarizer\": core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { return core.AgentResult{ Data: map[string]interface{}{ \"summary\": \"brief summary\", }, }, nil }), } runner := core.CreateCollaborativeRunner(agents, 30*time.Second) event := core.NewEvent(\"analyze\", map[string]interface{}{ \"text\": \"This is great!\", }) results, err := runner.ProcessEvent(context.Background(), event) require.NoError(t, err) assert.Len(t, results, 2) assert.Equal(t, \"positive sentiment\", results[\"analyzer\"].Data[\"analysis\"]) assert.Equal(t, \"brief summary\", results[\"summarizer\"].Data[\"summary\"]) }\rTesting Memory Integration func TestAgentWithMemory(t *testing.T) { // Create in-memory provider for testing memoryConfig := core.AgentMemoryConfig{ Provider: \"memory\", Connection: \"memory\", Dimensions: 384, Embedding: core.EmbeddingConfig{ Provider: \"dummy\", }, } memory, err := core.NewMemory(memoryConfig) require.NoError(t, err) defer memory.Close() // Create memory-enabled agent agent := NewMemoryEnabledAgent(memory) ctx := memory.SetSession(context.Background(), \"test-session\") // First interaction - store information event1 := core.NewEvent(\"remember\", map[string]interface{}{ \"info\": \"User likes coffee\", }) result1, err := agent.Run(ctx, event1, core.NewState()) require.NoError(t, err) assert.Contains(t, result1.Data[\"response\"], \"remembered\") // Second interaction - recall information event2 := core.NewEvent(\"recall\", map[string]interface{}{ \"query\": \"What does the user like?\", }) result2, err := agent.Run(ctx, event2, core.NewState()) require.NoError(t, err) assert.Contains(t, result2.Data[\"response\"], \"coffee\") }\rTesting MCP Tool Integration func TestAgentWithMCPTools(t *testing.T) { // Create mock MCP manager mockMCP := \u0026MockMCPManager{ tools: []core.ToolSchema{ { Name: \"search\", Description: \"Search for information\", }, }, responses: map[string]interface{}{ \"search\": map[string]interface{}{ \"results\": []string{\"Mock search result\"}, }, }, } mockLLM := \u0026MockLLMProvider{ response: `I'll search for that information. \u003ctool_call\u003e {\"name\": \"search\", \"args\": {\"query\": \"test query\"}} \u003c/tool_call\u003e Based on the search results, here's what I found...`, } agent := NewToolEnabledAgent(mockLLM, mockMCP) event := core.NewEvent(\"query\", map[string]interface{}{ \"question\": \"Search for test information\", }) result, err := agent.Run(context.Background(), event, core.NewState()) require.NoError(t, err) assert.True(t, result.Data[\"tools_used\"].(bool)) assert.Contains(t, result.Data[\"tool_results\"], \"Mock search result\") } type MockMCPManager struct { tools []core.ToolSchema responses map[string]interface{} } func (m *MockMCPManager) ListTools(ctx context.Context) ([]core.ToolSchema, error) { return m.tools, nil } func (m *MockMCPManager) CallTool(ctx context.Context, name string, args map[string]interface{}) (interface{}, error) { if response, exists := m.responses[name]; exists { return response, nil } return nil, fmt.Errorf(\"tool not found: %s\", name) }\rPerformance Testing Load Testing func TestAgentPerformance(t *testing.T) { if testing.Short() { t.Skip(\"Skipping performance test in short mode\") } agent := NewPerformantAgent() // Test concurrent execution concurrency := 10 iterations := 100 var wg sync.WaitGroup results := make(chan time.Duration, concurrency*iterations) for i := 0; i \u003c concurrency; i++ { wg.Add(1) go func() { defer wg.Done() for j := 0; j \u003c iterations; j++ { start := time.Now() event := core.NewEvent(\"process\", map[string]interface{}{ \"data\": fmt.Sprintf(\"test_%d_%d\", i, j), }) _, err := agent.Run(context.Background(), event, core.NewState()) duration := time.Since(start) require.NoError(t, err) results \u003c- duration } }() } wg.Wait() close(results) // Analyze performance var total time.Duration var max time.Duration count := 0 for duration := range results { total += duration if duration \u003e max { max = duration } count++ } average := total / time.Duration(count) t.Logf(\"Performance Results:\") t.Logf(\" Total requests: %d\", count) t.Logf(\" Average duration: %v\", average) t.Logf(\" Max duration: %v\", max) t.Logf(\" Requests per second: %.2f\", float64(count)/total.Seconds()) // Assert performance requirements assert.Less(t, average, 100*time.Millisecond, \"Average response time should be under 100ms\") assert.Less(t, max, 500*time.Millisecond, \"Max response time should be under 500ms\") }\rMemory Usage Testing func TestAgentMemoryUsage(t *testing.T) { agent := NewMemoryEfficientAgent() // Measure initial memory var m1 runtime.MemStats runtime.GC() runtime.ReadMemStats(\u0026m1) // Run many iterations for i := 0; i \u003c 1000; i++ { event := core.NewEvent(\"process\", map[string]interface{}{ \"data\": strings.Repeat(\"x\", 1000), // 1KB of data }) _, err := agent.Run(context.Background(), event, core.NewState()) require.NoError(t, err) } // Force garbage collection and measure final memory runtime.GC() var m2 runtime.MemStats runtime.ReadMemStats(\u0026m2) // Calculate memory increase memoryIncrease := m2.Alloc - m1.Alloc t.Logf(\"Memory usage:\") t.Logf(\" Initial: %d KB\", m1.Alloc/1024) t.Logf(\" Final: %d KB\", m2.Alloc/1024) t.Logf(\" Increase: %d KB\", memoryIncrease/1024) // Assert reasonable memory usage (adjust threshold as needed) assert.Less(t, memoryIncrease, uint64(10*1024*1024), \"Memory increase should be less than 10MB\") }\rEnd-to-End Testing Complete Workflow Testing func TestCompleteWorkflow(t *testing.T) { // Set up complete system memoryConfig := core.AgentMemoryConfig{ Provider: \"memory\", Connection: \"memory\", Dimensions: 384, Embedding: core.EmbeddingConfig{ Provider: \"dummy\", }, } memory, err := core.NewMemory(memoryConfig) require.NoError(t, err) defer memory.Close() mockLLM := \u0026MockLLMProvider{ response: \"I understand your request and will process it accordingly.\", } // Create complete agent system agents := map[string]core.AgentHandler{ \"intake\": NewIntakeAgent(mockLLM), \"processor\": NewProcessorAgent(mockLLM, memory), \"responder\": NewResponderAgent(mockLLM), } runner := core.CreateSequentialRunner(agents, []string{\"intake\", \"processor\", \"responder\"}, 60*time.Second) // Test complete user interaction ctx := memory.SetSession(context.Background(), \"e2e-test-session\") event := core.NewEvent(\"user_request\", map[string]interface{}{ \"message\": \"I need help with my project\", \"user_id\": \"test_user\", }) results, err := runner.ProcessEvent(ctx, event) require.NoError(t, err) assert.Len(t, results, 3) // Verify each stage completed successfully assert.Contains(t, results[\"intake\"].Data, \"processed\") assert.Contains(t, results[\"processor\"].Data, \"analyzed\") assert.Contains(t, results[\"responder\"].Data, \"response\") // Verify final response quality finalResponse := results[\"responder\"].Data[\"response\"].(string) assert.NotEmpty(t, finalResponse) assert.Greater(t, len(finalResponse), 10) }\rTest Utilities and Helpers Test Data Builders // Event builder for consistent test data type EventBuilder struct { eventType string data map[string]interface{} metadata map[string]interface{} } func NewEventBuilder(eventType string) *EventBuilder { return \u0026EventBuilder{ eventType: eventType, data: make(map[string]interface{}), metadata: make(map[string]interface{}), } } func (eb *EventBuilder) WithData(key string, value interface{}) *EventBuilder { eb.data[key] = value return eb } func (eb *EventBuilder) WithMetadata(key string, value interface{}) *EventBuilder { eb.metadata[key] = value return eb } func (eb *EventBuilder) Build() core.Event { event := core.NewEvent(eb.eventType, eb.data) for k, v := range eb.metadata { event.Metadata[k] = v } return event } // Usage in tests func TestWithEventBuilder(t *testing.T) { event := NewEventBuilder(\"process\"). WithData(\"input\", \"test data\"). WithData(\"priority\", \"high\"). WithMetadata(\"user_id\", \"123\"). Build() // Use event in test... }\rAgent Test Harness type AgentTestHarness struct { agent core.AgentHandler mockLLM *MockLLMProvider memory core.Memory ctx context.Context } func NewAgentTestHarness(agent core.AgentHandler) *AgentTestHarness { mockLLM := \u0026MockLLMProvider{} memoryConfig := core.AgentMemoryConfig{ Provider: \"memory\", Connection: \"memory\", Dimensions: 384, Embedding: core.EmbeddingConfig{ Provider: \"dummy\", }, } memory, _ := core.NewMemory(memoryConfig) ctx := memory.SetSession(context.Background(), \"test-session\") return \u0026AgentTestHarness{ agent: agent, mockLLM: mockLLM, memory: memory, ctx: ctx, } } func (h *AgentTestHarness) SetLLMResponse(response string) { h.mockLLM.response = response } func (h *AgentTestHarness) SetLLMError(err error) { h.mockLLM.err = err } func (h *AgentTestHarness) Execute(event core.Event) (core.AgentResult, error) { return h.agent.Run(h.ctx, event, core.NewState()) } func (h *AgentTestHarness) Cleanup() { if h.memory != nil { h.memory.Close() } } // Usage in tests func TestWithHarness(t *testing.T) { harness := NewAgentTestHarness(NewMyAgent()) defer harness.Cleanup() harness.SetLLMResponse(\"Expected response\") event := core.NewEvent(\"test\", map[string]interface{}{ \"input\": \"test data\", }) result, err := harness.Execute(event) require.NoError(t, err) assert.Equal(t, \"Expected response\", result.Data[\"response\"]) }\rContinuous Integration GitHub Actions Test Configuration # .github/workflows/test.yml name: Test on: push: branches: [ main, develop ] pull_request: branches: [ main ] jobs: test: runs-on: ubuntu-latest services: postgres: image: pgvector/pgvector:pg15 env: POSTGRES_PASSWORD: password POSTGRES_DB: agentflow_test options: \u003e- --health-cmd pg_isready --health-interval 10s --health-timeout 5s --health-retries 5 ports: - 5432:5432 steps: - uses: actions/checkout@v3 - name: Set up Go uses: actions/setup-go@v3 with: go-version: 1.21 - name: Install dependencies run: go mod download - name: Run unit tests run: go test -v -short ./... - name: Run integration tests run: go test -v ./... env: TEST_DB_URL: postgres://postgres:password@localhost:5432/agentflow_test?sslmode=disable - name: Run performance tests run: go test -v -run TestPerformance ./... - name: Generate coverage report run: go test -coverprofile=coverage.out ./... - name: Upload coverage to Codecov uses: codecov/codecov-action@v3 with: file: ./coverage.out\rBest Practices Test Organization // Organize tests by functionality func TestAgentCore(t *testing.T) { t.Run(\"BasicExecution\", testBasicExecution) t.Run(\"ErrorHandling\", testErrorHandling) t.Run(\"StateManagement\", testStateManagement) } func TestAgentIntegration(t *testing.T) { t.Run(\"WithMemory\", testWithMemory) t.Run(\"WithTools\", testWithTools) t.Run(\"WithOrchestration\", testWithOrchestration) } func TestAgentPerformance(t *testing.T) { if testing.Short() { t.Skip(\"Skipping performance tests in short mode\") } t.Run(\"LoadTest\", testLoadTest) t.Run(\"MemoryUsage\", testMemoryUsage) t.Run(\"Concurrency\", testConcurrency) }\rTest Data Management // Use table-driven tests for multiple scenarios func TestDataProcessing(t *testing.T) { testCases := []struct { name string input interface{} expected interface{} wantErr bool }{ // Test cases here... } for _, tc := range testCases { t.Run(tc.name, func(t *testing.T) { // Test implementation... }) } } // Use test fixtures for complex data func loadTestFixture(t *testing.T, filename string) map[string]interface{} { data, err := os.ReadFile(filepath.Join(\"testdata\", filename)) require.NoError(t, err) var result map[string]interface{} err = json.Unmarshal(data, \u0026result) require.NoError(t, err) return result }\rNext Steps Debugging - Debug agent interactions effectively Best Practices - Development best practices Production Deployment - Production deployment",
    "description": "Testing Agents Comprehensive testing strategies for multi-agent systems\nThis guide covers testing approaches for AgenticGoKit applications, from unit testing individual agents to integration testing complex multi-agent workflows.\nTesting Philosophy AgenticGoKit testing follows these principles:\nTest at multiple levels: Unit, integration, and end-to-end Mock external dependencies: LLM providers, databases, external APIs Test orchestration patterns: Verify agent interactions work correctly Performance testing: Ensure agents meet performance requirements Deterministic testing: Use mocks to ensure reproducible results Quick Start (10 minutes) 1. Basic Agent Unit Test package main import ( \"context\" \"testing\" \"github.com/stretchr/testify/assert\" \"github.com/stretchr/testify/require\" \"github.com/kunalkushwaha/agenticgokit/core\" ) func TestGreetingAgent(t *testing.T) { // Create mock LLM provider mockLLM := \u0026MockLLMProvider{ response: \"Hello, World! Nice to meet you.\", } // Create agent agent := NewGreetingAgent(\"greeter\", mockLLM) // Create test event event := core.NewEvent(\"greeting\", map[string]interface{}{ \"name\": \"World\", }) // Execute agent result, err := agent.Run(context.Background(), event, core.NewState()) // Verify results require.NoError(t, err) assert.Contains(t, result.Data[\"response\"], \"Hello\") assert.Contains(t, result.Data[\"response\"], \"World\") } // Mock LLM Provider for testing type MockLLMProvider struct { response string err error } func (m *MockLLMProvider) Generate(ctx context.Context, prompt string) (string, error) { if m.err != nil { return \"\", m.err } return m.response, nil } func (m *MockLLMProvider) Name() string { return \"mock\" }\r2. Multi-Agent Integration Test func TestMultiAgentWorkflow(t *testing.T) { // Create mock providers mockLLM := \u0026MockLLMProvider{ response: \"Processed successfully\", } // Create agents agents := map[string]core.AgentHandler{ \"analyzer\": NewAnalyzerAgent(mockLLM), \"processor\": NewProcessorAgent(mockLLM), } // Create sequential runner runner := core.CreateSequentialRunner(agents, []string{\"analyzer\", \"processor\"}, 30*time.Second) // Test workflow event := core.NewEvent(\"analyze\", map[string]interface{}{ \"data\": \"test data\", }) results, err := runner.ProcessEvent(context.Background(), event) require.NoError(t, err) assert.Len(t, results, 2) assert.Contains(t, results, \"analyzer\") assert.Contains(t, results, \"processor\") }\rUnit Testing Patterns Testing Agent Logic func TestDataProcessingAgent(t *testing.T) { tests := []struct { name string input map[string]interface{} expected map[string]interface{} wantErr bool }{ { name: \"valid data processing\", input: map[string]interface{}{ \"numbers\": []int{1, 2, 3, 4, 5}, }, expected: map[string]interface{}{ \"sum\": 15, \"average\": 3.0, \"count\": 5, }, wantErr: false, }, { name: \"empty data\", input: map[string]interface{}{ \"numbers\": []int{}, }, expected: nil, wantErr: true, }, { name: \"invalid input type\", input: map[string]interface{}{ \"numbers\": \"not a slice\", }, expected: nil, wantErr: true, }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { agent := NewDataProcessingAgent() event := core.NewEvent(\"process\", tt.input) result, err := agent.Run(context.Background(), event, core.NewState()) if tt.wantErr { assert.Error(t, err) return } require.NoError(t, err) for key, expectedValue := range tt.expected { assert.Equal(t, expectedValue, result.Data[key]) } }) } }\rTesting State Management func TestAgentStateHandling(t *testing.T) { agent := NewStatefulAgent() // Test state initialization initialState := core.NewState() initialState.Set(\"counter\", 0) event := core.NewEvent(\"increment\", nil) // First execution result1, err := agent.Run(context.Background(), event, initialState) require.NoError(t, err) assert.Equal(t, 1, result1.Data[\"counter\"]) // Second execution with updated state result2, err := agent.Run(context.Background(), event, result1.State) require.NoError(t, err) assert.Equal(t, 2, result2.Data[\"counter\"]) // Verify state persistence assert.Equal(t, 2, result2.State.Get(\"counter\")) }\rTesting Error Handling func TestAgentErrorHandling(t *testing.T) { // Test with failing LLM provider failingLLM := \u0026MockLLMProvider{ err: errors.New(\"LLM service unavailable\"), } agent := NewResilientAgent(failingLLM) event := core.NewEvent(\"process\", map[string]interface{}{ \"input\": \"test\", }) result, err := agent.Run(context.Background(), event, core.NewState()) // Agent should handle LLM failure gracefully require.NoError(t, err) assert.Contains(t, result.Data[\"response\"], \"service temporarily unavailable\") assert.True(t, result.Data[\"fallback_used\"].(bool)) }\rIntegration Testing Testing Orchestration Patterns func TestSequentialOrchestration(t *testing.T) { // Create agents that depend on each other agents := map[string]core.AgentHandler{ \"step1\": core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { input := event.Data[\"input\"].(string) return core.AgentResult{ Data: map[string]interface{}{ \"step1_output\": input + \"_processed\", }, }, nil }), \"step2\": core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { step1Output := state.Data[\"step1_output\"].(string) return core.AgentResult{ Data: map[string]interface{}{ \"final_output\": step1Output + \"_finalized\", }, }, nil }), } runner := core.CreateSequentialRunner(agents, []string{\"step1\", \"step2\"}, 30*time.Second) event := core.NewEvent(\"process\", map[string]interface{}{ \"input\": \"test\", }) results, err := runner.ProcessEvent(context.Background(), event) require.NoError(t, err) assert.Equal(t, \"test_processed_finalized\", results[\"step2\"].Data[\"final_output\"]) } func TestCollaborativeOrchestration(t *testing.T) { // Create agents that work in parallel agents := map[string]core.AgentHandler{ \"analyzer\": core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { return core.AgentResult{ Data: map[string]interface{}{ \"analysis\": \"positive sentiment\", }, }, nil }), \"summarizer\": core.AgentHandlerFunc(func(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { return core.AgentResult{ Data: map[string]interface{}{ \"summary\": \"brief summary\", }, }, nil }), } runner := core.CreateCollaborativeRunner(agents, 30*time.Second) event := core.NewEvent(\"analyze\", map[string]interface{}{ \"text\": \"This is great!\", }) results, err := runner.ProcessEvent(context.Background(), event) require.NoError(t, err) assert.Len(t, results, 2) assert.Equal(t, \"positive sentiment\", results[\"analyzer\"].Data[\"analysis\"]) assert.Equal(t, \"brief summary\", results[\"summarizer\"].Data[\"summary\"]) }\rTesting Memory Integration func TestAgentWithMemory(t *testing.T) { // Create in-memory provider for testing memoryConfig := core.AgentMemoryConfig{ Provider: \"memory\", Connection: \"memory\", Dimensions: 384, Embedding: core.EmbeddingConfig{ Provider: \"dummy\", }, } memory, err := core.NewMemory(memoryConfig) require.NoError(t, err) defer memory.Close() // Create memory-enabled agent agent := NewMemoryEnabledAgent(memory) ctx := memory.SetSession(context.Background(), \"test-session\") // First interaction - store information event1 := core.NewEvent(\"remember\", map[string]interface{}{ \"info\": \"User likes coffee\", }) result1, err := agent.Run(ctx, event1, core.NewState()) require.NoError(t, err) assert.Contains(t, result1.Data[\"response\"], \"remembered\") // Second interaction - recall information event2 := core.NewEvent(\"recall\", map[string]interface{}{ \"query\": \"What does the user like?\", }) result2, err := agent.Run(ctx, event2, core.NewState()) require.NoError(t, err) assert.Contains(t, result2.Data[\"response\"], \"coffee\") }\rTesting MCP Tool Integration func TestAgentWithMCPTools(t *testing.T) { // Create mock MCP manager mockMCP := \u0026MockMCPManager{ tools: []core.ToolSchema{ { Name: \"search\", Description: \"Search for information\", }, }, responses: map[string]interface{}{ \"search\": map[string]interface{}{ \"results\": []string{\"Mock search result\"}, }, }, } mockLLM := \u0026MockLLMProvider{ response: `I'll search for that information. \u003ctool_call\u003e {\"name\": \"search\", \"args\": {\"query\": \"test query\"}} \u003c/tool_call\u003e Based on the search results, here's what I found...`, } agent := NewToolEnabledAgent(mockLLM, mockMCP) event := core.NewEvent(\"query\", map[string]interface{}{ \"question\": \"Search for test information\", }) result, err := agent.Run(context.Background(), event, core.NewState()) require.NoError(t, err) assert.True(t, result.Data[\"tools_used\"].(bool)) assert.Contains(t, result.Data[\"tool_results\"], \"Mock search result\") } type MockMCPManager struct { tools []core.ToolSchema responses map[string]interface{} } func (m *MockMCPManager) ListTools(ctx context.Context) ([]core.ToolSchema, error) { return m.tools, nil } func (m *MockMCPManager) CallTool(ctx context.Context, name string, args map[string]interface{}) (interface{}, error) { if response, exists := m.responses[name]; exists { return response, nil } return nil, fmt.Errorf(\"tool not found: %s\", name) }\rPerformance Testing Load Testing func TestAgentPerformance(t *testing.T) { if testing.Short() { t.Skip(\"Skipping performance test in short mode\") } agent := NewPerformantAgent() // Test concurrent execution concurrency := 10 iterations := 100 var wg sync.WaitGroup results := make(chan time.Duration, concurrency*iterations) for i := 0; i \u003c concurrency; i++ { wg.Add(1) go func() { defer wg.Done() for j := 0; j \u003c iterations; j++ { start := time.Now() event := core.NewEvent(\"process\", map[string]interface{}{ \"data\": fmt.Sprintf(\"test_%d_%d\", i, j), }) _, err := agent.Run(context.Background(), event, core.NewState()) duration := time.Since(start) require.NoError(t, err) results \u003c- duration } }() } wg.Wait() close(results) // Analyze performance var total time.Duration var max time.Duration count := 0 for duration := range results { total += duration if duration \u003e max { max = duration } count++ } average := total / time.Duration(count) t.Logf(\"Performance Results:\") t.Logf(\" Total requests: %d\", count) t.Logf(\" Average duration: %v\", average) t.Logf(\" Max duration: %v\", max) t.Logf(\" Requests per second: %.2f\", float64(count)/total.Seconds()) // Assert performance requirements assert.Less(t, average, 100*time.Millisecond, \"Average response time should be under 100ms\") assert.Less(t, max, 500*time.Millisecond, \"Max response time should be under 500ms\") }\rMemory Usage Testing func TestAgentMemoryUsage(t *testing.T) { agent := NewMemoryEfficientAgent() // Measure initial memory var m1 runtime.MemStats runtime.GC() runtime.ReadMemStats(\u0026m1) // Run many iterations for i := 0; i \u003c 1000; i++ { event := core.NewEvent(\"process\", map[string]interface{}{ \"data\": strings.Repeat(\"x\", 1000), // 1KB of data }) _, err := agent.Run(context.Background(), event, core.NewState()) require.NoError(t, err) } // Force garbage collection and measure final memory runtime.GC() var m2 runtime.MemStats runtime.ReadMemStats(\u0026m2) // Calculate memory increase memoryIncrease := m2.Alloc - m1.Alloc t.Logf(\"Memory usage:\") t.Logf(\" Initial: %d KB\", m1.Alloc/1024) t.Logf(\" Final: %d KB\", m2.Alloc/1024) t.Logf(\" Increase: %d KB\", memoryIncrease/1024) // Assert reasonable memory usage (adjust threshold as needed) assert.Less(t, memoryIncrease, uint64(10*1024*1024), \"Memory increase should be less than 10MB\") }\rEnd-to-End Testing Complete Workflow Testing func TestCompleteWorkflow(t *testing.T) { // Set up complete system memoryConfig := core.AgentMemoryConfig{ Provider: \"memory\", Connection: \"memory\", Dimensions: 384, Embedding: core.EmbeddingConfig{ Provider: \"dummy\", }, } memory, err := core.NewMemory(memoryConfig) require.NoError(t, err) defer memory.Close() mockLLM := \u0026MockLLMProvider{ response: \"I understand your request and will process it accordingly.\", } // Create complete agent system agents := map[string]core.AgentHandler{ \"intake\": NewIntakeAgent(mockLLM), \"processor\": NewProcessorAgent(mockLLM, memory), \"responder\": NewResponderAgent(mockLLM), } runner := core.CreateSequentialRunner(agents, []string{\"intake\", \"processor\", \"responder\"}, 60*time.Second) // Test complete user interaction ctx := memory.SetSession(context.Background(), \"e2e-test-session\") event := core.NewEvent(\"user_request\", map[string]interface{}{ \"message\": \"I need help with my project\", \"user_id\": \"test_user\", }) results, err := runner.ProcessEvent(ctx, event) require.NoError(t, err) assert.Len(t, results, 3) // Verify each stage completed successfully assert.Contains(t, results[\"intake\"].Data, \"processed\") assert.Contains(t, results[\"processor\"].Data, \"analyzed\") assert.Contains(t, results[\"responder\"].Data, \"response\") // Verify final response quality finalResponse := results[\"responder\"].Data[\"response\"].(string) assert.NotEmpty(t, finalResponse) assert.Greater(t, len(finalResponse), 10) }\rTest Utilities and Helpers Test Data Builders // Event builder for consistent test data type EventBuilder struct { eventType string data map[string]interface{} metadata map[string]interface{} } func NewEventBuilder(eventType string) *EventBuilder { return \u0026EventBuilder{ eventType: eventType, data: make(map[string]interface{}), metadata: make(map[string]interface{}), } } func (eb *EventBuilder) WithData(key string, value interface{}) *EventBuilder { eb.data[key] = value return eb } func (eb *EventBuilder) WithMetadata(key string, value interface{}) *EventBuilder { eb.metadata[key] = value return eb } func (eb *EventBuilder) Build() core.Event { event := core.NewEvent(eb.eventType, eb.data) for k, v := range eb.metadata { event.Metadata[k] = v } return event } // Usage in tests func TestWithEventBuilder(t *testing.T) { event := NewEventBuilder(\"process\"). WithData(\"input\", \"test data\"). WithData(\"priority\", \"high\"). WithMetadata(\"user_id\", \"123\"). Build() // Use event in test... }\rAgent Test Harness type AgentTestHarness struct { agent core.AgentHandler mockLLM *MockLLMProvider memory core.Memory ctx context.Context } func NewAgentTestHarness(agent core.AgentHandler) *AgentTestHarness { mockLLM := \u0026MockLLMProvider{} memoryConfig := core.AgentMemoryConfig{ Provider: \"memory\", Connection: \"memory\", Dimensions: 384, Embedding: core.EmbeddingConfig{ Provider: \"dummy\", }, } memory, _ := core.NewMemory(memoryConfig) ctx := memory.SetSession(context.Background(), \"test-session\") return \u0026AgentTestHarness{ agent: agent, mockLLM: mockLLM, memory: memory, ctx: ctx, } } func (h *AgentTestHarness) SetLLMResponse(response string) { h.mockLLM.response = response } func (h *AgentTestHarness) SetLLMError(err error) { h.mockLLM.err = err } func (h *AgentTestHarness) Execute(event core.Event) (core.AgentResult, error) { return h.agent.Run(h.ctx, event, core.NewState()) } func (h *AgentTestHarness) Cleanup() { if h.memory != nil { h.memory.Close() } } // Usage in tests func TestWithHarness(t *testing.T) { harness := NewAgentTestHarness(NewMyAgent()) defer harness.Cleanup() harness.SetLLMResponse(\"Expected response\") event := core.NewEvent(\"test\", map[string]interface{}{ \"input\": \"test data\", }) result, err := harness.Execute(event) require.NoError(t, err) assert.Equal(t, \"Expected response\", result.Data[\"response\"]) }\rContinuous Integration GitHub Actions Test Configuration # .github/workflows/test.yml name: Test on: push: branches: [ main, develop ] pull_request: branches: [ main ] jobs: test: runs-on: ubuntu-latest services: postgres: image: pgvector/pgvector:pg15 env: POSTGRES_PASSWORD: password POSTGRES_DB: agentflow_test options: \u003e- --health-cmd pg_isready --health-interval 10s --health-timeout 5s --health-retries 5 ports: - 5432:5432 steps: - uses: actions/checkout@v3 - name: Set up Go uses: actions/setup-go@v3 with: go-version: 1.21 - name: Install dependencies run: go mod download - name: Run unit tests run: go test -v -short ./... - name: Run integration tests run: go test -v ./... env: TEST_DB_URL: postgres://postgres:password@localhost:5432/agentflow_test?sslmode=disable - name: Run performance tests run: go test -v -run TestPerformance ./... - name: Generate coverage report run: go test -coverprofile=coverage.out ./... - name: Upload coverage to Codecov uses: codecov/codecov-action@v3 with: file: ./coverage.out\rBest Practices Test Organization // Organize tests by functionality func TestAgentCore(t *testing.T) { t.Run(\"BasicExecution\", testBasicExecution) t.Run(\"ErrorHandling\", testErrorHandling) t.Run(\"StateManagement\", testStateManagement) } func TestAgentIntegration(t *testing.T) { t.Run(\"WithMemory\", testWithMemory) t.Run(\"WithTools\", testWithTools) t.Run(\"WithOrchestration\", testWithOrchestration) } func TestAgentPerformance(t *testing.T) { if testing.Short() { t.Skip(\"Skipping performance tests in short mode\") } t.Run(\"LoadTest\", testLoadTest) t.Run(\"MemoryUsage\", testMemoryUsage) t.Run(\"Concurrency\", testConcurrency) }\rTest Data Management // Use table-driven tests for multiple scenarios func TestDataProcessing(t *testing.T) { testCases := []struct { name string input interface{} expected interface{} wantErr bool }{ // Test cases here... } for _, tc := range testCases { t.Run(tc.name, func(t *testing.T) { // Test implementation... }) } } // Use test fixtures for complex data func loadTestFixture(t *testing.T, filename string) map[string]interface{} { data, err := os.ReadFile(filepath.Join(\"testdata\", filename)) require.NoError(t, err) var result map[string]interface{} err = json.Unmarshal(data, \u0026result) require.NoError(t, err) return result }\rNext Steps Debugging - Debug agent interactions effectively Best Practices - Development best practices Production Deployment - Production deployment",
    "tags": [],
    "title": "testing-agents",
    "uri": "/AgenticGoKitDocs/guides/development/testing-agents/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials \u003e advanced",
    "content": "Testing Strategies for Multi-Agent Systems Comprehensive testing approaches for building reliable and maintainable agent systems\nTesting multi-agent systems presents unique challenges due to their distributed nature, asynchronous communication, and external dependencies. This guide covers testing strategies, patterns, and tools specifically designed for AgenticGoKit applications.\n🧪 Testing Pyramid for Agent Systems flowchart TD\rsubgraph \"End-to-End Tests (10%)\"\rE2E1[Full System Tests]\rE2E2[User Journey Tests]\rE2E3[Performance Tests]\rend\rsubgraph \"Integration Tests (30%)\"\rINT1[Agent Interaction Tests]\rINT2[External Service Tests]\rINT3[Orchestration Tests]\rINT4[Memory System Tests]\rINT5[MCP Tool Tests]\rend\rsubgraph \"Unit Tests (60%)\"\rUNIT1[Individual Agent Tests]\rUNIT2[Business Logic Tests]\rUNIT3[Utility Function Tests]\rUNIT4[Error Handling Tests]\rUNIT5[State Management Tests]\rend\rE2E1 --\u003e INT1\rE2E2 --\u003e INT2\rE2E3 --\u003e INT3\rINT1 --\u003e UNIT1\rINT2 --\u003e UNIT2\rINT3 --\u003e UNIT3\rINT4 --\u003e UNIT4\rINT5 --\u003e UNIT5\rclassDef e2e fill:#ffebee,stroke:#c62828,stroke-width:2px\rclassDef integration fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px\rclassDef unit fill:#e3f2fd,stroke:#1565c0,stroke-width:2px\rclass E2E1,E2E2,E2E3 e2e\rclass INT1,INT2,INT3,INT4,INT5 integration\rclass UNIT1,UNIT2,UNIT3,UNIT4,UNIT5 unit\r🔧 Unit Testing Strategies 1. Testing Individual Agents package agents import ( \"context\" \"testing\" \"time\" \"github.com/stretchr/testify/assert\" \"github.com/stretchr/testify/mock\" \"github.com/kunalkushwaha/agenticgokit/core\" ) // Mock LLM Provider for testing type MockLLMProvider struct { mock.Mock } func (m *MockLLMProvider) GenerateResponse(ctx context.Context, prompt string, options map[string]interface{}) (string, error) { args := m.Called(ctx, prompt, options) return args.String(0), args.Error(1) } func TestAnalyzerAgent_Execute(t *testing.T) { tests := []struct { name string event core.Event state *core.State mockResponse string mockError error expectedResult map[string]interface{} expectedError string }{ { name: \"successful analysis\", event: core.NewEvent(\"analyze\", map[string]interface{}{ \"data\": \"Sample data to analyze\", }), state: \u0026core.State{ SessionID: \"test-session\", Data: map[string]interface{}{ \"context\": \"test context\", }, }, mockResponse: \"Analysis complete: The data shows positive trends\", expectedResult: map[string]interface{}{ \"analysis\": \"Analysis complete: The data shows positive trends\", \"agent\": \"analyzer\", \"status\": \"completed\", }, }, { name: \"empty data handling\", event: core.NewEvent(\"analyze\", map[string]interface{}{ \"data\": \"\", }), state: \u0026core.State{SessionID: \"test-session\"}, expectedError: \"no data provided for analysis\", }, { name: \"LLM provider error\", event: core.NewEvent(\"analyze\", map[string]interface{}{ \"data\": \"Sample data\", }), state: \u0026core.State{SessionID: \"test-session\"}, mockError: errors.New(\"LLM service unavailable\"), expectedError: \"LLM request failed: LLM service unavailable\", }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { // Setup mock mockProvider := new(MockLLMProvider) if tt.mockResponse != \"\" || tt.mockError != nil { mockProvider.On(\"GenerateResponse\", mock.Anything, mock.AnythingOfType(\"string\"), mock.Anything). Return(tt.mockResponse, tt.mockError) } // Create agent with mock agent := NewAnalyzerAgent(\"analyzer\", mockProvider) // Execute test result, err := agent.Execute(context.Background(), tt.event, tt.state) // Assertions if tt.expectedError != \"\" { assert.Error(t, err) assert.Contains(t, err.Error(), tt.expectedError) assert.Nil(t, result) } else { assert.NoError(t, err) assert.NotNil(t, result) for key, expectedValue := range tt.expectedResult { assert.Equal(t, expectedValue, result.Data[key]) } } // Verify mock expectations mockProvider.AssertExpectations(t) }) } }\r2. Testing State Management func TestStateTransformations(t *testing.T) { tests := []struct { name string initialState *core.State transformation func(*core.State) *core.State expectedData map[string]interface{} }{ { name: \"add analysis result\", initialState: \u0026core.State{ SessionID: \"test\", Data: map[string]interface{}{ \"input\": \"test data\", }, }, transformation: func(state *core.State) *core.State { newState := state.Clone() newState.Data[\"analysis\"] = \"completed\" newState.Data[\"confidence\"] = 0.95 return newState }, expectedData: map[string]interface{}{ \"input\": \"test data\", \"analysis\": \"completed\", \"confidence\": 0.95, }, }, { name: \"merge states\", initialState: \u0026core.State{ SessionID: \"test\", Data: map[string]interface{}{ \"step1\": \"completed\", }, }, transformation: func(state *core.State) *core.State { additionalData := map[string]interface{}{ \"step2\": \"completed\", \"final\": true, } return state.Merge(additionalData) }, expectedData: map[string]interface{}{ \"step1\": \"completed\", \"step2\": \"completed\", \"final\": true, }, }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { result := tt.transformation(tt.initialState) for key, expected := range tt.expectedData { assert.Equal(t, expected, result.Data[key]) } }) } }\r3. Testing Error Handling func TestErrorHandling(t *testing.T) { tests := []struct { name string setupMock func(*MockLLMProvider) expectedError string shouldRetry bool }{ { name: \"timeout error\", setupMock: func(m *MockLLMProvider) { m.On(\"GenerateResponse\", mock.Anything, mock.Anything, mock.Anything). Return(\"\", context.DeadlineExceeded) }, expectedError: \"context deadline exceeded\", shouldRetry: true, }, { name: \"rate limit error\", setupMock: func(m *MockLLMProvider) { m.On(\"GenerateResponse\", mock.Anything, mock.Anything, mock.Anything). Return(\"\", errors.New(\"rate limit exceeded\")) }, expectedError: \"rate limit exceeded\", shouldRetry: true, }, { name: \"authentication error\", setupMock: func(m *MockLLMProvider) { m.On(\"GenerateResponse\", mock.Anything, mock.Anything, mock.Anything). Return(\"\", errors.New(\"invalid API key\")) }, expectedError: \"invalid API key\", shouldRetry: false, }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { mockProvider := new(MockLLMProvider) tt.setupMock(mockProvider) agent := NewResilientAgent(\"test\", mockProvider) event := core.NewEvent(\"test\", \"data\") state := \u0026core.State{SessionID: \"test\"} _, err := agent.Execute(context.Background(), event, state) assert.Error(t, err) assert.Contains(t, err.Error(), tt.expectedError) // Test retry behavior if tt.shouldRetry { // Verify that retries were attempted assert.True(t, mockProvider.AssertNumberOfCalls(t, \"GenerateResponse\", 3)) } else { // Verify that no retries were attempted assert.True(t, mockProvider.AssertNumberOfCalls(t, \"GenerateResponse\", 1)) } }) } }\r🔗 Integration Testing 1. Testing Agent Interactions func TestAgentOrchestration(t *testing.T) { // Setup test agents mockProvider := new(MockLLMProvider) mockProvider.On(\"GenerateResponse\", mock.Anything, mock.MatchedBy(func(prompt string) bool { return strings.Contains(prompt, \"analyze\") }), mock.Anything).Return(\"Analysis: Data processed\", nil) mockProvider.On(\"GenerateResponse\", mock.Anything, mock.MatchedBy(func(prompt string) bool { return strings.Contains(prompt, \"validate\") }), mock.Anything).Return(\"Validation: Data is valid\", nil) analyzer := NewAnalyzerAgent(\"analyzer\", mockProvider) validator := NewValidatorAgent(\"validator\", mockProvider) // Create sequential runner agents := map[string]core.AgentHandler{ \"analyzer\": analyzer, \"validator\": validator, } config := core.RunnerConfig{ OrchestrationMode: core.OrchestrationSequential, SequentialAgents: []string{\"analyzer\", \"validator\"}, Timeout: 30 * time.Second, } runner := core.NewRunnerWithConfig(config) for name, agent := range agents { runner.RegisterAgent(name, agent) } // Test sequential execution event := core.NewEvent(\"process\", map[string]interface{}{ \"data\": \"test data for processing\", }) results, err := runner.ProcessEvent(context.Background(), event) assert.NoError(t, err) assert.Len(t, results, 2) // Verify execution order and data flow analyzerResult := results[0] validatorResult := results[1] assert.Equal(t, \"analyzer\", analyzerResult.Agent) assert.Contains(t, analyzerResult.Data[\"analysis\"], \"Data processed\") assert.Equal(t, \"validator\", validatorResult.Agent) assert.Contains(t, validatorResult.Data[\"validation\"], \"Data is valid\") }\r2. Testing Memory System Integration func TestMemorySystemIntegration(t *testing.T) { // Setup in-memory test database memoryProvider := memory.NewInMemoryProvider() // Create agent with memory agent := NewMemoryEnabledAgent(\"test-agent\", mockLLMProvider, memoryProvider) // Test storing and retrieving memories t.Run(\"store and retrieve memory\", func(t *testing.T) { event := core.NewEvent(\"remember\", map[string]interface{}{ \"fact\": \"The sky is blue\", \"category\": \"general_knowledge\", }) state := \u0026core.State{ SessionID: \"test-session\", UserID: \"test-user\", } result, err := agent.Execute(context.Background(), event, state) assert.NoError(t, err) assert.Equal(t, \"stored\", result.Data[\"status\"]) // Verify memory was stored memories, err := memoryProvider.Search(context.Background(), \"sky blue\", 5) assert.NoError(t, err) assert.Len(t, memories, 1) assert.Contains(t, memories[0].Content, \"sky is blue\") }) t.Run(\"RAG retrieval\", func(t *testing.T) { // Setup mock to expect context from memory mockProvider.On(\"GenerateResponse\", mock.Anything, mock.MatchedBy(func(prompt string) bool { return strings.Contains(prompt, \"sky is blue\") }), mock.Anything).Return(\"Based on the stored knowledge, the sky appears blue due to light scattering\", nil) event := core.NewEvent(\"query\", map[string]interface{}{ \"question\": \"What color is the sky?\", }) state := \u0026core.State{SessionID: \"test-session\"} result, err := agent.Execute(context.Background(), event, state) assert.NoError(t, err) assert.Contains(t, result.Data[\"response\"], \"blue due to light scattering\") }) }\r3. Testing MCP Tool Integration func TestMCPToolIntegration(t *testing.T) { // Setup mock MCP server mockServer := \u0026MockMCPServer{} mockServer.On(\"ExecuteTool\", \"web_search\", map[string]interface{}{ \"query\": \"AgenticGoKit testing\", }).Return(map[string]interface{}{ \"results\": []string{\"AgenticGoKit is a Go framework for building multi-agent systems\"}, }, nil) // Create agent with MCP tools toolManager := mcp.NewToolManager() toolManager.RegisterServer(\"test-server\", mockServer) agent := NewToolEnabledAgent(\"test-agent\", mockLLMProvider, toolManager) event := core.NewEvent(\"search\", map[string]interface{}{ \"query\": \"AgenticGoKit testing\", \"tool\": \"web_search\", }) state := \u0026core.State{SessionID: \"test-session\"} result, err := agent.Execute(context.Background(), event, state) assert.NoError(t, err) assert.Contains(t, result.Data[\"search_results\"], \"Go framework\") mockServer.AssertExpectations(t) }\r🌐 End-to-End Testing 1. Full System Tests func TestFullSystemWorkflow(t *testing.T) { // Setup complete system system := setupTestSystem(t) defer system.Cleanup() // Test complete user workflow t.Run(\"research assistant workflow\", func(t *testing.T) { // Step 1: User asks a question request := \u0026api.Request{ Type: \"research_query\", Data: map[string]interface{}{ \"question\": \"What are the latest developments in AI agent frameworks?\", \"depth\": \"comprehensive\", }, UserID: \"test-user\", SessionID: \"test-session\", } response, err := system.ProcessRequest(context.Background(), request) assert.NoError(t, err) // Verify response structure assert.NotNil(t, response) assert.Equal(t, \"success\", response.Status) assert.Contains(t, response.Data, \"research_results\") assert.Contains(t, response.Data, \"sources\") assert.Contains(t, response.Data, \"summary\") // Verify agent execution trace trace := response.Trace assert.NotEmpty(t, trace) // Should have executed: researcher -\u003e analyzer -\u003e validator -\u003e formatter expectedAgents := []string{\"researcher\", \"analyzer\", \"validator\", \"formatter\"} assert.Len(t, trace, len(expectedAgents)) for i, expectedAgent := range expectedAgents { assert.Equal(t, expectedAgent, trace[i].Agent) assert.Equal(t, \"completed\", trace[i].Status) } }) t.Run(\"error handling workflow\", func(t *testing.T) { // Simulate external service failure system.SimulateServiceFailure(\"web_search\") request := \u0026api.Request{ Type: \"research_query\", Data: map[string]interface{}{ \"question\": \"Test question\", }, UserID: \"test-user\", SessionID: \"test-session-2\", } response, err := system.ProcessRequest(context.Background(), request) // Should handle gracefully with fallback assert.NoError(t, err) assert.Equal(t, \"partial_success\", response.Status) assert.Contains(t, response.Data, \"fallback_response\") assert.Contains(t, response.Warnings, \"web search unavailable\") }) }\r2. Performance Testing func TestSystemPerformance(t *testing.T) { system := setupTestSystem(t) defer system.Cleanup() t.Run(\"concurrent request handling\", func(t *testing.T) { concurrency := 10 requestsPerWorker := 5 var wg sync.WaitGroup results := make(chan time.Duration, concurrency*requestsPerWorker) errors := make(chan error, concurrency*requestsPerWorker) for i := 0; i \u003c concurrency; i++ { wg.Add(1) go func(workerID int) { defer wg.Done() for j := 0; j \u003c requestsPerWorker; j++ { start := time.Now() request := \u0026api.Request{ Type: \"simple_query\", Data: map[string]interface{}{ \"question\": fmt.Sprintf(\"Test question %d-%d\", workerID, j), }, UserID: fmt.Sprintf(\"user-%d\", workerID), SessionID: fmt.Sprintf(\"session-%d-%d\", workerID, j), } _, err := system.ProcessRequest(context.Background(), request) duration := time.Since(start) if err != nil { errors \u003c- err } else { results \u003c- duration } } }(i) } wg.Wait() close(results) close(errors) // Collect results var durations []time.Duration for duration := range results { durations = append(durations, duration) } var errorList []error for err := range errors { errorList = append(errorList, err) } // Assertions assert.Empty(t, errorList, \"No errors should occur during concurrent processing\") assert.Len(t, durations, concurrency*requestsPerWorker) // Performance assertions avgDuration := calculateAverage(durations) maxDuration := calculateMax(durations) assert.Less(t, avgDuration, 5*time.Second, \"Average response time should be under 5 seconds\") assert.Less(t, maxDuration, 10*time.Second, \"Max response time should be under 10 seconds\") t.Logf(\"Performance results: avg=%v, max=%v, total_requests=%d\", avgDuration, maxDuration, len(durations)) }) }\r🎭 Test Doubles and Mocking 1. Mock External Services type MockExternalService struct { mock.Mock responses map[string]interface{} delays map[string]time.Duration failures map[string]error } func NewMockExternalService() *MockExternalService { return \u0026MockExternalService{ responses: make(map[string]interface{}), delays: make(map[string]time.Duration), failures: make(map[string]error), } } func (m *MockExternalService) SetResponse(endpoint string, response interface{}) { m.responses[endpoint] = response } func (m *MockExternalService) SetDelay(endpoint string, delay time.Duration) { m.delays[endpoint] = delay } func (m *MockExternalService) SetFailure(endpoint string, err error) { m.failures[endpoint] = err } func (m *MockExternalService) Call(ctx context.Context, endpoint string, params map[string]interface{}) (interface{}, error) { // Simulate delay if delay, exists := m.delays[endpoint]; exists { select { case \u003c-time.After(delay): case \u003c-ctx.Done(): return nil, ctx.Err() } } // Simulate failure if err, exists := m.failures[endpoint]; exists { return nil, err } // Return mock response if response, exists := m.responses[endpoint]; exists { return response, nil } return nil, errors.New(\"endpoint not mocked\") }\r2. Test Fixtures and Builders type TestSystemBuilder struct { agents map[string]core.AgentHandler mockServices map[string]*MockExternalService config core.RunnerConfig } func NewTestSystemBuilder() *TestSystemBuilder { return \u0026TestSystemBuilder{ agents: make(map[string]core.AgentHandler), mockServices: make(map[string]*MockExternalService), config: core.RunnerConfig{ OrchestrationMode: core.OrchestrationSequential, Timeout: 30 * time.Second, }, } } func (b *TestSystemBuilder) WithAgent(name string, agent core.AgentHandler) *TestSystemBuilder { b.agents[name] = agent return b } func (b *TestSystemBuilder) WithMockService(name string, service *MockExternalService) *TestSystemBuilder { b.mockServices[name] = service return b } func (b *TestSystemBuilder) WithOrchestration(mode core.OrchestrationMode, agents []string) *TestSystemBuilder { b.config.OrchestrationMode = mode switch mode { case core.OrchestrationSequential: b.config.SequentialAgents = agents case core.OrchestrationCollaborative: b.config.CollaborativeAgents = agents } return b } func (b *TestSystemBuilder) Build() *TestSystem { runner := core.NewRunnerWithConfig(b.config) for name, agent := range b.agents { runner.RegisterAgent(name, agent) } return \u0026TestSystem{ runner: runner, mockServices: b.mockServices, } } type TestSystem struct { runner core.Runner mockServices map[string]*MockExternalService } func (ts *TestSystem) ProcessRequest(ctx context.Context, request *api.Request) (*api.Response, error) { event := core.NewEvent(request.Type, request.Data) state := \u0026core.State{ SessionID: request.SessionID, UserID: request.UserID, } results, err := ts.runner.ProcessEvent(ctx, event) if err != nil { return nil, err } return \u0026api.Response{ Status: \"success\", Data: results[len(results)-1].Data, Trace: convertToTrace(results), }, nil } func (ts *TestSystem) SimulateServiceFailure(serviceName string) { if service, exists := ts.mockServices[serviceName]; exists { service.SetFailure(\"*\", errors.New(\"simulated service failure\")) } } func (ts *TestSystem) Cleanup() { // Cleanup resources for _, service := range ts.mockServices { service.AssertExpectations(\u0026testing.T{}) } }\r📊 Test Data Management 1. Test Data Builders type EventBuilder struct { eventType string data map[string]interface{} metadata map[string]interface{} } func NewEventBuilder(eventType string) *EventBuilder { return \u0026EventBuilder{ eventType: eventType, data: make(map[string]interface{}), metadata: make(map[string]interface{}), } } func (b *EventBuilder) WithData(key string, value interface{}) *EventBuilder { b.data[key] = value return b } func (b *EventBuilder) WithMetadata(key string, value interface{}) *EventBuilder { b.metadata[key] = value return b } func (b *EventBuilder) Build() core.Event { event := core.NewEvent(b.eventType, b.data) for key, value := range b.metadata { event.Metadata[key] = value } return event } type StateBuilder struct { sessionID string userID string data map[string]interface{} } func NewStateBuilder() *StateBuilder { return \u0026StateBuilder{ data: make(map[string]interface{}), } } func (b *StateBuilder) WithSession(sessionID string) *StateBuilder { b.sessionID = sessionID return b } func (b *StateBuilder) WithUser(userID string) *StateBuilder { b.userID = userID return b } func (b *StateBuilder) WithData(key string, value interface{}) *StateBuilder { b.data[key] = value return b } func (b *StateBuilder) Build() *core.State { return \u0026core.State{ SessionID: b.sessionID, UserID: b.userID, Data: b.data, } }\r2. Test Scenarios func TestCommonScenarios(t *testing.T) { scenarios := []struct { name string setup func() (*TestSystem, core.Event, *core.State) validate func(*testing.T, *core.AgentResult, error) }{ { name: \"successful_analysis\", setup: func() (*TestSystem, core.Event, *core.State) { system := NewTestSystemBuilder(). WithAgent(\"analyzer\", NewMockAnalyzer(\"success\")). Build() event := NewEventBuilder(\"analyze\"). WithData(\"content\", \"test data\"). Build() state := NewStateBuilder(). WithSession(\"test-session\"). Build() return system, event, state }, validate: func(t *testing.T, result *core.AgentResult, err error) { assert.NoError(t, err) assert.Equal(t, \"completed\", result.Data[\"status\"]) }, }, { name: \"timeout_handling\", setup: func() (*TestSystem, core.Event, *core.State) { system := NewTestSystemBuilder(). WithAgent(\"slow_agent\", NewSlowMockAgent(10*time.Second)). Build() event := NewEventBuilder(\"process\"). WithData(\"timeout\", \"5s\"). Build() state := NewStateBuilder(). WithSession(\"timeout-test\"). Build() return system, event, state }, validate: func(t *testing.T, result *core.AgentResult, err error) { assert.Error(t, err) assert.Contains(t, err.Error(), \"timeout\") }, }, } for _, scenario := range scenarios { t.Run(scenario.name, func(t *testing.T) { system, event, state := scenario.setup() defer system.Cleanup() result, err := system.ProcessSingleEvent(context.Background(), event, state) scenario.validate(t, result, err) }) } }\r🎯 Best Practices 1. Test Organization // Package structure for tests // agents/ // analyzer_test.go - Unit tests for analyzer agent // validator_test.go - Unit tests for validator agent // integration_test.go - Integration tests between agents // orchestration/ // sequential_test.go - Sequential orchestration tests // collaborative_test.go - Collaborative orchestration tests // system/ // e2e_test.go - End-to-end system tests // performance_test.go - Performance and load tests // testutils/ // builders.go - Test data builders // mocks.go - Mock implementations // fixtures.go - Test fixtures and helpers\r2. Test Configuration // Test configuration management type TestConfig struct { DatabaseURL string RedisURL string MockServices bool LogLevel string TestTimeout time.Duration } func LoadTestConfig() *TestConfig { return \u0026TestConfig{ DatabaseURL: getEnvOrDefault(\"TEST_DATABASE_URL\", \"postgres://localhost/agentflow_test\"), RedisURL: getEnvOrDefault(\"TEST_REDIS_URL\", \"redis://localhost:6379/1\"), MockServices: getEnvOrDefault(\"MOCK_SERVICES\", \"true\") == \"true\", LogLevel: getEnvOrDefault(\"TEST_LOG_LEVEL\", \"error\"), TestTimeout: 30 * time.Second, } }\r3. Continuous Integration # .github/workflows/test.yml name: Test Suite on: [push, pull_request] jobs: unit-tests: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - uses: actions/setup-go@v3 with: go-version: '1.21' - name: Run unit tests run: go test -v -race -coverprofile=coverage.out ./... - name: Upload coverage uses: codecov/codecov-action@v3 integration-tests: runs-on: ubuntu-latest services: postgres: image: pgvector/pgvector:pg15 env: POSTGRES_PASSWORD: test POSTGRES_DB: agentflow_test options: \u003e- --health-cmd pg_isready --health-interval 10s --health-timeout 5s --health-retries 5 redis: image: redis:7-alpine options: \u003e- --health-cmd \"redis-cli ping\" --health-interval 10s --health-timeout 5s --health-retries 5 steps: - uses: actions/checkout@v3 - uses: actions/setup-go@v3 with: go-version: '1.21' - name: Run integration tests run: go test -v -tags=integration ./... env: TEST_DATABASE_URL: postgres://postgres:test@localhost:5432/agentflow_test TEST_REDIS_URL: redis://localhost:6379/1 e2e-tests: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - uses: actions/setup-go@v3 with: go-version: '1.21' - name: Start test environment run: docker-compose -f docker-compose.test.yml up -d - name: Wait for services run: ./scripts/wait-for-services.sh - name: Run E2E tests run: go test -v -tags=e2e ./... - name: Cleanup run: docker-compose -f docker-compose.test.yml down\rTesting multi-agent systems requires a comprehensive approach that covers unit, integration, and end-to-end scenarios. By following these strategies and patterns, you can build reliable and maintainable agent systems with confidence.\n🚀 Next Steps Circuit Breaker Patterns - Add fault tolerance to your tested systems Load Balancing and Scaling - Test scaled deployments Production Monitoring - Monitor your tested systems in production Debugging Guide - Debug issues found during testing",
    "description": "Testing Strategies for Multi-Agent Systems Comprehensive testing approaches for building reliable and maintainable agent systems\nTesting multi-agent systems presents unique challenges due to their distributed nature, asynchronous communication, and external dependencies. This guide covers testing strategies, patterns, and tools specifically designed for AgenticGoKit applications.\n🧪 Testing Pyramid for Agent Systems flowchart TD\rsubgraph \"End-to-End Tests (10%)\"\rE2E1[Full System Tests]\rE2E2[User Journey Tests]\rE2E3[Performance Tests]\rend\rsubgraph \"Integration Tests (30%)\"\rINT1[Agent Interaction Tests]\rINT2[External Service Tests]\rINT3[Orchestration Tests]\rINT4[Memory System Tests]\rINT5[MCP Tool Tests]\rend\rsubgraph \"Unit Tests (60%)\"\rUNIT1[Individual Agent Tests]\rUNIT2[Business Logic Tests]\rUNIT3[Utility Function Tests]\rUNIT4[Error Handling Tests]\rUNIT5[State Management Tests]\rend\rE2E1 --\u003e INT1\rE2E2 --\u003e INT2\rE2E3 --\u003e INT3\rINT1 --\u003e UNIT1\rINT2 --\u003e UNIT2\rINT3 --\u003e UNIT3\rINT4 --\u003e UNIT4\rINT5 --\u003e UNIT5\rclassDef e2e fill:#ffebee,stroke:#c62828,stroke-width:2px\rclassDef integration fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px\rclassDef unit fill:#e3f2fd,stroke:#1565c0,stroke-width:2px\rclass E2E1,E2E2,E2E3 e2e\rclass INT1,INT2,INT3,INT4,INT5 integration\rclass UNIT1,UNIT2,UNIT3,UNIT4,UNIT5 unit\r🔧 Unit Testing Strategies 1. Testing Individual Agents package agents import ( \"context\" \"testing\" \"time\" \"github.com/stretchr/testify/assert\" \"github.com/stretchr/testify/mock\" \"github.com/kunalkushwaha/agenticgokit/core\" ) // Mock LLM Provider for testing type MockLLMProvider struct { mock.Mock } func (m *MockLLMProvider) GenerateResponse(ctx context.Context, prompt string, options map[string]interface{}) (string, error) { args := m.Called(ctx, prompt, options) return args.String(0), args.Error(1) } func TestAnalyzerAgent_Execute(t *testing.T) { tests := []struct { name string event core.Event state *core.State mockResponse string mockError error expectedResult map[string]interface{} expectedError string }{ { name: \"successful analysis\", event: core.NewEvent(\"analyze\", map[string]interface{}{ \"data\": \"Sample data to analyze\", }), state: \u0026core.State{ SessionID: \"test-session\", Data: map[string]interface{}{ \"context\": \"test context\", }, }, mockResponse: \"Analysis complete: The data shows positive trends\", expectedResult: map[string]interface{}{ \"analysis\": \"Analysis complete: The data shows positive trends\", \"agent\": \"analyzer\", \"status\": \"completed\", }, }, { name: \"empty data handling\", event: core.NewEvent(\"analyze\", map[string]interface{}{ \"data\": \"\", }), state: \u0026core.State{SessionID: \"test-session\"}, expectedError: \"no data provided for analysis\", }, { name: \"LLM provider error\", event: core.NewEvent(\"analyze\", map[string]interface{}{ \"data\": \"Sample data\", }), state: \u0026core.State{SessionID: \"test-session\"}, mockError: errors.New(\"LLM service unavailable\"), expectedError: \"LLM request failed: LLM service unavailable\", }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { // Setup mock mockProvider := new(MockLLMProvider) if tt.mockResponse != \"\" || tt.mockError != nil { mockProvider.On(\"GenerateResponse\", mock.Anything, mock.AnythingOfType(\"string\"), mock.Anything). Return(tt.mockResponse, tt.mockError) } // Create agent with mock agent := NewAnalyzerAgent(\"analyzer\", mockProvider) // Execute test result, err := agent.Execute(context.Background(), tt.event, tt.state) // Assertions if tt.expectedError != \"\" { assert.Error(t, err) assert.Contains(t, err.Error(), tt.expectedError) assert.Nil(t, result) } else { assert.NoError(t, err) assert.NotNil(t, result) for key, expectedValue := range tt.expectedResult { assert.Equal(t, expectedValue, result.Data[key]) } } // Verify mock expectations mockProvider.AssertExpectations(t) }) } }\r2. Testing State Management func TestStateTransformations(t *testing.T) { tests := []struct { name string initialState *core.State transformation func(*core.State) *core.State expectedData map[string]interface{} }{ { name: \"add analysis result\", initialState: \u0026core.State{ SessionID: \"test\", Data: map[string]interface{}{ \"input\": \"test data\", }, }, transformation: func(state *core.State) *core.State { newState := state.Clone() newState.Data[\"analysis\"] = \"completed\" newState.Data[\"confidence\"] = 0.95 return newState }, expectedData: map[string]interface{}{ \"input\": \"test data\", \"analysis\": \"completed\", \"confidence\": 0.95, }, }, { name: \"merge states\", initialState: \u0026core.State{ SessionID: \"test\", Data: map[string]interface{}{ \"step1\": \"completed\", }, }, transformation: func(state *core.State) *core.State { additionalData := map[string]interface{}{ \"step2\": \"completed\", \"final\": true, } return state.Merge(additionalData) }, expectedData: map[string]interface{}{ \"step1\": \"completed\", \"step2\": \"completed\", \"final\": true, }, }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { result := tt.transformation(tt.initialState) for key, expected := range tt.expectedData { assert.Equal(t, expected, result.Data[key]) } }) } }\r3. Testing Error Handling func TestErrorHandling(t *testing.T) { tests := []struct { name string setupMock func(*MockLLMProvider) expectedError string shouldRetry bool }{ { name: \"timeout error\", setupMock: func(m *MockLLMProvider) { m.On(\"GenerateResponse\", mock.Anything, mock.Anything, mock.Anything). Return(\"\", context.DeadlineExceeded) }, expectedError: \"context deadline exceeded\", shouldRetry: true, }, { name: \"rate limit error\", setupMock: func(m *MockLLMProvider) { m.On(\"GenerateResponse\", mock.Anything, mock.Anything, mock.Anything). Return(\"\", errors.New(\"rate limit exceeded\")) }, expectedError: \"rate limit exceeded\", shouldRetry: true, }, { name: \"authentication error\", setupMock: func(m *MockLLMProvider) { m.On(\"GenerateResponse\", mock.Anything, mock.Anything, mock.Anything). Return(\"\", errors.New(\"invalid API key\")) }, expectedError: \"invalid API key\", shouldRetry: false, }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { mockProvider := new(MockLLMProvider) tt.setupMock(mockProvider) agent := NewResilientAgent(\"test\", mockProvider) event := core.NewEvent(\"test\", \"data\") state := \u0026core.State{SessionID: \"test\"} _, err := agent.Execute(context.Background(), event, state) assert.Error(t, err) assert.Contains(t, err.Error(), tt.expectedError) // Test retry behavior if tt.shouldRetry { // Verify that retries were attempted assert.True(t, mockProvider.AssertNumberOfCalls(t, \"GenerateResponse\", 3)) } else { // Verify that no retries were attempted assert.True(t, mockProvider.AssertNumberOfCalls(t, \"GenerateResponse\", 1)) } }) } }\r🔗 Integration Testing 1. Testing Agent Interactions func TestAgentOrchestration(t *testing.T) { // Setup test agents mockProvider := new(MockLLMProvider) mockProvider.On(\"GenerateResponse\", mock.Anything, mock.MatchedBy(func(prompt string) bool { return strings.Contains(prompt, \"analyze\") }), mock.Anything).Return(\"Analysis: Data processed\", nil) mockProvider.On(\"GenerateResponse\", mock.Anything, mock.MatchedBy(func(prompt string) bool { return strings.Contains(prompt, \"validate\") }), mock.Anything).Return(\"Validation: Data is valid\", nil) analyzer := NewAnalyzerAgent(\"analyzer\", mockProvider) validator := NewValidatorAgent(\"validator\", mockProvider) // Create sequential runner agents := map[string]core.AgentHandler{ \"analyzer\": analyzer, \"validator\": validator, } config := core.RunnerConfig{ OrchestrationMode: core.OrchestrationSequential, SequentialAgents: []string{\"analyzer\", \"validator\"}, Timeout: 30 * time.Second, } runner := core.NewRunnerWithConfig(config) for name, agent := range agents { runner.RegisterAgent(name, agent) } // Test sequential execution event := core.NewEvent(\"process\", map[string]interface{}{ \"data\": \"test data for processing\", }) results, err := runner.ProcessEvent(context.Background(), event) assert.NoError(t, err) assert.Len(t, results, 2) // Verify execution order and data flow analyzerResult := results[0] validatorResult := results[1] assert.Equal(t, \"analyzer\", analyzerResult.Agent) assert.Contains(t, analyzerResult.Data[\"analysis\"], \"Data processed\") assert.Equal(t, \"validator\", validatorResult.Agent) assert.Contains(t, validatorResult.Data[\"validation\"], \"Data is valid\") }\r2. Testing Memory System Integration func TestMemorySystemIntegration(t *testing.T) { // Setup in-memory test database memoryProvider := memory.NewInMemoryProvider() // Create agent with memory agent := NewMemoryEnabledAgent(\"test-agent\", mockLLMProvider, memoryProvider) // Test storing and retrieving memories t.Run(\"store and retrieve memory\", func(t *testing.T) { event := core.NewEvent(\"remember\", map[string]interface{}{ \"fact\": \"The sky is blue\", \"category\": \"general_knowledge\", }) state := \u0026core.State{ SessionID: \"test-session\", UserID: \"test-user\", } result, err := agent.Execute(context.Background(), event, state) assert.NoError(t, err) assert.Equal(t, \"stored\", result.Data[\"status\"]) // Verify memory was stored memories, err := memoryProvider.Search(context.Background(), \"sky blue\", 5) assert.NoError(t, err) assert.Len(t, memories, 1) assert.Contains(t, memories[0].Content, \"sky is blue\") }) t.Run(\"RAG retrieval\", func(t *testing.T) { // Setup mock to expect context from memory mockProvider.On(\"GenerateResponse\", mock.Anything, mock.MatchedBy(func(prompt string) bool { return strings.Contains(prompt, \"sky is blue\") }), mock.Anything).Return(\"Based on the stored knowledge, the sky appears blue due to light scattering\", nil) event := core.NewEvent(\"query\", map[string]interface{}{ \"question\": \"What color is the sky?\", }) state := \u0026core.State{SessionID: \"test-session\"} result, err := agent.Execute(context.Background(), event, state) assert.NoError(t, err) assert.Contains(t, result.Data[\"response\"], \"blue due to light scattering\") }) }\r3. Testing MCP Tool Integration func TestMCPToolIntegration(t *testing.T) { // Setup mock MCP server mockServer := \u0026MockMCPServer{} mockServer.On(\"ExecuteTool\", \"web_search\", map[string]interface{}{ \"query\": \"AgenticGoKit testing\", }).Return(map[string]interface{}{ \"results\": []string{\"AgenticGoKit is a Go framework for building multi-agent systems\"}, }, nil) // Create agent with MCP tools toolManager := mcp.NewToolManager() toolManager.RegisterServer(\"test-server\", mockServer) agent := NewToolEnabledAgent(\"test-agent\", mockLLMProvider, toolManager) event := core.NewEvent(\"search\", map[string]interface{}{ \"query\": \"AgenticGoKit testing\", \"tool\": \"web_search\", }) state := \u0026core.State{SessionID: \"test-session\"} result, err := agent.Execute(context.Background(), event, state) assert.NoError(t, err) assert.Contains(t, result.Data[\"search_results\"], \"Go framework\") mockServer.AssertExpectations(t) }\r🌐 End-to-End Testing 1. Full System Tests func TestFullSystemWorkflow(t *testing.T) { // Setup complete system system := setupTestSystem(t) defer system.Cleanup() // Test complete user workflow t.Run(\"research assistant workflow\", func(t *testing.T) { // Step 1: User asks a question request := \u0026api.Request{ Type: \"research_query\", Data: map[string]interface{}{ \"question\": \"What are the latest developments in AI agent frameworks?\", \"depth\": \"comprehensive\", }, UserID: \"test-user\", SessionID: \"test-session\", } response, err := system.ProcessRequest(context.Background(), request) assert.NoError(t, err) // Verify response structure assert.NotNil(t, response) assert.Equal(t, \"success\", response.Status) assert.Contains(t, response.Data, \"research_results\") assert.Contains(t, response.Data, \"sources\") assert.Contains(t, response.Data, \"summary\") // Verify agent execution trace trace := response.Trace assert.NotEmpty(t, trace) // Should have executed: researcher -\u003e analyzer -\u003e validator -\u003e formatter expectedAgents := []string{\"researcher\", \"analyzer\", \"validator\", \"formatter\"} assert.Len(t, trace, len(expectedAgents)) for i, expectedAgent := range expectedAgents { assert.Equal(t, expectedAgent, trace[i].Agent) assert.Equal(t, \"completed\", trace[i].Status) } }) t.Run(\"error handling workflow\", func(t *testing.T) { // Simulate external service failure system.SimulateServiceFailure(\"web_search\") request := \u0026api.Request{ Type: \"research_query\", Data: map[string]interface{}{ \"question\": \"Test question\", }, UserID: \"test-user\", SessionID: \"test-session-2\", } response, err := system.ProcessRequest(context.Background(), request) // Should handle gracefully with fallback assert.NoError(t, err) assert.Equal(t, \"partial_success\", response.Status) assert.Contains(t, response.Data, \"fallback_response\") assert.Contains(t, response.Warnings, \"web search unavailable\") }) }\r2. Performance Testing func TestSystemPerformance(t *testing.T) { system := setupTestSystem(t) defer system.Cleanup() t.Run(\"concurrent request handling\", func(t *testing.T) { concurrency := 10 requestsPerWorker := 5 var wg sync.WaitGroup results := make(chan time.Duration, concurrency*requestsPerWorker) errors := make(chan error, concurrency*requestsPerWorker) for i := 0; i \u003c concurrency; i++ { wg.Add(1) go func(workerID int) { defer wg.Done() for j := 0; j \u003c requestsPerWorker; j++ { start := time.Now() request := \u0026api.Request{ Type: \"simple_query\", Data: map[string]interface{}{ \"question\": fmt.Sprintf(\"Test question %d-%d\", workerID, j), }, UserID: fmt.Sprintf(\"user-%d\", workerID), SessionID: fmt.Sprintf(\"session-%d-%d\", workerID, j), } _, err := system.ProcessRequest(context.Background(), request) duration := time.Since(start) if err != nil { errors \u003c- err } else { results \u003c- duration } } }(i) } wg.Wait() close(results) close(errors) // Collect results var durations []time.Duration for duration := range results { durations = append(durations, duration) } var errorList []error for err := range errors { errorList = append(errorList, err) } // Assertions assert.Empty(t, errorList, \"No errors should occur during concurrent processing\") assert.Len(t, durations, concurrency*requestsPerWorker) // Performance assertions avgDuration := calculateAverage(durations) maxDuration := calculateMax(durations) assert.Less(t, avgDuration, 5*time.Second, \"Average response time should be under 5 seconds\") assert.Less(t, maxDuration, 10*time.Second, \"Max response time should be under 10 seconds\") t.Logf(\"Performance results: avg=%v, max=%v, total_requests=%d\", avgDuration, maxDuration, len(durations)) }) }\r🎭 Test Doubles and Mocking 1. Mock External Services type MockExternalService struct { mock.Mock responses map[string]interface{} delays map[string]time.Duration failures map[string]error } func NewMockExternalService() *MockExternalService { return \u0026MockExternalService{ responses: make(map[string]interface{}), delays: make(map[string]time.Duration), failures: make(map[string]error), } } func (m *MockExternalService) SetResponse(endpoint string, response interface{}) { m.responses[endpoint] = response } func (m *MockExternalService) SetDelay(endpoint string, delay time.Duration) { m.delays[endpoint] = delay } func (m *MockExternalService) SetFailure(endpoint string, err error) { m.failures[endpoint] = err } func (m *MockExternalService) Call(ctx context.Context, endpoint string, params map[string]interface{}) (interface{}, error) { // Simulate delay if delay, exists := m.delays[endpoint]; exists { select { case \u003c-time.After(delay): case \u003c-ctx.Done(): return nil, ctx.Err() } } // Simulate failure if err, exists := m.failures[endpoint]; exists { return nil, err } // Return mock response if response, exists := m.responses[endpoint]; exists { return response, nil } return nil, errors.New(\"endpoint not mocked\") }\r2. Test Fixtures and Builders type TestSystemBuilder struct { agents map[string]core.AgentHandler mockServices map[string]*MockExternalService config core.RunnerConfig } func NewTestSystemBuilder() *TestSystemBuilder { return \u0026TestSystemBuilder{ agents: make(map[string]core.AgentHandler), mockServices: make(map[string]*MockExternalService), config: core.RunnerConfig{ OrchestrationMode: core.OrchestrationSequential, Timeout: 30 * time.Second, }, } } func (b *TestSystemBuilder) WithAgent(name string, agent core.AgentHandler) *TestSystemBuilder { b.agents[name] = agent return b } func (b *TestSystemBuilder) WithMockService(name string, service *MockExternalService) *TestSystemBuilder { b.mockServices[name] = service return b } func (b *TestSystemBuilder) WithOrchestration(mode core.OrchestrationMode, agents []string) *TestSystemBuilder { b.config.OrchestrationMode = mode switch mode { case core.OrchestrationSequential: b.config.SequentialAgents = agents case core.OrchestrationCollaborative: b.config.CollaborativeAgents = agents } return b } func (b *TestSystemBuilder) Build() *TestSystem { runner := core.NewRunnerWithConfig(b.config) for name, agent := range b.agents { runner.RegisterAgent(name, agent) } return \u0026TestSystem{ runner: runner, mockServices: b.mockServices, } } type TestSystem struct { runner core.Runner mockServices map[string]*MockExternalService } func (ts *TestSystem) ProcessRequest(ctx context.Context, request *api.Request) (*api.Response, error) { event := core.NewEvent(request.Type, request.Data) state := \u0026core.State{ SessionID: request.SessionID, UserID: request.UserID, } results, err := ts.runner.ProcessEvent(ctx, event) if err != nil { return nil, err } return \u0026api.Response{ Status: \"success\", Data: results[len(results)-1].Data, Trace: convertToTrace(results), }, nil } func (ts *TestSystem) SimulateServiceFailure(serviceName string) { if service, exists := ts.mockServices[serviceName]; exists { service.SetFailure(\"*\", errors.New(\"simulated service failure\")) } } func (ts *TestSystem) Cleanup() { // Cleanup resources for _, service := range ts.mockServices { service.AssertExpectations(\u0026testing.T{}) } }\r📊 Test Data Management 1. Test Data Builders type EventBuilder struct { eventType string data map[string]interface{} metadata map[string]interface{} } func NewEventBuilder(eventType string) *EventBuilder { return \u0026EventBuilder{ eventType: eventType, data: make(map[string]interface{}), metadata: make(map[string]interface{}), } } func (b *EventBuilder) WithData(key string, value interface{}) *EventBuilder { b.data[key] = value return b } func (b *EventBuilder) WithMetadata(key string, value interface{}) *EventBuilder { b.metadata[key] = value return b } func (b *EventBuilder) Build() core.Event { event := core.NewEvent(b.eventType, b.data) for key, value := range b.metadata { event.Metadata[key] = value } return event } type StateBuilder struct { sessionID string userID string data map[string]interface{} } func NewStateBuilder() *StateBuilder { return \u0026StateBuilder{ data: make(map[string]interface{}), } } func (b *StateBuilder) WithSession(sessionID string) *StateBuilder { b.sessionID = sessionID return b } func (b *StateBuilder) WithUser(userID string) *StateBuilder { b.userID = userID return b } func (b *StateBuilder) WithData(key string, value interface{}) *StateBuilder { b.data[key] = value return b } func (b *StateBuilder) Build() *core.State { return \u0026core.State{ SessionID: b.sessionID, UserID: b.userID, Data: b.data, } }\r2. Test Scenarios func TestCommonScenarios(t *testing.T) { scenarios := []struct { name string setup func() (*TestSystem, core.Event, *core.State) validate func(*testing.T, *core.AgentResult, error) }{ { name: \"successful_analysis\", setup: func() (*TestSystem, core.Event, *core.State) { system := NewTestSystemBuilder(). WithAgent(\"analyzer\", NewMockAnalyzer(\"success\")). Build() event := NewEventBuilder(\"analyze\"). WithData(\"content\", \"test data\"). Build() state := NewStateBuilder(). WithSession(\"test-session\"). Build() return system, event, state }, validate: func(t *testing.T, result *core.AgentResult, err error) { assert.NoError(t, err) assert.Equal(t, \"completed\", result.Data[\"status\"]) }, }, { name: \"timeout_handling\", setup: func() (*TestSystem, core.Event, *core.State) { system := NewTestSystemBuilder(). WithAgent(\"slow_agent\", NewSlowMockAgent(10*time.Second)). Build() event := NewEventBuilder(\"process\"). WithData(\"timeout\", \"5s\"). Build() state := NewStateBuilder(). WithSession(\"timeout-test\"). Build() return system, event, state }, validate: func(t *testing.T, result *core.AgentResult, err error) { assert.Error(t, err) assert.Contains(t, err.Error(), \"timeout\") }, }, } for _, scenario := range scenarios { t.Run(scenario.name, func(t *testing.T) { system, event, state := scenario.setup() defer system.Cleanup() result, err := system.ProcessSingleEvent(context.Background(), event, state) scenario.validate(t, result, err) }) } }\r🎯 Best Practices 1. Test Organization // Package structure for tests // agents/ // analyzer_test.go - Unit tests for analyzer agent // validator_test.go - Unit tests for validator agent // integration_test.go - Integration tests between agents // orchestration/ // sequential_test.go - Sequential orchestration tests // collaborative_test.go - Collaborative orchestration tests // system/ // e2e_test.go - End-to-end system tests // performance_test.go - Performance and load tests // testutils/ // builders.go - Test data builders // mocks.go - Mock implementations // fixtures.go - Test fixtures and helpers\r2. Test Configuration // Test configuration management type TestConfig struct { DatabaseURL string RedisURL string MockServices bool LogLevel string TestTimeout time.Duration } func LoadTestConfig() *TestConfig { return \u0026TestConfig{ DatabaseURL: getEnvOrDefault(\"TEST_DATABASE_URL\", \"postgres://localhost/agentflow_test\"), RedisURL: getEnvOrDefault(\"TEST_REDIS_URL\", \"redis://localhost:6379/1\"), MockServices: getEnvOrDefault(\"MOCK_SERVICES\", \"true\") == \"true\", LogLevel: getEnvOrDefault(\"TEST_LOG_LEVEL\", \"error\"), TestTimeout: 30 * time.Second, } }\r3. Continuous Integration # .github/workflows/test.yml name: Test Suite on: [push, pull_request] jobs: unit-tests: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - uses: actions/setup-go@v3 with: go-version: '1.21' - name: Run unit tests run: go test -v -race -coverprofile=coverage.out ./... - name: Upload coverage uses: codecov/codecov-action@v3 integration-tests: runs-on: ubuntu-latest services: postgres: image: pgvector/pgvector:pg15 env: POSTGRES_PASSWORD: test POSTGRES_DB: agentflow_test options: \u003e- --health-cmd pg_isready --health-interval 10s --health-timeout 5s --health-retries 5 redis: image: redis:7-alpine options: \u003e- --health-cmd \"redis-cli ping\" --health-interval 10s --health-timeout 5s --health-retries 5 steps: - uses: actions/checkout@v3 - uses: actions/setup-go@v3 with: go-version: '1.21' - name: Run integration tests run: go test -v -tags=integration ./... env: TEST_DATABASE_URL: postgres://postgres:test@localhost:5432/agentflow_test TEST_REDIS_URL: redis://localhost:6379/1 e2e-tests: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - uses: actions/setup-go@v3 with: go-version: '1.21' - name: Start test environment run: docker-compose -f docker-compose.test.yml up -d - name: Wait for services run: ./scripts/wait-for-services.sh - name: Run E2E tests run: go test -v -tags=e2e ./... - name: Cleanup run: docker-compose -f docker-compose.test.yml down\rTesting multi-agent systems requires a comprehensive approach that covers unit, integration, and end-to-end scenarios. By following these strategies and patterns, you can build reliable and maintainable agent systems with confidence.",
    "tags": [],
    "title": "testing-strategies",
    "uri": "/AgenticGoKitDocs/tutorials/advanced/testing-strategies/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials \u003e mcp",
    "content": "Tool Development in AgenticGoKit Overview Tools are the building blocks that extend agent capabilities beyond text generation. This tutorial covers how to develop custom tools in AgenticGoKit, from simple utilities to complex integrations with external services.\nBy creating custom tools, you can enable your agents to perform specific tasks, access specialized information, and interact with external systems in a controlled manner.\nPrerequisites Understanding of MCP Overview Basic knowledge of Go interfaces and error handling Familiarity with State Management Tool Interface In AgenticGoKit, all tools implement the Tool interface:\ntype Tool interface { // Name returns the tool's unique identifier Name() string // Description provides information about the tool's functionality Description() string // ParameterSchema defines the expected input parameters ParameterSchema() map[string]ParameterDefinition // Execute runs the tool with the provided parameters Execute(ctx context.Context, params map[string]interface{}) (interface{}, error) } type ParameterDefinition struct { Type string `json:\"type\"` // string, number, boolean, array, object Description string `json:\"description\"` // Parameter description Required bool `json:\"required\"` // Whether parameter is required Default interface{} `json:\"default\"` // Default value if not provided Enum []string `json:\"enum\"` // Possible values (optional) }\rCreating a Basic Tool 1. Simple Calculator Tool package tools import ( \"context\" \"fmt\" \"strconv\" \"github.com/kunalkushwaha/agenticgokit/core\" ) // CalculatorTool provides basic arithmetic operations type CalculatorTool struct{} func NewCalculatorTool() *CalculatorTool { return \u0026CalculatorTool{} } func (t *CalculatorTool) Name() string { return \"calculator\" } func (t *CalculatorTool) Description() string { return \"Performs basic arithmetic operations (add, subtract, multiply, divide)\" } func (t *CalculatorTool) ParameterSchema() map[string]core.ParameterDefinition { return map[string]core.ParameterDefinition{ \"operation\": { Type: \"string\", Description: \"The arithmetic operation to perform\", Required: true, Enum: []string{\"add\", \"subtract\", \"multiply\", \"divide\"}, }, \"a\": { Type: \"number\", Description: \"First operand\", Required: true, }, \"b\": { Type: \"number\", Description: \"Second operand\", Required: true, }, } } func (t *CalculatorTool) Execute(ctx context.Context, params map[string]interface{}) (interface{}, error) { // Extract operation operation, ok := params[\"operation\"].(string) if !ok { return nil, fmt.Errorf(\"operation must be a string\") } // Extract operands and convert to float64 a, err := getFloat(params[\"a\"]) if err != nil { return nil, fmt.Errorf(\"invalid first operand: %w\", err) } b, err := getFloat(params[\"b\"]) if err != nil { return nil, fmt.Errorf(\"invalid second operand: %w\", err) } // Perform operation var result float64 switch operation { case \"add\": result = a + b case \"subtract\": result = a - b case \"multiply\": result = a * b case \"divide\": if b == 0 { return nil, fmt.Errorf(\"division by zero\") } result = a / b default: return nil, fmt.Errorf(\"unsupported operation: %s\", operation) } // Return result return map[string]interface{}{ \"result\": result, }, nil } // Helper function to convert interface{} to float64 func getFloat(value interface{}) (float64, error) { switch v := value.(type) { case float64: return v, nil case float32: return float64(v), nil case int: return float64(v), nil case int64: return float64(v), nil case string: return strconv.ParseFloat(v, 64) default: return 0, fmt.Errorf(\"cannot convert %T to float64\", value) } }\r2. Weather Information Tool package tools import ( \"context\" \"encoding/json\" \"fmt\" \"net/http\" \"net/url\" \"time\" \"github.com/kunalkushwaha/agenticgokit/core\" ) // WeatherTool provides weather information for a location type WeatherTool struct { apiKey string httpClient *http.Client } func NewWeatherTool(apiKey string) *WeatherTool { return \u0026WeatherTool{ apiKey: apiKey, httpClient: \u0026http.Client{ Timeout: 10 * time.Second, }, } } func (t *WeatherTool) Name() string { return \"weather\" } func (t *WeatherTool) Description() string { return \"Gets current weather information for a specified location\" } func (t *WeatherTool) ParameterSchema() map[string]core.ParameterDefinition { return map[string]core.ParameterDefinition{ \"location\": { Type: \"string\", Description: \"City name or location\", Required: true, }, \"units\": { Type: \"string\", Description: \"Temperature units (metric, imperial, standard)\", Required: false, Default: \"metric\", Enum: []string{\"metric\", \"imperial\", \"standard\"}, }, } } func (t *WeatherTool) Execute(ctx context.Context, params map[string]interface{}) (interface{}, error) { // Extract location location, ok := params[\"location\"].(string) if !ok || location == \"\" { return nil, fmt.Errorf(\"location must be a non-empty string\") } // Extract units (with default) units := \"metric\" if unitsParam, ok := params[\"units\"].(string); ok \u0026\u0026 unitsParam != \"\" { units = unitsParam } // Build API URL apiURL := fmt.Sprintf( \"https://api.openweathermap.org/data/2.5/weather?q=%s\u0026units=%s\u0026appid=%s\", url.QueryEscape(location), url.QueryEscape(units), url.QueryEscape(t.apiKey), ) // Create request req, err := http.NewRequestWithContext(ctx, \"GET\", apiURL, nil) if err != nil { return nil, fmt.Errorf(\"failed to create request: %w\", err) } // Execute request resp, err := t.httpClient.Do(req) if err != nil { return nil, fmt.Errorf(\"weather API request failed: %w\", err) } defer resp.Body.Close() // Check status code if resp.StatusCode != http.StatusOK { return nil, fmt.Errorf(\"weather API returned status %d\", resp.StatusCode) } // Parse response var weatherData map[string]interface{} if err := json.NewDecoder(resp.Body).Decode(\u0026weatherData); err != nil { return nil, fmt.Errorf(\"failed to parse weather data: %w\", err) } // Extract relevant information result := map[string]interface{}{ \"location\": location, \"units\": units, } // Extract temperature if main, ok := weatherData[\"main\"].(map[string]interface{}); ok { if temp, ok := main[\"temp\"].(float64); ok { result[\"temperature\"] = temp } if humidity, ok := main[\"humidity\"].(float64); ok { result[\"humidity\"] = humidity } } // Extract weather description if weather, ok := weatherData[\"weather\"].([]interface{}); ok \u0026\u0026 len(weather) \u003e 0 { if firstWeather, ok := weather[0].(map[string]interface{}); ok { if description, ok := firstWeather[\"description\"].(string); ok { result[\"description\"] = description } } } return result, nil }\rBest Practices 1. Tool Design Principles Single Responsibility: Each tool should do one thing well Clear Interface: Define clear parameter schemas and return values Robust Error Handling: Provide meaningful error messages Statelessness: Prefer stateless tools when possible Security: Validate inputs and limit access to sensitive operations Performance: Optimize for speed and resource usage Documentation: Provide clear descriptions and examples 2. Parameter Schema Design Required vs. Optional: Only mark parameters as required if they’re truly necessary Defaults: Provide sensible defaults for optional parameters Validation: Use enums and type constraints to prevent errors Documentation: Clearly describe each parameter’s purpose and format Consistency: Use consistent naming and types across tools 3. Error Handling Specific Errors: Return specific error messages that explain what went wrong Context: Include context in error messages (e.g., parameter names) Recovery: Implement graceful recovery from transient errors Logging: Log errors for debugging and monitoring User-Friendly: Make error messages understandable to end users 4. Testing Tools package tools_test import ( \"context\" \"testing\" \"github.com/kunalkushwaha/agenticgokit/tools\" \"github.com/stretchr/testify/assert\" ) func TestCalculatorTool(t *testing.T) { calculator := tools.NewCalculatorTool() ctx := context.Background() // Test addition result, err := calculator.Execute(ctx, map[string]interface{}{ \"operation\": \"add\", \"a\": 5, \"b\": 3, }) assert.NoError(t, err) assert.Equal(t, 8.0, result.(map[string]interface{})[\"result\"]) // Test division by zero _, err = calculator.Execute(ctx, map[string]interface{}{ \"operation\": \"divide\", \"a\": 5, \"b\": 0, }) assert.Error(t, err) assert.Contains(t, err.Error(), \"division by zero\") }\rConclusion Developing custom tools is a powerful way to extend agent capabilities in AgenticGoKit. By following the patterns and best practices in this tutorial, you can create tools that are robust, secure, and easy to use.\nKey takeaways:\nImplement the Tool interface for all custom tools Use parameter schemas to define and validate inputs Follow best practices for error handling and testing Design tools with single responsibility and clear interfaces Next Steps Tool Integration - Learn how to integrate tools with agents Advanced Tool Patterns - Explore complex tool usage patterns Error Handling - Implement robust error handling Further Reading API Reference: MCP Examples: Tool Usage Advanced Patterns - Advanced multi-agent patterns",
    "description": "Tool Development in AgenticGoKit Overview Tools are the building blocks that extend agent capabilities beyond text generation. This tutorial covers how to develop custom tools in AgenticGoKit, from simple utilities to complex integrations with external services.\nBy creating custom tools, you can enable your agents to perform specific tasks, access specialized information, and interact with external systems in a controlled manner.\nPrerequisites Understanding of MCP Overview Basic knowledge of Go interfaces and error handling Familiarity with State Management Tool Interface In AgenticGoKit, all tools implement the Tool interface:",
    "tags": [],
    "title": "tool-development",
    "uri": "/AgenticGoKitDocs/tutorials/mcp/tool-development/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials \u003e getting-started",
    "content": "Tool Integration Tutorial (15 minutes) Overview Learn how to connect your agents to external tools and APIs using MCP (Model Context Protocol). You’ll set up built-in tools, create custom tools, and optimize tool performance with caching.\nPrerequisites Complete the Memory and RAG tutorial Basic understanding of APIs and external services Optional: Docker for advanced tool setups Learning Objectives By the end of this tutorial, you’ll understand:\nHow to enable MCP tool integration Using built-in tools (web search, file operations) Creating custom tools for your specific needs Tool caching and performance optimization What You’ll Build A tool-enabled agent system that can:\nSearch the web for real-time information Process files and documents Call custom APIs for specialized tasks Cache results for better performance Part 1: Basic Tool Integration (5 minutes) Start with built-in tools to understand MCP concepts.\nCreate a Tool-Enabled Project # Create project with MCP tools agentcli create tool-agent --mcp-enabled --agents 2 \\ --mcp-tools \"web_search,summarize\" cd tool-agent\rUnderstanding MCP Configuration The generated agentflow.toml includes MCP settings:\n[mcp] enabled = true enable_discovery = true connection_timeout = 5000 max_retries = 3 retry_delay = 1000 enable_caching = true cache_timeout = 300000 max_connections = 10 # Example MCP servers - configure as needed [[mcp.servers]] name = \"docker\" type = \"tcp\" host = \"localhost\" port = 8811 enabled = false [[mcp.servers]] name = \"filesystem\" type = \"stdio\" command = \"npx @modelcontextprotocol/server-filesystem /path/to/allowed/files\" enabled = false [[mcp.servers]] name = \"brave-search\" type = \"stdio\" command = \"npx @modelcontextprotocol/server-brave-search\" enabled = false\rImportant Note: The --mcp-tools flag specifies which tools you want to use, but tools are discovered at runtime from MCP servers, not configured individually in the TOML file. The servers above are examples and are disabled by default - you’ll need to enable and configure the servers that provide the tools you want to use.\nTest Basic Tools # Set your API key export OPENAI_API_KEY=your-api-key-here # Run the tool-enabled system go run main.go\rThe agents can now use web search and summarization tools automatically when needed.\nView Tool Usage # Check tool calls in traces agentcli trace --verbose \u003csession-id\u003e # Check MCP server status (run from project directory) agentcli mcp servers # List available tools (run from project directory) agentcli mcp tools # Note: MCP commands require an active MCP configuration and may need # the agent system to be running to show live server connections\rPart 2: Production Tool Setup (5 minutes) Set up production-ready tools with caching, metrics, and load balancing.\nCreate a Production Tool System # Create production MCP setup agentcli create production-tools --mcp-production --with-cache --with-metrics \\ --agents 3 --mcp-tools \"web_search,summarize,translate\" cd production-tools\rUnderstanding Production Configuration The production MCP setup generates the same basic TOML configuration as the standard setup:\n[mcp] enabled = true enable_discovery = true connection_timeout = 5000 max_retries = 3 retry_delay = 1000 enable_caching = true cache_timeout = 300000 max_connections = 10 # Example MCP servers - configure as needed [[mcp.servers]] name = \"docker\" type = \"tcp\" host = \"localhost\" port = 8811 enabled = false [[mcp.servers]] name = \"filesystem\" type = \"stdio\" command = \"npx @modelcontextprotocol/server-filesystem /path/to/allowed/files\" enabled = false [[mcp.servers]] name = \"brave-search\" type = \"stdio\" command = \"npx @modelcontextprotocol/server-brave-search\" enabled = false\rProduction Features: The --mcp-production, --with-cache, and --with-metrics flags enable additional runtime features and optimizations, but the core TOML configuration remains the same. Production features are handled at the application level rather than through additional TOML sections.\nStart Production Services # Start with Docker Compose (if generated) docker-compose up -d # Or run directly export OPENAI_API_KEY=your-api-key-here go run main.go\rMonitor Tool Performance # Check cache statistics (run from project directory) agentcli cache stats # View metrics (if enabled and running) curl http://localhost:8080/metrics # Monitor MCP servers (run from project directory) agentcli mcp health # Note: These commands require the agent system to be running # and may need active MCP connections to show meaningful data\rPart 3: Custom Tool Development (5 minutes) Create custom tools for your specific use cases.\nCreate a Custom Tool Project # Create project with custom tool setup agentcli create custom-tools --mcp-enabled --agents 2 cd custom-tools\rAdd a Custom Weather Tool Create tools/weather.go:\npackage tools import ( \"context\" \"encoding/json\" \"fmt\" \"net/http\" \"net/url\" \"time\" ) // WeatherTool provides weather information type WeatherTool struct { apiKey string httpClient *http.Client } func NewWeatherTool(apiKey string) *WeatherTool { return \u0026WeatherTool{ apiKey: apiKey, httpClient: \u0026http.Client{ Timeout: 10 * time.Second, }, } } func (w *WeatherTool) Name() string { return \"weather\" } func (w *WeatherTool) Description() string { return \"Gets current weather information for a specified location\" } func (w *WeatherTool) ParameterSchema() map[string]interface{} { return map[string]interface{}{ \"type\": \"object\", \"properties\": map[string]interface{}{ \"location\": map[string]interface{}{ \"type\": \"string\", \"description\": \"The city, zip code, or coordinates\", }, }, \"required\": []string{\"location\"}, } } func (w *WeatherTool) Execute(ctx context.Context, params map[string]interface{}) (interface{}, error) { location, ok := params[\"location\"].(string) if !ok || location == \"\" { return nil, fmt.Errorf(\"location parameter is required\") } // Build API URL (using a free weather API) apiURL := fmt.Sprintf(\"https://api.weatherapi.com/v1/current.json?key=%s\u0026q=%s\", w.apiKey, url.QueryEscape(location)) req, err := http.NewRequestWithContext(ctx, \"GET\", apiURL, nil) if err != nil { return nil, fmt.Errorf(\"failed to create request: %w\", err) } resp, err := w.httpClient.Do(req) if err != nil { return nil, fmt.Errorf(\"weather API request failed: %w\", err) } defer resp.Body.Close() if resp.StatusCode != http.StatusOK { return nil, fmt.Errorf(\"weather API returned status %d\", resp.StatusCode) } var weatherData map[string]interface{} if err := json.NewDecoder(resp.Body).Decode(\u0026weatherData); err != nil { return nil, fmt.Errorf(\"failed to parse weather data: %w\", err) } return weatherData, nil }\rRegister the Custom Tool Update main.go to include your custom tool:\n// Add to your main.go import \"your-project/tools\" func main() { // ... existing code ... // Create tool manager toolManager := core.NewToolManager() // Register custom weather tool weatherTool := tools.NewWeatherTool(os.Getenv(\"WEATHER_API_KEY\")) toolManager.RegisterTool(weatherTool) // Register with agents for _, agent := range agents { agent.SetToolManager(toolManager) } // ... rest of code ... }\rTest Custom Tools # Set API keys export OPENAI_API_KEY=your-openai-key export WEATHER_API_KEY=your-weather-api-key # Run with custom tools go run main.go\rYour agents can now use the custom weather tool alongside built-in tools.\nTool Categories and Use Cases Built-in Tools Tool Description Use Case web_search Search the internet Real-time information, research summarize Summarize text content Document processing, content analysis translate Translate between languages Multi-language support file_operations Read/write files Document processing, data import Custom Tool Examples Tool Type Example Implementation API Integration Weather, Stock prices HTTP client with JSON parsing Database Access User lookup, Data queries SQL client with connection pooling File Processing PDF parsing, Image analysis Specialized libraries External Services Email sending, SMS Service-specific SDKs Tool Performance Optimization Caching Configuration Caching is configured in the main MCP section:\n[mcp] enabled = true enable_discovery = true connection_timeout = 5000 max_retries = 3 retry_delay = 1000 enable_caching = true cache_timeout = 300000 # 5 minutes default cache timeout max_connections = 10\rNote: Individual tool cache settings are handled at runtime through the MCP manager, not through separate TOML sections. The cache_timeout setting applies to all tools by default, and specific tool caching behavior is managed programmatically.\nLoad Balancing and Circuit Breaker Protection Note: Load balancing and circuit breaker features are handled at the application runtime level when using --mcp-production flag. The TOML configuration remains the same:\n[mcp] enabled = true enable_discovery = true connection_timeout = 5000 # Connection timeout in milliseconds max_retries = 3 # Maximum retry attempts retry_delay = 1000 # Delay between retries in milliseconds enable_caching = true cache_timeout = 300000 max_connections = 10 # Maximum concurrent connections\rProduction features like load balancing, circuit breakers, and advanced retry policies are configured programmatically when you use the --mcp-production flag during project creation.\nAdvanced Tool Patterns Tool Composition // Combine multiple tools for complex operations type ResearchTool struct { webSearch Tool summarizer Tool translator Tool } func (r *ResearchTool) Execute(ctx context.Context, params map[string]interface{}) (interface{}, error) { // 1. Search for information searchResults, err := r.webSearch.Execute(ctx, params) if err != nil { return nil, err } // 2. Summarize results summaryParams := map[string]interface{}{ \"text\": searchResults, } summary, err := r.summarizer.Execute(ctx, summaryParams) if err != nil { return nil, err } // 3. Translate if needed if targetLang, ok := params[\"language\"]; ok { translateParams := map[string]interface{}{ \"text\": summary, \"target_lang\": targetLang, } return r.translator.Execute(ctx, translateParams) } return summary, nil }\rAsync Tool Execution // Execute multiple tools concurrently func (a *Agent) executeToolsConcurrently(ctx context.Context, tools []ToolCall) (map[string]interface{}, error) { results := make(map[string]interface{}) var wg sync.WaitGroup var mu sync.Mutex for _, toolCall := range tools { wg.Add(1) go func(tc ToolCall) { defer wg.Done() result, err := a.toolManager.ExecuteTool(ctx, tc.Name, tc.Params) mu.Lock() if err != nil { results[tc.Name] = fmt.Sprintf(\"Error: %v\", err) } else { results[tc.Name] = result } mu.Unlock() }(toolCall) } wg.Wait() return results, nil }\rTroubleshooting Common Issues Tool not found:\n# Check tool registration agentcli mcp tools # Verify MCP server status agentcli mcp servers # Check configuration cat agentflow.toml | grep -A 10 \"\\[mcp\\]\"\rTool execution timeout:\n# Increase timeout in configuration [mcp] connection_timeout = 60000 # Increase to 60 seconds (in milliseconds)\rCache not working:\n# Check cache statistics agentcli cache stats # Clear cache if needed agentcli cache clear --all\rPerformance Issues Slow tool execution:\nEnable caching for frequently used tools Use connection pooling for database tools Implement circuit breakers for unreliable services High resource usage:\nLimit concurrent tool executions Use tool result caching Monitor with metrics Tool Security Best Practices Input Validation func (t *CustomTool) Execute(ctx context.Context, params map[string]interface{}) (interface{}, error) { // Always validate inputs input, ok := params[\"input\"].(string) if !ok { return nil, fmt.Errorf(\"input parameter must be a string\") } // Sanitize inputs input = sanitizeInput(input) // Validate against schema if err := validateInput(input); err != nil { return nil, fmt.Errorf(\"invalid input: %w\", err) } // ... tool logic ... }\rAPI Key Management // Use environment variables for sensitive data func NewAPITool() *APITool { return \u0026APITool{ apiKey: os.Getenv(\"API_KEY\"), // Never hardcode keys client: \u0026http.Client{ Timeout: 30 * time.Second, }, } }\rNext Steps Now that your agents can use tools:\nGo Production: Learn Production Deployment for scaling Advanced Patterns: Explore Advanced Patterns for complex workflows Custom MCP Servers: Learn MCP Tool Development Key Takeaways MCP Integration: Standardized way to connect agents with external tools Built-in Tools: Ready-to-use tools for common tasks Custom Tools: Easy to create for specific needs Performance: Caching and load balancing for production use Security: Always validate inputs and manage credentials properly Further Reading MCP Fundamentals - Deep dive into MCP concepts Creating Custom Tools - Advanced tool development Tool Integration Patterns - Advanced patterns",
    "description": "Tool Integration Tutorial (15 minutes) Overview Learn how to connect your agents to external tools and APIs using MCP (Model Context Protocol). You’ll set up built-in tools, create custom tools, and optimize tool performance with caching.\nPrerequisites Complete the Memory and RAG tutorial Basic understanding of APIs and external services Optional: Docker for advanced tool setups Learning Objectives By the end of this tutorial, you’ll understand:\nHow to enable MCP tool integration Using built-in tools (web search, file operations) Creating custom tools for your specific needs Tool caching and performance optimization What You’ll Build A tool-enabled agent system that can:",
    "tags": [],
    "title": "tool-integration",
    "uri": "/AgenticGoKitDocs/tutorials/getting-started/tool-integration/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials \u003e mcp",
    "content": "Tool Integration in AgenticGoKit Overview Integrating tools with agents is a key capability in AgenticGoKit that enables agents to perform actions beyond text generation. This tutorial covers how to connect tools to agents, configure tool access, handle tool results, and implement tool-aware prompting.\nEffective tool integration allows agents to interact with external systems, access specialized information, and perform complex tasks while maintaining a natural conversation flow.\nPrerequisites Understanding of MCP Overview Familiarity with Tool Development Knowledge of Agent Lifecycle Basic understanding of State Management MCP Integration Architecture ┌─────────────┐ ┌───────────────┐ ┌─────────────┐\r│ │ │ │ │ │\r│ Agent │────▶│ MCP Manager │────▶│ Tool │\r│ │ │ │ │ │\r└─────────────┘ └───────────────┘ └─────────────┘\r▲ │ │\r│ ▼ ▼\r│ ┌───────────────┐ ┌─────────────┐\r└──────────────│ Tool Result │◀───│ External │\r│ Processor │ │ Service │\r└───────────────┘ └─────────────┘\rBasic Tool Integration 1. Setting Up MCP Manager package main import ( \"context\" \"fmt\" \"log\" \"os\" \"time\" \"github.com/kunalkushwaha/agenticgokit/core\" \"github.com/kunalkushwaha/agenticgokit/tools\" ) func main() { // Create MCP manager mcpManager := core.NewMCPManager() // Register tools mcpManager.RegisterTool(\"calculator\", tools.NewCalculatorTool()) mcpManager.RegisterTool(\"weather\", tools.NewWeatherTool(os.Getenv(\"WEATHER_API_KEY\"))) mcpManager.RegisterTool(\"counter\", tools.NewCounterTool()) // Create LLM provider llmProvider, err := core.NewOpenAIProvider( os.Getenv(\"OPENAI_API_KEY\"), \"gpt-4\", core.WithTemperature(0.2), core.WithMaxTokens(1000), ) if err != nil { log.Fatalf(\"Failed to create LLM provider: %v\", err) } // Create agent with MCP capability agent, err := core.NewAgent(\"assistant\"). WithLLM(llmProvider). WithMCP(mcpManager). WithMCPConfig(core.MCPConfig{ Tools: []string{\"calculator\", \"weather\", \"counter\"}, MaxToolCalls: 5, ToolTimeout: 10 * time.Second, }). Build() if err != nil { log.Fatalf(\"Failed to create agent: %v\", err) } // Create runner runner := core.NewRunner(100) runner.RegisterAgent(\"assistant\", agent) // Start runner ctx := context.Background() runner.Start(ctx) defer runner.Stop() // Create event with user query event := core.NewEvent( \"assistant\", core.EventData{\"message\": \"What's 25 * 16 and what's the weather in New York?\"}, map[string]string{\"session_id\": \"test-session\"}, ) // Register callback to handle agent response runner.RegisterCallback(core.HookAfterAgentRun, \"response-handler\", func(ctx context.Context, args core.CallbackArgs) (core.State, error) { if response, ok := args.AgentResult.OutputState.Get(\"response\"); ok { fmt.Printf(\"Agent response: %s\\n\", response) } return args.State, nil }, ) // Emit event runner.Emit(event) // Wait for response time.Sleep(5 * time.Second) }\r2. Tool-Aware Prompting func createToolAwareAgent() (core.AgentHandler, error) { // Create MCP manager mcpManager := core.NewMCPManager() // Register tools mcpManager.RegisterTool(\"calculator\", tools.NewCalculatorTool()) mcpManager.RegisterTool(\"weather\", tools.NewWeatherTool(os.Getenv(\"WEATHER_API_KEY\"))) // Create LLM provider llmProvider, err := core.NewOpenAIProvider( os.Getenv(\"OPENAI_API_KEY\"), \"gpt-4\", core.WithTemperature(0.2), core.WithMaxTokens(1000), ) if err != nil { return nil, fmt.Errorf(\"failed to create LLM provider: %w\", err) } // Create tool-aware system prompt systemPrompt := `You are a helpful assistant with access to the following tools: 1. calculator: Performs basic arithmetic operations (add, subtract, multiply, divide) - Parameters: operation (string), a (number), b (number) 2. weather: Gets current weather information for a specified location - Parameters: location (string), units (string, optional) When you need to use a tool, use the following format: \u003ctool\u003ecalculator\u003c/tool\u003e \u003cparameters\u003e { \"operation\": \"add\", \"a\": 5, \"b\": 3 } \u003c/parameters\u003e Wait for the tool result before continuing. Tool results will be provided in this format: \u003ctool_result\u003e { \"result\": 8 } \u003c/tool_result\u003e Answer user questions directly when you can, and use tools when necessary.` // Create agent with MCP capability and tool-aware prompt return core.NewAgent(\"assistant\"). WithLLM(llmProvider). WithSystemPrompt(systemPrompt). WithMCP(mcpManager). WithMCPConfig(core.MCPConfig{ Tools: []string{\"calculator\", \"weather\"}, MaxToolCalls: 5, ToolTimeout: 10 * time.Second, }). Build() }\r3. Handling Tool Results func setupToolResultHandling(runner *core.Runner) { // Register callback for tool execution runner.RegisterCallback(core.HookBeforeToolExecution, \"tool-execution-logger\", func(ctx context.Context, args core.CallbackArgs) (core.State, error) { toolName := args.ToolName toolParams := args.ToolParams fmt.Printf(\"Executing tool: %s with params: %v\\n\", toolName, toolParams) return args.State, nil }, ) // Register callback for tool result runner.RegisterCallback(core.HookAfterToolExecution, \"tool-result-logger\", func(ctx context.Context, args core.CallbackArgs) (core.State, error) { toolName := args.ToolName toolResult := args.ToolResult toolError := args.Error if toolError != nil { fmt.Printf(\"Tool %s failed: %v\\n\", toolName, toolError) } else { fmt.Printf(\"Tool %s result: %v\\n\", toolName, toolResult) } return args.State, nil }, ) // Register callback for tool error runner.RegisterCallback(core.HookToolError, \"tool-error-handler\", func(ctx context.Context, args core.CallbackArgs) (core.State, error) { toolName := args.ToolName toolError := args.Error fmt.Printf(\"Tool error: %s - %v\\n\", toolName, toolError) // Add error information to state newState := args.State.Clone() newState.Set(\"tool_error\", fmt.Sprintf(\"The %s tool encountered an error: %v\", toolName, toolError)) return newState, nil }, ) }\rAdvanced Tool Integration 1. Dynamic Tool Selection type DynamicToolSelector struct { tools map[string]core.Tool llmProvider core.LLMProvider } func NewDynamicToolSelector(llmProvider core.LLMProvider) *DynamicToolSelector { return \u0026DynamicToolSelector{ tools: make(map[string]core.Tool), llmProvider: llmProvider, } } func (dts *DynamicToolSelector) RegisterTool(name string, tool core.Tool) { dts.tools[name] = tool } func (dts *DynamicToolSelector) SelectTools(ctx context.Context, query string) ([]string, error) { // Create tool descriptions var toolDescriptions strings.Builder for name, tool := range dts.tools { toolDescriptions.WriteString(fmt.Sprintf(\"- %s: %s\\n\", name, tool.Description())) } // Create prompt for tool selection prompt := fmt.Sprintf(`Given the following user query and available tools, select the most appropriate tools to answer the query. User query: %s Available tools: %s Return only the names of the tools that should be used, separated by commas (e.g., \"calculator,weather\"). If no tools are needed, return \"none\". Selected tools:`, query, toolDescriptions.String()) // Get tool selection from LLM response, err := dts.llmProvider.Generate(ctx, prompt) if err != nil { return nil, fmt.Errorf(\"tool selection failed: %w\", err) } // Parse response response = strings.TrimSpace(response) if response == \"none\" { return []string{}, nil } // Split by comma and trim spaces selectedTools := strings.Split(response, \",\") for i := range selectedTools { selectedTools[i] = strings.TrimSpace(selectedTools[i]) } // Validate selected tools validTools := make([]string, 0, len(selectedTools)) for _, name := range selectedTools { if _, exists := dts.tools[name]; exists { validTools = append(validTools, name) } } return validTools, nil }\r2. Tool Result Processing type ToolResultProcessor struct { llmProvider core.LLMProvider } func NewToolResultProcessor(llmProvider core.LLMProvider) *ToolResultProcessor { return \u0026ToolResultProcessor{ llmProvider: llmProvider, } } func (trp *ToolResultProcessor) ProcessResult(ctx context.Context, toolName string, result interface{}, query string) (string, error) { // Convert result to string representation resultStr := fmt.Sprintf(\"%v\", result) if resultMap, ok := result.(map[string]interface{}); ok { resultBytes, err := json.MarshalIndent(resultMap, \"\", \" \") if err == nil { resultStr = string(resultBytes) } } // Create prompt for result processing prompt := fmt.Sprintf(`You are processing the result of a tool execution. Format the result in a clear, human-readable way that answers the user's query. User query: %s Tool used: %s Tool result: %s Provide a concise, helpful response based on this result:`, query, toolName, resultStr) // Get formatted response from LLM return trp.llmProvider.Generate(ctx, prompt) }\r3. Tool Caching and Performance type CachedTool struct { tool core.Tool cache map[string]CacheEntry cacheTTL time.Duration mu sync.RWMutex } type CacheEntry struct { Result interface{} Timestamp time.Time } func NewCachedTool(tool core.Tool, cacheTTL time.Duration) *CachedTool { return \u0026CachedTool{ tool: tool, cache: make(map[string]CacheEntry), cacheTTL: cacheTTL, } } func (ct *CachedTool) Name() string { return ct.tool.Name() } func (ct *CachedTool) Description() string { return ct.tool.Description() } func (ct *CachedTool) ParameterSchema() map[string]core.ParameterDefinition { return ct.tool.ParameterSchema() } func (ct *CachedTool) Execute(ctx context.Context, params map[string]interface{}) (interface{}, error) { // Create cache key from parameters cacheKey := ct.createCacheKey(params) // Check cache if result, found := ct.getCachedResult(cacheKey); found { return result, nil } // Execute tool result, err := ct.tool.Execute(ctx, params) if err != nil { return nil, err } // Cache result ct.setCachedResult(cacheKey, result) return result, nil } func (ct *CachedTool) createCacheKey(params map[string]interface{}) string { // Create deterministic key from parameters keys := make([]string, 0, len(params)) for k := range params { keys = append(keys, k) } sort.Strings(keys) var keyBuilder strings.Builder for _, k := range keys { keyBuilder.WriteString(fmt.Sprintf(\"%s:%v;\", k, params[k])) } // Hash the key for consistent length hasher := sha256.New() hasher.Write([]byte(keyBuilder.String())) return fmt.Sprintf(\"%x\", hasher.Sum(nil)) } func (ct *CachedTool) getCachedResult(key string) (interface{}, bool) { ct.mu.RLock() defer ct.mu.RUnlock() entry, exists := ct.cache[key] if !exists { return nil, false } // Check if entry is expired if time.Since(entry.Timestamp) \u003e ct.cacheTTL { return nil, false } return entry.Result, true } func (ct *CachedTool) setCachedResult(key string, result interface{}) { ct.mu.Lock() defer ct.mu.Unlock() ct.cache[key] = CacheEntry{ Result: result, Timestamp: time.Now(), } // Clean up expired entries periodically if len(ct.cache)%100 == 0 { go ct.cleanupExpiredEntries() } } func (ct *CachedTool) cleanupExpiredEntries() { ct.mu.Lock() defer ct.mu.Unlock() now := time.Now() for key, entry := range ct.cache { if now.Sub(entry.Timestamp) \u003e ct.cacheTTL { delete(ct.cache, key) } } }\rTool Configuration and Management 1. Tool Configuration type ToolConfig struct { Name string `json:\"name\"` Enabled bool `json:\"enabled\"` MaxCalls int `json:\"max_calls\"` Timeout time.Duration `json:\"timeout\"` RateLimit RateLimitConfig `json:\"rate_limit\"` Cache CacheConfig `json:\"cache\"` Parameters map[string]interface{} `json:\"parameters\"` } type RateLimitConfig struct { MaxRequests int `json:\"max_requests\"` Interval time.Duration `json:\"interval\"` } type CacheConfig struct { Enabled bool `json:\"enabled\"` TTL time.Duration `json:\"ttl\"` } type ToolManager struct { tools map[string]core.Tool configs map[string]ToolConfig mu sync.RWMutex } func NewToolManager() *ToolManager { return \u0026ToolManager{ tools: make(map[string]core.Tool), configs: make(map[string]ToolConfig), } } func (tm *ToolManager) RegisterTool(tool core.Tool, config ToolConfig) error { tm.mu.Lock() defer tm.mu.Unlock() name := tool.Name() // Apply configuration wrappers wrappedTool := tool // Add caching if enabled if config.Cache.Enabled { wrappedTool = NewCachedTool(wrappedTool, config.Cache.TTL) } // Add rate limiting if configured if config.RateLimit.MaxRequests \u003e 0 { wrappedTool = NewRateLimitedTool(wrappedTool, config.RateLimit.MaxRequests, config.RateLimit.Interval) } // Add validation wrappedTool = NewValidatedTool(wrappedTool, true) tm.tools[name] = wrappedTool tm.configs[name] = config return nil } func (tm *ToolManager) GetTool(name string) (core.Tool, error) { tm.mu.RLock() defer tm.mu.RUnlock() tool, exists := tm.tools[name] if !exists { return nil, fmt.Errorf(\"tool not found: %s\", name) } config := tm.configs[name] if !config.Enabled { return nil, fmt.Errorf(\"tool disabled: %s\", name) } return tool, nil } func (tm *ToolManager) ListEnabledTools() []string { tm.mu.RLock() defer tm.mu.RUnlock() var enabled []string for name, config := range tm.configs { if config.Enabled { enabled = append(enabled, name) } } return enabled }\r2. Tool Discovery and Registration type ToolDiscovery struct { registry map[string]ToolFactory mu sync.RWMutex } type ToolFactory func(config map[string]interface{}) (core.Tool, error) func NewToolDiscovery() *ToolDiscovery { td := \u0026ToolDiscovery{ registry: make(map[string]ToolFactory), } // Register built-in tool factories td.RegisterFactory(\"calculator\", func(config map[string]interface{}) (core.Tool, error) { return tools.NewCalculatorTool(), nil }) td.RegisterFactory(\"weather\", func(config map[string]interface{}) (core.Tool, error) { apiKey, ok := config[\"api_key\"].(string) if !ok || apiKey == \"\" { return nil, fmt.Errorf(\"weather tool requires api_key parameter\") } return tools.NewWeatherTool(apiKey), nil }) td.RegisterFactory(\"counter\", func(config map[string]interface{}) (core.Tool, error) { return tools.NewCounterTool(), nil }) return td } func (td *ToolDiscovery) RegisterFactory(name string, factory ToolFactory) { td.mu.Lock() defer td.mu.Unlock() td.registry[name] = factory } func (td *ToolDiscovery) CreateTool(name string, config map[string]interface{}) (core.Tool, error) { td.mu.RLock() factory, exists := td.registry[name] td.mu.RUnlock() if !exists { return nil, fmt.Errorf(\"unknown tool type: %s\", name) } return factory(config) } func (td *ToolDiscovery) ListAvailableTools() []string { td.mu.RLock() defer td.mu.RUnlock() tools := make([]string, 0, len(td.registry)) for name := range td.registry { tools = append(tools, name) } return tools }\rProduction Integration Patterns 1. Tool Health Monitoring type ToolHealthMonitor struct { tools map[string]core.Tool metrics map[string]*ToolMetrics mu sync.RWMutex } type ToolMetrics struct { TotalCalls int64 `json:\"total_calls\"` SuccessCalls int64 `json:\"success_calls\"` ErrorCalls int64 `json:\"error_calls\"` AverageTime time.Duration `json:\"average_time\"` LastError string `json:\"last_error\"` LastErrorTime time.Time `json:\"last_error_time\"` } func NewToolHealthMonitor() *ToolHealthMonitor { return \u0026ToolHealthMonitor{ tools: make(map[string]core.Tool), metrics: make(map[string]*ToolMetrics), } } func (thm *ToolHealthMonitor) WrapTool(tool core.Tool) core.Tool { thm.mu.Lock() defer thm.mu.Unlock() name := tool.Name() thm.tools[name] = tool thm.metrics[name] = \u0026ToolMetrics{} return \u0026MonitoredTool{ tool: tool, monitor: thm, } } type MonitoredTool struct { tool core.Tool monitor *ToolHealthMonitor } func (mt *MonitoredTool) Name() string { return mt.tool.Name() } func (mt *MonitoredTool) Description() string { return mt.tool.Description() } func (mt *MonitoredTool) ParameterSchema() map[string]core.ParameterDefinition { return mt.tool.ParameterSchema() } func (mt *MonitoredTool) Execute(ctx context.Context, params map[string]interface{}) (interface{}, error) { start := time.Now() name := mt.tool.Name() // Execute tool result, err := mt.tool.Execute(ctx, params) // Record metrics duration := time.Since(start) mt.monitor.recordExecution(name, duration, err) return result, err } func (thm *ToolHealthMonitor) recordExecution(toolName string, duration time.Duration, err error) { thm.mu.Lock() defer thm.mu.Unlock() metrics := thm.metrics[toolName] metrics.TotalCalls++ if err != nil { metrics.ErrorCalls++ metrics.LastError = err.Error() metrics.LastErrorTime = time.Now() } else { metrics.SuccessCalls++ } // Update average time (simple moving average) if metrics.TotalCalls == 1 { metrics.AverageTime = duration } else { metrics.AverageTime = time.Duration( (int64(metrics.AverageTime)*metrics.TotalCalls + int64(duration)) / (metrics.TotalCalls + 1), ) } } func (thm *ToolHealthMonitor) GetMetrics(toolName string) (*ToolMetrics, error) { thm.mu.RLock() defer thm.mu.RUnlock() metrics, exists := thm.metrics[toolName] if !exists { return nil, fmt.Errorf(\"tool not found: %s\", toolName) } // Return a copy to avoid race conditions return \u0026ToolMetrics{ TotalCalls: metrics.TotalCalls, SuccessCalls: metrics.SuccessCalls, ErrorCalls: metrics.ErrorCalls, AverageTime: metrics.AverageTime, LastError: metrics.LastError, LastErrorTime: metrics.LastErrorTime, }, nil } func (thm *ToolHealthMonitor) GetHealthStatus() map[string]string { thm.mu.RLock() defer thm.mu.RUnlock() status := make(map[string]string) for name, metrics := range thm.metrics { if metrics.TotalCalls == 0 { status[name] = \"unknown\" continue } errorRate := float64(metrics.ErrorCalls) / float64(metrics.TotalCalls) switch { case errorRate == 0: status[name] = \"healthy\" case errorRate \u003c 0.1: status[name] = \"warning\" default: status[name] = \"unhealthy\" } } return status }\r2. Complete Integration Example package main import ( \"context\" \"encoding/json\" \"fmt\" \"log\" \"os\" \"time\" \"github.com/kunalkushwaha/agenticgokit/core\" \"github.com/kunalkushwaha/agenticgokit/tools\" ) func main() { // Create tool discovery and manager discovery := NewToolDiscovery() manager := NewToolManager() monitor := NewToolHealthMonitor() // Configure tools toolConfigs := []struct { name string config ToolConfig params map[string]interface{} }{ { name: \"calculator\", config: ToolConfig{ Name: \"calculator\", Enabled: true, MaxCalls: 100, Timeout: 5 * time.Second, Cache: CacheConfig{ Enabled: true, TTL: 1 * time.Hour, }, }, }, { name: \"weather\", config: ToolConfig{ Name: \"weather\", Enabled: true, MaxCalls: 50, Timeout: 10 * time.Second, RateLimit: RateLimitConfig{ MaxRequests: 10, Interval: 1 * time.Minute, }, Cache: CacheConfig{ Enabled: true, TTL: 15 * time.Minute, }, }, params: map[string]interface{}{ \"api_key\": os.Getenv(\"WEATHER_API_KEY\"), }, }, } // Create and register tools for _, tc := range toolConfigs { tool, err := discovery.CreateTool(tc.name, tc.params) if err != nil { log.Fatalf(\"Failed to create tool %s: %v\", tc.name, err) } // Wrap with monitoring monitoredTool := monitor.WrapTool(tool) // Register with manager if err := manager.RegisterTool(monitoredTool, tc.config); err != nil { log.Fatalf(\"Failed to register tool %s: %v\", tc.name, err) } } // Create MCP manager and register tools mcpManager := core.NewMCPManager() for _, toolName := range manager.ListEnabledTools() { tool, err := manager.GetTool(toolName) if err != nil { log.Printf(\"Failed to get tool %s: %v\", toolName, err) continue } mcpManager.RegisterTool(toolName, tool) } // Create LLM provider llmProvider, err := core.NewOpenAIProvider( os.Getenv(\"OPENAI_API_KEY\"), \"gpt-4\", core.WithTemperature(0.2), core.WithMaxTokens(1000), ) if err != nil { log.Fatalf(\"Failed to create LLM provider: %v\", err) } // Create agent agent, err := core.NewAgent(\"assistant\"). WithLLM(llmProvider). WithMCP(mcpManager). WithMCPConfig(core.MCPConfig{ Tools: manager.ListEnabledTools(), MaxToolCalls: 5, ToolTimeout: 10 * time.Second, }). Build() if err != nil { log.Fatalf(\"Failed to create agent: %v\", err) } // Create runner with tool monitoring runner := core.NewRunner(100) runner.RegisterAgent(\"assistant\", agent) // Setup tool result handling setupToolResultHandling(runner) // Start runner ctx := context.Background() runner.Start(ctx) defer runner.Stop() // Example interaction event := core.NewEvent( \"assistant\", core.EventData{\"message\": \"Calculate 15 * 23 and tell me the weather in London\"}, map[string]string{\"session_id\": \"demo-session\"}, ) runner.Emit(event) // Wait and show metrics time.Sleep(10 * time.Second) // Display tool health metrics fmt.Println(\"\\nTool Health Status:\") healthStatus := monitor.GetHealthStatus() for tool, status := range healthStatus { fmt.Printf(\"- %s: %s\\n\", tool, status) if metrics, err := monitor.GetMetrics(tool); err == nil { fmt.Printf(\" Calls: %d (Success: %d, Errors: %d)\\n\", metrics.TotalCalls, metrics.SuccessCalls, metrics.ErrorCalls) fmt.Printf(\" Average Time: %v\\n\", metrics.AverageTime) if metrics.LastError != \"\" { fmt.Printf(\" Last Error: %s (%v)\\n\", metrics.LastError, metrics.LastErrorTime) } } } }\rBest Practices 1. Tool Integration Guidelines Selective Tool Access: Only provide agents with tools they actually need Tool Validation: Always validate tool parameters and results Error Handling: Implement robust error handling and recovery Performance Monitoring: Monitor tool performance and health Security: Implement proper authentication and authorization Caching: Use caching for expensive or frequently called tools Rate Limiting: Protect external services with rate limiting 2. Prompting Best Practices Clear Tool Descriptions: Provide clear, concise tool descriptions Parameter Documentation: Document all parameters with examples Usage Examples: Include examples of when and how to use tools Error Guidance: Explain how to handle tool errors Tool Selection: Guide the agent on tool selection criteria 3. Production Considerations Monitoring: Implement comprehensive tool monitoring Logging: Log all tool executions for debugging Metrics: Track tool usage and performance metrics Health Checks: Implement tool health checking Fallbacks: Provide fallback mechanisms for tool failures Configuration: Make tool configuration externally manageable Troubleshooting Common Issues Tool Not Found: Ensure tools are properly registered with the MCP manager Parameter Validation Errors: Check parameter schemas and validation rules Timeout Errors: Adjust tool timeout settings for slow operations Rate Limit Exceeded: Configure appropriate rate limits for external services Authentication Failures: Verify API keys and authentication credentials Debugging Tips Enable detailed logging for tool executions Use tool health monitoring to identify problematic tools Test tools independently before integrating with agents Monitor tool performance metrics regularly Implement proper error handling and recovery mechanisms Conclusion Tool integration is a powerful feature that extends agent capabilities beyond text generation. By following the patterns and best practices in this tutorial, you can create robust, scalable tool integrations that enhance your agent systems.\nKey takeaways:\nUse the MCP manager to register and manage tools Implement proper tool configuration and monitoring Handle tool results and errors gracefully Follow security and performance best practices Monitor tool health and performance in production Next Steps Advanced Tool Patterns - Explore complex tool usage patterns Tool Development - Learn how to create custom tools State Management - Understand state flow with tools Further Reading API Reference: MCP Examples: Tool Usage Production Deployment",
    "description": "Tool Integration in AgenticGoKit Overview Integrating tools with agents is a key capability in AgenticGoKit that enables agents to perform actions beyond text generation. This tutorial covers how to connect tools to agents, configure tool access, handle tool results, and implement tool-aware prompting.\nEffective tool integration allows agents to interact with external systems, access specialized information, and perform complex tasks while maintaining a natural conversation flow.\nPrerequisites Understanding of MCP Overview Familiarity with Tool Development Knowledge of Agent Lifecycle Basic understanding of State Management MCP Integration Architecture ┌─────────────┐ ┌───────────────┐ ┌─────────────┐\r│ │ │ │ │ │\r│ Agent │────▶│ MCP Manager │────▶│ Tool │\r│ │ │ │ │ │\r└─────────────┘ └───────────────┘ └─────────────┘\r▲ │ │\r│ ▼ ▼\r│ ┌───────────────┐ ┌─────────────┐\r└──────────────│ Tool Result │◀───│ External │\r│ Processor │ │ Service │\r└───────────────┘ └─────────────┘\rBasic Tool Integration 1. Setting Up MCP Manager package main import ( \"context\" \"fmt\" \"log\" \"os\" \"time\" \"github.com/kunalkushwaha/agenticgokit/core\" \"github.com/kunalkushwaha/agenticgokit/tools\" ) func main() { // Create MCP manager mcpManager := core.NewMCPManager() // Register tools mcpManager.RegisterTool(\"calculator\", tools.NewCalculatorTool()) mcpManager.RegisterTool(\"weather\", tools.NewWeatherTool(os.Getenv(\"WEATHER_API_KEY\"))) mcpManager.RegisterTool(\"counter\", tools.NewCounterTool()) // Create LLM provider llmProvider, err := core.NewOpenAIProvider( os.Getenv(\"OPENAI_API_KEY\"), \"gpt-4\", core.WithTemperature(0.2), core.WithMaxTokens(1000), ) if err != nil { log.Fatalf(\"Failed to create LLM provider: %v\", err) } // Create agent with MCP capability agent, err := core.NewAgent(\"assistant\"). WithLLM(llmProvider). WithMCP(mcpManager). WithMCPConfig(core.MCPConfig{ Tools: []string{\"calculator\", \"weather\", \"counter\"}, MaxToolCalls: 5, ToolTimeout: 10 * time.Second, }). Build() if err != nil { log.Fatalf(\"Failed to create agent: %v\", err) } // Create runner runner := core.NewRunner(100) runner.RegisterAgent(\"assistant\", agent) // Start runner ctx := context.Background() runner.Start(ctx) defer runner.Stop() // Create event with user query event := core.NewEvent( \"assistant\", core.EventData{\"message\": \"What's 25 * 16 and what's the weather in New York?\"}, map[string]string{\"session_id\": \"test-session\"}, ) // Register callback to handle agent response runner.RegisterCallback(core.HookAfterAgentRun, \"response-handler\", func(ctx context.Context, args core.CallbackArgs) (core.State, error) { if response, ok := args.AgentResult.OutputState.Get(\"response\"); ok { fmt.Printf(\"Agent response: %s\\n\", response) } return args.State, nil }, ) // Emit event runner.Emit(event) // Wait for response time.Sleep(5 * time.Second) }\r2. Tool-Aware Prompting func createToolAwareAgent() (core.AgentHandler, error) { // Create MCP manager mcpManager := core.NewMCPManager() // Register tools mcpManager.RegisterTool(\"calculator\", tools.NewCalculatorTool()) mcpManager.RegisterTool(\"weather\", tools.NewWeatherTool(os.Getenv(\"WEATHER_API_KEY\"))) // Create LLM provider llmProvider, err := core.NewOpenAIProvider( os.Getenv(\"OPENAI_API_KEY\"), \"gpt-4\", core.WithTemperature(0.2), core.WithMaxTokens(1000), ) if err != nil { return nil, fmt.Errorf(\"failed to create LLM provider: %w\", err) } // Create tool-aware system prompt systemPrompt := `You are a helpful assistant with access to the following tools: 1. calculator: Performs basic arithmetic operations (add, subtract, multiply, divide) - Parameters: operation (string), a (number), b (number) 2. weather: Gets current weather information for a specified location - Parameters: location (string), units (string, optional) When you need to use a tool, use the following format: \u003ctool\u003ecalculator\u003c/tool\u003e \u003cparameters\u003e { \"operation\": \"add\", \"a\": 5, \"b\": 3 } \u003c/parameters\u003e Wait for the tool result before continuing. Tool results will be provided in this format: \u003ctool_result\u003e { \"result\": 8 } \u003c/tool_result\u003e Answer user questions directly when you can, and use tools when necessary.` // Create agent with MCP capability and tool-aware prompt return core.NewAgent(\"assistant\"). WithLLM(llmProvider). WithSystemPrompt(systemPrompt). WithMCP(mcpManager). WithMCPConfig(core.MCPConfig{ Tools: []string{\"calculator\", \"weather\"}, MaxToolCalls: 5, ToolTimeout: 10 * time.Second, }). Build() }\r3. Handling Tool Results func setupToolResultHandling(runner *core.Runner) { // Register callback for tool execution runner.RegisterCallback(core.HookBeforeToolExecution, \"tool-execution-logger\", func(ctx context.Context, args core.CallbackArgs) (core.State, error) { toolName := args.ToolName toolParams := args.ToolParams fmt.Printf(\"Executing tool: %s with params: %v\\n\", toolName, toolParams) return args.State, nil }, ) // Register callback for tool result runner.RegisterCallback(core.HookAfterToolExecution, \"tool-result-logger\", func(ctx context.Context, args core.CallbackArgs) (core.State, error) { toolName := args.ToolName toolResult := args.ToolResult toolError := args.Error if toolError != nil { fmt.Printf(\"Tool %s failed: %v\\n\", toolName, toolError) } else { fmt.Printf(\"Tool %s result: %v\\n\", toolName, toolResult) } return args.State, nil }, ) // Register callback for tool error runner.RegisterCallback(core.HookToolError, \"tool-error-handler\", func(ctx context.Context, args core.CallbackArgs) (core.State, error) { toolName := args.ToolName toolError := args.Error fmt.Printf(\"Tool error: %s - %v\\n\", toolName, toolError) // Add error information to state newState := args.State.Clone() newState.Set(\"tool_error\", fmt.Sprintf(\"The %s tool encountered an error: %v\", toolName, toolError)) return newState, nil }, ) }\rAdvanced Tool Integration 1. Dynamic Tool Selection type DynamicToolSelector struct { tools map[string]core.Tool llmProvider core.LLMProvider } func NewDynamicToolSelector(llmProvider core.LLMProvider) *DynamicToolSelector { return \u0026DynamicToolSelector{ tools: make(map[string]core.Tool), llmProvider: llmProvider, } } func (dts *DynamicToolSelector) RegisterTool(name string, tool core.Tool) { dts.tools[name] = tool } func (dts *DynamicToolSelector) SelectTools(ctx context.Context, query string) ([]string, error) { // Create tool descriptions var toolDescriptions strings.Builder for name, tool := range dts.tools { toolDescriptions.WriteString(fmt.Sprintf(\"- %s: %s\\n\", name, tool.Description())) } // Create prompt for tool selection prompt := fmt.Sprintf(`Given the following user query and available tools, select the most appropriate tools to answer the query. User query: %s Available tools: %s Return only the names of the tools that should be used, separated by commas (e.g., \"calculator,weather\"). If no tools are needed, return \"none\". Selected tools:`, query, toolDescriptions.String()) // Get tool selection from LLM response, err := dts.llmProvider.Generate(ctx, prompt) if err != nil { return nil, fmt.Errorf(\"tool selection failed: %w\", err) } // Parse response response = strings.TrimSpace(response) if response == \"none\" { return []string{}, nil } // Split by comma and trim spaces selectedTools := strings.Split(response, \",\") for i := range selectedTools { selectedTools[i] = strings.TrimSpace(selectedTools[i]) } // Validate selected tools validTools := make([]string, 0, len(selectedTools)) for _, name := range selectedTools { if _, exists := dts.tools[name]; exists { validTools = append(validTools, name) } } return validTools, nil }\r2. Tool Result Processing type ToolResultProcessor struct { llmProvider core.LLMProvider } func NewToolResultProcessor(llmProvider core.LLMProvider) *ToolResultProcessor { return \u0026ToolResultProcessor{ llmProvider: llmProvider, } } func (trp *ToolResultProcessor) ProcessResult(ctx context.Context, toolName string, result interface{}, query string) (string, error) { // Convert result to string representation resultStr := fmt.Sprintf(\"%v\", result) if resultMap, ok := result.(map[string]interface{}); ok { resultBytes, err := json.MarshalIndent(resultMap, \"\", \" \") if err == nil { resultStr = string(resultBytes) } } // Create prompt for result processing prompt := fmt.Sprintf(`You are processing the result of a tool execution. Format the result in a clear, human-readable way that answers the user's query. User query: %s Tool used: %s Tool result: %s Provide a concise, helpful response based on this result:`, query, toolName, resultStr) // Get formatted response from LLM return trp.llmProvider.Generate(ctx, prompt) }\r3. Tool Caching and Performance type CachedTool struct { tool core.Tool cache map[string]CacheEntry cacheTTL time.Duration mu sync.RWMutex } type CacheEntry struct { Result interface{} Timestamp time.Time } func NewCachedTool(tool core.Tool, cacheTTL time.Duration) *CachedTool { return \u0026CachedTool{ tool: tool, cache: make(map[string]CacheEntry), cacheTTL: cacheTTL, } } func (ct *CachedTool) Name() string { return ct.tool.Name() } func (ct *CachedTool) Description() string { return ct.tool.Description() } func (ct *CachedTool) ParameterSchema() map[string]core.ParameterDefinition { return ct.tool.ParameterSchema() } func (ct *CachedTool) Execute(ctx context.Context, params map[string]interface{}) (interface{}, error) { // Create cache key from parameters cacheKey := ct.createCacheKey(params) // Check cache if result, found := ct.getCachedResult(cacheKey); found { return result, nil } // Execute tool result, err := ct.tool.Execute(ctx, params) if err != nil { return nil, err } // Cache result ct.setCachedResult(cacheKey, result) return result, nil } func (ct *CachedTool) createCacheKey(params map[string]interface{}) string { // Create deterministic key from parameters keys := make([]string, 0, len(params)) for k := range params { keys = append(keys, k) } sort.Strings(keys) var keyBuilder strings.Builder for _, k := range keys { keyBuilder.WriteString(fmt.Sprintf(\"%s:%v;\", k, params[k])) } // Hash the key for consistent length hasher := sha256.New() hasher.Write([]byte(keyBuilder.String())) return fmt.Sprintf(\"%x\", hasher.Sum(nil)) } func (ct *CachedTool) getCachedResult(key string) (interface{}, bool) { ct.mu.RLock() defer ct.mu.RUnlock() entry, exists := ct.cache[key] if !exists { return nil, false } // Check if entry is expired if time.Since(entry.Timestamp) \u003e ct.cacheTTL { return nil, false } return entry.Result, true } func (ct *CachedTool) setCachedResult(key string, result interface{}) { ct.mu.Lock() defer ct.mu.Unlock() ct.cache[key] = CacheEntry{ Result: result, Timestamp: time.Now(), } // Clean up expired entries periodically if len(ct.cache)%100 == 0 { go ct.cleanupExpiredEntries() } } func (ct *CachedTool) cleanupExpiredEntries() { ct.mu.Lock() defer ct.mu.Unlock() now := time.Now() for key, entry := range ct.cache { if now.Sub(entry.Timestamp) \u003e ct.cacheTTL { delete(ct.cache, key) } } }\rTool Configuration and Management 1. Tool Configuration type ToolConfig struct { Name string `json:\"name\"` Enabled bool `json:\"enabled\"` MaxCalls int `json:\"max_calls\"` Timeout time.Duration `json:\"timeout\"` RateLimit RateLimitConfig `json:\"rate_limit\"` Cache CacheConfig `json:\"cache\"` Parameters map[string]interface{} `json:\"parameters\"` } type RateLimitConfig struct { MaxRequests int `json:\"max_requests\"` Interval time.Duration `json:\"interval\"` } type CacheConfig struct { Enabled bool `json:\"enabled\"` TTL time.Duration `json:\"ttl\"` } type ToolManager struct { tools map[string]core.Tool configs map[string]ToolConfig mu sync.RWMutex } func NewToolManager() *ToolManager { return \u0026ToolManager{ tools: make(map[string]core.Tool), configs: make(map[string]ToolConfig), } } func (tm *ToolManager) RegisterTool(tool core.Tool, config ToolConfig) error { tm.mu.Lock() defer tm.mu.Unlock() name := tool.Name() // Apply configuration wrappers wrappedTool := tool // Add caching if enabled if config.Cache.Enabled { wrappedTool = NewCachedTool(wrappedTool, config.Cache.TTL) } // Add rate limiting if configured if config.RateLimit.MaxRequests \u003e 0 { wrappedTool = NewRateLimitedTool(wrappedTool, config.RateLimit.MaxRequests, config.RateLimit.Interval) } // Add validation wrappedTool = NewValidatedTool(wrappedTool, true) tm.tools[name] = wrappedTool tm.configs[name] = config return nil } func (tm *ToolManager) GetTool(name string) (core.Tool, error) { tm.mu.RLock() defer tm.mu.RUnlock() tool, exists := tm.tools[name] if !exists { return nil, fmt.Errorf(\"tool not found: %s\", name) } config := tm.configs[name] if !config.Enabled { return nil, fmt.Errorf(\"tool disabled: %s\", name) } return tool, nil } func (tm *ToolManager) ListEnabledTools() []string { tm.mu.RLock() defer tm.mu.RUnlock() var enabled []string for name, config := range tm.configs { if config.Enabled { enabled = append(enabled, name) } } return enabled }\r2. Tool Discovery and Registration type ToolDiscovery struct { registry map[string]ToolFactory mu sync.RWMutex } type ToolFactory func(config map[string]interface{}) (core.Tool, error) func NewToolDiscovery() *ToolDiscovery { td := \u0026ToolDiscovery{ registry: make(map[string]ToolFactory), } // Register built-in tool factories td.RegisterFactory(\"calculator\", func(config map[string]interface{}) (core.Tool, error) { return tools.NewCalculatorTool(), nil }) td.RegisterFactory(\"weather\", func(config map[string]interface{}) (core.Tool, error) { apiKey, ok := config[\"api_key\"].(string) if !ok || apiKey == \"\" { return nil, fmt.Errorf(\"weather tool requires api_key parameter\") } return tools.NewWeatherTool(apiKey), nil }) td.RegisterFactory(\"counter\", func(config map[string]interface{}) (core.Tool, error) { return tools.NewCounterTool(), nil }) return td } func (td *ToolDiscovery) RegisterFactory(name string, factory ToolFactory) { td.mu.Lock() defer td.mu.Unlock() td.registry[name] = factory } func (td *ToolDiscovery) CreateTool(name string, config map[string]interface{}) (core.Tool, error) { td.mu.RLock() factory, exists := td.registry[name] td.mu.RUnlock() if !exists { return nil, fmt.Errorf(\"unknown tool type: %s\", name) } return factory(config) } func (td *ToolDiscovery) ListAvailableTools() []string { td.mu.RLock() defer td.mu.RUnlock() tools := make([]string, 0, len(td.registry)) for name := range td.registry { tools = append(tools, name) } return tools }\rProduction Integration Patterns 1. Tool Health Monitoring type ToolHealthMonitor struct { tools map[string]core.Tool metrics map[string]*ToolMetrics mu sync.RWMutex } type ToolMetrics struct { TotalCalls int64 `json:\"total_calls\"` SuccessCalls int64 `json:\"success_calls\"` ErrorCalls int64 `json:\"error_calls\"` AverageTime time.Duration `json:\"average_time\"` LastError string `json:\"last_error\"` LastErrorTime time.Time `json:\"last_error_time\"` } func NewToolHealthMonitor() *ToolHealthMonitor { return \u0026ToolHealthMonitor{ tools: make(map[string]core.Tool), metrics: make(map[string]*ToolMetrics), } } func (thm *ToolHealthMonitor) WrapTool(tool core.Tool) core.Tool { thm.mu.Lock() defer thm.mu.Unlock() name := tool.Name() thm.tools[name] = tool thm.metrics[name] = \u0026ToolMetrics{} return \u0026MonitoredTool{ tool: tool, monitor: thm, } } type MonitoredTool struct { tool core.Tool monitor *ToolHealthMonitor } func (mt *MonitoredTool) Name() string { return mt.tool.Name() } func (mt *MonitoredTool) Description() string { return mt.tool.Description() } func (mt *MonitoredTool) ParameterSchema() map[string]core.ParameterDefinition { return mt.tool.ParameterSchema() } func (mt *MonitoredTool) Execute(ctx context.Context, params map[string]interface{}) (interface{}, error) { start := time.Now() name := mt.tool.Name() // Execute tool result, err := mt.tool.Execute(ctx, params) // Record metrics duration := time.Since(start) mt.monitor.recordExecution(name, duration, err) return result, err } func (thm *ToolHealthMonitor) recordExecution(toolName string, duration time.Duration, err error) { thm.mu.Lock() defer thm.mu.Unlock() metrics := thm.metrics[toolName] metrics.TotalCalls++ if err != nil { metrics.ErrorCalls++ metrics.LastError = err.Error() metrics.LastErrorTime = time.Now() } else { metrics.SuccessCalls++ } // Update average time (simple moving average) if metrics.TotalCalls == 1 { metrics.AverageTime = duration } else { metrics.AverageTime = time.Duration( (int64(metrics.AverageTime)*metrics.TotalCalls + int64(duration)) / (metrics.TotalCalls + 1), ) } } func (thm *ToolHealthMonitor) GetMetrics(toolName string) (*ToolMetrics, error) { thm.mu.RLock() defer thm.mu.RUnlock() metrics, exists := thm.metrics[toolName] if !exists { return nil, fmt.Errorf(\"tool not found: %s\", toolName) } // Return a copy to avoid race conditions return \u0026ToolMetrics{ TotalCalls: metrics.TotalCalls, SuccessCalls: metrics.SuccessCalls, ErrorCalls: metrics.ErrorCalls, AverageTime: metrics.AverageTime, LastError: metrics.LastError, LastErrorTime: metrics.LastErrorTime, }, nil } func (thm *ToolHealthMonitor) GetHealthStatus() map[string]string { thm.mu.RLock() defer thm.mu.RUnlock() status := make(map[string]string) for name, metrics := range thm.metrics { if metrics.TotalCalls == 0 { status[name] = \"unknown\" continue } errorRate := float64(metrics.ErrorCalls) / float64(metrics.TotalCalls) switch { case errorRate == 0: status[name] = \"healthy\" case errorRate \u003c 0.1: status[name] = \"warning\" default: status[name] = \"unhealthy\" } } return status }\r2. Complete Integration Example package main import ( \"context\" \"encoding/json\" \"fmt\" \"log\" \"os\" \"time\" \"github.com/kunalkushwaha/agenticgokit/core\" \"github.com/kunalkushwaha/agenticgokit/tools\" ) func main() { // Create tool discovery and manager discovery := NewToolDiscovery() manager := NewToolManager() monitor := NewToolHealthMonitor() // Configure tools toolConfigs := []struct { name string config ToolConfig params map[string]interface{} }{ { name: \"calculator\", config: ToolConfig{ Name: \"calculator\", Enabled: true, MaxCalls: 100, Timeout: 5 * time.Second, Cache: CacheConfig{ Enabled: true, TTL: 1 * time.Hour, }, }, }, { name: \"weather\", config: ToolConfig{ Name: \"weather\", Enabled: true, MaxCalls: 50, Timeout: 10 * time.Second, RateLimit: RateLimitConfig{ MaxRequests: 10, Interval: 1 * time.Minute, }, Cache: CacheConfig{ Enabled: true, TTL: 15 * time.Minute, }, }, params: map[string]interface{}{ \"api_key\": os.Getenv(\"WEATHER_API_KEY\"), }, }, } // Create and register tools for _, tc := range toolConfigs { tool, err := discovery.CreateTool(tc.name, tc.params) if err != nil { log.Fatalf(\"Failed to create tool %s: %v\", tc.name, err) } // Wrap with monitoring monitoredTool := monitor.WrapTool(tool) // Register with manager if err := manager.RegisterTool(monitoredTool, tc.config); err != nil { log.Fatalf(\"Failed to register tool %s: %v\", tc.name, err) } } // Create MCP manager and register tools mcpManager := core.NewMCPManager() for _, toolName := range manager.ListEnabledTools() { tool, err := manager.GetTool(toolName) if err != nil { log.Printf(\"Failed to get tool %s: %v\", toolName, err) continue } mcpManager.RegisterTool(toolName, tool) } // Create LLM provider llmProvider, err := core.NewOpenAIProvider( os.Getenv(\"OPENAI_API_KEY\"), \"gpt-4\", core.WithTemperature(0.2), core.WithMaxTokens(1000), ) if err != nil { log.Fatalf(\"Failed to create LLM provider: %v\", err) } // Create agent agent, err := core.NewAgent(\"assistant\"). WithLLM(llmProvider). WithMCP(mcpManager). WithMCPConfig(core.MCPConfig{ Tools: manager.ListEnabledTools(), MaxToolCalls: 5, ToolTimeout: 10 * time.Second, }). Build() if err != nil { log.Fatalf(\"Failed to create agent: %v\", err) } // Create runner with tool monitoring runner := core.NewRunner(100) runner.RegisterAgent(\"assistant\", agent) // Setup tool result handling setupToolResultHandling(runner) // Start runner ctx := context.Background() runner.Start(ctx) defer runner.Stop() // Example interaction event := core.NewEvent( \"assistant\", core.EventData{\"message\": \"Calculate 15 * 23 and tell me the weather in London\"}, map[string]string{\"session_id\": \"demo-session\"}, ) runner.Emit(event) // Wait and show metrics time.Sleep(10 * time.Second) // Display tool health metrics fmt.Println(\"\\nTool Health Status:\") healthStatus := monitor.GetHealthStatus() for tool, status := range healthStatus { fmt.Printf(\"- %s: %s\\n\", tool, status) if metrics, err := monitor.GetMetrics(tool); err == nil { fmt.Printf(\" Calls: %d (Success: %d, Errors: %d)\\n\", metrics.TotalCalls, metrics.SuccessCalls, metrics.ErrorCalls) fmt.Printf(\" Average Time: %v\\n\", metrics.AverageTime) if metrics.LastError != \"\" { fmt.Printf(\" Last Error: %s (%v)\\n\", metrics.LastError, metrics.LastErrorTime) } } } }\rBest Practices 1. Tool Integration Guidelines Selective Tool Access: Only provide agents with tools they actually need Tool Validation: Always validate tool parameters and results Error Handling: Implement robust error handling and recovery Performance Monitoring: Monitor tool performance and health Security: Implement proper authentication and authorization Caching: Use caching for expensive or frequently called tools Rate Limiting: Protect external services with rate limiting 2. Prompting Best Practices Clear Tool Descriptions: Provide clear, concise tool descriptions Parameter Documentation: Document all parameters with examples Usage Examples: Include examples of when and how to use tools Error Guidance: Explain how to handle tool errors Tool Selection: Guide the agent on tool selection criteria 3. Production Considerations Monitoring: Implement comprehensive tool monitoring Logging: Log all tool executions for debugging Metrics: Track tool usage and performance metrics Health Checks: Implement tool health checking Fallbacks: Provide fallback mechanisms for tool failures Configuration: Make tool configuration externally manageable Troubleshooting Common Issues Tool Not Found: Ensure tools are properly registered with the MCP manager Parameter Validation Errors: Check parameter schemas and validation rules Timeout Errors: Adjust tool timeout settings for slow operations Rate Limit Exceeded: Configure appropriate rate limits for external services Authentication Failures: Verify API keys and authentication credentials Debugging Tips Enable detailed logging for tool executions Use tool health monitoring to identify problematic tools Test tools independently before integrating with agents Monitor tool performance metrics regularly Implement proper error handling and recovery mechanisms Conclusion Tool integration is a powerful feature that extends agent capabilities beyond text generation. By following the patterns and best practices in this tutorial, you can create robust, scalable tool integrations that enhance your agent systems.",
    "tags": [],
    "title": "tool-integration",
    "uri": "/AgenticGoKitDocs/tutorials/mcp/tool-integration/index.html"
  },
  {
    "breadcrumb": "content \u003e guides",
    "content": "Troubleshooting Guide Quick solutions for common AgenticGoKit issues\nThis guide helps you diagnose and resolve common problems when building with AgenticGoKit. Issues are organized by category with step-by-step solutions and prevention tips.\n🚨 Quick Diagnostics Health Check Commands # Check AgenticGoKit installation agentcli version # Verify project structure agentcli validate # Test basic functionality go run . -m \"Hello, world!\" --dry-run\rCommon Error Patterns Error Pattern Likely Cause Quick Fix connection refused Service not running Start required services API key not found Missing environment variables Set API keys agent not found Registration issue Check agent registration timeout Performance or network issue Increase timeouts out of memory Resource exhaustion Optimize memory usage 🔧 Installation \u0026 Setup Issues AgenticGoKit CLI Not Found Symptoms:\nagentcli: command not found\rSolutions:\nInstall AgenticGoKit CLI: # Using Go go install github.com/kunalkushwaha/agenticgokit/cmd/agentcli@latest # Verify installation agentcli version\rCheck PATH: # Add Go bin to PATH export PATH=$PATH:$(go env GOPATH)/bin # Make permanent (add to ~/.bashrc or ~/.zshrc) echo 'export PATH=$PATH:$(go env GOPATH)/bin' \u003e\u003e ~/.bashrc\rAlternative installation: # Clone and build git clone https://github.com/kunalkushwaha/agenticgokit.git cd agenticgokit go build -o agentcli ./cmd/agentcli sudo mv agentcli /usr/local/bin/\rGo Module Issues Symptoms:\ngo: module not found go: cannot find module providing package\rSolutions:\nInitialize Go module: go mod init your-project-name go mod tidy\rUpdate dependencies: go get -u github.com/kunalkushwaha/agenticgokit@latest go mod tidy\rClear module cache: go clean -modcache go mod download\rProject Creation Fails Symptoms:\nError: failed to create project directory Permission denied\rSolutions:\nCheck permissions: # Ensure write permissions in current directory ls -la chmod 755 .\rUse different directory: cd ~/projects agentcli create my-project\rRun with elevated permissions (if necessary): sudo agentcli create my-project sudo chown -R $USER:$USER my-project\r🤖 Agent Execution Issues Agent Not Responding Symptoms:\nNo output from agents Workflow hangs indefinitely Silent failures Diagnosis:\nEnable debug logging: export AGENTFLOW_LOG_LEVEL=debug go run . -m \"test message\"\rCheck agent registration: // Verify agents are registered func main() { agents := map[string]core.AgentHandler{ \"agent1\": myAgent1, \"agent2\": myAgent2, } // Debug: Print registered agents for name := range agents { log.Printf(\"Registered agent: %s\", name) } runner := core.CreateSequentialRunner(agents, []string{\"agent1\", \"agent2\"}, 30*time.Second) // ... }\rSolutions:\nVerify agent implementation: func (a *MyAgent) Execute(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { log.Printf(\"Agent %s executing with event type: %s\", a.name, event.Type) // Ensure you return a result return \u0026core.AgentResult{ Data: map[string]interface{}{ \"response\": \"Agent executed successfully\", }, }, nil }\rCheck event routing: // Ensure event types match what agents expect event := core.NewEvent(\"process\", map[string]interface{}{ \"input\": \"test data\", })\rAdd timeout handling: ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second) defer cancel() result, err := agent.Execute(ctx, event, state)\rInfinite Loops Symptoms:\nHigh CPU usage Agents keep executing without finishing No final result Diagnosis:\n// Add loop detection type LoopDetector struct { executions map[string]int maxExecs int } func (ld *LoopDetector) CheckLoop(agentName string) bool { ld.executions[agentName]++ if ld.executions[agentName] \u003e ld.maxExecs { log.Printf(\"WARNING: Agent %s executed %d times - possible loop\", agentName, ld.executions[agentName]) return true } return false }\rSolutions:\nSet maximum iterations: runner := core.CreateSequentialRunner(agents, agentOrder, 30*time.Second) runner.SetMaxIterations(10) // Prevent infinite loops\rAdd termination conditions: func (a *MyAgent) Execute(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { // Check if work is already done if state.Data[\"completed\"] == true { return \u0026core.AgentResult{ Data: map[string]interface{}{ \"message\": \"Work already completed\", }, }, nil } // Do work and mark as completed result := \u0026core.AgentResult{ Data: map[string]interface{}{ \"response\": \"Work done\", \"completed\": true, }, } return result, nil }\rUse proper orchestration: // For sequential processing, ensure proper agent order agents := []string{\"input\", \"process\", \"output\"} // Clear sequence // For collaborative, ensure agents don't interfere runner := core.CreateCollaborativeRunner(agentMap, 30*time.Second)\r🔌 LLM Provider Issues API Key Errors Symptoms:\nError: API key not found Error: unauthorized Error: invalid API key\rSolutions:\nSet environment variables: # OpenAI export OPENAI_API_KEY=\"your-key-here\" # Azure OpenAI export AZURE_OPENAI_API_KEY=\"your-key-here\" export AZURE_OPENAI_ENDPOINT=\"https://your-resource.openai.azure.com/\" export AZURE_OPENAI_DEPLOYMENT=\"your-deployment-name\" # Ollama (if using local) export OLLAMA_HOST=\"http://localhost:11434\"\rVerify keys are loaded: func main() { apiKey := os.Getenv(\"OPENAI_API_KEY\") if apiKey == \"\" { log.Fatal(\"OPENAI_API_KEY environment variable not set\") } log.Printf(\"API key loaded: %s...\", apiKey[:8]) // Show first 8 chars }\rUse .env files: # Create .env file cat \u003e .env \u003c\u003c EOF OPENAI_API_KEY=your-key-here AZURE_OPENAI_API_KEY=your-azure-key EOF # Load in your application go get github.com/joho/godotenv\rimport \"github.com/joho/godotenv\" func init() { if err := godotenv.Load(); err != nil { log.Printf(\"No .env file found\") } }\rRate Limiting Symptoms:\nError: rate limit exceeded Error: too many requests HTTP 429 errors\rSolutions:\nImplement exponential backoff: import \"time\" func retryWithBackoff(fn func() error, maxRetries int) error { for i := 0; i \u003c maxRetries; i++ { err := fn() if err == nil { return nil } if strings.Contains(err.Error(), \"rate limit\") { delay := time.Duration(1\u003c\u003ci) * time.Second // Exponential backoff log.Printf(\"Rate limited, waiting %v before retry %d/%d\", delay, i+1, maxRetries) time.Sleep(delay) continue } return err // Non-rate-limit error } return fmt.Errorf(\"max retries exceeded\") }\rAdd request throttling: import \"golang.org/x/time/rate\" type ThrottledProvider struct { provider core.ModelProvider limiter *rate.Limiter } func NewThrottledProvider(provider core.ModelProvider, requestsPerSecond float64) *ThrottledProvider { return \u0026ThrottledProvider{ provider: provider, limiter: rate.NewLimiter(rate.Limit(requestsPerSecond), 1), } } func (tp *ThrottledProvider) GenerateResponse(ctx context.Context, prompt string, options map[string]interface{}) (string, error) { if err := tp.limiter.Wait(ctx); err != nil { return \"\", err } return tp.provider.GenerateResponse(ctx, prompt, options) }\rUse multiple API keys: type RotatingProvider struct { providers []core.ModelProvider current int mutex sync.Mutex } func (rp *RotatingProvider) GenerateResponse(ctx context.Context, prompt string, options map[string]interface{}) (string, error) { rp.mutex.Lock() provider := rp.providers[rp.current] rp.current = (rp.current + 1) % len(rp.providers) rp.mutex.Unlock() return provider.GenerateResponse(ctx, prompt, options) }\rConnection Timeouts Symptoms:\nError: context deadline exceeded Error: connection timeout Error: request timeout\rSolutions:\nIncrease timeouts: # In agentflow.toml [runtime] timeout_seconds = 60 # Increase from default 30 [providers.openai] timeout_seconds = 45 [providers.azure] timeout_seconds = 45\rImplement timeout handling: func (a *MyAgent) Execute(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { // Create timeout context timeoutCtx, cancel := context.WithTimeout(ctx, 30*time.Second) defer cancel() // Use timeout context for LLM calls response, err := a.llmProvider.GenerateResponse(timeoutCtx, prompt, nil) if err != nil { if errors.Is(err, context.DeadlineExceeded) { return nil, fmt.Errorf(\"LLM request timed out after 30 seconds\") } return nil, err } return \u0026core.AgentResult{Data: map[string]interface{}{\"response\": response}}, nil }\rAdd retry logic: func callWithRetry(ctx context.Context, provider core.ModelProvider, prompt string, maxRetries int) (string, error) { for i := 0; i \u003c maxRetries; i++ { response, err := provider.GenerateResponse(ctx, prompt, nil) if err == nil { return response, nil } if errors.Is(err, context.DeadlineExceeded) \u0026\u0026 i \u003c maxRetries-1 { log.Printf(\"Timeout on attempt %d/%d, retrying...\", i+1, maxRetries) continue } return \"\", err } return \"\", fmt.Errorf(\"all retry attempts failed\") }\r💾 Memory \u0026 Database Issues Database Connection Failed Symptoms:\nError: failed to connect to database Error: connection refused Error: database does not exist\rSolutions:\nCheck database is running: # For PostgreSQL docker compose ps docker compose logs postgres # Test connection manually psql -h localhost -U agentflow -d agentflow -c \"SELECT 1;\"\rVerify connection string: [agent_memory] provider = \"pgvector\" connection = \"postgres://agentflow:password@localhost:5432/agentflow?sslmode=disable\"\rInitialize database: # Run setup script ./setup.sh # Or manually psql -h localhost -U agentflow -d agentflow -f init-db.sql\rCheck Docker networking: # If using Docker, check network connectivity docker network ls docker compose exec postgres pg_isready -U agentflow\rVector Search Not Working Symptoms:\nError: vector extension not found Error: operator does not exist for vector Poor search results\rSolutions:\nVerify pgvector extension: -- Connect to database psql -h localhost -U agentflow -d agentflow -- Check extension SELECT * FROM pg_extension WHERE extname = 'vector'; -- Install if missing CREATE EXTENSION IF NOT EXISTS vector;\rCheck vector dimensions: -- Verify table structure \\d+ embeddings -- Check if dimensions match your embedding model SELECT embedding FROM embeddings LIMIT 1;\rRebuild indexes: -- Drop and recreate vector indexes DROP INDEX IF EXISTS embeddings_embedding_idx; CREATE INDEX embeddings_embedding_idx ON embeddings USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);\rMemory Usage Too High Symptoms:\nApplication crashes with out of memory Slow performance High RAM usage Solutions:\nMonitor memory usage: import \"runtime\" func logMemoryUsage() { var m runtime.MemStats runtime.ReadMemStats(\u0026m) log.Printf(\"Memory: Alloc=%d KB, Sys=%d KB, NumGC=%d\", m.Alloc/1024, m.Sys/1024, m.NumGC) } // Call periodically go func() { for range time.Tick(30 * time.Second) { logMemoryUsage() } }()\rOptimize batch sizes: [agent_memory.embedding] max_batch_size = 50 # Reduce from default 100 [agent_memory] max_connections = 10 # Reduce connection pool\rImplement memory cleanup: func (a *MyAgent) Execute(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { defer runtime.GC() // Force garbage collection after execution // Your agent logic... return result, nil }\r🔧 MCP Tool Issues Tools Not Available Symptoms:\nError: tool not found Error: MCP server not configured No tools discovered\rSolutions:\nCheck MCP configuration: # Verify MCP is enabled cat agentflow.toml | grep -A 10 \"\\[mcp\\]\" # Check server status agentcli mcp servers # List available tools agentcli mcp tools\rEnable MCP servers: [[mcp.servers]] name = \"brave-search\" type = \"stdio\" command = \"npx @modelcontextprotocol/server-brave-search\" enabled = true # Make sure this is true\rInstall MCP servers: # Install required MCP servers npm install -g @modelcontextprotocol/server-brave-search npm install -g @modelcontextprotocol/server-filesystem # Test server directly npx @modelcontextprotocol/server-brave-search\rTool Execution Fails Symptoms:\nError: tool execution failed Error: timeout executing tool Tool returned empty result\rSolutions:\nCheck tool parameters: // Ensure proper parameter format params := map[string]interface{}{ \"query\": \"search term\", \"num_results\": 5, } result, err := toolManager.ExecuteTool(ctx, \"web_search\", params)\rAdd error handling: func (a *MyAgent) Execute(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { result, err := a.toolManager.ExecuteTool(ctx, \"web_search\", params) if err != nil { log.Printf(\"Tool execution failed: %v\", err) // Provide fallback response return \u0026core.AgentResult{ Data: map[string]interface{}{ \"error\": \"Tool temporarily unavailable\", \"fallback\": \"Unable to search at this time\", }, }, nil } return \u0026core.AgentResult{Data: map[string]interface{}{\"result\": result}}, nil }\rIncrease timeouts: [mcp] connection_timeout = 30000 # 30 seconds max_retries = 5 retry_delay = 2000\r🚀 Performance Issues Slow Response Times Symptoms:\nLong delays between agent responses High latency Timeouts Diagnosis:\n// Add timing measurements func (a *MyAgent) Execute(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { start := time.Now() defer func() { log.Printf(\"Agent %s took %v\", a.name, time.Since(start)) }() // Your agent logic... }\rSolutions:\nOptimize LLM calls: // Use shorter prompts prompt := fmt.Sprintf(\"Summarize: %s\", truncateText(input, 1000)) // Cache responses type CachedProvider struct { provider core.ModelProvider cache map[string]string mutex sync.RWMutex } func (cp *CachedProvider) GenerateResponse(ctx context.Context, prompt string, options map[string]interface{}) (string, error) { cp.mutex.RLock() if cached, exists := cp.cache[prompt]; exists { cp.mutex.RUnlock() return cached, nil } cp.mutex.RUnlock() response, err := cp.provider.GenerateResponse(ctx, prompt, options) if err == nil { cp.mutex.Lock() cp.cache[prompt] = response cp.mutex.Unlock() } return response, err }\rParallel processing: // Process multiple agents concurrently func processAgentsConcurrently(ctx context.Context, agents []core.AgentHandler, event core.Event, state *core.State) ([]*core.AgentResult, error) { results := make([]*core.AgentResult, len(agents)) errors := make([]error, len(agents)) var wg sync.WaitGroup for i, agent := range agents { wg.Add(1) go func(idx int, a core.AgentHandler) { defer wg.Done() result, err := a.Execute(ctx, event, state) results[idx] = result errors[idx] = err }(i, agent) } wg.Wait() // Check for errors for _, err := range errors { if err != nil { return nil, err } } return results, nil }\rDatabase optimization: -- Add indexes for common queries CREATE INDEX IF NOT EXISTS idx_agent_memory_session ON agent_memory(session_id, created_at); -- Optimize vector search CREATE INDEX embeddings_embedding_hnsw_idx ON embeddings USING hnsw (embedding vector_cosine_ops);\rHigh Resource Usage Solutions:\nLimit concurrent operations: // Use semaphore to limit concurrency semaphore := make(chan struct{}, 5) // Max 5 concurrent operations func (a *MyAgent) Execute(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { semaphore \u003c- struct{}{} // Acquire defer func() { \u003c-semaphore }() // Release // Your agent logic... }\rOptimize memory usage: // Use object pools for frequently allocated objects var resultPool = sync.Pool{ New: func() interface{} { return \u0026core.AgentResult{ Data: make(map[string]interface{}), } }, } func (a *MyAgent) Execute(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { result := resultPool.Get().(*core.AgentResult) defer resultPool.Put(result) // Clear previous data for k := range result.Data { delete(result.Data, k) } // Use result... result.Data[\"response\"] = \"...\" // Return a copy, not the pooled object return \u0026core.AgentResult{Data: copyMap(result.Data)}, nil }\r🛠️ Development Issues Build Failures Symptoms:\ngo: build failed undefined: core.AgentHandler package not found\rSolutions:\nUpdate dependencies: go get -u github.com/kunalkushwaha/agenticgokit@latest go mod tidy\rCheck imports: import ( \"github.com/kunalkushwaha/agenticgokit/core\" // Not: \"github.com/kunalkushwaha/agenticgokit/internal/core\" )\rVerify Go version: go version # Should be 1.21 or later\rTesting Issues Solutions:\nMock LLM provider for tests: type MockProvider struct { responses map[string]string } func (m *MockProvider) GenerateResponse(ctx context.Context, prompt string, options map[string]interface{}) (string, error) { if response, exists := m.responses[prompt]; exists { return response, nil } return \"mock response\", nil } // Use in tests func TestMyAgent(t *testing.T) { mockProvider := \u0026MockProvider{ responses: map[string]string{ \"test prompt\": \"expected response\", }, } agent := NewMyAgent(\"test\", mockProvider) // Test agent... }\rTest with timeout: func TestAgentTimeout(t *testing.T) { ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second) defer cancel() result, err := agent.Execute(ctx, event, state) if err != nil { t.Fatalf(\"Agent execution failed: %v\", err) } // Verify result... }\r📞 Getting Help Before Asking for Help Check logs: export AGENTFLOW_LOG_LEVEL=debug go run . -m \"test\" 2\u003e\u00261 | tee debug.log\rGather system info: go version agentcli version docker --version cat agentflow.toml\rCreate minimal reproduction: // Simplest possible example that shows the issue func main() { provider := core.NewMockProvider() agent := agents.NewSimpleAgent(\"test\", provider) event := core.NewEvent(\"test\", map[string]interface{}{\"input\": \"test\"}) state := core.NewState() result, err := agent.Execute(context.Background(), event, state) if err != nil { log.Fatal(err) } log.Printf(\"Result: %+v\", result) }\rCommunity Resources GitHub Issues - Bug reports and feature requests GitHub Discussions - Questions and community help Discord Community - Real-time chat and support Issue Template When reporting issues, include:\n**AgenticGoKit Version:** (agentcli version output)\r**Go Version:** (go version output)\r**Operating System:** (OS and version)\r**Expected Behavior:**\rWhat you expected to happen\r**Actual Behavior:**\rWhat actually happened\r**Steps to Reproduce:**\r1. Step one\r2. Step two\r3. Step three\r**Code Sample:**\r```go\r// Minimal code that reproduces the issue\rLogs:\n// Relevant log output with debug level enabled\rConfiguration:\n// Relevant parts of agentflow.toml\r---\r*This troubleshooting guide covers common issues with AgenticGoKit. The framework is actively developed, so some issues may be resolved in newer versions.*",
    "description": "Troubleshooting Guide Quick solutions for common AgenticGoKit issues\nThis guide helps you diagnose and resolve common problems when building with AgenticGoKit. Issues are organized by category with step-by-step solutions and prevention tips.\n🚨 Quick Diagnostics Health Check Commands # Check AgenticGoKit installation agentcli version # Verify project structure agentcli validate # Test basic functionality go run . -m \"Hello, world!\" --dry-run\rCommon Error Patterns Error Pattern Likely Cause Quick Fix connection refused Service not running Start required services API key not found Missing environment variables Set API keys agent not found Registration issue Check agent registration timeout Performance or network issue Increase timeouts out of memory Resource exhaustion Optimize memory usage 🔧 Installation \u0026 Setup Issues AgenticGoKit CLI Not Found Symptoms:",
    "tags": [],
    "title": "troubleshooting",
    "uri": "/AgenticGoKitDocs/guides/troubleshooting/index.html"
  },
  {
    "breadcrumb": "content",
    "content": "Tutorials Navigation: Documentation Home → Tutorials\nLearning-oriented guides to help you understand and use AgenticGoKit effectively.\nLearning Paths Beginner Path Start here if you’re new to AgenticGoKit:\nYour First Agent - Create a simple agent Multi-Agent Collaboration - Learn agent orchestration Memory and RAG - Add knowledge capabilities Tool Integration - Connect external tools Production Deployment - Deploy to production Core Concepts Path Understand the fundamental concepts:\nAgent Lifecycle - How agents work internally State Management - Data flow between agents Error Handling - Robust error management Orchestration Patterns - Multi-agent coordination Advanced Path For experienced users building complex systems:\nCircuit Breaker Patterns - Fault tolerance Retry Policies - Resilient operations Testing Strategies - Testing multi-agent systems Load Balancing \u0026 Scaling - Production scaling Tutorial Structure Each tutorial includes:\nLearning objectives - What you’ll accomplish Prerequisites - What you need to know first Step-by-step instructions - Clear, actionable steps Working code examples - Complete, runnable code What’s next - Suggested follow-up tutorials Getting Help Check the Troubleshooting Guide for common issues Review the API Reference for detailed documentation See How-To Guides for specific tasks",
    "description": "Tutorials Navigation: Documentation Home → Tutorials\nLearning-oriented guides to help you understand and use AgenticGoKit effectively.\nLearning Paths Beginner Path Start here if you’re new to AgenticGoKit:\nYour First Agent - Create a simple agent Multi-Agent Collaboration - Learn agent orchestration Memory and RAG - Add knowledge capabilities Tool Integration - Connect external tools Production Deployment - Deploy to production Core Concepts Path Understand the fundamental concepts:\nAgent Lifecycle - How agents work internally State Management - Data flow between agents Error Handling - Robust error management Orchestration Patterns - Multi-agent coordination Advanced Path For experienced users building complex systems:",
    "tags": [],
    "title": "tutorials",
    "uri": "/AgenticGoKitDocs/tutorials/index.html"
  },
  {
    "breadcrumb": "content \u003e guides \u003e setup",
    "content": "Vector Databases Configure persistent vector storage for RAG and memory systems\nThis guide walks you through setting up vector databases with AgenticGoKit for persistent memory and RAG (Retrieval-Augmented Generation) capabilities. You’ll learn to configure PostgreSQL with pgvector and Weaviate for different use cases.\nPrerequisites Docker installed on your system Basic understanding of AgenticGoKit memory systems Command line familiarity What You’ll Build A vector database setup that supports:\nDocument storage and retrieval Semantic search capabilities Persistent agent memory RAG-powered question answering Database Options Comparison Feature PostgreSQL + pgvector Weaviate In-Memory Persistence ✅ Full ✅ Full ❌ Temporary Scalability ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐ Setup Complexity ⭐⭐⭐ ⭐⭐ ⭐⭐⭐⭐⭐ Query Performance ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ Resource Usage ⭐⭐⭐ ⭐⭐ ⭐⭐⭐⭐⭐ Best For Production apps Large scale Development Quick Start 1. Create Memory-Enabled Project # Create project with pgvector memory agentcli create vector-db-demo --memory-enabled --memory-provider pgvector \\\\ --rag-enabled --embedding-provider ollama cd vector-db-demo\r2. Start Database Services The project includes a docker-compose.yml file:\n# Start PostgreSQL with pgvector docker compose up -d # Verify the database is running docker compose ps\r3. Initialize Database # Run the setup script (generated with your project) ./setup.sh # On Linux/Mac # or setup.bat # On Windows\r4. Test the Setup # Install dependencies go mod tidy # Set up Ollama (if using local embeddings) ollama pull nomic-embed-text:latest # Test the system go run . -m \\\"Tell me about vector databases\\\"\rPostgreSQL + pgvector Setup Detailed Configuration The generated docker-compose.yml includes:\nversion: '3.8' services: postgres: image: pgvector/pgvector:pg15 environment: POSTGRES_DB: agentflow POSTGRES_USER: agentflow POSTGRES_PASSWORD: password volumes: - postgres_data:/var/lib/postgresql/data - ./init-db.sql:/docker-entrypoint-initdb.d/init-db.sql ports: - \\\"5432:5432\\\" healthcheck: test: [\\\"CMD-SHELL\\\", \\\"pg_isready -U agentflow\\\"] interval: 30s timeout: 10s retries: 3 volumes: postgres_data:\rDatabase Initialization The init-db.sql file sets up the required extensions and tables:\n-- Enable pgvector extension CREATE EXTENSION IF NOT EXISTS vector; -- Create embeddings table CREATE TABLE IF NOT EXISTS embeddings ( id SERIAL PRIMARY KEY, content TEXT NOT NULL, embedding vector(384), -- Adjust dimensions based on your model metadata JSONB, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ); -- Create index for vector similarity search CREATE INDEX IF NOT EXISTS embeddings_embedding_idx ON embeddings USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100); -- Create memory table for agent conversations CREATE TABLE IF NOT EXISTS agent_memory ( id SERIAL PRIMARY KEY, agent_name VARCHAR(255) NOT NULL, session_id VARCHAR(255), content TEXT NOT NULL, embedding vector(384), metadata JSONB, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ); CREATE INDEX IF NOT EXISTS agent_memory_embedding_idx ON agent_memory USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);\rConfiguration in AgenticGoKit The generated agentflow.toml includes:\n[memory] provider = \\\"pgvector\\\" connection = \\\"postgres://agentflow:password@localhost:5432/agentflow?sslmode=disable\\\" max_results = 5 dimensions = 384 # Matches your embedding model auto_embed = true enable_knowledge_base = true [memory.embedding] provider = \\\"ollama\\\" model = \\\"nomic-embed-text:latest\\\" base_url = \\\"http://localhost:11434\\\" cache_embeddings = true max_batch_size = 100 timeout_seconds = 30\rTesting PostgreSQL Setup # Test database connection psql -h localhost -U agentflow -d agentflow -c \\\"SELECT version();\\\" # Check pgvector extension psql -h localhost -U agentflow -d agentflow -c \\\"SELECT * FROM pg_extension WHERE extname = 'vector';\\\" # Test vector operations psql -h localhost -U agentflow -d agentflow -c \\\"SELECT '[1,2,3]'::vector \u003c-\u003e '[4,5,6]'::vector;\\\"\rWeaviate Setup Create Weaviate Project # Create project with Weaviate agentcli create weaviate-demo --memory-enabled --memory-provider weaviate \\\\ --rag-enabled --embedding-provider openai cd weaviate-demo\rWeaviate Docker Compose version: '3.8' services: weaviate: image: semitechnologies/weaviate:latest ports: - \\\"8080:8080\\\" environment: QUERY_DEFAULTS_LIMIT: 25 AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true' PERSISTENCE_DATA_PATH: '/var/lib/weaviate' DEFAULT_VECTORIZER_MODULE: 'none' ENABLE_MODULES: 'text2vec-openai,text2vec-cohere,text2vec-huggingface' CLUSTER_HOSTNAME: 'node1' volumes: - weaviate_data:/var/lib/weaviate volumes: weaviate_data:\rWeaviate Configuration [memory] provider = \\\"weaviate\\\" connection = \\\"http://localhost:8080\\\" max_results = 5 dimensions = 1536 # OpenAI ada-002 dimensions auto_embed = true [memory.embedding] provider = \\\"openai\\\" model = \\\"text-embedding-ada-002\\\" cache_embeddings = true\rTesting Weaviate Setup # Start Weaviate docker compose up -d # Check Weaviate health curl http://localhost:8080/v1/meta # Test with your application export OPENAI_API_KEY=your-key-here go run . -m \\\"What can you remember?\\\"\rAdvanced Configuration Optimizing PostgreSQL Performance -- Tune pgvector settings for better performance ALTER SYSTEM SET shared_preload_libraries = 'vector'; ALTER SYSTEM SET max_connections = 200; ALTER SYSTEM SET shared_buffers = '256MB'; ALTER SYSTEM SET effective_cache_size = '1GB'; ALTER SYSTEM SET maintenance_work_mem = '64MB'; -- Restart PostgreSQL after changes\rCustom Embedding Dimensions If you’re using a different embedding model, update the dimensions:\n-- For different embedding models -- OpenAI ada-002: 1536 dimensions -- Sentence Transformers: 384 or 768 dimensions -- Custom models: varies ALTER TABLE embeddings ALTER COLUMN embedding TYPE vector(1536); ALTER TABLE agent_memory ALTER COLUMN embedding TYPE vector(1536); -- Recreate indexes with new dimensions DROP INDEX IF EXISTS embeddings_embedding_idx; CREATE INDEX embeddings_embedding_idx ON embeddings USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);\rConnection Pooling For production use, configure connection pooling:\n[memory] provider = \\\"pgvector\\\" connection = \\\"postgres://agentflow:password@localhost:5432/agentflow?sslmode=disable\\\" max_connections = 20 idle_connections = 5 connection_lifetime = \\\"1h\\\"\rProduction Considerations Security # Use environment variables for credentials export DB_PASSWORD=$(openssl rand -base64 32) export DB_CONNECTION=\\\"postgres://agentflow:${DB_PASSWORD}@localhost:5432/agentflow?sslmode=require\\\"\r[memory] provider = \\\"pgvector\\\" connection = \\\"${DB_CONNECTION}\\\" # Uses environment variable\rBackup Strategy # PostgreSQL backup pg_dump -h localhost -U agentflow agentflow \u003e backup.sql # Restore psql -h localhost -U agentflow agentflow \u003c backup.sql # Automated backup script #!/bin/bash DATE=$(date +%Y%m%d_%H%M%S) pg_dump -h localhost -U agentflow agentflow | gzip \u003e \\\"backup_${DATE}.sql.gz\\\"\rMonitoring -- Monitor vector index usage SELECT schemaname, tablename, indexname, idx_scan, idx_tup_read, idx_tup_fetch FROM pg_stat_user_indexes WHERE indexname LIKE '%embedding%'; -- Check table sizes SELECT schemaname, tablename, pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size FROM pg_tables WHERE tablename IN ('embeddings', 'agent_memory');\rTroubleshooting Common Issues Database connection failed:\n# Check if database is running docker compose ps # Check logs docker compose logs postgres # Test connection manually psql -h localhost -U agentflow -d agentflow -c \\\"SELECT 1;\\\"\rpgvector extension not found:\n# Ensure you're using the pgvector image docker compose down docker compose pull docker compose up -d\rSlow vector queries:\n-- Check if indexes exist \\\\d+ embeddings -- Recreate index with more lists for larger datasets DROP INDEX embeddings_embedding_idx; CREATE INDEX embeddings_embedding_idx ON embeddings USING ivfflat (embedding vector_cosine_ops) WITH (lists = 1000); -- Increase for larger datasets\rMemory usage too high:\n# Reduce batch sizes and connection limits [memory.embedding] max_batch_size = 50 # Reduce from 100 [memory] max_connections = 10 # Reduce connection pool\rPerformance Tuning For large datasets (\u003e1M vectors):\n-- Use HNSW index for better performance (PostgreSQL 14+) CREATE INDEX embeddings_embedding_hnsw_idx ON embeddings USING hnsw (embedding vector_cosine_ops);\rFor high-throughput applications:\n[memory] connection_timeout = \\\"5s\\\" query_timeout = \\\"10s\\\" max_connections = 50 idle_connections = 10\rMigration Between Providers From In-Memory to PostgreSQL # Create new project with PostgreSQL agentcli create migrated-project --memory-enabled --memory-provider pgvector # Export data from old system (if applicable) # Import into new PostgreSQL setup\rFrom PostgreSQL to Weaviate # Export vectors from PostgreSQL psql -h localhost -U agentflow -d agentflow -c \\\"COPY (SELECT content, embedding, metadata FROM embeddings) TO '/tmp/vectors.csv' WITH CSV HEADER;\\\" # Import into Weaviate (requires custom script) # See Weaviate documentation for bulk import\rNext Steps With your vector database set up:\nImplement Document Ingestion: Add document processing capabilities Optimize RAG Performance: Fine-tune retrieval and generation Build Knowledge Systems: Create comprehensive knowledge bases Monitor Performance: Set up monitoring and alerting Related Guides Memory Systems - Memory system tutorials RAG Implementation - RAG setup guide LLM Providers - LLM configuration MCP Tools - Tool integration Vector database setup is a foundational step for building knowledge-aware agents. Choose the provider that best fits your scale and requirements.",
    "description": "Vector Databases Configure persistent vector storage for RAG and memory systems\nThis guide walks you through setting up vector databases with AgenticGoKit for persistent memory and RAG (Retrieval-Augmented Generation) capabilities. You’ll learn to configure PostgreSQL with pgvector and Weaviate for different use cases.\nPrerequisites Docker installed on your system Basic understanding of AgenticGoKit memory systems Command line familiarity What You’ll Build A vector database setup that supports:\nDocument storage and retrieval Semantic search capabilities Persistent agent memory RAG-powered question answering Database Options Comparison Feature PostgreSQL + pgvector Weaviate In-Memory Persistence ✅ Full ✅ Full ❌ Temporary Scalability ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐ Setup Complexity ⭐⭐⭐ ⭐⭐ ⭐⭐⭐⭐⭐ Query Performance ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ Resource Usage ⭐⭐⭐ ⭐⭐ ⭐⭐⭐⭐⭐ Best For Production apps Large scale Development Quick Start 1. Create Memory-Enabled Project # Create project with pgvector memory agentcli create vector-db-demo --memory-enabled --memory-provider pgvector \\\\ --rag-enabled --embedding-provider ollama cd vector-db-demo\r2. Start Database Services The project includes a docker-compose.yml file:",
    "tags": [],
    "title": "vector-databases",
    "uri": "/AgenticGoKitDocs/guides/setup/vector-databases/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials \u003e memory-systems",
    "content": "Vector Databases in AgenticGoKit Overview Vector databases enable sophisticated similarity search and retrieval-augmented generation (RAG) by storing and querying high-dimensional embeddings. This tutorial covers setting up and using vector databases with AgenticGoKit, including pgvector and Weaviate integration.\nVector databases are essential for production-ready memory systems that need to handle large amounts of data with fast, semantically-aware search capabilities.\nPrerequisites Understanding of Basic Memory Operations Knowledge of vector embeddings and similarity search Basic database administration skills Familiarity with Docker (for setup examples) Vector Database Concepts What are Vector Embeddings? Vector embeddings are numerical representations of text, images, or other data that capture semantic meaning in high-dimensional space:\n\"The cat sat on the mat\" → [0.1, -0.3, 0.8, ..., 0.2] (1536 dimensions)\r\"A feline rested on the rug\" → [0.2, -0.2, 0.7, ..., 0.3] (similar vector)\rSimilarity Search Vector databases use distance metrics to find similar content:\nCosine Similarity: Measures angle between vectors Euclidean Distance: Measures straight-line distance Dot Product: Measures vector alignment pgvector Setup and Configuration 1. Database Setup with Docker # Create docker-compose.yml for pgvector version: '3.8' services: postgres: image: pgvector/pgvector:pg15 environment: POSTGRES_DB: agentdb POSTGRES_USER: agent_user POSTGRES_PASSWORD: agent_pass ports: - \"5432:5432\" volumes: - postgres_data:/var/lib/postgresql/data - ./init.sql:/docker-entrypoint-initdb.d/init.sql volumes: postgres_data:\r-- init.sql CREATE EXTENSION IF NOT EXISTS vector; -- Create memory table CREATE TABLE agent_memory ( id UUID PRIMARY KEY DEFAULT gen_random_uuid(), content TEXT NOT NULL, content_type VARCHAR(100) NOT NULL, embedding vector(1536), -- OpenAI embedding dimensions metadata JSONB, session_id VARCHAR(255), created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(), updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW() ); -- Create indexes for performance CREATE INDEX idx_agent_memory_content_type ON agent_memory(content_type); CREATE INDEX idx_agent_memory_session_id ON agent_memory(session_id); CREATE INDEX idx_agent_memory_created_at ON agent_memory(created_at); CREATE INDEX idx_agent_memory_embedding ON agent_memory USING ivfflat (embedding vector_cosine_ops); -- Create conversation history table CREATE TABLE conversation_history ( id UUID PRIMARY KEY DEFAULT gen_random_uuid(), session_id VARCHAR(255) NOT NULL, role VARCHAR(50) NOT NULL, content TEXT NOT NULL, metadata JSONB, timestamp TIMESTAMP WITH TIME ZONE DEFAULT NOW() ); CREATE INDEX idx_conversation_session_id ON conversation_history(session_id); CREATE INDEX idx_conversation_timestamp ON conversation_history(timestamp);\r2. pgvector Configuration package main import ( \"context\" \"log\" \"os\" \"github.com/kunalkushwaha/agenticgokit/core\" ) func main() { // Configure pgvector memory config := core.AgentMemoryConfig{ Provider: \"pgvector\", Connection: \"postgres://agent_user:agent_pass@localhost:5432/agentdb?sslmode=disable\", EnableRAG: true, Dimensions: 1536, // OpenAI embedding dimensions ChunkSize: 1000, ChunkOverlap: 200, Embedding: core.EmbeddingConfig{ Provider: \"openai\", Model: \"text-embedding-3-small\", APIKey: os.Getenv(\"OPENAI_API_KEY\"), Dimensions: 1536, BatchSize: 100, }, Options: map[string]interface{}{ \"max_connections\": 10, \"timeout\": \"30s\", \"retry_attempts\": 3, \"index_type\": \"ivfflat\", // or \"hnsw\" \"index_lists\": 100, // for ivfflat }, } // Create memory instance memory, err := core.NewMemory(config) if err != nil { log.Fatalf(\"Failed to create pgvector memory: %v\", err) } // Test the connection ctx := context.Background() stats, err := memory.GetStats(ctx) if err != nil { log.Fatalf(\"Failed to get stats: %v\", err) } log.Printf(\"Connected to pgvector: %d items stored\", stats.ItemCount) }\r3. Advanced pgvector Operations func demonstratePgvectorFeatures(memory core.Memory) error { ctx := context.Background() // Store documents with rich metadata documents := []struct { content string docType string metadata map[string]string }{ { content: \"Machine learning is a subset of artificial intelligence that enables computers to learn from data.\", docType: \"definition\", metadata: map[string]string{ \"topic\": \"machine-learning\", \"difficulty\": \"beginner\", \"source\": \"textbook\", }, }, { content: \"Neural networks are computing systems inspired by biological neural networks.\", docType: \"definition\", metadata: map[string]string{ \"topic\": \"neural-networks\", \"difficulty\": \"intermediate\", \"source\": \"research-paper\", }, }, { content: \"Deep learning uses neural networks with multiple layers to model complex patterns.\", docType: \"definition\", metadata: map[string]string{ \"topic\": \"deep-learning\", \"difficulty\": \"advanced\", \"source\": \"textbook\", }, }, } // Store documents for _, doc := range documents { err := memory.Store(ctx, doc.content, doc.docType, core.WithMetadata(doc.metadata), core.WithTimestamp(time.Now()), ) if err != nil { return fmt.Errorf(\"failed to store document: %w\", err) } } // Perform semantic search results, err := memory.Search(ctx, \"What is AI and how does it learn?\", core.WithLimit(3), core.WithScoreThreshold(0.7), core.WithMetadataFilter(map[string]string{ \"difficulty\": \"beginner\", }), ) if err != nil { return fmt.Errorf(\"search failed: %w\", err) } fmt.Printf(\"Found %d relevant documents:\\n\", len(results)) for _, result := range results { fmt.Printf(\"- %s (Score: %.3f)\\n\", result.Content, result.Score) fmt.Printf(\" Topic: %s, Difficulty: %s\\n\", result.Metadata[\"topic\"], result.Metadata[\"difficulty\"]) } return nil }\rWeaviate Integration 1. Weaviate Setup with Docker # docker-compose.yml for Weaviate version: '3.4' services: weaviate: command: - --host - 0.0.0.0 - --port - '8080' - --scheme - http image: semitechnologies/weaviate:1.22.4 ports: - 8080:8080 restart: on-failure:0 environment: QUERY_DEFAULTS_LIMIT: 25 AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true' PERSISTENCE_DATA_PATH: '/var/lib/weaviate' DEFAULT_VECTORIZER_MODULE: 'none' ENABLE_MODULES: 'text2vec-openai,generative-openai' CLUSTER_HOSTNAME: 'node1' volumes: - weaviate_data:/var/lib/weaviate volumes: weaviate_data:\r2. Weaviate Configuration func setupWeaviateMemory() (core.Memory, error) { config := core.AgentMemoryConfig{ Provider: \"weaviate\", Connection: \"http://localhost:8080\", EnableRAG: true, Embedding: core.EmbeddingConfig{ Provider: \"openai\", Model: \"text-embedding-3-small\", APIKey: os.Getenv(\"OPENAI_API_KEY\"), Dimensions: 1536, BatchSize: 100, }, Options: map[string]interface{}{ \"class_name\": \"AgentMemory\", \"timeout\": \"30s\", \"retry_attempts\": 3, \"batch_size\": 100, }, } memory, err := core.NewMemory(config) if err != nil { return nil, fmt.Errorf(\"failed to create Weaviate memory: %w\", err) } return memory, nil }\r3. Advanced Weaviate Features func demonstrateWeaviateFeatures(memory core.Memory) error { ctx := context.Background() // Store multi-modal content content := []struct { text string docType string metadata map[string]string }{ { text: \"A red sports car driving on a mountain road\", docType: \"image-description\", metadata: map[string]string{ \"category\": \"automotive\", \"color\": \"red\", \"setting\": \"mountain\", }, }, { text: \"A blue sedan parked in a city street\", docType: \"image-description\", metadata: map[string]string{ \"category\": \"automotive\", \"color\": \"blue\", \"setting\": \"urban\", }, }, } // Store content for _, item := range content { err := memory.Store(ctx, item.text, item.docType, core.WithMetadata(item.metadata), ) if err != nil { return fmt.Errorf(\"failed to store: %w\", err) } } // Perform complex search with filters results, err := memory.Search(ctx, \"vehicle in urban environment\", core.WithLimit(5), core.WithMetadataFilter(map[string]string{ \"category\": \"automotive\", \"setting\": \"urban\", }), ) if err != nil { return fmt.Errorf(\"search failed: %w\", err) } fmt.Printf(\"Found %d matching items:\\n\", len(results)) for _, result := range results { fmt.Printf(\"- %s (Score: %.3f)\\n\", result.Content, result.Score) } return nil }\rEmbedding Strategies 1. OpenAI Embeddings func setupOpenAIEmbeddings() core.EmbeddingConfig { return core.EmbeddingConfig{ Provider: \"openai\", Model: \"text-embedding-3-small\", // or text-embedding-3-large APIKey: os.Getenv(\"OPENAI_API_KEY\"), Dimensions: 1536, // 3072 for large model BatchSize: 100, Options: map[string]string{ \"user\": \"agenticgokit-user\", // for usage tracking }, } }\r2. Hugging Face Embeddings func setupHuggingFaceEmbeddings() core.EmbeddingConfig { return core.EmbeddingConfig{ Provider: \"huggingface\", Model: \"sentence-transformers/all-MiniLM-L6-v2\", APIKey: os.Getenv(\"HUGGINGFACE_API_KEY\"), // optional for hosted inference Dimensions: 384, BatchSize: 32, Options: map[string]string{ \"normalize_embeddings\": \"true\", \"pooling_mode\": \"mean\", }, } }\r3. Local Embeddings func setupLocalEmbeddings() core.EmbeddingConfig { return core.EmbeddingConfig{ Provider: \"local\", Model: \"all-MiniLM-L6-v2\", Dimensions: 384, BatchSize: 16, Options: map[string]string{ \"model_path\": \"./models/sentence-transformer\", \"device\": \"cpu\", // or \"cuda\" \"max_seq_length\": \"512\", }, } }\rPerformance Optimization 1. Index Configuration // pgvector index optimization func optimizePgvectorIndexes(db *sql.DB) error { // Create HNSW index for better performance _, err := db.Exec(` CREATE INDEX CONCURRENTLY idx_agent_memory_embedding_hnsw ON agent_memory USING hnsw (embedding vector_cosine_ops) WITH (m = 16, ef_construction = 64); `) if err != nil { return fmt.Errorf(\"failed to create HNSW index: %w\", err) } // Update table statistics _, err = db.Exec(\"ANALYZE agent_memory;\") if err != nil { return fmt.Errorf(\"failed to analyze table: %w\", err) } return nil }\r2. Batch Operations func performBatchOperations(memory core.Memory) error { ctx := context.Background() // Prepare batch data documents := make([]core.Document, 1000) for i := 0; i \u003c 1000; i++ { documents[i] = core.Document{ Content: fmt.Sprintf(\"Document %d content\", i), ContentType: \"batch-document\", Metadata: map[string]string{ \"batch_id\": \"batch-001\", \"index\": fmt.Sprintf(\"%d\", i), }, } } // Batch store operation err := memory.StoreBatch(ctx, documents, core.WithBatchSize(100), core.WithConcurrency(4), ) if err != nil { return fmt.Errorf(\"batch store failed: %w\", err) } fmt.Printf(\"Successfully stored %d documents in batch\\n\", len(documents)) return nil }\r3. Connection Pooling type OptimizedMemoryConfig struct { core.AgentMemoryConfig ConnectionPool struct { MaxConnections int `yaml:\"max_connections\"` MaxIdleConnections int `yaml:\"max_idle_connections\"` ConnectionTimeout time.Duration `yaml:\"connection_timeout\"` IdleTimeout time.Duration `yaml:\"idle_timeout\"` } `yaml:\"connection_pool\"` } func createOptimizedMemory() (core.Memory, error) { config := OptimizedMemoryConfig{ AgentMemoryConfig: core.AgentMemoryConfig{ Provider: \"pgvector\", Connection: \"postgres://user:pass@localhost:5432/agentdb\", EnableRAG: true, Dimensions: 1536, }, } // Configure connection pool config.ConnectionPool.MaxConnections = 20 config.ConnectionPool.MaxIdleConnections = 5 config.ConnectionPool.ConnectionTimeout = 30 * time.Second config.ConnectionPool.IdleTimeout = 5 * time.Minute return core.NewMemoryWithConfig(config) }\rMemory Monitoring and Metrics 1. Performance Metrics type VectorDBMetrics struct { SearchLatency []time.Duration IndexSize int64 QueryThroughput float64 CacheHitRate float64 mu sync.RWMutex } func (m *VectorDBMetrics) RecordSearch(duration time.Duration) { m.mu.Lock() defer m.mu.Unlock() m.SearchLatency = append(m.SearchLatency, duration) // Keep only recent measurements if len(m.SearchLatency) \u003e 1000 { m.SearchLatency = m.SearchLatency[len(m.SearchLatency)-1000:] } } func (m *VectorDBMetrics) GetAverageLatency() time.Duration { m.mu.RLock() defer m.mu.RUnlock() if len(m.SearchLatency) == 0 { return 0 } var total time.Duration for _, latency := range m.SearchLatency { total += latency } return total / time.Duration(len(m.SearchLatency)) }\r2. Health Monitoring func monitorVectorDBHealth(memory core.Memory) { ticker := time.NewTicker(1 * time.Minute) defer ticker.Stop() for range ticker.C { ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second) // Check basic connectivity stats, err := memory.GetStats(ctx) if err != nil { log.Printf(\"Health check failed: %v\", err) cancel() continue } // Check search performance start := time.Now() _, err = memory.Search(ctx, \"health check query\", core.WithLimit(1)) searchLatency := time.Since(start) if err != nil { log.Printf(\"Search health check failed: %v\", err) } else if searchLatency \u003e 1*time.Second { log.Printf(\"Search latency high: %v\", searchLatency) } // Log health status log.Printf(\"Vector DB Health: %d items, search latency: %v\", stats.ItemCount, searchLatency) cancel() } }\rProduction Deployment 1. High Availability Setup # docker-compose.prod.yml for pgvector HA version: '3.8' services: postgres-primary: image: pgvector/pgvector:pg15 environment: POSTGRES_DB: agentdb POSTGRES_USER: agent_user POSTGRES_PASSWORD: ${POSTGRES_PASSWORD} POSTGRES_REPLICATION_USER: replicator POSTGRES_REPLICATION_PASSWORD: ${REPLICATION_PASSWORD} volumes: - postgres_primary_data:/var/lib/postgresql/data - ./postgresql.conf:/etc/postgresql/postgresql.conf command: postgres -c config_file=/etc/postgresql/postgresql.conf postgres-replica: image: pgvector/pgvector:pg15 environment: PGUSER: agent_user POSTGRES_PASSWORD: ${POSTGRES_PASSWORD} POSTGRES_MASTER_SERVICE: postgres-primary POSTGRES_REPLICATION_USER: replicator POSTGRES_REPLICATION_PASSWORD: ${REPLICATION_PASSWORD} volumes: - postgres_replica_data:/var/lib/postgresql/data depends_on: - postgres-primary volumes: postgres_primary_data: postgres_replica_data:\r2. Backup and Recovery func setupBackupStrategy(memory core.Memory) error { // Create backup scheduler scheduler := cron.New() // Daily full backup scheduler.AddFunc(\"0 2 * * *\", func() { err := performFullBackup(memory) if err != nil { log.Printf(\"Full backup failed: %v\", err) } }) // Hourly incremental backup scheduler.AddFunc(\"0 * * * *\", func() { err := performIncrementalBackup(memory) if err != nil { log.Printf(\"Incremental backup failed: %v\", err) } }) scheduler.Start() return nil } func performFullBackup(memory core.Memory) error { ctx := context.Background() // Export all data backupFile := fmt.Sprintf(\"backup-full-%s.json\", time.Now().Format(\"2006-01-02-15-04-05\")) data, err := memory.ExportAll(ctx) if err != nil { return fmt.Errorf(\"export failed: %w\", err) } // Write to file err = ioutil.WriteFile(backupFile, data, 0644) if err != nil { return fmt.Errorf(\"write backup failed: %w\", err) } log.Printf(\"Full backup completed: %s\", backupFile) return nil }\rTroubleshooting Common Issues 1. Performance Issues func diagnosePerfomanceIssues(memory core.Memory) { ctx := context.Background() // Check index usage stats, err := memory.GetStats(ctx) if err != nil { log.Printf(\"Failed to get stats: %v\", err) return } log.Printf(\"Memory Stats:\") log.Printf(\" Items: %d\", stats.ItemCount) log.Printf(\" Size: %d MB\", stats.SizeBytes/1024/1024) log.Printf(\" Index Size: %d MB\", stats.IndexSizeBytes/1024/1024) // Test search performance queries := []string{ \"machine learning algorithms\", \"neural network architecture\", \"data processing pipeline\", } for _, query := range queries { start := time.Now() results, err := memory.Search(ctx, query, core.WithLimit(10)) duration := time.Since(start) if err != nil { log.Printf(\"Query failed: %s - %v\", query, err) } else { log.Printf(\"Query: %s - %d results in %v\", query, len(results), duration) } } }\r2. Connection Issues func handleConnectionIssues(config core.AgentMemoryConfig) core.Memory { var memory core.Memory var err error maxRetries := 5 baseDelay := 1 * time.Second for attempt := 0; attempt \u003c maxRetries; attempt++ { memory, err = core.NewMemory(config) if err == nil { log.Printf(\"Connected to vector database on attempt %d\", attempt+1) return memory } delay := baseDelay * time.Duration(1\u003c\u003cattempt) // Exponential backoff log.Printf(\"Connection attempt %d failed: %v, retrying in %v\", attempt+1, err, delay) time.Sleep(delay) } log.Fatalf(\"Failed to connect after %d attempts: %v\", maxRetries, err) return nil }\rBest Practices 1. Schema Design -- Optimized schema for pgvector CREATE TABLE agent_memory ( id UUID PRIMARY KEY DEFAULT gen_random_uuid(), content TEXT NOT NULL, content_type VARCHAR(100) NOT NULL, embedding vector(1536), metadata JSONB, session_id VARCHAR(255), created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(), updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(), -- Add constraints CONSTRAINT content_not_empty CHECK (length(content) \u003e 0), CONSTRAINT valid_content_type CHECK (content_type ~ '^[a-z][a-z0-9_-]*$') ); -- Optimized indexes CREATE INDEX CONCURRENTLY idx_agent_memory_embedding_hnsw ON agent_memory USING hnsw (embedding vector_cosine_ops) WITH (m = 16, ef_construction = 64); CREATE INDEX idx_agent_memory_content_type_session ON agent_memory(content_type, session_id); CREATE INDEX idx_agent_memory_metadata_gin ON agent_memory USING gin (metadata);\r2. Embedding Optimization func optimizeEmbeddings() core.EmbeddingConfig { return core.EmbeddingConfig{ Provider: \"openai\", Model: \"text-embedding-3-small\", // Good balance of cost/performance Dimensions: 1536, BatchSize: 100, // Optimize for API rate limits Options: map[string]string{ \"max_retries\": \"3\", \"retry_delay\": \"1s\", \"timeout\": \"30s\", \"normalize\": \"true\", // Normalize embeddings for cosine similarity }, } }\r3. Query Optimization func optimizeQueries(memory core.Memory) { ctx := context.Background() // Use appropriate limits results, err := memory.Search(ctx, \"query\", core.WithLimit(10), // Don't retrieve more than needed core.WithScoreThreshold(0.7), // Filter low-relevance results ) // Use metadata filters to reduce search space results, err = memory.Search(ctx, \"query\", core.WithContentType(\"specific-type\"), core.WithMetadataFilter(map[string]string{ \"category\": \"relevant-category\", }), ) // Cache frequent queries cacheKey := fmt.Sprintf(\"search:%s\", query) if cached := getFromCache(cacheKey); cached != nil { return cached } results, err = memory.Search(ctx, query) if err == nil { setCache(cacheKey, results, 5*time.Minute) } }\rConclusion Vector databases provide the foundation for sophisticated memory systems in AgenticGoKit. Key takeaways:\nChoose the right vector database for your use case (pgvector for SQL integration, Weaviate for specialized features) Optimize embeddings and indexes for performance Implement proper monitoring and backup strategies Use appropriate query patterns and caching Vector databases enable semantic search, RAG systems, and intelligent memory that can significantly enhance agent capabilities.\nNext Steps RAG Implementation - Build retrieval-augmented generation systems Knowledge Bases - Create comprehensive knowledge systems Memory Optimization - Advanced performance tuning Further Reading pgvector Documentation Weaviate Documentation OpenAI Embeddings Guide",
    "description": "Vector Databases in AgenticGoKit Overview Vector databases enable sophisticated similarity search and retrieval-augmented generation (RAG) by storing and querying high-dimensional embeddings. This tutorial covers setting up and using vector databases with AgenticGoKit, including pgvector and Weaviate integration.\nVector databases are essential for production-ready memory systems that need to handle large amounts of data with fast, semantically-aware search capabilities.\nPrerequisites Understanding of Basic Memory Operations Knowledge of vector embeddings and similarity search Basic database administration skills Familiarity with Docker (for setup examples) Vector Database Concepts What are Vector Embeddings? Vector embeddings are numerical representations of text, images, or other data that capture semantic meaning in high-dimensional space:",
    "tags": [],
    "title": "vector-databases",
    "uri": "/AgenticGoKitDocs/tutorials/memory-systems/vector-databases/index.html"
  },
  {
    "breadcrumb": "content \u003e guides \u003e development",
    "content": "AgenticGoKit Workflow Visualization Guide Overview AgenticGoKit provides comprehensive workflow visualization capabilities using Mermaid diagrams. You can visualize multi-agent compositions, orchestrations, and complex workflow patterns to better understand and document your agent systems.\nFeatures 1. Composition Builder Visualization Sequential workflows: Step-by-step agent processing pipelines Parallel processing: Fan-out/fan-in patterns for concurrent execution Loop patterns: Retry and iteration logic with conditions 2. Orchestration Builder Visualization Collaborative orchestration: Broadcast events to multiple agents Routing patterns: Conditional routing based on event types Mixed orchestration modes: Sequential, parallel, and loop orchestrations 3. AgentBuilder Integration Multi-agent composition visualization: Preview compositions before building Pre-build diagram generation: Generate diagrams during agent construction Configuration validation: Visualize settings like timeouts and error strategies 4. Advanced Features Custom diagram configuration: Control direction, themes, and metadata display Pre-built workflow patterns: Map-reduce, pipeline, and other common patterns File export: Save diagrams as .mmd files for documentation Styling and metadata: Professional diagrams with execution details Quick Start Examples Basic Sequential Workflow // Create agents orderAgent := createAgent(\"OrderProcessor\") paymentAgent := createAgent(\"PaymentProcessor\") shippingAgent := createAgent(\"ShippingService\") // Build sequential composition pipeline := core.NewComposition(\"order-pipeline\"). WithAgents(orderAgent, paymentAgent, shippingAgent). AsSequential(). WithTimeout(2 * time.Minute) // Generate diagram diagram := pipeline.GenerateMermaidDiagram() fmt.Println(diagram)\rParallel Processing // Create analysis agents sentimentAgent := createAgent(\"SentimentAnalyzer\") keywordAgent := createAgent(\"KeywordExtractor\") languageAgent := createAgent(\"LanguageDetector\") // Build parallel composition analysis := core.NewComposition(\"content-analysis\"). WithAgents(sentimentAgent, keywordAgent, languageAgent). AsParallel(). WithTimeout(30 * time.Second). WithErrorStrategy(core.ErrorStrategyCollectAll) // Generate custom diagram (left-to-right) config := core.MermaidConfig{ DiagramType: core.MermaidFlowchart, Title: \"Content Analysis System\", Direction: \"LR\", ShowMetadata: true, ShowAgentTypes: true, } diagram := analysis.GenerateMermaidDiagramWithConfig(config)\rAgentBuilder with Visualization // Build agent with multi-agent composition builder := core.NewAgent(\"DataProcessor\"). WithParallelAgents(dataAgent, analyticsAgent, reportAgent). WithMultiAgentConfig(core.MultiAgentConfig{ Timeout: 90 * time.Second, MaxConcurrency: 8, ErrorStrategy: core.ErrorStrategyCollectAll, StateStrategy: core.StateStrategyMerge, }) // Check if it can be visualized if builder.CanVisualize() { diagram := builder.GenerateMermaidDiagram() // Save to file os.WriteFile(\"workflow.mmd\", []byte(diagram), 0644) } // Build the actual agent agent, err := builder.Build()\rLoop with Conditions qualityAgent := createAgent(\"QualityChecker\") // Define stop condition condition := func(state core.State) bool { if score, exists := state.Get(\"quality_score\"); exists { if qualityScore, ok := score.(float64); ok { return qualityScore \u003e= 0.95 // Stop at 95% quality } } return false } // Build loop composition qualityLoop := core.NewComposition(\"quality-monitor\"). WithAgents(qualityAgent). AsLoop(10, condition). // Max 10 iterations WithTimeout(5 * time.Minute) diagram := qualityLoop.GenerateMermaidDiagram()\rOrchestration Patterns // Collaborative microservices serviceHandlers := map[string]core.AgentHandler{ \"user-service\": core.ConvertAgentToHandler(userAgent), \"order-service\": core.ConvertAgentToHandler(orderAgent), \"payment-service\": core.ConvertAgentToHandler(paymentAgent), } collaboration := core.NewOrchestrationBuilder(core.OrchestrationCollaborate). WithAgents(serviceHandlers). WithTimeout(1 * time.Minute). WithMaxConcurrency(20) diagram := collaboration.GenerateMermaidDiagram() // API routing routing := core.NewOrchestrationBuilder(core.OrchestrationRoute). WithAgents(serviceHandlers). WithTimeout(30 * time.Second) routingDiagram := routing.GenerateMermaidDiagram()\rConfiguration Options MermaidConfig Structure type MermaidConfig struct { DiagramType MermaidDiagramType // flowchart, sequenceDiagram, etc. Title string // Custom diagram title Direction string // \"TD\", \"LR\", \"BT\", \"RL\" Theme string // \"default\", \"dark\", \"forest\" ShowMetadata bool // Include timeout/error info ShowAgentTypes bool // Show agent type details CompactMode bool // Generate compact diagrams }\rAvailable Directions \"TD\" or \"TB\": Top to Bottom (default) \"LR\": Left to Right \"BT\": Bottom to Top \"RL\": Right to Left Themes \"default\": Standard Mermaid theme \"dark\": Dark theme for presentations \"forest\": Green theme \"base\": Minimal theme Workflow Patterns Pre-built Patterns // Map-Reduce pattern agents := []core.Agent{dataAgent, processor1, processor2, reducer} mapReduceDiagram := core.GenerateWorkflowPatternDiagram(\"map-reduce\", agents) // Pipeline pattern pipelineAgents := []core.Agent{input, transform, validate, output} pipelineDiagram := core.GenerateWorkflowPatternDiagram(\"pipeline\", pipelineAgents)\rFile Export and Integration Save Diagrams to Files // Create output directory outputDir := \"workflow_diagrams\" os.MkdirAll(outputDir, 0755) // Save diagram as Markdown file filename := filepath.Join(outputDir, \"my_workflow.md\") err := core.SaveDiagramAsMarkdown(filename, \"My Workflow\", diagram) // Save with metadata metadata := map[string]interface{}{ \"Pattern\": \"Sequential Processing\", \"Agents\": 4, \"Timeout\": \"2 minutes\", \"Error Strategy\": \"Fail Fast\", } err = core.SaveDiagramWithMetadata(filename, \"My Workflow\", \"Description of the workflow\", diagram, metadata)\rView Diagrams VS Code/GitHub/GitLab: Open .md files directly - Mermaid diagrams render automatically Online: Copy Mermaid code to Mermaid Live Editor Documentation: Include .md files in project documentation Presentations: Export from Mermaid Live as PNG/SVG Integration in Documentation # My Workflow Here's how our order processing works: \\`\\`\\`mermaid --- title: Order Processing Pipeline --- flowchart TD INPUT[\"🎯 Order Request\"] AGENT1[\"🤖 InventoryChecker\"] INPUT --\u003e AGENT1 AGENT2[\"🤖 PaymentProcessor\"] AGENT1 --\u003e AGENT2 AGENT3[\"🤖 ShippingService\"] AGENT2 --\u003e AGENT3 OUTPUT[\"✅ Order Complete\"] AGENT3 --\u003e OUTPUT \\`\\`\\`\rBest Practices 1. Use Descriptive Agent Names ✅ \"OrderProcessor\", \"PaymentGateway\" ❌ \"Agent1\", \"Worker\" 2. Choose Appropriate Directions Sequential workflows: Top-to-bottom (\"TD\") Data flows: Left-to-right (\"LR\") Process hierarchies: Top-to-bottom (\"TD\") 3. Include Metadata for Complex Systems config := core.MermaidConfig{ ShowMetadata: true, // Show timeouts, error strategies ShowAgentTypes: true, // Show agent capabilities }\rSave Diagrams During Development // Always save diagrams for documentation if builder.CanVisualize() { diagram := builder.GenerateMermaidDiagram() filename := fmt.Sprintf(\"docs/%s.md\", builder.Name()) core.SaveDiagramAsMarkdown(filename, builder.Name(), diagram) }\r5. Use Custom Titles for Clarity config := core.MermaidConfig{ Title: \"E-commerce Order Processing Pipeline v2.1\", }\rTroubleshooting Common Issues Empty Diagram: Check if composition has agents and a mode set\nif builder.CanVisualize() { // Safe to generate diagram }\rMissing Styling: Ensure metadata is enabled\nconfig.ShowMetadata = true config.ShowAgentTypes = true\rComplex Diagrams: Use compact mode for large compositions\nconfig.CompactMode = true\rValidation // Validate before visualization if err := builder.Validate(); err != nil { fmt.Printf(\"Builder error: %v\\n\", err) return } // Check visualization capability if !builder.CanVisualize() { fmt.Println(\"No multi-agent composition to visualize\") return }\rAPI Reference Composition Builder GenerateMermaidDiagram() string GenerateMermaidDiagramWithConfig(config MermaidConfig) string Orchestration Builder GenerateMermaidDiagram() string GenerateMermaidDiagramWithConfig(config MermaidConfig) string Agent Builder CanVisualize() bool GenerateMermaidDiagram() string GenerateMermaidDiagramWithConfig(config MermaidConfig) string Workflow Patterns GenerateWorkflowPatternDiagram(pattern string, agents []Agent) string File Export Utilities SaveDiagramAsMarkdown(filename, title, diagram string) error SaveDiagramWithMetadata(filename, title, description, diagram string, metadata map[string]interface{}) error ConvertMmdToMarkdown(mmdFile, title string) error Supported patterns: \"map-reduce\", \"pipeline\", \"scatter-gather\"\nExamples Repository See the examples/visualization/ directory for complete working examples:\ndemo.go: Basic visualization examples comprehensive_demo.go: Advanced patterns and configurations Generated .mmd files for reference Next Steps Explore Examples: Run the demo programs to see all features Create Custom Patterns: Build your own workflow visualizations Integrate Documentation: Add diagrams to your project documentation Share Diagrams: Export and share workflow visualizations with your team",
    "description": "AgenticGoKit Workflow Visualization Guide Overview AgenticGoKit provides comprehensive workflow visualization capabilities using Mermaid diagrams. You can visualize multi-agent compositions, orchestrations, and complex workflow patterns to better understand and document your agent systems.\nFeatures 1. Composition Builder Visualization Sequential workflows: Step-by-step agent processing pipelines Parallel processing: Fan-out/fan-in patterns for concurrent execution Loop patterns: Retry and iteration logic with conditions 2. Orchestration Builder Visualization Collaborative orchestration: Broadcast events to multiple agents Routing patterns: Conditional routing based on event types Mixed orchestration modes: Sequential, parallel, and loop orchestrations 3. AgentBuilder Integration Multi-agent composition visualization: Preview compositions before building Pre-build diagram generation: Generate diagrams during agent construction Configuration validation: Visualize settings like timeouts and error strategies 4. Advanced Features Custom diagram configuration: Control direction, themes, and metadata display Pre-built workflow patterns: Map-reduce, pipeline, and other common patterns File export: Save diagrams as .mmd files for documentation Styling and metadata: Professional diagrams with execution details Quick Start Examples Basic Sequential Workflow // Create agents orderAgent := createAgent(\"OrderProcessor\") paymentAgent := createAgent(\"PaymentProcessor\") shippingAgent := createAgent(\"ShippingService\") // Build sequential composition pipeline := core.NewComposition(\"order-pipeline\"). WithAgents(orderAgent, paymentAgent, shippingAgent). AsSequential(). WithTimeout(2 * time.Minute) // Generate diagram diagram := pipeline.GenerateMermaidDiagram() fmt.Println(diagram)\rParallel Processing // Create analysis agents sentimentAgent := createAgent(\"SentimentAnalyzer\") keywordAgent := createAgent(\"KeywordExtractor\") languageAgent := createAgent(\"LanguageDetector\") // Build parallel composition analysis := core.NewComposition(\"content-analysis\"). WithAgents(sentimentAgent, keywordAgent, languageAgent). AsParallel(). WithTimeout(30 * time.Second). WithErrorStrategy(core.ErrorStrategyCollectAll) // Generate custom diagram (left-to-right) config := core.MermaidConfig{ DiagramType: core.MermaidFlowchart, Title: \"Content Analysis System\", Direction: \"LR\", ShowMetadata: true, ShowAgentTypes: true, } diagram := analysis.GenerateMermaidDiagramWithConfig(config)\rAgentBuilder with Visualization // Build agent with multi-agent composition builder := core.NewAgent(\"DataProcessor\"). WithParallelAgents(dataAgent, analyticsAgent, reportAgent). WithMultiAgentConfig(core.MultiAgentConfig{ Timeout: 90 * time.Second, MaxConcurrency: 8, ErrorStrategy: core.ErrorStrategyCollectAll, StateStrategy: core.StateStrategyMerge, }) // Check if it can be visualized if builder.CanVisualize() { diagram := builder.GenerateMermaidDiagram() // Save to file os.WriteFile(\"workflow.mmd\", []byte(diagram), 0644) } // Build the actual agent agent, err := builder.Build()\rLoop with Conditions qualityAgent := createAgent(\"QualityChecker\") // Define stop condition condition := func(state core.State) bool { if score, exists := state.Get(\"quality_score\"); exists { if qualityScore, ok := score.(float64); ok { return qualityScore \u003e= 0.95 // Stop at 95% quality } } return false } // Build loop composition qualityLoop := core.NewComposition(\"quality-monitor\"). WithAgents(qualityAgent). AsLoop(10, condition). // Max 10 iterations WithTimeout(5 * time.Minute) diagram := qualityLoop.GenerateMermaidDiagram()\rOrchestration Patterns // Collaborative microservices serviceHandlers := map[string]core.AgentHandler{ \"user-service\": core.ConvertAgentToHandler(userAgent), \"order-service\": core.ConvertAgentToHandler(orderAgent), \"payment-service\": core.ConvertAgentToHandler(paymentAgent), } collaboration := core.NewOrchestrationBuilder(core.OrchestrationCollaborate). WithAgents(serviceHandlers). WithTimeout(1 * time.Minute). WithMaxConcurrency(20) diagram := collaboration.GenerateMermaidDiagram() // API routing routing := core.NewOrchestrationBuilder(core.OrchestrationRoute). WithAgents(serviceHandlers). WithTimeout(30 * time.Second) routingDiagram := routing.GenerateMermaidDiagram()\rConfiguration Options MermaidConfig Structure type MermaidConfig struct { DiagramType MermaidDiagramType // flowchart, sequenceDiagram, etc. Title string // Custom diagram title Direction string // \"TD\", \"LR\", \"BT\", \"RL\" Theme string // \"default\", \"dark\", \"forest\" ShowMetadata bool // Include timeout/error info ShowAgentTypes bool // Show agent type details CompactMode bool // Generate compact diagrams }\rAvailable Directions \"TD\" or \"TB\": Top to Bottom (default) \"LR\": Left to Right \"BT\": Bottom to Top \"RL\": Right to Left Themes \"default\": Standard Mermaid theme \"dark\": Dark theme for presentations \"forest\": Green theme \"base\": Minimal theme Workflow Patterns Pre-built Patterns // Map-Reduce pattern agents := []core.Agent{dataAgent, processor1, processor2, reducer} mapReduceDiagram := core.GenerateWorkflowPatternDiagram(\"map-reduce\", agents) // Pipeline pattern pipelineAgents := []core.Agent{input, transform, validate, output} pipelineDiagram := core.GenerateWorkflowPatternDiagram(\"pipeline\", pipelineAgents)\rFile Export and Integration Save Diagrams to Files // Create output directory outputDir := \"workflow_diagrams\" os.MkdirAll(outputDir, 0755) // Save diagram as Markdown file filename := filepath.Join(outputDir, \"my_workflow.md\") err := core.SaveDiagramAsMarkdown(filename, \"My Workflow\", diagram) // Save with metadata metadata := map[string]interface{}{ \"Pattern\": \"Sequential Processing\", \"Agents\": 4, \"Timeout\": \"2 minutes\", \"Error Strategy\": \"Fail Fast\", } err = core.SaveDiagramWithMetadata(filename, \"My Workflow\", \"Description of the workflow\", diagram, metadata)\rView Diagrams VS Code/GitHub/GitLab: Open .md files directly - Mermaid diagrams render automatically Online: Copy Mermaid code to Mermaid Live Editor Documentation: Include .md files in project documentation Presentations: Export from Mermaid Live as PNG/SVG Integration in Documentation # My Workflow Here's how our order processing works: \\`\\`\\`mermaid --- title: Order Processing Pipeline --- flowchart TD INPUT[\"🎯 Order Request\"] AGENT1[\"🤖 InventoryChecker\"] INPUT --\u003e AGENT1 AGENT2[\"🤖 PaymentProcessor\"] AGENT1 --\u003e AGENT2 AGENT3[\"🤖 ShippingService\"] AGENT2 --\u003e AGENT3 OUTPUT[\"✅ Order Complete\"] AGENT3 --\u003e OUTPUT \\`\\`\\`\rBest Practices 1. Use Descriptive Agent Names ✅ \"OrderProcessor\", \"PaymentGateway\" ❌ \"Agent1\", \"Worker\" 2. Choose Appropriate Directions Sequential workflows: Top-to-bottom (\"TD\") Data flows: Left-to-right (\"LR\") Process hierarchies: Top-to-bottom (\"TD\") 3. Include Metadata for Complex Systems config := core.MermaidConfig{ ShowMetadata: true, // Show timeouts, error strategies ShowAgentTypes: true, // Show agent capabilities }\rSave Diagrams During Development // Always save diagrams for documentation if builder.CanVisualize() { diagram := builder.GenerateMermaidDiagram() filename := fmt.Sprintf(\"docs/%s.md\", builder.Name()) core.SaveDiagramAsMarkdown(filename, builder.Name(), diagram) }\r5. Use Custom Titles for Clarity config := core.MermaidConfig{ Title: \"E-commerce Order Processing Pipeline v2.1\", }\rTroubleshooting Common Issues Empty Diagram: Check if composition has agents and a mode set",
    "tags": [],
    "title": "visualization",
    "uri": "/AgenticGoKitDocs/guides/development/visualization/index.html"
  },
  {
    "breadcrumb": "content \u003e guides \u003e development",
    "content": "Web Search Integration Give your agents the ability to search the web for real-time information\nThis guide shows you how to integrate web search capabilities into your AgenticGoKit agents using MCP (Model Context Protocol) tools. You’ll learn to set up web search, handle results, and optimize for different use cases.\nPrerequisites Basic AgenticGoKit project setup Understanding of MCP tool integration API key for a search service (optional for basic setup) What You’ll Build An agent system that can:\nSearch the web for current information Process and summarize search results Handle search errors gracefully Cache results for performance Quick Start 1. Create a Project with Web Search # Create project with MCP web search enabled agentcli create web-search-agent --mcp-enabled --agents 2 \\\\ --mcp-tools \\\"web_search,summarize\\\" cd web-search-agent\r2. Configure Search Provider The generated agentflow.toml includes MCP server configuration:\n[mcp] enabled = true enable_discovery = true connection_timeout = 5000 max_retries = 3 enable_caching = true cache_timeout = 300000 # Configure search server (example) [[mcp.servers]] name = \\\"brave-search\\\" type = \\\"stdio\\\" command = \\\"npx @modelcontextprotocol/server-brave-search\\\" enabled = true # Enable this server\r3. Set Up Environment # If using Brave Search (free tier available) export BRAVE_API_KEY=your-brave-api-key # Or use other search providers export SERP_API_KEY=your-serp-api-key\r4. Test Web Search # Set your LLM provider key export OPENAI_API_KEY=your-openai-key # Run the agent go run . -m \\\"What are the latest developments in AI?\\\"\rAvailable Search Providers Provider MCP Server Free Tier Rate Limits Brave Search @modelcontextprotocol/server-brave-search 2,000 queries/month 1 req/sec SerpAPI @modelcontextprotocol/server-serpapi 100 queries/month Varies DuckDuckGo @modelcontextprotocol/server-duckduckgo Unlimited Rate limited Custom Search Integration If you need a specific search provider, you can create a custom tool:\npackage tools import ( \\\"context\\\" \\\"encoding/json\\\" \\\"fmt\\\" \\\"net/http\\\" \\\"net/url\\\" ) type CustomSearchTool struct { apiKey string httpClient *http.Client } func NewCustomSearchTool(apiKey string) *CustomSearchTool { return \u0026CustomSearchTool{ apiKey: apiKey, httpClient: \u0026http.Client{Timeout: 10 * time.Second}, } } func (t *CustomSearchTool) Name() string { return \\\"custom_search\\\" } func (t *CustomSearchTool) Description() string { return \\\"Search the web using custom search API\\\" } func (t *CustomSearchTool) Execute(ctx context.Context, params map[string]interface{}) (interface{}, error) { query, ok := params[\\\"query\\\"].(string) if !ok || query == \\\"\\\" { return nil, fmt.Errorf(\\\"query parameter is required\\\") } numResults := 5 if nr, ok := params[\\\"num_results\\\"].(float64); ok { numResults = int(nr) } // Build search URL (example using a hypothetical API) searchURL := fmt.Sprintf(\\\"https://api.example-search.com/search?q=%s\u0026count=%d\u0026key=%s\\\", url.QueryEscape(query), numResults, t.apiKey) req, err := http.NewRequestWithContext(ctx, \\\"GET\\\", searchURL, nil) if err != nil { return nil, fmt.Errorf(\\\"failed to create request: %w\\\", err) } resp, err := t.httpClient.Do(req) if err != nil { return nil, fmt.Errorf(\\\"search request failed: %w\\\", err) } defer resp.Body.Close() if resp.StatusCode != http.StatusOK { return nil, fmt.Errorf(\\\"search API returned status %d\\\", resp.StatusCode) } var searchResults map[string]interface{} if err := json.NewDecoder(resp.Body).Decode(\u0026searchResults); err != nil { return nil, fmt.Errorf(\\\"failed to parse search results: %w\\\", err) } return searchResults, nil }\rAdvanced Patterns Search Result Processing Create an agent specifically for processing search results:\ntype SearchProcessorAgent struct { name string llmProvider core.ModelProvider } func (a *SearchProcessorAgent) Execute(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { searchResults, ok := event.Data[\\\"search_results\\\"] if !ok { return nil, fmt.Errorf(\\\"no search results provided\\\") } prompt := fmt.Sprintf(` Analyze these search results and provide a comprehensive summary: %v Please provide: 1. Key findings and insights 2. Source credibility assessment 3. Conflicting information (if any) 4. Recommended follow-up searches Format your response as structured JSON. `, searchResults) response, err := a.llmProvider.GenerateResponse(ctx, prompt, nil) if err != nil { return nil, fmt.Errorf(\\\"failed to process search results: %w\\\", err) } return \u0026core.AgentResult{ Data: map[string]interface{}{ \\\"processed_results\\\": response, \\\"source_count\\\": extractSourceCount(searchResults), }, }, nil }\rError Handling Implement robust error handling for search operations:\nfunc (a *SearchAgent) Execute(ctx context.Context, event core.Event, state *core.State) (*core.AgentResult, error) { query := event.Data[\\\"query\\\"].(string) // Try primary search method results, err := a.performSearch(ctx, query) if err != nil { // Log the error but don't fail immediately log.Printf(\\\"Primary search failed: %v\\\", err) // Try fallback search method results, err = a.performFallbackSearch(ctx, query) if err != nil { // If both fail, return a graceful response return \u0026core.AgentResult{ Data: map[string]interface{}{ \\\"error\\\": \\\"Search temporarily unavailable\\\", \\\"message\\\": \\\"Unable to perform web search at this time. Please try again later.\\\", }, }, nil } } return \u0026core.AgentResult{ Data: map[string]interface{}{ \\\"search_results\\\": results, \\\"query\\\": query, \\\"timestamp\\\": time.Now().Unix(), }, }, nil }\rPerformance Optimization Rate Limiting Implement rate limiting to stay within API limits:\nimport \\\"golang.org/x/time/rate\\\" type RateLimitedSearchTool struct { *CustomSearchTool limiter *rate.Limiter } func NewRateLimitedSearchTool(apiKey string, requestsPerSecond float64) *RateLimitedSearchTool { return \u0026RateLimitedSearchTool{ CustomSearchTool: NewCustomSearchTool(apiKey), limiter: rate.NewLimiter(rate.Limit(requestsPerSecond), 1), } } func (t *RateLimitedSearchTool) Execute(ctx context.Context, params map[string]interface{}) (interface{}, error) { // Wait for rate limiter if err := t.limiter.Wait(ctx); err != nil { return nil, fmt.Errorf(\\\"rate limit wait failed: %w\\\", err) } return t.CustomSearchTool.Execute(ctx, params) }\rCaching Strategy Configure caching for search results to improve performance and reduce API costs:\n[mcp] enabled = true enable_caching = true cache_timeout = 1800000 # 30 minutes for search results # Longer cache for stable queries [mcp.cache_rules] \\\"news_*\\\" = 300000 # 5 minutes for news queries \\\"weather_*\\\" = 1800000 # 30 minutes for weather \\\"facts_*\\\" = 86400000 # 24 hours for factual queries\rTroubleshooting Common Issues Search not working:\n# Check MCP server status agentcli mcp list --status # Verify tool availability agentcli mcp list --tools # Check configuration cat agentflow.toml | grep -A 10 \\\"\\\\[mcp\\\\]\\\"\rAPI rate limits:\nImplement exponential backoff Use multiple API keys with rotation Cache results aggressively Filter queries to reduce API calls Poor search quality:\nRefine search queries programmatically Use multiple search sources Implement result ranking Add domain-specific filters Security Considerations API Key Management # Use environment variables export SEARCH_API_KEY=\\\"your-key-here\\\" # Or use a secrets manager export SEARCH_API_KEY=$(vault kv get -field=key secret/search-api)\rInput Validation func validateSearchQuery(query string) error { if len(query) == 0 { return fmt.Errorf(\\\"search query cannot be empty\\\") } if len(query) \u003e 500 { return fmt.Errorf(\\\"search query too long (max 500 characters)\\\") } // Check for potentially harmful content if containsHarmfulContent(query) { return fmt.Errorf(\\\"search query contains inappropriate content\\\") } return nil }\rNext Steps Now that you have web search capabilities:\nCombine with RAG: Use search results to enhance your knowledge base Add Fact Checking: Cross-reference search results with trusted sources Implement News Monitoring: Set up automated news tracking Build Research Workflows: Create multi-step research processes Related Guides Research Assistant - Complete research system MCP Tools - MCP tool integration Best Practices - Development guidelines Debugging - Troubleshooting tools This guide covers web search integration in AgenticGoKit, enabling your agents to access real-time information from the web.",
    "description": "Web Search Integration Give your agents the ability to search the web for real-time information\nThis guide shows you how to integrate web search capabilities into your AgenticGoKit agents using MCP (Model Context Protocol) tools. You’ll learn to set up web search, handle results, and optimize for different use cases.\nPrerequisites Basic AgenticGoKit project setup Understanding of MCP tool integration API key for a search service (optional for basic setup) What You’ll Build An agent system that can:",
    "tags": [],
    "title": "web-search-integration",
    "uri": "/AgenticGoKitDocs/guides/development/web-search-integration/index.html"
  },
  {
    "breadcrumb": "content \u003e tutorials \u003e getting-started",
    "content": "Your First Agent Tutorial (15 minutes) Navigation: Documentation Home → Tutorials → Getting Started → Your First Agent\nOverview Learn how to build your first AgenticGoKit agent using the powerful agentcli create command. This tutorial showcases AgenticGoKit’s biggest USP - getting started with production-ready agentic code in seconds.\nPrerequisites Go 1.21 or later installed OpenAI API key (or other LLM provider) Basic understanding of Go programming (helpful but not required) Learning Objectives By the end of this tutorial, you’ll understand:\nHow to use agentcli create to scaffold agent projects instantly The generated project structure and its components How to customize and extend generated agents Basic agent concepts through working code How to run and test your agent What You’ll Build A fully functional agent system that:\nIs generated instantly with agentcli create Accepts user input and processes requests using an LLM Includes proper error handling and logging Comes with tests and documentation Is ready for production deployment Step 1: Install AgenticGoKit CLI (1 minute) The agentcli tool is AgenticGoKit’s secret weapon for instant agent creation:\n# Install the CLI go install github.com/kunalkushwaha/agenticgokit/cmd/agentcli@latest # Verify installation agentcli --version\rSet up your environment:\nexport OPENAI_API_KEY=your-api-key-here\rStep 2: Generate Your Agent Project (30 seconds) This is where AgenticGoKit shines! Instead of writing boilerplate code, let’s generate a complete agent project:\n# Create a single agent project agentcli create my-first-agent # Or create with specific options agentcli create my-first-agent --provider openai --agents 1 --template basic cd my-first-agent\rThat’s it! You now have a complete, working agent project. Let’s see what was generated:\nGenerated Project Structure my-first-agent/\r├── main.go # Main application entry point with multi-agent orchestration\r├── agent1.go # First agent implementation\r├── agent2.go # Second agent implementation ├── agentflow.toml # Configuration file for providers and settings\r├── go.mod # Go module file\r├── README.md # Project documentation with usage instructions\rLet’s Look at the Generated Code main.go - The application entry point:\npackage main import ( \"context\" \"flag\" \"fmt\" \"os\" \"sync\" \"time\" \"github.com/kunalkushwaha/agenticgokit/core\" ) func main() { ctx := context.Background() core.SetLogLevel(core.INFO) logger := core.Logger() logger.Info().Msg(\"Starting my-first-agent multi-agent system...\") messageFlag := flag.String(\"m\", \"\", \"Message to process\") flag.Parse() // Read provider from config config, err := core.LoadConfig(\"agentflow.toml\") if err != nil { fmt.Printf(\"Failed to load configuration: %v\\n\", err) os.Exit(1) } llmProvider, err := initializeProvider(config.AgentFlow.Provider) if err != nil { fmt.Printf(\"Failed to initialize LLM provider '%s': %v\\n\", config.AgentFlow.Provider, err) fmt.Printf(\"Make sure you have set the appropriate environment variables:\\n\") switch config.AgentFlow.Provider { case \"openai\": fmt.Printf(\" OPENAI_API_KEY=your-api-key\\n\") case \"azure\": fmt.Printf(\" AZURE_OPENAI_API_KEY=your-api-key\\n\") fmt.Printf(\" AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/\\n\") } os.Exit(1) } // Create agents with result collection agents := make(map[string]core.AgentHandler) results := make([]AgentOutput, 0) var resultsMutex sync.Mutex // Wrap agents with result collectors agent1 := NewAgent1(llmProvider) wrappedAgent1 := \u0026ResultCollectorHandler{ originalHandler: agent1, agentName: \"agent1\", outputs: \u0026results, mutex: \u0026resultsMutex, } agents[\"agent1\"] = wrappedAgent1 agent2 := NewAgent2(llmProvider) wrappedAgent2 := \u0026ResultCollectorHandler{ originalHandler: agent2, agentName: \"agent2\", outputs: \u0026results, mutex: \u0026resultsMutex, } agents[\"agent2\"] = wrappedAgent2 // Create collaborative runner runner := core.CreateCollaborativeRunner(agents, 30*time.Second) var message string if *messageFlag != \"\" { message = *messageFlag } else { fmt.Print(\"Enter your message: \") fmt.Scanln(\u0026message) } if message == \"\" { message = \"Hello! Please provide information about current topics.\" } // Start the runner and process runner.Start(ctx) defer runner.Stop() event := core.NewEvent(\"agent1\", core.EventData{ \"message\": message, }, map[string]string{ \"route\": \"agent1\", }) if err := runner.Emit(event); err != nil { logger.Error().Err(err).Msg(\"Workflow execution failed\") fmt.Printf(\"Error: %v\\n\", err) os.Exit(1) } // Wait for processing and display results time.Sleep(5 * time.Second) fmt.Printf(\"\\n=== Agent Responses ===\\n\") resultsMutex.Lock() for _, result := range results { fmt.Printf(\"\\n🤖 %s:\\n%s\\n\", result.AgentName, result.Content) fmt.Printf(\"⏰ %s\\n\", result.Timestamp.Format(\"15:04:05\")) } resultsMutex.Unlock() fmt.Printf(\"\\n=== Workflow Completed ===\\n\") } func initializeProvider(providerType string) (core.ModelProvider, error) { return core.NewProviderFromWorkingDir() }\ragent1.go - Your first agent implementation:\npackage main import ( \"context\" \"fmt\" \"strings\" agentflow \"github.com/kunalkushwaha/agenticgokit/core\" ) // Agent1Handler represents the agent1 agent handler // Purpose: Handles specialized task processing within the workflow type Agent1Handler struct { llm agentflow.ModelProvider } // NewAgent1 creates a new Agent1 instance func NewAgent1(llmProvider agentflow.ModelProvider) *Agent1Handler { return \u0026Agent1Handler{llm: llmProvider} } // Run implements the agentflow.AgentHandler interface func (a *Agent1Handler) Run(ctx context.Context, event agentflow.Event, state agentflow.State) (agentflow.AgentResult, error) { logger := agentflow.Logger() logger.Debug().Str(\"agent\", \"agent1\").Str(\"event_id\", event.GetID()).Msg(\"Agent processing started\") // Get message from event or state var inputToProcess interface{} eventData := event.GetData() if msg, ok := eventData[\"message\"]; ok { inputToProcess = msg } else if stateMessage, exists := state.Get(\"message\"); exists { inputToProcess = stateMessage } else { inputToProcess = \"No message provided\" } // System prompt with detailed responsibilities systemPrompt := `You are Agent1, handles specialized task processing within the workflow. Core Responsibilities: - Process the original user request and provide initial analysis - Use available MCP tools to gather current and accurate information - For financial queries: get current prices, market data, and trends - Provide concrete answers with actual data rather than generic advice - Set a strong foundation for subsequent agents Tool Usage Strategy: - For stock prices/financial data: Use search tools to find current information - For current events/news: Use search tools for latest updates - For specific web content: Use fetch_content tool with URLs - Always prefer real data over general advice - Document tool usage and results clearly Response Quality: - Provide specific, data-driven answers when possible - Extract and present key information clearly - Be conversational but professional - Integrate tool results naturally into responses Route Mode: Process tasks and route to appropriate next steps in the workflow.` // Get available MCP tools to include in prompt var toolsPrompt string mcpManager := agentflow.GetMCPManager() if mcpManager != nil { availableTools := mcpManager.GetAvailableTools() logger.Debug().Str(\"agent\", \"agent1\").Int(\"tool_count\", len(availableTools)).Msg(\"MCP Tools discovered\") toolsPrompt = agentflow.FormatToolsPromptForLLM(availableTools) } else { logger.Warn().Str(\"agent\", \"agent1\").Msg(\"MCP Manager is not available\") } // Create initial LLM prompt with available tools information userPrompt := fmt.Sprintf(\"User query: %v\", inputToProcess) userPrompt += toolsPrompt prompt := agentflow.Prompt{ System: systemPrompt, User: userPrompt, } // Call LLM to get initial response and potential tool calls response, err := a.llm.Call(ctx, prompt) if err != nil { return agentflow.AgentResult{}, fmt.Errorf(\"Agent1 LLM call failed: %w\", err) } // Parse LLM response for tool calls and execute them toolCalls := agentflow.ParseLLMToolCalls(response.Content) var mcpResults []string // Execute any requested tools if len(toolCalls) \u003e 0 \u0026\u0026 mcpManager != nil { logger.Info().Str(\"agent\", \"agent1\").Int(\"tool_calls\", len(toolCalls)).Msg(\"Executing LLM-requested tools\") for _, toolCall := range toolCalls { if toolName, ok := toolCall[\"name\"].(string); ok { var args map[string]interface{} if toolArgs, exists := toolCall[\"args\"]; exists { if argsMap, ok := toolArgs.(map[string]interface{}); ok { args = argsMap } else { args = make(map[string]interface{}) } } else { args = make(map[string]interface{}) } // Execute tool using the global ExecuteMCPTool function result, err := agentflow.ExecuteMCPTool(ctx, toolName, args) if err != nil { logger.Error().Str(\"agent\", \"agent1\").Str(\"tool_name\", toolName).Err(err).Msg(\"Tool execution failed\") mcpResults = append(mcpResults, fmt.Sprintf(\"Tool '%s' failed: %v\", toolName, err)) } else if result.Success { logger.Info().Str(\"agent\", \"agent1\").Str(\"tool_name\", toolName).Msg(\"Tool execution successful\") var resultContent string if len(result.Content) \u003e 0 { resultContent = result.Content[0].Text } else { resultContent = \"Tool executed successfully but returned no content\" } mcpResults = append(mcpResults, fmt.Sprintf(\"Tool '%s' result: %s\", toolName, resultContent)) } } } } // Generate final response if tools were used var finalResponse string if len(mcpResults) \u003e 0 { // Create enhanced prompt with tool results enhancedPrompt := agentflow.Prompt{ System: systemPrompt, User: fmt.Sprintf(\"Original query: %v\\n\\nTool results:\\n%s\\n\\nPlease provide a comprehensive response incorporating these tool results:\", inputToProcess, strings.Join(mcpResults, \"\\n\")), } // Get final response from LLM finalLLMResponse, err := a.llm.Call(ctx, enhancedPrompt) if err != nil { return agentflow.AgentResult{}, fmt.Errorf(\"Agent1 final LLM call failed: %w\", err) } finalResponse = finalLLMResponse.Content } else { finalResponse = response.Content } // Store agent response in state for potential use by subsequent agents outputState := agentflow.NewState() outputState.Set(\"agent1_response\", finalResponse) outputState.Set(\"message\", finalResponse) // Route to the next agent (Agent2) in the workflow outputState.SetMeta(agentflow.RouteMetadataKey, \"agent2\") logger.Info().Str(\"agent\", \"agent1\").Msg(\"Agent processing completed successfully\") return agentflow.AgentResult{ OutputState: outputState, }, nil }\ragentflow.toml - Configuration file:\n# AgenticGoKit Configuration [agent_flow] name = \"my-first-agent\" version = \"1.0.0\" provider = \"openai\" [logging] level = \"info\" format = \"json\" [runtime] max_concurrent_agents = 10 timeout_seconds = 30 [providers.azure] # API key will be read from AZURE_OPENAI_API_KEY environment variable # Endpoint will be read from AZURE_OPENAI_ENDPOINT environment variable # Deployment will be read from AZURE_OPENAI_DEPLOYMENT environment variable [providers.openai] # API key will be read from OPENAI_API_KEY environment variable [providers.ollama] endpoint = \"http://localhost:11434\" model = \"llama2\" [providers.mock] # Mock provider for testing - no configuration needed\rThe generated code also includes a ResultCollectorHandler that wraps your agents to capture and display their outputs:\n// ResultCollectorHandler wraps an agent handler to capture its outputs type ResultCollectorHandler struct { originalHandler core.AgentHandler agentName string outputs *[]AgentOutput mutex *sync.Mutex } // AgentOutput holds the output from an agent type AgentOutput struct { AgentName string Content string Timestamp time.Time } // Run implements the AgentHandler interface and captures the output func (r *ResultCollectorHandler) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { // Call the original handler result, err := r.originalHandler.Run(ctx, event, state) // Extract meaningful content from the result var content string if err != nil { content = fmt.Sprintf(\"Error: %v\", err) } else if result.Error != \"\" { content = fmt.Sprintf(\"Agent Error: %s\", result.Error) } else { // Try to extract content from the result's output state if result.OutputState != nil { if responseData, exists := result.OutputState.Get(\"response\"); exists { if responseStr, ok := responseData.(string); ok { content = responseStr } } // Additional fallbacks for different state keys... } } // Store the output with timestamp r.mutex.Lock() *r.outputs = append(*r.outputs, AgentOutput{ AgentName: r.agentName, Content: content, Timestamp: time.Now(), }) r.mutex.Unlock() return result, err }\rStep 3: Run Your Agent (1 minute) The generated project is ready to run immediately:\n# Install dependencies (if needed) go mod tidy # Run your agent go run main.go\rYou’ll see an interactive interface where you can interact with your agents:\n# Run with a message flag go run . -m \"What's the weather like today?\" # Or run interactively go run . Enter your message: Hello! What can you do?\rThe output will show both agents processing your request:\n=== Agent Responses ===\r🤖 agent1:\rHello! I'm Agent1, specialized in task processing within the workflow. I can help you with:\r- Processing and analyzing your requests\r- Gathering current information using available tools\r- Providing data-driven answers when possible\r- Setting up a foundation for more detailed analysis\rI work collaboratively with Agent2 to provide comprehensive responses. What would you like help with today?\r⏰ 14:23:15\r🤖 agent2:\rBased on Agent1's initial analysis, I can provide you with comprehensive final responses. I specialize in:\r- Synthesizing insights from previous agents\r- Presenting information in a clear, organized manner\r- Ensuring responses fully address your questions\r- Using additional tools if critical information is missing\rTogether, we form a collaborative system designed to give you thorough, well-researched answers. How can we assist you today?\r⏰ 14:23:17\r=== Workflow Completed ===\rThat’s it! You have a fully functional agent running in under 2 minutes!\nStep 4: Explore Advanced CLI Options (3 minutes) The agentcli create command has many powerful options. Here are some examples of what you can do:\nMulti-Agent Systems Create a system with multiple agents working together:\n# Create a multi-agent system agentcli create research-team --agents 3 cd research-team go run main.go\rWith Different Orchestration Modes Specify how your agents should collaborate:\n# Create a system with sequential orchestration agentcli create workflow-agent --orchestration-mode sequential cd workflow-agent go run main.go\rWith Memory Integration Create an agent with persistent memory capabilities:\n# Create agent with memory agentcli create memory-agent --memory-enabled cd memory-agent go run main.go\rWith Tool Integration Create an agent that can use external tools:\n# Create agent with tool integration agentcli create tool-agent --mcp-enabled cd tool-agent go run main.go\rGet Help with All Options To see all available options:\n# View all options agentcli create --help # Or use interactive mode agentcli create --interactive\rStep 5: Customize Your Generated Agent (5 minutes) The generated code is fully customizable. Let’s enhance your agent:\n1. Customize the Agent Behavior Find the agent implementation file in your generated project and customize the system prompt to give your agent personality:\n// Find the Run method in your agent implementation func (a *YourAgent) Run(ctx context.Context, event core.Event, state core.State) (core.AgentResult, error) { message, ok := event.GetData()[\"message\"].(string) if !ok { return core.AgentResult{}, fmt.Errorf(\"no message found in event data\") } // Custom system prompt with personality systemPrompt := `You are Alex, a friendly and knowledgeable AI assistant with expertise in: - Software development and programming - Technology trends and best practices - Problem-solving and debugging You respond with enthusiasm and provide practical, actionable advice. Always ask follow-up questions to better understand the user's needs.` // Generate response using your custom prompt // The exact implementation will depend on your generated code // but will follow this general pattern response, err := a.generateResponse(ctx, systemPrompt, message) if err != nil { return core.AgentResult{}, err } // Update state with response state.Set(\"response\", response) return core.AgentResult{OutputState: state}, nil }\r2. Add Error Handling and Fallbacks Enhance your agent with better error handling:\n// Add a retry function to your agent func (a *YourAgent) generateWithRetry(ctx context.Context, prompt string, maxRetries int) (string, error) { var lastErr error for i := 0; i \u003c maxRetries; i++ { response, err := a.llmProvider.Generate(ctx, prompt) if err == nil { return response, nil } lastErr = err // Exponential backoff if i \u003c maxRetries-1 { time.Sleep(time.Duration(i+1) * time.Second) } } return \"\", fmt.Errorf(\"failed after %d retries: %w\", maxRetries, lastErr) } // Use it in your Run method response, err := a.generateWithRetry(ctx, fullPrompt, 3) if err != nil { // Provide fallback response fallback := \"I'm having trouble processing your request right now. Could you please try again?\" state.Set(\"response\", fallback) state.Set(\"error_occurred\", true) // Return success with fallback instead of error return core.AgentResult{OutputState: state}, nil }\r3. Test Your Customizations Test your customized agent to ensure it works as expected:\n# Run your modified agent go run main.go\r🎉 Congratulations! You’ve experienced AgenticGoKit’s biggest USP - instant agent creation! In just a few minutes, you:\n✅ Generated a complete agent project with agentcli create ✅ Got production-ready code with proper structure and configuration ✅ Ran a functional agent immediately without writing boilerplate ✅ Learned advanced CLI options for different use cases ✅ Customized the generated code to fit your needs Why agentcli create is Game-Changing Traditional Approach (Hours) mkdir my-agent cd my-agent go mod init my-agent # Write main.go (100+ lines) # Write agent.go (50+ lines) # Write config.toml # Write tests # Write README # Debug imports and interfaces # Handle errors and edge cases\rAgenticGoKit Approach (Seconds) agentcli create my-agent cd my-agent go run main.go # It just works!\rKey Features You Get for Free 🏗️ Project Structure: Organized, professional layout ⚙️ Configuration: TOML-based config with sensible defaults 🧪 Tests: Generated test files with examples 📚 Documentation: README with usage instructions 🐳 Docker: Optional containerization setup 📊 Monitoring: Optional metrics and logging 🔧 Tools: MCP integration ready to go 💾 Memory: Vector database integration available CLI Command Reference # Basic agent agentcli create my-agent # Multi-agent system agentcli create team --agents 3 # With memory agentcli create smart-agent --memory-enabled # With tools agentcli create tool-agent --mcp-enabled # Get help agentcli create --help # Interactive mode agentcli create --interactive\rNext Steps Now that you’ve seen the power of agentcli create, explore:\n🚀 Immediate Next Steps Multi-Agent Orchestration - Create collaborative agent teams Memory Systems - Add persistent memory and RAG MCP Tools - Connect external tools and APIs 🎓 Learning Path Core Concepts - Understand agent fundamentals Memory Systems - Build knowledge-aware agents Advanced Patterns - Complex orchestration patterns 🏭 Production Ready Best Practices - Code quality and patterns Production Deployment - Deploy and scale your agents Troubleshooting Common Issues:\n“agentcli: command not found”: Run go install github.com/kunalkushwaha/agenticgokit/cmd/agentcli@latest “OpenAI API key not found”: Set export OPENAI_API_KEY=your-key “Module not found”: Run go mod tidy in the generated project “Permission denied”: Make sure you have write permissions in the directory Pro Tips:\nUse agentcli create --interactive for guided project creation Check agentcli create --help for all available options Generated projects include comprehensive README files All generated code is fully customizable and production-ready Need Help?\nCLI Reference - Complete command documentation Troubleshooting Guide - Common issues and solutions Discord Community - Get help from the community GitHub Discussions - Technical discussions 🚀 Ready to build something amazing? Try agentcli create --interactive for your next project!",
    "description": "Your First Agent Tutorial (15 minutes) Navigation: Documentation Home → Tutorials → Getting Started → Your First Agent\nOverview Learn how to build your first AgenticGoKit agent using the powerful agentcli create command. This tutorial showcases AgenticGoKit’s biggest USP - getting started with production-ready agentic code in seconds.\nPrerequisites Go 1.21 or later installed OpenAI API key (or other LLM provider) Basic understanding of Go programming (helpful but not required) Learning Objectives By the end of this tutorial, you’ll understand:",
    "tags": [],
    "title": "your-first-agent",
    "uri": "/AgenticGoKitDocs/tutorials/getting-started/your-first-agent/index.html"
  },
  {
    "breadcrumb": "content",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/AgenticGoKitDocs/categories/index.html"
  },
  {
    "breadcrumb": "content",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/AgenticGoKitDocs/tags/index.html"
  }
]
