# Streaming Demo - Real-time LLM Response Streaming

This example demonstrates the real-time streaming capabilities of AgenticGoKit vNext, showing how tokens are delivered as they're generated by the LLM.

## ğŸ¯ What This Demo Shows

### ğŸ”¥ Core Streaming Features
- **Real-time token delivery**: See responses being generated character by character
- **Multiple streaming options**: Advanced configuration for different use cases
- **Provider comparison**: Compare streaming performance across Ollama, OpenAI, and Azure
- **Interactive mode**: Real-time conversation with streaming responses
- **Performance metrics**: Track streaming speed, chunk counts, and latency

### ğŸª Demo Modes

#### 1. Basic Streaming ğŸ“¡
**Best for: Understanding how streaming works**

Shows the fundamental streaming experience:
- Tokens arrive in real-time as they're generated
- Visual feedback of the streaming process
- Performance statistics (chunks/second, latency)
- Complete response assembly

#### 2. Streaming with Options âš™ï¸
**Best for: Advanced streaming configuration**

Demonstrates advanced streaming features:
- Custom buffer sizes for optimal performance
- Thought streaming (see the AI "thinking")
- Tool call streaming (when tools are used)
- Metadata streaming (additional response info)
- Custom timeouts and flush intervals

#### 3. Multiple Providers ğŸŒ
**Best for: Comparing different LLM providers**

Side-by-side comparison of streaming across:
- **Ollama**: Local model streaming (gemma2:2b)
- **OpenAI**: GPT-4o-mini streaming via API
- **Azure OpenAI**: Azure-hosted model streaming
- Performance metrics for each provider

#### 4. Interactive Streaming ğŸ’¬
**Best for: Experiencing real-time conversation**

Interactive chat with streaming responses:
- Type questions and see immediate streaming responses
- Real-time conversation flow
- Commands: `quit` to exit, `clear` to clear screen

## ğŸš€ Quick Start

### Prerequisites

1. **Install Ollama** (for local streaming):
```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Pull a fast model for demos
ollama pull gemma2:2b
```

2. **Optional: Set API Keys** (for cloud providers):
```bash
export OPENAI_API_KEY="your-openai-key"
export AZURE_OPENAI_API_KEY="your-azure-key"
export AZURE_OPENAI_ENDPOINT="your-azure-endpoint"
```

### Run the Demo

```bash
cd examples/vnext/streaming-demo
go run main.go
```

## ğŸ® How to Use

1. **Run the demo**: `go run main.go`
2. **Choose a demo mode** (1-4)
3. **Watch the streaming in action**!

### Demo Menu
```
Choose a streaming demo:
1. Basic Streaming - See tokens arrive in real-time
2. Streaming with Options - Advanced streaming configuration  
3. Multiple Providers - Compare Ollama, OpenAI, Azure streaming
4. Interactive Streaming - Real-time conversation
```

## ğŸ”¬ What You'll See

### Real-time Token Streaming
```
ğŸ“¡ Streaming response:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
The robot named Pixel had always been fascinated by colors. Every day, it would watch the sunrise paint the sky in brilliant hues...
```

### Performance Metrics
```
ğŸ“Š Streaming Statistics:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Total chunks: 45
â€¢ Duration: 3.2s  
â€¢ Characters: 387
â€¢ Avg chunk size: 8.6 chars
â€¢ Tokens per second: 14.1
```

### Provider Comparison
```
ğŸ¦™ Ollama: 32 chunks in 2.1s (15.2 chunks/sec)
ğŸ¤– OpenAI: 28 chunks in 1.8s (15.6 chunks/sec)  
â˜ï¸ Azure: 31 chunks in 2.0s (15.5 chunks/sec)
```

### Interactive Mode
```
ğŸ§‘ You: What's the weather like?
ğŸ¤– Agent: I don't have access to real-time weather data, but I'd be happy to help you find current weather information...
```

## ğŸ›ï¸ Streaming Options Explained

The demo shows these advanced streaming features:

```go
streamOpts := vnext.NewStreamOptions().
    WithBufferSize(100).              // Larger buffer for performance
    WithThoughts(true).               // Enable thought streaming  
    WithToolCalls(true).              // Enable tool call streaming
    WithStreamTimeout(30*time.Second). // Custom timeout
    WithFlushInterval(50*time.Millisecond) // How often to flush chunks
```

### Option Details

- **BufferSize**: Controls internal buffering for performance
- **Thoughts**: Shows AI reasoning process in real-time
- **ToolCalls**: Streams tool usage as it happens
- **StreamTimeout**: Maximum time to wait for complete response
- **FlushInterval**: How frequently chunks are delivered

## ğŸ­ Chunk Types

The demo shows different types of streaming content:

- **Text**: The main response content
- **Thoughts**: AI reasoning process  
- **Tool Calls**: When AI uses tools
- **Metadata**: Additional response information
- **Done**: Signals stream completion
- **Error**: Error conditions

## ğŸ”§ Technical Details

### Streaming Architecture
```go
// Start streaming
stream, err := agent.RunStream(ctx, prompt)

// Process chunks in real-time
for chunk := range stream.Chunks() {
    switch chunk.Type {
    case vnext.ChunkTypeDelta:
        fmt.Print(chunk.Delta)  // Print token as it arrives
    case vnext.ChunkTypeDone:
        // Stream completed
    }
}
```

### Error Handling
```go
for chunk := range stream.Chunks() {
    if chunk.Error != nil {
        fmt.Printf("Stream error: %v\n", chunk.Error)
        break
    }
    // Process chunk...
}
```

## ğŸ¯ Key Benefits Demonstrated

1. **Real-time Feedback**: Users see responses being generated
2. **Better UX**: No waiting for complete responses
3. **Performance**: Efficient token-by-token processing  
4. **Flexibility**: Different streaming modes for different needs
5. **Provider Agnostic**: Same API works with all LLM providers

## ğŸ› Troubleshooting

### Ollama Issues
```
Error: failed to connect to Ollama
Solution: Start Ollama service (ollama serve)
```

### Model Not Found
```
Error: model 'gemma2:2b' not found  
Solution: Pull the model (ollama pull gemma2:2b)
```

### API Key Issues
```
Error: unauthorized
Solution: Set proper API keys in environment variables
```

### Slow Streaming
```
Issue: Streaming seems slow
Solution: Try smaller models or adjust buffer size
```

## ğŸ“ Learning Outcomes

After running this demo, you'll understand:

- How real-time streaming works in practice
- The difference between streaming and non-streaming responses
- How to configure streaming for different use cases
- Performance characteristics of different providers
- How to build interactive streaming applications

## ğŸ”— Related Examples

- [Ollama QuickStart](../ollama-quickstart/) - Basic agent setup
- [Ollama Short Answer](../ollama-short-answer/) - Builder pattern
- [Memory Demo](../../memory_demo/) - Agent memory with streaming

## ğŸ“š API Reference

- [`agent.RunStream()`](../../../core/vnext/agent.go) - Basic streaming
- [`agent.RunStreamWithOptions()`](../../../core/vnext/agent.go) - Advanced streaming
- [`vnext.StreamOptions`](../../../core/vnext/streaming.go) - Streaming configuration
- [`vnext.ChunkType`](../../../core/vnext/streaming.go) - Chunk type definitions

## ğŸŒŸ Next Steps

1. **Experiment** with different prompts and see how streaming responds
2. **Try different models** to see performance differences  
3. **Add memory** to create stateful streaming conversations
4. **Integrate tools** to see tool call streaming in action
5. **Build your own** streaming application using these patterns

## ğŸ’¡ Pro Tips

- Use smaller models (like gemma2:2b) for faster local streaming
- Set appropriate timeouts based on expected response length
- Buffer size affects memory vs latency tradeoffs
- Interactive mode is great for testing different prompts
- Monitor performance metrics to optimize your streaming setup

Enjoy exploring real-time AI streaming! ğŸš€