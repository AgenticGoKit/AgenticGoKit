# Simple Streaming Example

This example demonstrates the basic streaming functionality of AgenticGoKit vNext.

## What it does

- Creates a simple chat agent using Ollama (gemma3:1b model)
- Sends a question about streaming
- Displays tokens as they arrive in real-time
- Shows performance statistics (token count, duration, speed)

## How to run

```bash
cd examples/vnext/simple-streaming
go run main.go
```

## Requirements

- Ollama running locally on port 11434
- gemma3:1b model available (`ollama pull gemma3:1b`)

## Expected Output

You'll see tokens streaming in real-time followed by statistics like:

```
ğŸš€ Simple Streaming Example
===========================

â“ Question: Explain what streaming means in the context of AI responses

ğŸ’¬ Streaming Answer:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Streaming in AI responses means... [tokens appear in real-time]

âœ… Streaming completed!
ğŸ“Š Statistics:
â€¢ Response length: 234 characters
â€¢ Tokens received: 45
â€¢ Time taken: 2.1s
â€¢ Speed: 21.4 tokens/second

ğŸ‰ This is how streaming works! Tokens arrive in real-time instead of waiting for the complete response.
```

This demonstrates the core streaming concept where tokens appear immediately as they're generated, rather than waiting for the complete response.