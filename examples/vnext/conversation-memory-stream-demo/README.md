# Conversation Memory Demo with Streaming

This example demonstrates a conversational agent with memory capabilities and **real-time streaming responses**.

## Features

- ✅ **Real-time streaming** - Responses stream token-by-token for interactive UX
- ✅ **Conversation history** - Agent remembers previous messages
- ✅ **Memory retrieval** - RAG-based context enrichment
- ✅ **Session-scoped memory** - Each conversation maintains separate context
- ✅ **Memory query tracking** - See how many memory operations were performed

## What This Demo Shows

### 1. Streaming Responses
Unlike the basic demo that waits for the complete response, this version:
- Streams tokens as they're generated by the LLM
- Provides immediate feedback to users
- Shows thinking process (optional)
- Better user experience for longer responses

### 2. Memory Integration
The agent uses memory in two ways:
- **Chat History**: Sequential conversation context (always retrieved)
- **RAG Queries**: Semantic similarity search for relevant past information

### 3. Query Counting
The demo tracks and displays:
- Number of memory queries per turn (typically 2: RAG query + chat history)
- Response time including streaming overhead
- Token usage statistics

## Running the Demo

### Prerequisites

1. **Ollama running** with the model available:
   ```bash
   ollama pull gpt-oss:120b-cloud
   # Or use another model like: ollama pull llama3.2
   ```

2. **Build the demo**:
   ```bash
   cd examples/vnext/conversation-memory-stream-demo
   go build
   ```

3. **Run**:
   ```bash
   ./conversation-memory-stream-demo
   ```

## Example Session

```
🤖 Interactive Chat Agent with Memory (Streaming)
==================================================

✅ Agent initialized successfully!

💬 Start chatting! Type 'quit' or 'exit' to end the conversation.
💡 Watch the responses stream in real-time, word by word!

👤 You: Hello, my name is Alice

🤖 Assistant (Turn 1): Hi Alice! 👋 Nice to meet you...
[Response streams in real-time, word by word]

📊 Memory: Used (2 queries)
⏱️  Response time: 2.5s
🎯 Tokens used: 45
────────────────────────────────────────────────────────────

👤 You: I work with Go and AI

🤖 Assistant (Turn 2): That's awesome Alice! Go and AI...
[Response streams as it's generated]

📊 Memory: Used (2 queries)
⏱️  Response time: 3.1s
🎯 Tokens used: 78
────────────────────────────────────────────────────────────

👤 You: What do you know about me?

🤖 Assistant (Turn 3): Based on our conversation, Alice...
[Agent recalls your name and interests from memory]

📊 Memory: Used (2 queries)
⏱️  Response time: 2.8s
────────────────────────────────────────────────────────────
```

## Configuration Options

The streaming demo uses these configuration options:

```go
Streaming: &vnext.StreamingConfig{
    Enabled:          true,
    BufferSize:       100,   // Channel buffer size
    FlushInterval:    50,    // Flush every 50ms
    IncludeThoughts:  true,  // Show thinking process
    IncludeToolCalls: false, // Don't show tool calls
    IncludeMetadata:  false, // Don't stream metadata
}
```

### Streaming Options

- **BufferSize**: Size of the chunk channel buffer (default: 100)
- **FlushInterval**: Milliseconds between flushes (default: 100ms)
- **IncludeThoughts**: Stream the agent's internal reasoning
- **IncludeToolCalls**: Stream tool execution in real-time
- **IncludeMetadata**: Include metadata in stream chunks

## How It Works

### 1. Stream Creation
```go
stream, err := agent.RunStream(ctx, userInput,
    vnext.WithThoughts(),      // Optional: show thinking
    vnext.WithBufferSize(100), // Buffer size
)
```

### 2. Processing Chunks
The demo processes different chunk types:
- **ChunkTypeDelta**: Actual response text (streamed word-by-word)
- **ChunkTypeThought**: Agent's internal reasoning
- **ChunkTypeToolCall**: Tool execution notifications
- **ChunkTypeToolRes**: Tool execution results
- **ChunkTypeError**: Error notifications
- **ChunkTypeDone**: Stream completion signal

### 3. Final Result
After streaming completes, get the full result:
```go
result := stream.Result()
// Contains: full content, duration, memory queries, tokens used
```

## Comparison: Streaming vs Non-Streaming

| Feature | Non-Streaming | Streaming |
|---------|--------------|-----------|
| **Response Display** | All at once | Token-by-token |
| **User Feedback** | Wait for complete response | Immediate feedback |
| **Perceived Latency** | Higher | Lower |
| **Memory Tracking** | ✅ Same | ✅ Same |
| **Use Case** | Batch processing | Interactive chat |

## Memory Query Details

The counter shows **2 queries per turn**:

1. **RAG Query** (`memoryProvider.Query()`):
   - Semantic similarity search for relevant memories
   - Even if 0 results returned, the query is counted
   - Uses cosine similarity on stored embeddings

2. **Chat History** (`memoryProvider.GetHistory()`):
   - Retrieves recent conversation messages
   - Provides sequential context
   - Limited by `HistoryLimit` (20 messages in demo)

## Customization

### Change the Model
```go
LLM: vnext.LLMConfig{
    Provider:    "ollama",
    Model:       "llama3.2", // Change model here
    Temperature: 0.7,
    MaxTokens:   150,
}
```

### Adjust Memory Settings
```go
Memory: &vnext.MemoryConfig{
    Provider: "memory",
    RAG: &vnext.RAGConfig{
        MaxTokens:       2000, // Increase context size
        PersonalWeight:  0.8,  // Weight for conversation history
        KnowledgeWeight: 0.2,  // Weight for knowledge base
        HistoryLimit:    50,   // Keep more messages
    },
}
```

### Tune Streaming Behavior
```go
Streaming: &vnext.StreamingConfig{
    FlushInterval:    20,   // Faster streaming (20ms)
    IncludeThoughts:  false, // Hide thinking process
}
```

## See Also

- [Basic Conversation Memory Demo](../conversation-memory-demo/) - Non-streaming version
- [Streaming Guide](../../../docs/guides/streaming-guide.md) - Complete streaming documentation
- [Memory Guide](../../../docs/guides/MemoryAndRAG.md) - Memory system documentation
